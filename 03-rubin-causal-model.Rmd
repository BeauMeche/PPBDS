# Rubin Causal Model {#rubin-causal-model}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(gt)
library(tidyverse)
```


<!-- DK: Notes for the summer. 

This will only include non-probabilistic discussion, but will go all the way through a discussion of bias/confounding/assignment mechanism, et cetera. The current version is a good start. 

It is a separate projects to then interweave the probabilistic versions into other chapters. In the sampling chapter, for example, we would just have the blood pressure example but with uncertainty. The one parameter and two parameters would have similar RCM sections, each time with associated confidence intervals for the things we are trying to estimate. And each would discuss what sort of assignment mechanisms might mess us up. 

First, I think this should be split into two parts. First, go through everything with no uncertainty. Your blood pressure is a number, a precise number. And so is your bp if you take the treatment. This proceeds all the way through everything. Second, we go back and start again, this time explaining that there are no precise numbers in the world. Your bp right now is not a fixed number! It depends on what device is being used, who is measuring it and so on. It is better represented as a pdf. Then go through the whole thing again, literally every single table, except with pdfs this time. We assign the first version is the first 1/3 of the course and the second half in the second 1/3, after reading about probabilities in chapter 5. 

Second, that leaves it unclear what we cover in the final third. Ideas? Maybe this is when we hit concepts like regression to the mean and the garden of forking paths. Maybe there is a single "lesson" like this each week. 

Third, let's think a little more clearly about what the bite-sized chunks are and how to distribute them. Does each of the first two parts naturally split itself into 4 chunks. First chunk is simple RT. Last chunk is . . .

Leftover comments:

assigment mechanism applies to both the assignment of treatment, as we know, and to the assignment of sampling. What mechanism caused us to, for example, sample this commuter and not that commuter? Ought to mention "assignment" every week, right next to potential outcomes. These are the two key parts of the RCM. 

-->

The Rubin Causal Model (RCM) is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes, named after Donald Rubin.^[This chapter remixes some material from [Rubin Causal Model (Wikipedia)](https://en.wikipedia.org/wiki/Rubin_causal_model). <br /> The name "Rubin Causal Model" was first coined by Paul W. Holland (1986).]  

<!-- AR: Perhaps the credit to Wikipedia should be added to the attributions page
rather than here.-->

## What is a causal effect?

The Rubin Causal Model (RCM) is based on the idea of **potential outcomes.**  For example, let's say you take a blood pressure medicine and then measure your blood pressure. To calculate the causal effect of taking the medicine, we need to compare the outcome for you in one alternative future (where you took the medicine) to another (where you did not). Since it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing. This dilemma is the "fundamental problem of causal inference."

In most circumstances, we are interested in comparing two futures, one generally termed "treatment" and the other "control." These labels are somewhat arbitrary. The difference between the potential outcome under treatment and the potential outcome under control is called a "causal effect" or a "treatment effect."  The scenario that didn't actually happen, and thus that we didn't observe, is called a "counterfactual."

Thus, according to the RCM, the **causal effect** of your taking or not taking the medicine is the *difference* between what your blood pressure would have been under "treatment" (taking the medicine) and "control" (not taking the medicine). If your systolic blood pressure would have been 140 without the medicine and was 120 with it, then the causal effect of taking the medicine is a 20-point reduction in your systolic blood pressure.

### Everything is a missing data problem

<!-- AR: This perhaps could be introduced even earlier, before the students are
assigned the RCM chapter.  Could be a motivating idea for the whole book. -->

Before we discuss the RCM in more detail, it's worth discussing in general the problem of **missing data**.  Why is that?  When you don't observe certain counterfactuals, that's just one example of a missing piece of data.  Missing data problems arise whenever you want to make statistical inferences.

For example, let's say that before you test a new blood pressure medicine, you want to know the average systolic blood pressure of United States adults.  At first, this seems like a much easier problem---there's nothing causal here!  If you knew the true values, you could build a dataset like this: 


```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Person 1", "Person 2", "Person 3", "...", "Person N"),
       `Blood Pressure` = c("130", "110", "115", "...", "140")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Person**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Then, your answer is simply the average of all the values. 

But do we have this table?  No!  What we actually have is this:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Person 1", "Person 2", "Person 3", "...", "Person N"),
       `Blood Pressure` = c("?", "?", "?", "...", "?")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Person**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

That is, we have a lot of *missing data*.

But maybe we could survey 1,000 people, and ask them to report their blood pressures:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Respondent 1", "Respondent 2", "Respondent 3", "...", "Respondent 1,000"),
       `Blood Pressure` = c("130", "110", "115", "...", "140")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Respondent**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

At least then we could have an average blood pressure for the respondents in our sample, right?

Not so fast! It is probably not reasonable to assume that every blood pressure measurement is perfectly accurate.  Some survey respondents may misremember or round the values, or use measurements from a doctor's visit long ago.  (How many American adults own blood pressure monitors?)  Furthermore, blood pressure monitors aren't 100% accurate.

One's blood pressure reading also isn't a single, constant number, even if it could be perfectly measured: it depends on the time of day, whether one exercised or ate before taking the measurement, and so on.

So what we really should do is think of each measurement as being drawn from a distribution of potential values for each person; we could construct a *confidence interval* around each of the values to try to reflect these sources of error.  Let's say that a fair CI is 15 points in either direction.  (It may actually be higher; this is just for illustration.)  Then our table really looks something like this:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Respondent 1", "Respondent 2", "Respondent 3", "...", "Respondent 1,000"),
       `Blood Pressure` = c("130 (115, 145)", "110 (95, 125)", "115 (100, 130)", "...", "140 (125, 155)")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Respondent**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

OK, so with all this information, we can compute an average blood pressure reading for our respondents, and we can also get a sense of how confident we should be in that average.  With a big enough sample, we can probably get a pretty good measure of the average blood pressure for that sample.  Have we solved our problem?  That is, can we now assume that our estimate of the average blood pressure in our sample is a good estimate of the average blood pressure in the U.S. adult population?

<!-- AR: Talking about sampling here so we can reference it later -->

Well, let's see how many of the missing values we have filled in to our table of the population:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Person 1", "Person 2", "Person 3", "...", "Person N"),
       `Blood Pressure` = c("?", "?", "?", "...", "?")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Person**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Alas, since the U.S. population is over 300 million, we can't even see our 1,000 respondents!  If we want to assume that our estimate for our respondents is a good estimate for the population, we need to assume that their blood pressures are, on average, the same as the blood pressures of the people we *didn't* survey.

For that to be true, it means that the *sampling mechanism* can't be related to someone's blood pressure.  For example, let's say that we surveyed 1,000 Harvard undergraduates.  Do we think that Harvard undergraduates have average blood pressure? No! Harvard undergraduates are younger than average, and younger people have lower blood pressures on average.

So we should *randomly* sample the population.  Then, there won't be any systematic relationship between being chosen for our sample and blood pressure.  (There may be a relationship by chance, but taking a bigger sample reduces that possibility.)

But even randomly choosing whom to survey may not totally solve our problem.  Some people may not fill out our survey, after all.  So our table for our survey may look like this:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Respondent 1", "Respondent 2", "Respondent 3", "...", "Respondent 1,000"),
       `Blood Pressure` = c("130", "?", "115", "...", "140")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("**Respondent**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

So not only do we have all the ? values in the population table, we also have a ? for one of our survey respondents (Respondent 2).

Can we just fill in the ? for Respondent 2 with the average of the other respondents?  Only if the factors that led Respondent 2 not to answer the survey had nothing to do with blood pressure.  Let's say that poorer respondents were less likely to fill out the survey, for example.  If poorer respondents also had higher blood pressure on average, our estimate of the average blood pressure *among those who answered the survey* will be biased downwards as an estimate for the population, even if we surveyed people at random. 

So missing data is not just a problem for estimating causal effects.  The mechanisms by which you find out some things and don't find out other things are critical to understand when trying to make statistical inferences.

### Potential outcomes: introduction to the Rubin Table

Suppose that Joe is participating in an FDA test for a new hypertension drug. If we were omniscient, we would know the outcomes for Joe under both treatment (the new drug) and control (either no treatment or the current standard treatment). The causal effect, or treatment effect, is the difference between these two potential outcomes. We can present this with the following table of potential outcomes, which we will call a **Rubin Table**:

<!-- AR: Should we stick with this notation? -->

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = "Joe",
       ytreat = "130",
       ycontrol = "135",
       ydiff = "-5") %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```


$Y_{t}(u)$ is Joe's systolic blood pressure if he takes the new pill. In general, this notation expresses the potential outcome which results from a treatment, $t$, on a unit, $u$.

Similarly, $Y_{c}(u)$ is the effect of a different treatment, $c$ or control, on a unit, $u$. In this case, $Y_{c}(u)$ is Joe's blood pressure if he doesn't take the pill.

$Y_{t}(u)-Y_{c}(u)$ is the causal effect of taking the new drug.  Thus, Joe would have a blood pressure of $130$ if he took the pill and $135$ if he did not; the causal effect of taking the pill is $-5$.  Joe should take the pill!

From this table we only know the causal effect on Joe. Everyone else in the study might have an increase in blood pressure if they take the pill. However, regardless of what the causal effect is for the other subjects, the causal effect for Joe is lower blood pressure, relative to what his blood pressure would have been if he had not taken the pill.

### Estimands

Let's look again at our basic Rubin Table for Joe:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = "Joe",
       ytreat = "130",
       ycontrol = "135",
       ydiff = "-5") %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

We have Joe's potential outcome under treatment, Joe's potential outcome under control, and the difference between the two.  The difference is generally called the "causal effect" or "treatment effect."  But it is in fact only one possible causal **estimand**.  An estimand is some quantity in the real world that we are trying to measure.  Note that we could also estimate the ratio of potential outcomes for Joe:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = "Joe",
       ytreat = "130",
       ycontrol = "135",
       yratio = "26/27") %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                yratio = md("$$Y_t(u) / Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Or the percentage change in his potential outcomes:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = "Joe",
       ytreat = "130",
       ycontrol = "135",
       yperc = "-3.7%") %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                yperc = md("$$(Y_t(u) - Y_c(u)) / Y_c(u) \\times 100$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Now, let's consider a larger sample of patients: 

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "130", "130", "140", "145", "135"),
       ycontrol = c("135", "145", "145", "150", "140", "143"),
       ydiff = c("-5", "-15", "-15", "-10", "+5", "-8")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

From this Rubin Table, there are many possible estimands.  Here are just a sampling:

- One potential outcome for one person, such as Joe's potential outcome under treatment ($130$).  
- A causal effect for one person, such as for Mary.  This is the difference between the potential outcomes, which we have provided as its own column ($130 - 145 = -15$).
- The most positive causal effect.  Here, that is $+5$, from Bob ($145 - 140 = +5$).
- The most negative causal effect.  Here, that is $-15$, from either Mary or Sally (both $130 -145 = -15$).
- The median causal effect ($-10$).
- The median percentage change.  To do this, calculate the percentage change for each person.  You'll get 5 percentages: $-3.7\%$, $-10.3\%$, $-10.3\%$, $-6.7\%$, and $3.6\%$.  The median is $-6.7\%$.
- The total number of people for whom the causal effect is positive: $4$.
- And so on. There are a lot of things one might care about!

<!-- AR: the ATE is just another estimand--and a particularly important one--so
I think it should be mentioned here.  We can talk about calculating it from
actual data later.-->

<!-- AR: Calling this the "average treatment effect" rather than "average causal
effect" throughout -->

One very common estimand has its own name, the **average treatment effect**.  The average treatment effect (often abbreviated **ATE**) is the mean of all the causal effects.  Here, the mean is $-8$.

All of these can be derived if you know the full Rubin Table, but in real life only one of these potential outcomes can actually be observed.  Still, the **estimand** is the value that you would calculate, if you had the full Rubin Table.

Of course, we don't have the full Rubin Table. It is impossible, by definition, to observe the effects of more than one treatment on a single subject. Joe cannot both take the pill and not take the pill at the same time! Therefore, the data we actually observe would look something like this: 

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5"),
       ydiff = c("?", "?", "?", "?", "?", "-5.83")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Question marks are responses that could not be observed. The **Fundamental Problem of Causal Inference**^[Holland 1986.] is that directly observing unit-level causal effects is impossible.

### The infinite Rubin Table: many kinds of missing data

While this is what our *observed* Rubin Table may look like, just filling in these question marks likely wouldn't be enough to answer our scientific question of interest.  We are likely interested in whether the new blood pressure drug works *in general*, not just for these five people. Just as when we were considering estimating the average blood pressure of every adult in the U.S., we'll need to think about whether our *sample* is representative of the population.  Thus, we usually have (at least) two main sources of missing data:

1. For the units in our sample, we only see one potential outcome
1. For the units outside our sample, we see *no* potential outcomes

Furthermore, even if we did want to know just about these five people, do we only care about their outcomes this year?  No!  We also care about Joe's potential outcomes one year from now, two years from now, and so on.

So our full Rubin Table includes people we know (Joe) and people we don't (for example, Tyrone), both now and in the future:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe this year", "Joe next year", "Joe two years from now", "Tyrone this year", "Tyrone next year", "Tyrone two years from now", "...", "Person n at time t"),
       ytreat = c("130", "?", "?", "?", "?", "?", "?", "?"),
       ycontrol = c("?", "?", "?", "?", "?", "?", "?", "?"),
       ydiff = c("?", "?", "?", "?", "?", "?", "?", "?")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

In fact, because time is continuous, there is a row for Joe now, Joe one second from now, Joe one day from now and so on. The Rubin Table extends downward forever. Thus, in order to estimate any causal effect, we need assumptions, so we aren't dealing with an infinite table.

The most obvious way to eliminate some rows from the table is to assume the causal effect for Joe now is the same as all the ones for Joe in the future. Is that plausible? Sort of. Joe now and Joe in one second are pretty similar! Joe now and Joe in 30 years are less so.  Unfortunately, there's no magic way to get a good estimate of every missing value in the infinite Rubin Table!

Not only can we extend the Rubin Table by adding people not in our sample to the rows, but we can also add additional treatments to the columns.  Let's go back to the original five people in our sample. What if instead of testing one blood pressure drug, we wanted to test two?  We'll call the original treatment $t$ and the new treatment $t'$.

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ytreat2 = c("?", "?", "?", "?", "?", "?"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
             ytreat = md("$$Y_t(u)$$"),
             ytreat2 = md("$$Y_{t'}(u)$$"),
             ycontrol = md("$$Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Note that for Joe, we now have three causal effects we can estimate: the difference between the original treatment and the new treatment, the difference between the original treatment and control, and the difference between the new treatment and control.  And that's just when looking at differences!  Recall there are many other potential estimands we could be interested in, such as ratios, percent changes, and so on.

Even if you have just one medication that you are testing, there still could be multiple treatments.  For example, the patients could take a different number of pills:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ytreat2 = c("?", "?", "?", "?", "?", "?"),
       ytreat3 = c("?", "?", "?", "?", "?", "?"),
       ytreat4 = c("?", "?", "?", "?", "?", "?"),
       ytreat5 = c("?", "?", "?", "?", "?", "?"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
             ytreat = md("$$Y_{\\text{1 pill}}(u)$$"),
             ytreat2 = md("$$Y_{\\text{2 pills}}(u)$$"),
             ytreat3 = md("$$Y_{\\text{3 pills}}(u)$$"),
             ytreat4 = md("$$Y_{\\text{4 pills}}(u)$$"),
             ytreat5 = md("$$Y_{\\text{5 pills}}(u)$$"),
             ycontrol = md("$$Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Again, there are many possible estimands.  It's worth noting when the treatments have some numeric relationship to each other, though, there may be reasonable assumptions we can use to help fill in the missing values---but we aren't there yet!

Instead of considering the treatment in terms of discrete pills, we could also consider different doses: 1,000 mg, 1,001 mg, and so on:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ytreat1001 = c("?", "?", "?", "?", "?", "?"),
       ytreat1002 = c("?", "?", "?", "?", "?", "?"),
       yellipsis = c("?", "?", "?", "?", "?", "?"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
             ytreat = md("$$Y_{\\text{1,000 mg}}(u)$$"),
             ytreat1001 = md("$$Y_{\\text{1,001 mg}}(u)$$"),
             ytreat1002 = md("$$Y_{\\text{1,002 mg}}(u)$$"),
             yellipsis = md("$$Y_{\\text{... mg}}(u)$$"),
             ycontrol = md("$$Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Indeed, there are an infinite number of possible treatments. The Rubin Table extends to the right forever. Again, assumptions come to our rescue. Or rather, we just throw up our hands and only try to estimate a few things.  This is why it is crucial to define one's estimand precisely: if we are interested in the difference in potential outcomes between one 1,000 mg pill of the new drug versus control, we can ignore all the other possible columns in the infinite Rubin Table.

Thus, whenever you are considering a causal question, the best way to think about it is to start with the infinite Rubin Table.  First we throw out the rows we think are duplicates (such as all the observations for Joe one second from now, two seconds from now, etc.) or that are outside the scope of what we are interested in for now (maybe we don't care about outcomes 30 years in the future for this study).  Second, we throw out the columns that we don't care about, which are all the possible treatments we aren't considering.  Finally, we define precisely---in terms of potential outcomes---our estimand.  It may be something simple, such as the average treatment effect, or something more complex.  Once we have done these steps, we can start thinking about how to fill in the question marks.  But remember that the infinite Rubin Table is always there, and you should be conscious of which rows and columns you are throwing out!


## How do we fill in the missing values?

Let's return to our finite Rubin Table with five people and one treatment:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5"),
       ydiff = c("?", "?", "?", "?", "?", "-5.83")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

How can we fill in the question marks?  Because of the fundamental problem of causal inference, we can never *know* what the values are for the missing values.  Furthermore, even if we could know, everyone is different, so every causal effect is different.  But we can't make any decisions unless we make some assumptions.  "Assumption" just means that we need a "model," and all models have parameters.

One model might be that the causal effect is the same for everyone. There is a single parameter, $\tau$, which we then estimate. Once we have an estimate, we can fill in the Rubin Table because, knowing it, we know what the unseen potential outcome is for each person. Now our estimand is $\tau$; whether this is a sensible estimand depends on how close the real world conforms to our assumption that the causal effect is the same for everyone.

We'll talk more about how you might calculate $\tau$ later.  Right now, just know that if you had an estimate for $\tau$, you could add it to the observed value of every observation in the control group (or subtract it from the observed value of every observation in the treatment group), and thus fill in all the missing values.
  
A second model might be that the causal effect is the same within categories. For example, perhaps it is $\tau_1$ for men and $\tau_2$ for women.  A third model might be that the causal effect is a number $\tau$ multiplied by age: $\tau \times age$. The key concept is that we can't make any progress unless we make some assumptions. That is an inescapable result of the fundamental problem of causal inference. So we make some assumptions, which give us some models.

Note that *some* estimands may not require filling in all the question marks in the Rubin Table.  For example, we'll see that with randomized assignment to treatment, we can get a good estimate of the *average* treatment effect even without filling in every question mark---the average treatment effect is just a single number.  If you wanted, however, to use your estimate of the average treatment effect to fill in the missing *individual* treatment effects, you would need an additional assumption, such as that the treatment effect is constant $\tau$ for everyone.

### Average treatment effect

The average treatment effect is the average difference in *potential* outcomes between the treated group and the control group.  As we noted before, this is a popular estimand.  Why?

1. There's an obvious *estimator* for this estimand: the difference in *observed* outcomes between the treated group and the control group.
1. If treatment is *randomly assigned*, the estimator is *unbiased*: you can be pretty confident in the estimate if you have a large enough treatment and control group.
1. If you are willing to assume that the causal effect is the same for everyone (a big assumption!), you can use your estimate of the ATE to fill in the missing individual values in your Rubin Table.

Note that just because the ATE is a convenient estimand doesn't necessarily mean that it is a useful one.  In particular, consider point #3.  Let's say that a treatment is very beneficial to women but mildly harmful to men.  You'd want to give the treatment only to women.  However, the average treatment effect for the whole sample, even if you estimate it correctly, will be a single positive number (since the beneficial effect for women is larger than the harmful effect for women).  So while our model to estimate the ATE is simple and thus alluring---with randomized treatment assignment, a difference in means will do the trick---don't assume that it's always the right model for your problem.

Let's now walk through how we would estimate the ATE in our blood pressure example.

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5"),
       ydiff = c("?", "?", "?", "?", "?", "-5.83")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

The simplest way to estimate the ATE is to take the mean of the treated group ($121.67$) and the mean of the control group ($127.5$) and then take the difference in those means ($-5.83$).  We'll call this estimate of the ATE $\widehat{ATE}$, pronounced "ATE-hat."

So estimating the ATE is easy.  But is our $\widehat{ATE}$ a good estimate of the true ATE?  After all, if we knew all the missing values in the Rubin Table, we could calculate the true ATE perfectly.  But those missing values may be wildly different from the observed values.  Consider this unobservable true Rubin Table:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "135", "140", "115", "128"),
       ycontrol = c("120", "110", "125", "130", "105", "118"),
       ydiff = c("+10", "+10", "+10", "+10", "+10", "+10")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

In this example, there is indeed a single causal effect for everyone: $+10$.  Note that the *observed* values are all the same, but the unobserved values were such that our estimated ATE, $-5.83$, is pretty far from the real ATE, $+10$.

We'll consider in a little bit when it is reasonable to assume that our estimate of the ATE is a good one.  (The best way, as we'll see, is randomized assignment to treatment.)

But for now, let's assume we have a good estimate.  How can we use that to fill in the missing values in the Rubin Table?  Well, we'll need another assumption: that there's one treatment effect, $\tau$, that is the same for everyone.  Let's fill in our missing values is by adding $\widehat{ATE}$ to the observed value under control (for units in the control group) or by subtracting $\widehat{ATE}$ from the observed value under treatment (for units in the treatment group), like so:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "119.17", "124.17", "115", "121.67"),
       ycontrol = c("135.83", "125.83", "125", "130", "120.83", "127.5"),
       ydiff = c("-5.83", "-5.83", "-5.83", "-5.83", "-5.83", "-5.83")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Note that for some estimands, filling in the same causal effect for everyone would be nonsensical.  For example, let's say you were interested in the lowest potential blood pressure under treatment.  That's not an unreasonable thing to want to know!  But assuming that everyone has the same causal effect means that your estimate of the lowest potential blood pressure is either a) the smallest observed treatment value or b) the smallest observed control value minus your estimate of the ATE, whichever is smaller. This clearly isn't a reasonable estimate of the lowest potential blood pressure under treatment!  Thus, we need a different model if we have a different estimand.

In general, do we think that if Joe didn't take the drug, his systolic blood pressure would be exactly $135.83$? Of course not! We have some uncertainty around these numbers.  There are two main sources of uncertainty we have to take into account when making a prediction for Joe:

1. **Uncertainty in estimating the ATE**.  Even if treatment is randomly assigned, and thus $\widehat{ATE}$ is an unbiased estimate of the true ATE, we still may not have a very *precise* estimate if our sample is small.  With this miniscule sample (five sujects!), the uncertainty might be gigantic, perhaps someting like ($-25$, $13$), which would lead to a prediction for Joe of ($117, 155$).  As we get a larger sample size, this uncertainty decreases.  
1. **Individual variation**.  Even if we have a perfect estimate of the *average* treatment effect, it still may be the case that the effect *for Joe* is higher or lower than average.  We can assume this away if we say that the treatment effect is a constant $\tau$ for everyone, but that is not likely to be true in the real world --- and this source of uncertainty does not go away simply by collecting more observations.  So with a large sample, let's say that we calculated a confidence interval around $\widehat{ATE}$ of ($-6, -5.67$), leading to an interval for Joe's outcome under control of ($135.67, 136$).  The uncertainty due to individual variation may still be a great deal higher---say ($120, 152$).  These numbers are simply illustrative, but they highlight an important point: even if you have a good estimate of the ATE, you should still be much more uncertain about the causal effect for any *particular individual*.

The main takeaway is that we can get rid of #1 by collecting more data, but the only way to get rid of #2 is through making assumptions, potentially very strong ones.

Also, remember the infinite Rubin Table!  We may not just be interested in the effect on Joe, a person in our sample, but on Tyrone, for whom we observe neither potential outcome.  Even if we have a good estimate of the ATE for *this sample*, it may be a bad estimate of the ATE for the *population* if not all people have the same chance of being included in our sample.  So reducing our uncertainty about Joe may still not answer the question we really care about.

### The assignment mechanism

Remember that *everything is a missing data problem*.   We sidestepped the following question before: Is the difference in sample means between treated units and control units, $\widehat{ATE}$, a good estimate of the ATE?  That depends entirely on the method by which units are assigned treatment, which is called the **assignment mechanism**.  That is the mechanism whereby some values are missing and some values are observed.

As we discussed earlier, this already comes up in non-causal context when considering *sampling*.  If we are trying to estimate the average systolic blood pressure in the U.S., we usually do so by taking a sample.  If the process by which people enter our sample is related to their blood pressures, even indirectly, then estimates from our sample won't be good estimates for the population.  We called this the *sampling mechanism*.

The sampling mechanism still matters in causal inference.  You may want to estimate the causal effect for Tyrone, for example, rather than Joe.  But before you even get to the sampling mechanism, there is another missing data problem that can affect causal inference.  That is the *assignment mechanism*, the process by which some units receive treatment and others do not.

If the assignment mechanism is related to the potential outcomes, it may be difficult to conduct causal inference.  The following example of a non-random assignment mechanism illustrates the potential problems.

Consider the use of the *perfect doctor* as an assignment mechanism. The perfect doctor knows how each subject will respond to the drug or the control and assigns each subject to the treatment that will most benefit her. The perfect doctor knows this information about a sample of patients: 

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "Susie", "MEAN"),
       ytreat = c("130", "120", "100", "115", "125", "135", "120"),
       ycontrol = c("115", "125", "150", "125", "130", "105", "125"),
       ydiff = c("+15", "-5", "-50", "-10", "-10", "+30", "-15")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Based on this knowledge she would make the following treatment assignments: 

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "Susie", "MEAN"),
       ytreat = c("?", "120", "100", "115", "125", "?", "113.75"),
       ycontrol = c("115", "?", "?", "?", "?", "105", "110"),
       ydiff = c("?", "?", "?", "?", "?", "?", "+3.75")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

The perfect doctor distorts the averages of both $Y_t(u)$ and $Y_c(u)$ by filtering out poor responses to both the treatment and control. The difference in means thus is distorted in a direction that depends on the details. For instance, a subject like Susie who is harmed by taking the drug would be assigned to the control group by the perfect doctor and thus the negative effect of the drug would be masked.

Therefore, the difference in means is no longer a good estimate of the ATE. In fact, in this case it has the wrong sign! This is not merely a consequence of our small sample: even if the perfect doctor saw a million patients, we could not get a good estimate of the ATE.

This is an extreme example of a problem called **selection bias**.  The perfect doctor is not choosing which patients are receiving the drug at random. Rather, the perfect doctor is making treatment decisions based directly on the patient's potential outcomes.  Whenever the assignment mechanism is correlated with the potential outcomes, we say that there is **confounding**.  Confounding is a problem, since it means that our simple estimate of the ATE is biased.

Much like how the best way to avoid making poor inferences from a sample to a population is to take a random sample of the population, the best assignment mechanism for avoiding confounding is **randomization**. For each subject we could flip a coin to determine if she receives treatment. If we wanted five subjects to receive treatment, we could assign treatment to the first five names we pick out of a hat.

Randomized assignment is the best assignment mechanism for inferring the average treatment effect because if the sample is large enough, the difference in sample means between treated and control units ($\widehat{ATE}$) will be very close to the true ATE in that sample.  (You still can't know *individual* treatment effects without more assumptions, because of the fundamental problem of causal inference.)

Thus, if you are interested in causal inference, randomized trials are the best approach. In many circumstances, however, randomized trials are not possible due to ethical or practical concerns. In such scenarios there is by necessity a non-random assignment mechanism.

For instance, let's say you were interested in the effect of college attendence on earnings. People are not randomly assigned to attend college. Rather, people may choose to attend college based on their financial situation, parents' education, and so on.  This can introduce confounding if the assignment mechanism affects future earnings.  For example, if people choose to go to college at higher rates when they are on career paths where a college degree is particularly beneficial  -- or, in RCM language, whose potential outcomes are on average higher under treatment -- that would introduce confounding.

Many statistical methods have been developed for causal inference when there is a non-random assignment mechanism, such as propensity score matching. These methods attempt to correct for the assignment mechanism by finding control units similar to treatment units.  What you should not do is naively compare the sample means under treatment and control and assume that is a good estimate of the ATE.  Without randomization, this could be very misleading!

Now, let's consider our Rubin Table for Joe, Mary, Sally, Bob, and James again:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "?", "?", "115", "121.67"),
       ycontrol = c("?", "?", "125", "130", "?", "127.5"),
       ydiff = c("?", "?", "?", "?", "?", "-5.83")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Let's say we knew these values came about following a random assignment into treatment.  If that is the case, our estimate $\widehat{ATE} = -5.83$ is an *unbiased* estimate of the ATE.  But how confident in it should we be?  We'll consider that problem now.

### Uncertainty and permutation tests

Even with randomized assignment, when the sample size is small, our estimate $\widehat{ATE}$ may deviate considerably from the true ATE.  For example, we've considered just one possible random assignment in our blood pressure example, where Joe, Mary and James receive the treatment.  Here's another possible random assignment, where Joe, Mary and Sally receive the treatment:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "100", "?", "?", "115"),
       ycontrol = c("?", "?", "?", "130", "120", "123"),
       ydiff = c("?", "?", "?", "?", "?", "-8.33")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Note that with one assignment, $\widehat{ATE} = -5.83$, while with another $\widehat{ATE} = -8.33$.  Assume that these data are the truth: 

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("130", "120", "100", "110", "115", "115"),
       ycontrol = c("115", "125", "130", "130", "120", "123"),
       ydiff = c("+15", "-5", "-25", "-20", "-5", "-8")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Thus, the true average treatment effect is $-8$.  However, our estimates of the average treatment effect vary because our sample is small and the responses have a large variance. If the sample were larger and the variance were less, the average treatment effect would be closer to the true average treatment effect regardless of the specific units randomly assigned to treatment.

<!-- AR: This section attempts to describe permutation tests in a non-frequentist
way; thus, there are no references to a "test statistic" or to the "sharp null." Worth keeping?
-->

How can we estimate this uncertainty?  To make things simple, we'll consider a dataset with four patients.  Let's say we observed the following results, which we know came from a random treatment assignment:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Mary", "Sally", "Bob", "James", "MEAN"),
       ytreat = c("120", "100", "?", "?", "110"),
       ycontrol = c("?", "?", "130", "120", "125"),
       ydiff = c("?", "?", "?", "?", "-15")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("$$\\mathbf{Subject}$$"),
                ytreat = md("$$Y_t(u)$$"),
                ycontrol = md("$$Y_c(u)$$"),
                ydiff = md("$$Y_t(u) - Y_c(u)$$")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

The drug looks very useful!  However, how likely is it that we would observe these results if the drug did nothing at all?  That is, how confident should we be that the drug is actually helpful?

To answer this question, we can use a **permutation test**.^[See [this post](https://www.r-bloggers.com/what-is-a-permutation-test/) for a longer discussion.]  The intuition behind a permutation test is simple.  We observed four units, two of which were assigned to treatment and two that were assigned to control.  To conduct a permutation test, we calculate our quantity of interest (here, the difference in means between treated and control) for every possible arrangement of the labels "treatment" and "control" across the four numbers we actually saw.

This is easiest to understand visually:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(Permutation = c("#1", "#2", "#3", "#4", "#5", "#6"),
       `120` = c("T", "T", "T", "C", "C", "C"),
       `100` = c("T", "C", "C", "T", "T", "C"),
       `130` = c("C", "T", "C", "T", "C", "T"),
       `120 ` = c("C", "C", "T", "C", "T", "T"),
       ATE = c("-15", "+15", "+5", "-5", "-15", "+15")) %>%
       
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(Permutation = md("**Permutation**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(Permutation))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(Permutation)))
```

Here, permutation #1 is what we actually observed, and permutations #2-6 are all the possible rearrangement of the two "treatment" labels and the two "control" labels.  What do we see?  4/6 (67%) of the permutations produce calculated ATEs are at least as large as the effect we actually observed ($-15$), and two of them are the opposite sign!  We therefore should not have much confidence from these data alone that the drug really lowers blood pressure, even though the effect size appeared large.  The moral of the story?  Don't conduct an experiment on only four people!

Let's say that we had a larger experiment.  We can no longer calculate the results of the permutation test by hand, since the number of possible permutations will quickly become very large.  However, you can use R to calculate far more permutations than you can do by hand.  Furthermore, if the number of permutations becomes too much even for your computer, you can take a random sample of all the possible permutations instead; if the random sample is large enough, this will give you a result very close to what you would get if you considered all the permutations

From the permutations, or a random sample of permutations, you can construct a *confidence interval*.  Note that this confidence interval only takes into account the uncertainty from the assignment process---some people being randomly assigned to treatment and others to control.  This necessarily uses only data from your sample.  If you want to make inferences about a larger population, you have to assume that your sample is representative of the larger population.

### Everything is a missing data problem revisited: Internal and external validity

Recall the two main sources of missing data:

1. For the units in our sample, we only see one potential outcome
1. For the units outside our sample, we see *no* potential outcomes

If we have randomized assignment and a large sample, we can be confident that we have a good estimate of the average treatment effect *in that sample*, although we still need more assumptions if we are interested in more difficult estimands such as individual treatment effects.  We say that the experiment has high **internal validity**: the inferences we are making are likely to reflect the truth about that sample.  That reflects the first main source of missing data.

However, we may be interested in a population beyond our particular sample, the second main source of missing data.  For example, consider the hypertension drug example.  Let's say that we ran a randomized trial on 10,000 patients, with 5,000 receiving the new drug and 5,000 in the contol group.  We estimate an average treatment effect of $-5$ units of systolic blood pressure.  Should we recommend this new drug?

The answer to that question depends in part on the **external validity** of the study.  Are the 10,000 people in the study similar to the people for whom we will be making recommendations?  Let's say that the 10,000 study participants were chosen because they all lived near the location that the study took place. That's another form of **selection bias**. The sample is not randomly selected from the population in which we are interested.  Why is that a problem?  Those people may differ systematically from other people, including in ways that affect their response to blood pressure medicine. For example, what if the people in this area are younger on average than most people who take blood pressure medicine?

Note that this concern can be expressed in terms of the assignment mechanism.  People who live far away from the location of the study have a 0% chance of receiving the treatment.  Thus, the study *can't* directly speak to whether the treatment is effective for them; the only way we can make such claims is by making additional assumptions, such as that the non-participants will respond the same on average to the drug as the participants.

The circumstances of the experiment may also affect the external validity of the study.  Perhaps the study participants were all eating a diet that was controlled by the experimenters.  Then, while we have variation in one aspect of the treatment (whether the participants received the blood pressure medicine), we don't have variation in another (diet).  It may be that the blood pressure medicine works when on the controlled diet but doesn't work otherwise.

When dealing with human subjects, there is a particular concern regarding external validity: the [**Hawthorne effect**](https://en.wikipedia.org/wiki/Hawthorne_effect).  When human subjects know that they are part of an experiment, they may change their behavior.

This can lead to a version of the above concern.  Suppose again that the blood pressure drug works when combined with a controlled diet, say a low-salt diet, but it doesn't work for those who have a high-salt diet.  If the subjects in the experiment are paying greater attention to their blood pressure than they otherwise would and thus are eating lower sodium diets than other people, one could make the mistaken inference that it was the blood pressure drug alone that led to a decrease in blood pressure, and not the combination of the drug and the experimental subjects' changing behavior.

### More complex models for causal inference

In this chapter, we explained that there are many possible causal estimands that may be of interest, discussed one in greater detail (the ATE), and provided an estimator for that estimand---the difference in means between treatment and control, which we called $\widehat{ATE}$.  But more complex models can help both with estimating the ATE and with estimating more complex estimands.

We will cover these topics more when we discuss regression and other models, but moving beyond a simple difference in means can help a lot with causal inference:

1. More complex models can help correct for non-random treatment assignments, if the factors that lead to treatment assignment are known.
1. More complex models can also help with estimating different treatment effects for different groups.  Recall we discussed the possible assumption that are different treatment effects for men and women, $\tau_1$ and $\tau_2$.  While you could just calculate the difference in means for each group, that may lead to small sample sizes, especially if there are a lot of groups.  More complex models can help *pool* information across groups to get better estimates.
1. Finally, the difference in means doesn't work with continuous treatments, such as taking 1,000 mg or 1,001 mg of a drug.  Regression and other models can handle continuous treatments much better.

### Causal inference and prediction

Causal inference is often compared with prediction.  In prediction, we want to know an outcome, $Y(u)$.  In causal inference, we want to know a function of *potential* outcomes, such as a treatment effect $Y_t(u) - Y_c(u)$.

Note, however, that these are both kinds of missing data problems.  Prediction involves getting an estimate for an outcome variable that we don't have, and thus is missing, whether because it is in the future or because it is from data that we are unable to collect.  Thus, prediction is the term for using statistical inference to fill in missing data for outcomes.

Causal inference, however, is the term for filling in missing data for potential outcomes.  Unlike with prediction, only one potential outcome can *ever* be observed, even in principle.  (If you are forecasting the weather, you can compare your forecast from yesterday to the actual weather today.)  In a way, however, this is still a kind of prediction: it is just a prediction about a quantity you can't ever observe (a potential outcome).

In both causal inference and prediction, the process by which some data is missing and some is observed is crucial.  If we think that the missing data is similar to the observed data, we can make inferences more easily.  If not, we have to think through the dissimilarities and consider how to model them.

## Other issues with causal inference

### No causation without manipulation

In order for a potential outcome to make sense, it must be possible, at least *a priori*. For example, if there is no way for Joe, under any circumstance, to obtain the new drug, then $Y_{t}(u)$ is impossible for him. It can never happen. And if $Y_{t}(u)$ can never be observed, even in theory, then the causal effect of treatment on Joe's blood pressure is not defined. 

The causal effect of new drug is well defined because it is the simple difference of two potential outcomes, both of which might happen. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing or a different thing might happen.

This definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to happen, ever. For example, what is the causal effect of Joe's height on his weight? Naively, this seems similar to our blood pressure example. We just need to compare two potential outcomes: what would Joe's weight be under the treatment (where treatment is defined as being 3 inches taller) and what would Joe's weight be under the control (where control is defined as his current height).

A moment's reflection highlights the problem: we can't increase Joe's height. There is no way to observe, even conceptually, what Joe's weight would be if he were taller because there is no way to make him taller. We can't manipulate Joe's height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: *No causation without manipulation.*

### Stable unit treatment value assumption (SUTVA)

One important assumption for causal inference is that "the [potential outcome] observation on one unit should be unaffected by the particular assignment of treatments to the other units."^[Cox 1958, §2.4.] This is called the **Stable Unit Treatment Value Assumption** (**SUTVA**). 

In the context of our example, Joe's blood pressure should not depend on whether Mary receives the drug. But what if it does? Suppose that Joe and Mary live in the same house and Mary always cooks. The drug causes Mary to crave salty foods, so if she takes the drug she will cook with more salt than she would have otherwise. A high salt diet increases Joe's blood pressure. Therefore, his outcome will depend on both which treatment he received and which treatment Mary receives.

SUTVA violation makes causal inference more difficult. We can account for dependent observations by considering more treatments. We create 4 treatments by taking into account whether or not Mary receives treatment:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Joe"),
       ct = "140",
       tt = "130",
       cc = "125",
       tc = "120") %>%
  
  # Then, we use the gt function to make it pretty.
  
  gt() %>%
  cols_label(subject = md("**Subject**"),
             ct = md("Joe = c,<br />Mary = t"),
             tt = md("Joe = t,<br />Mary = t"),
             cc = md("Joe = c,<br />Mary = c"),
             tc = md("Joe = t,<br />Mary = c")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(cell_text(weight = "bold"),
            location = cells_body(columns = vars(subject)))
```

Recall that a causal effect is defined as the difference between two potential outcomes. In this case, there are multiple causal effects because there are more than two potential outcomes:

- One is the causal effect of the drug on Joe when Mary receives treatment ($130-140$).
- Another is the causal effect of the drug on Joe when Mary does not receive treatment ($120-125$).
- A third is the causal effect of Mary's treatment on Joe when Joe is not treated ($140-125$). In other words, we can define a causal effect on Joe even in situation in which Joe's treatment is identical in both situations.

Note that Mary taking the drug has a larger causal effect on Joe than Joe himself taking the drug, and what's more, the effect is in the opposite dierction!

By considering more potential outcomes in this way, we can cause SUTVA to hold. However, if any units other than Joe are dependent on Mary, then we must consider further potential outcomes. The greater the number of dependent units, the more potential outcomes we must consider and the more complex the calculations become (consider an experiment with 20 different people, each of whose treatment status can affect outcomes for every one else). In order (easily) to estimate the causal effect of a single treatment relative to a control, SUTVA should hold. 


## Conclusion

The causal effect of a treatment on a single unit at a point in time is the difference between the outcome variable with the treatment and without the treatment. The fundamental problem of causal inference is that it is impossible to observe the causal effect on a single unit. You either take the blood pressure drug or you don't. As a consequence, assumptions---models---must be used in order to estimate the missing data.

## References

<!-- AR: Need to integrate this with the rest of the book -->

Cox, D. R. (1958). *Planning of Experiments.* Wiley.

Holland, Paul W. (1986). "Statistics and Causal Inference." *J. Amer. Statist. Assoc.* 81 (396): 945–960. doi:10.1080/01621459.1986.10478354.
