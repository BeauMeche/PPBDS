# Classification {#classification}

$$
\newcommand{\lik}{\operatorname{Lik}}
\newcommand{\Lik}{\operatorname{Lik}}
$$
Many research questions have binary (yes/no or success/failure) responses: 

- a. Are students with poor grades more likely to binge drink?
- b. Is exposure to a particular chemical associated with a cancer diagnosis?
 
**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), Yes ($Y=1$) or No ($Y=0$), etc. Thus, examples (a) and (b) above would be considered to have binary responses (Does a student binge drink?  Was a patient diagnosed with cancer?). Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success $p$ based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure.

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapter \@ref(regression) and \@ref(multiple-regression).  Second, we will consider classification and regression trees (CART).  Finally, we will discuss random forests.

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \@ref(tidyverse-package) that loading the **tidyverse** package by running `library(tidyverse)` loads the following commonly used data science packages all at once:

* **ggplot2** for data visualization
* **dplyr** for data wrangling
* **tidyr** for converting data to "tidy" format
* **readr** for importing spreadsheet data into R
* As well as the more advanced **purrr**, **tibble**, **stringr**, and **forcats** packages

If needed, read Section \@ref(packages) for information on how to install and load R packages. 

```{r, eval=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(fivethirtyeight)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(fivethirtyeight)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text:
library(knitr)
library(kableExtra)
library(gridExtra)
```
   
## House elections: logistic regression

What affects whether a Democrat or Republican wins a race in the U.S. House of Representatives? This is an example of a binary response: either a Democrat wins (and a Republican loses) or a Republican wins (and a Democrat loses).^[It is rare that third party candidates mount serious bids in U.S. House bids, so it isn't a much of an oversimplication to think of the variable as binary.] In this section, we are going to consider several models predicting Democratic victory in House races.  First, we will consider a single categorical variable as a predictor: the region that a district lies in (Midwest, Northeast, South, or West).  Second, we will consider a single continuous variable (year).  Finally, we fill fit a model that contains an interaction of the two.

### What is logistic regression?

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. 

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

where the observed values $Y_i \sim$ Bernoulli with $p=p_i$ for a given set of predictors $X$.

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *logistic function*, is $\left(\frac{e^x}{e^x + 1}\right)$, and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.

### Exploratory data analysis

The data on House election results from 1976 to 2018 can be found in the `house_results` data frame in **politicaldata** package.  We'll create a version of this data frame called `house_ch13` that creates a new column `dem_win` that notes for each state if the Democratic candidate in a congressional district received more votes than the other candidates.  We'll also join it with the `state_info` data frame in the **fivethirtyeight** package to add the `region` of each state.

```{r}
library(politicaldata)

house_ch13 <- house_results %>%
  
  # Create dem_win variable
  
  mutate(dem = ifelse(is.na(dem), 0, dem),
         other = ifelse(is.na(other), 0, other),
         rep = ifelse(is.na(rep), 0, rep),
         dem_win = ifelse(dem > rep & dem > other, 1, 0)) %>%
  
  # Rename to join with state_info
  
  rename(state_abbrev = state_abb) %>%
  left_join(state_info) %>%
  select(region, state, district, year, dem_win)
```

Recall the three common steps in an exploratory data analysis we saw in Subsection \@ref(model1EDA):

1. Looking at the raw data values.
1. Computing summary statistics.
1. Creating data visualizations.

Let's first look at the raw data values by either looking at `house_ch13` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(house_ch13)
```

Let's also display a random sample of 5 rows of the 9,557 rows corresponding to different district-years. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval = FALSE}
house_ch13 %>% 
  sample_n(size = 5)
```
```{r, echo = FALSE}
house_ch13 %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "A random sample of 5 out of the 4,201 district-years",
    booktabs = TRUE,
    linesep = ""
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Now that we’ve looked at the raw values in our `house_ch13` data frame and got a sense of the data, let’s compute summary statistics. As we've done in our exploratory data analyses before, let’s use the `skim()` function from the `skimr` package, being sure to only select() the variables of interest in our model:

```{r}
house_ch13 %>% 
  select(dem_win, region, year) %>% 
  skim()
```

Observe that we have no missing data, that we have 9,557 observations, and that the mean of `dem_win` is 0.536, indicating that Democrats won 53.6% of the House elections in this period (1976--2018).

Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations.

For our categorical variable, we'll look at histograms of `dem_win` faceted by `region`:

```{r}
house_ch13 %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) +
  theme_minimal()
```

Wait!  That doesn't tell us very much, because our outcome variable only takes two values, 0 and 1.  Let's instead `group_by(district)` and `summarize()` to get a better sense of the distributions:

```{r}
house_ch13 %>%
  group_by(region, district) %>%
  summarize(dem_win = mean(dem_win)) %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) + 
  theme_minimal()
```

This is much more informative!  We can see that the Midwest is highly bimodal, with many districts either electing Democrats for every year in this period or for none.  The Northeast and West have many districts that always elect Democrats but few that never do.  The South is the only region with a peak in the middle, indicating that there are many districts in the South that elected Democrats for about half the time during 1976-2018.

What happens if we create a scatterplot of our outcome variable `dem_win` and a continuous predictor, `year`?

```{r}
house_ch13 %>%
  ggplot(aes(x = year, y = dem_win)) +
  geom_point() +
  labs(x = "Year", y = "Democratic Victory") +
  geom_smooth(method = "lm", se = FALSE)
```

This is completely incomprehensible!  When dealing with binary data, it is more helpful to construct an *empirical logit* plot instead of a regular scatterplot.  The steps for constructing such a plot are as follows:

1. `group_by` your continuous variable.
1. `summarize` the percentage of successes in your outcome variable.
1. Calculate the *empirical logit* for each group, using the logit function: $log\left(\frac{p}{1 - p}\right)$
1. Plot the results.

First, since we will use it frequently, let's define a function `logit()` that performs the logit transformation:

```{r}
logit <- function(p) log(p / (1 - p))
```

Next, let's look at the empirical logit plot:

```{r}
house_ch13 %>%
  group_by(year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = logit(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  theme_classic()
```

Now we see that after the logit transformation, there is roughly a linear relationship between our outcome variable and our explanatory variable `year`.  This means that a logistic regression model makes sense.  Some of the most visually apparent outliers will be familiar to students of American politics: 1994 (the "Republican Revolution"), 2008 (Obama's first election), and 2018. Yet in general it appears that over time, the Democrats have performed worse in House elections.

We can follow the same steps to look at this relationship within Census regions (Midwest, Northeast, South and West):
 
```{r}
house_ch13 %>%
  group_by(region, year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = logit(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  facet_wrap(~ region) +
  theme_classic()
```

We see roughly linear relationships after the logit transformation within-region as well, although the relationship looks more linear in the Midwest and South than in the Northeast and West.  We see that the Democratic Party's overall decline in House races is driven by the South and to a lesser extent the Midwest; Democratic performance has on average improved in the Northeast and West.  The sharp negative slope in the South will not be surprising if one is familiar with the collapse of the "Solid South."

### One categorical explanatory variable

Let's start our modeling by predicting `dem_win` with a single categorical explanatory variable, `region`.  As we'll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression.  In fact, we'll follow the same basic steps:

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `house_region_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_region_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

Note that the key difference is that instead of using `lm()`, we are now using `glm()`.  `glm()` operates very similarly to `lm()`, but it has an additional argument: `family`.  To run a logistic regression, we use `family = binomial`.

```{r, eval=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial data = house_ch13)
house_region_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch13)
```
```{r, echo=FALSE}
tidy(house_region_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Recall that in the linear regression context, we interpreted the coefficients as follows:  the intercept represented the mean for the omitted category, while the other coefficients all represented offsets from that value.  We can't use that intepretation here.  Recall our logistic regression model equation:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

Note that there's a familiar term for what we are taking the log of on the left hand side:

\[\textrm{Odds} = \frac{\textrm{probability of success}}{\textrm{probability of failure}}=
\frac{p}{1-p}.\] 

For example, there's a 25\% chance of flipping two heads in a row when flipping fair coins.  In odds terms, the odds of flipping two heads in a row are $\frac{0.25}{0.75} = \frac{1}{3}$. These are conventionally phrased in words in terms of the odds *against* success, such as "three to one odds against flipping two heads" or just "three to one odds."

Thus, our equation can also be read as:

\[
\log(\textrm{Odds})=\beta_0+\beta_1X 
\]

Let's say that $X$ is our categorical variable `region`, with the baseline category being the Midwest.  Based on this model, the log odds of a Democratic victory when $X = \textrm{Midwest}$ is:
\[
\log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) =\beta_0 = -0.076,
\]
and the log odds when $X = \textrm{South}$ is:
\[
\log\left(\frac{p_{South}}{1-p_{South}}\right)=\beta_0+\beta_{1,South} = -0.076 + 0.056 = -0.020.
\]

We can see that $\beta_{1,South}$ is the difference between the log odds of success when $X = \textrm{South}$ versus $X = \textrm{Midwest}$.  Using rules of logs:
\[
\begin{aligned}
\beta_{1,South} &= (\beta_0 + \beta_{1,South}) - \beta_0 \\ &=
\log\left(\frac{p_{South}}{1-p_{South}}\right) - \log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) \\ &=
\log\left(\frac{p_{South}/(1-p_{South})}{p_{Midwest}/{(1-p_{Midwest})}}\right) \\ &=
\log\left(\frac{\textrm{Odds}(South)}{\textrm{Odds}(Midwest)}\right)
\end{aligned}
\]

Since $e^{\log(x)} = x$, $e^{\beta_{1,South}}$ is the ratio of the odds of success when $X = \textrm{South}$ compared to $X = \textrm{Midwest}$. In general, __exponentiated coefficients in a logistic regression are odds ratios__. A general interpretation of an odds ratio is the odds of success for group A compared to the odds of success for group B---how many times greater the odds of success are in group A compared to group B.

Note that the logistic regression model can also be re-written in a __probability form__:

\[
p_X=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
\]

Now that we understand that expontentiated coefficients in logistic regressions are odds ratios, we can use the argument `exponentiate = TRUE` in the `tidy()` function to exponentiate the coefficients for us:

```{r, eval=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch13)
house_region_model %>%
  tidy(conf.int = TRUE,
       exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch13)
```
```{r, echo=FALSE}
tidy(house_region_model,
     conf.int = TRUE,
     exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table, exponentiated coefficients",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

This allows us to see that the odds of a Democratic victory in the Northeast are nearly twice as high as the odds of a Democratic victory in the Midwest, the reference category.  The odds of a Democratic victory in the West are about 1.4 times higher than the odds of a Democratic victory in the Midwest.  The odds of a Democratic victory in the South are similar to the odds of a Democratic victory in the Midwest.

### Residuals for logistic regression

With linear regression, we used residuals to assess model assumptions and identify outliers. For logistic regression, a __deviance residual__ is a residual for logistic regression based on the discrepancy between the observed values and those estimated using the likelihood.
A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred. 


### Linear regression vs. logistic regression

\[
\underline{\textrm{Response}} \\
\mathbf{Linear\ Regression:}\textrm{ numeric} \\
\mathbf{Logistic\ Regression:}\textrm{ binary} \\
\textrm{ } \\
\underline{\textrm{Model Fitting}} \\
\mathbf{Linear\ Regression:}\ \mu=\beta_0+\beta_1x \textrm{ using }\texttt{lm()}\\
\mathbf{Logistic\ Regression:}\ \log\left(\frac{p}{1-p}\right)=\beta_0+\beta_1x \text{ using }\texttt{glm()}\\
\textrm{ } \\
\underline{\textrm{EDA}} \\
\mathbf{Linear\ Regression:}\textrm{ plot $X$ vs. $Y$; add line} \\
\mathbf{Logistic\ Regression:}\textrm{ find $\log(\textrm{odds})$ for several subgroups; plot vs. $X$} \\
\textrm{ } \\
\underline{\textrm{Interpreting Coefficients}} \\
\mathbf{Linear\ Regression:}\ \beta_1=\textrm{ change in }\mu_y\textrm{ for unit change in $X$} \\
\mathbf{Logistic\ Regression:}\ e^{\beta_1}=\textrm{ percent change in odds for unit change in $X$} 
\]

## Case study: Trying to lose weight

In this case study, we examine characteristics of young people who are trying to lose weight. The prevalence of obesity among US youth suggests that wanting to lose weight is sensible and desirable for some young people such as those with a high body mass index (BMI). On the flip side, there are young people who do not need to lose weight but make ill-advised attempts to do so nonetheless. A multitude of studies on weight loss focus specifically on youth and propose a variety of motivations for the young wanting to lose weight; athletics and the media are two commonly cited sources of motivation for losing weight for young people.

Sports have been implicated as a reason for young people wanting to shed pounds, but not all studies are consistent with this idea. For example, a study by @Martinsen2009 reported that, despite preconceptions to the contrary, there was a higher rate of self-reported eating disorders among controls (non-elite athletes) as opposed to elite athletes. Interestingly, the kind of sport was not found to be a factor, as participants in leanness sports (for example, distance running, swimming, gymnastics, dance, and diving) did not differ in the proportion with eating disorders when compared to those in non-leanness sports. So, in our analysis, we will not make a distinction between different sports.

Other studies suggest that mass media is the culprit. They argue that students' exposure to unrealistically thin celebrities may provide unhealthy motivation for some, particularly young women, to try to slim down. An examination and analysis of a large number of related studies (referred to as a __meta-analysis__) [@Grabe2008] found a strong relationship between exposure to mass media and the amount of time that adolescents spend talking about what they see in the media, deciphering what it means, and figuring out how they can be more like the celebrities.

We are interested in the following questions: Are the odds that young females report trying to lose weight greater that the odds that males do? Is increasing BMI is associated with an interest in losing weight, regardless of sex? Does sports participation increase the desire to lose weight? Is media exposure is associated with more interest in losing weight?

We have sample of 500 teens from data collected in 2009 through the U.S. Youth Risk Behavior Surveillance System (YRBSS) [@YRBS2009]. The YRBSS is an annual national school-based survey conducted by the Centers for Disease Control and Prevention (CDC) and state, territorial, and local education and health agencies and tribal governments. More information on this survey can be found [here](http://www.cdc.gov/HealthyYouth/yrbs/index.htm).
 
### Data organization

Here are the three questions from the YRBSS we use for our investigation:

Q66. Which of the following are you trying to do about your weight?

- A. Lose weight
- B. Gain weight
- C. Stay the same weight
- D. I am not trying to do anything about my weight

Q81. On an average school day, how many hours do you watch TV?

- A. I do not watch TV on an average school day
- B. Less than 1 hour per day
- C. 1 hour per day
- D. 2 hours per day
- E. 3 hours per day
- F. 4 hours per day
- G. 5 or more hours per day

Q84. During the past 12 months, on how many sports teams did you play? (Include any teams run by your school or community groups.)

- A. 0 teams
- B. 1 team
- C. 2 teams
- D. 3 or more teams
 
Answers to Q66 are used to define our response variable: Y = 1 corresponds to "(A) trying to lose weight", while Y = 0 corresponds to the other non-missing values. Q84 provides information on students' sports participation and is treated as numerical, 0 through 3, with 3 representing 3 or more. As a proxy for media exposure we use answers to Q81 as numerical values 0, 0.5, 1, 2, 3, 4, and 5, with 5 representing 5 or more. Media exposure and sports participation are also considered as categorical factors, that is, as variables with distinct levels which can be denoted by indicator variables as opposed to their numerical values.
 
BMI is included in this study as the percentile for a given BMI for members of the same sex. This facilitates comparisons when modeling with males and females. We will use the terms *BMI* and *BMI percentile* interchangeably with the understanding that we are always referring to the percentile.

With our sample, we use only the cases that include all of the data for these four questions. This is referred to as a __complete case analysis__. That brings our sample of 500 to 426. There are limitations of complete case analyses that we address in the Discussion.   

### Exploratory data analysis

```{r, include=FALSE, comment=NA}
risk2009.data = foreign::read.spss("data/yrbs09.sav", to.data.frame=TRUE)
names(risk2009.data)

set.seed(1313)
risk2009.samp <- risk2009.data %>%
  sample_n(500)
rm(risk2009.data)

# Data Prep
sex = with(risk2009.samp,Q2)
sex=with(risk2009.samp,ifelse(sex=="Female",0,1))
sex.l=with(risk2009.samp,factor(sex, labels = c("Female", "Male")))
table(sex.l)

lose.wt4 = with(risk2009.samp,Q66)
table(lose.wt4)
lose.wt2= with(risk2009.samp,ifelse(Q66=="Lose weight",1,0))
table(lose.wt2)
lose.wt.l=with(risk2009.samp,factor(lose.wt2, labels = c("No weight loss", "Lose weight")))
table(lose.wt.l)

sport4 = with(risk2009.samp,Q84)
table(sport4)
sport =  with(risk2009.samp,ifelse(Q84=="0 teams",0,1))
sport.l=with(risk2009.samp,factor(sport, labels = c("No sports", "Sports")))
table(sport.l)

media=with(risk2009.samp,as.numeric(Q81)) # hours of TV per day
table(media)
media=media-2
table(media)
media=ifelse(media==0,0.5,media)
media=ifelse(media==-1,0,media)
table(media)

risk2009 <- risk2009.samp %>%
  mutate(sex = sex.l, lose.wt = lose.wt.l, sport = sport.l, media = media,
         lose.wt.01 = lose.wt2) %>%
  dplyr::select(lose.wt, lose.wt.01, sex, sport, media, bmipct) %>%
  filter(complete.cases(.))
rm(risk2009.samp)
```

```{r, include = FALSE}
#primary response
prop.table( mosaic::tally(~ lose.wt, data = risk2009) )   

#covariates
prop.table( mosaic::tally(~ sex, data = risk2009) )   
prop.table( mosaic::tally(~ sport, data = risk2009) )   
prop.table( mosaic::tally(~ media, data = risk2009) )   

risk2009 %>%
  summarise(meanBMI = mean(bmipct),
            medianBMI = median(bmipct))
```

Nearly half (44.5\%) of our sample of 426 youth report that they are trying to lose weight, 49.9\% percent of the sample are females, and 56.6\% play on one or more sports teams. 9.3 percent report that they do not watch any TV on school days, whereas another 9.3\% watched 5 or more hours each day.  Interestingly, the median BMI percentile for our 426 youth is 70.  The most dramatic difference in the proportions of those who are trying to lose weight is by sex; 58\% of the females want to lose weight in contrast to only 31\% of the males (see Figure \@ref(fig:mosaicsexlose)). This provides strong support for the inclusion of a sex term in every model considered.

```{r, mosaicsexlose, fig.align="center", out.width="60%", fig.cap='Weight loss plans vs. Sex',echo=FALSE, warning=FALSE, comment=NA}
ytable <- mosaic::tally(~ sex + lose.wt, data = risk2009)   
mosaicplot(ytable, color=c("blue", "light blue"), main="")
#prop.table(ytable, 1)

# Note that we use mosaic plots in this section to illustrate differences
# in sample sizes, but geom_barplot() could also be used to produce
# segmented bar charts.
```


```{r, echo=FALSE, include=FALSE, comment=NA}
risk2009 %>%
  group_by(sex, lose.wt) %>%
  summarise(mean = mean(bmipct), sd = sd(bmipct), n = n())
```

```{r, table3chp6, echo=FALSE, comment=NA}
sex <- c("Female"," ","Male"," ")
wls <- c("No weight loss","Lose weight","No weight loss","Lose weight")
mbmip <- c("48.2","76.3","54.9","84.4")
sds <- c("26.2", "19.5", "28.6", "17.7")
ncol <- c(90,124,148,67)
#ccbmiXsex.tab
table3chp6 <- data.frame(sex,wls,mbmip,sds,ncol)
colnames(table3chp6) <- c("Sex","Weight loss status","mean BMI percentile","SD","n")
kable(table3chp6, booktabs=T,caption="Mean BMI percentile by sex and desire to lose weight.")
```

Table \@ref(tab:table3chp6) displays the mean BMI of those wanting and not wanting to lose weight for males and females. The mean BMI is greater for those trying to lose weight compared to those not trying to lose weight, regardless of sex. The size of the difference is remarkably similar for the two sexes.

If we consider including a BMI term in our model(s), the logit should be linearly related to BMI. We can investigate this assumption by constructing an empirical logit plot. In order to calculate empirical logits, we first divide our data by sex. Within each sex, we generate 10 groups of equal sizes, the first holding the bottom 10\% in BMI percentile for that sex, the second holding the next lowest 10\%, etc.. Within each group, we calculate the proporton, $\hat{p}$ that reported wanting to lose weight, and then the empirical log odds, $log(\frac{\hat{p}}{1-\hat{p}})$, that a young person in that group wants to lose weight.

```{r, logitBMIsex, fig.align="center",out.width="60%", fig.cap='Empirical logits of trying to lose weight by BMI and Sex.',echo=FALSE, warning=FALSE, comment=NA}
risk2009 <- risk2009 %>%
  group_by(sex) %>%
  mutate(BMIcuts = cut_number(bmipct,10))
  
emplogit1 <- risk2009 %>%
  group_by(sex, BMIcuts) %>%
  summarise(prop.lose = mean(lose.wt.01), 
            n = n(),
            midpoint = median(bmipct)) %>%
  mutate(prop.lose = ifelse(prop.lose==0, .01, prop.lose),
         emplogit = log(prop.lose / (1 - prop.lose)))

ggplot(emplogit1, aes(x = midpoint, y = emplogit, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("BMI percentile") + ylab("Empirical logits")
```

Figure \@ref(fig:logitBMIsex) presents the empirical logits for the BMI intervals by sex. Both males and females exhibit an increasing linear trend on the logit scale indicating that increasing BMI is associated a greater desire to lose weight and that modeling log odds as a linear function of BMI is reasonable. The slope for the females appears to be similar to the slope for males, so we do not need to consider an interaction term between BMI and sex in the model. 
 
```{r, mosaicsexsports, fig.align="center", out.width="60%", fig.cap='Weight loss plans vs. sex and sports participation',echo=FALSE, warning=FALSE, comment=NA}
ytable <- mosaic::tally(~ sex + sport + lose.wt, data = risk2009)   
mosaicplot(ytable, color = T, main="")
#prop.table(ytable, c(1,2))
```

Out of those who play sports, 43\% want to lose weight whereas 46\% want to lose weight among those who do not play sports. Figure \@ref(fig:mosaicsexsports) compares the proportion of respondents who want to lose weight by their sex and sport participation. The data suggest that sports participation is associated with the same or even a slightly lower desire to lose weight, contrary to what had originally been hypothesized. While the overall levels of those wanting to lose weight differ considerably between the sexes, the differences between those in and out of sports within sex appear to be very small.  A term for sports participation or number of teams will be considered, but there is not compelling evidence that an interaction term will be needed.

It was posited that increased exposure to media, here measured as hours of TV daily, is associated with increased desire to lose weight, particularly for females. Overall, the percentage who want to lose weight ranges from 35\% of those watching 5 hours of TV per day to 52\% among those watching 3 hours daily. There is minimal variation in the proportion wanting to lose weight with both sexes combined. However, we are more interested in differences between the sexes (see Figure \@ref(fig:mediaXsex)).  We create empirical logits using the proportion of students trying to lose weight for each level of hours spent watching daily and look at the trends in the logits separately for males and females. From Figure \@ref(fig:logitmediasex), there does not appear to be a linear relationship for males or females. 

```{r,mediaXsex, fig.align="center",out.width="60%", fig.cap='Weight loss plans vs. daily hours of TV and sex.',echo=FALSE, warning=FALSE, comment=NA}
ytable <- mosaic::tally(~ sex + media + lose.wt, data = risk2009)   
mosaicplot(ytable, color = T, main="")
#prop.table(ytable, c(1,2))
```

```{r, logitmediasex, fig.align="center",out.width="60%", fig.cap='Empirical logits for the odds of trying to lose weight by TV watching and sex.',echo=FALSE, warning=FALSE}
emplogit2 <- risk2009 %>%
  group_by(sex, media) %>%
  summarise(prop.lose = mean(lose.wt.01), n = n()) %>%
  mutate(prop.lose = ifelse(prop.lose==0, .01, prop.lose)) %>%
  mutate(emplogit = log(prop.lose / (1 - prop.lose)))

ggplot(emplogit2, aes(x = media, y = emplogit, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("TV hours per day") + ylab("Empirical logits")
```

### Initial models

Our strategy for modeling is to use our questions of interest and what we have learned in the exploratory data analysis. For each model we interpret the coefficient of interest, look at the corresponding Wald test and, as a final step, compare the deviances for the different models we considered.

We first use a model where sex is our only predictor.
```{r, 6model1, echo=FALSE, comment=NA}
risk2009 <- risk2009 %>%
  mutate(female = ifelse(sex=="Female", 1, 0))
model1 <- glm(lose.wt.01 ~ female, family = binomial, data = risk2009)
tmp <- capture.output(summary(model1))
cat(tmp[c(3:4, 9:13, 18:20)], sep='\n')
#exp(confint(model1))
```
Our estimated logistic regression model is:
\[\log\left(\frac{{p}}{1-{p}}\right)=-0.79+1.11 (\textrm{female})\]
where ${p}$ is the estimated proportion of youth wanting to lose weight. We can interpret the coefficient on female by exponentiating $e^{1.1130} = 3.04$ (95% CI = $(2.05, 4.54)$) indicating that the odds of a female trying to lose weight is over three times the odds of a male trying to lose weight ($Z=5.506$, $p=3.67e-08$). We retain sex in the model and consider adding the BMI percentile:

```{r, include=FALSE}
exp(coef(model1))
exp(confint(model1))
```

```{r, 6model2int, echo=FALSE, include=FALSE, comment=NA}
model2int <- glm(lose.wt.01 ~ female + bmipct + female:bmipct, 
                 family = binomial, data = risk2009)
summary(model2int)
```

```{r, 6model2, echo=FALSE, comment=NA}
model2 <- glm(lose.wt.01 ~ female + bmipct, family = binomial, 
              data = risk2009)
tmp <- capture.output(summary(model2))
cat(tmp[c(3:5, 10:15, 20:22)], sep='\n')
```

We see that there is statistically significant evidence ($Z=9.067, p<.001$) that BMI is positively associated with the odds of trying to lose weight, after controlling for sex.  Clearly BMI percentile belongs in the model with sex.  

Our estimated logistic regression model is:
\[\log\left(\frac{{p}}{1-{p}}\right)= -4.46+1.54\textrm{female}+0.051\textrm{bmipct}\]

To interpret the coefficient on `bmipct`, we will consider a 10 unit increase in `bmipct`.  Because $e^{10*0.0509}=1.664$, then there is an estimated 66.4\% increase in the odds of wanting to lose weight for each additional 10 percentile points of BMI for members of the same sex. Just as we had done in other multiple regression models we need to interpret our coefficient *given that the other variables remain constant*. An interaction term for BMI by sex was tested (not shown) and it was not significant ($Z=-0.405$, $p=0.6856$), so the effect of BMI does not differ by sex.

We next add `sport` to our model. Sports participation was considered for inclusion in the model in three ways: an indicator of sports participation (0 = no teams, 1 = one or more teams), treating the number of teams (0, 1, 2, or 3) as numeric, and treating the number of teams as a factor.  The models below treat sports participation using an indicator variable, but all three models produced similar results.

```{r, 6model3, echo=FALSE, comment=NA, message=FALSE}
model3 <- glm(lose.wt.01 ~ female + bmipct + sport, 
              family = binomial, data = risk2009)
tmp <- capture.output(summary(model3))
cat(tmp[c(3:5, 10:15, 20:22)], sep='\n')

model3int <- glm(lose.wt.01 ~ female + bmipct + sport + female:sport + bmipct:sport, family = binomial, data = risk2009)

tmp1 <- capture.output(summary(model3int))
cat(tmp1[c(3:5, 10:17, 22:24)], sep='\n')
```

Sports teams were not significant in any of these models, nor were interaction terms (sex by sports) and (bmipct by sports). As a result, sports participation was no longer considered for inclusion in the model.

We last look at adding `media` to our model.

```{r, 6model4, echo=FALSE, comment=NA, message=FALSE}
model4 <- glm(lose.wt.01 ~ bmipct + female + media, 
              family = binomial, data = risk2009)
tmp <- capture.output(summary(model4))
cat(tmp[c(3:5, 10:16, 21:23)], sep='\n')
```

Media is not a statistically significant term ($p=0.456$, $Z=-0.745$). However, because our interest centers on how media may affect attempts to lose weight and how its effect might be different for females and males, we fit a model with a media term and a sex by media interaction term (not shown). Neither term was statistically significant, so we have no support in our data that media exposure as measured by hours spent watching TV is associated with the odds a teen is trying to lose weight after accounting for sex and BMI.



### Model discussion and summary

We found that the odds of wanting to lose weight are considerably greater for females compared to males. In addition, respondents with greater BMI percentiles express a greater desire to lose weight for members of the same sex. Regardless of sex or BMI percentile, sports participation and TV watching are not associated with different odds for wanting to lose weight. 

A limitation of this analysis is that we used complete cases in place of a method of imputing responses or modeling missingness. This reduced our sample from 500 to 429, and it may have introduced bias. For example, if respondents who watch a lot of TV were unwilling to reveal as much, and if they differed with respect to their desire to lose weight from those respondents who reported watching little TV, our inferences regarding the relationship between lots of TV and desire to lose weight may be biased.

Other limitations may result from definitions. Trying to lose weight is self-reported and may not correlate with any action undertaken to do so. The number of sports teams may not accurately reflect sports related pressures to lose weight. For example, elite athletes may focus on a single sport and be subject to greater pressures whereas athletes who casually participate in three sports may not feel any pressure to lose weight. Hours spent watching TV is not likely to encompass the totality of media exposure, particularly because exposure to celebrities occurs often online.  Furthermore, this analysis does not explore in any detail maladaptions---inappropriate motivations for wanting to lose weight. For example, we did not focus our study on subsets of respondents with low BMI who are attempting to lose weight. 

It would be instructive to use data science methodologies to explore the entire data set of 16,000 instead of sampling 500. However, the types of exploration and models used here could translate to the larger sample size.

Finally a limitation may be introduced as a result of the acknowledged variation in the administration of the YRBSS. States and local authorities are allowed to administer the survey as they see fit, which at times results in significant variation in sample selection and response.


## Classification and regression trees (CART)

```{r, echo=FALSE}
img_path <- "images"
```

### The curse of dimensionality

We described how methods such as LDA and QDA are not meant to be used with many predictors $p$ because the number of parameters that we need to estimate becomes too large. For example, with the digits example $p=784$, we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the _curse of dimensionality_. The _dimension_ here refers to the fact that when we have $p$ predictors, the distance between two observations is computed in $p$-dimensional space.

A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility. 

For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it's easy to see that our windows have to be of size 0.1:

```{r, echo=FALSE, out.width="50%", fig.height=1.5}
rafalib::mypar()
x <- seq(0,1,len=100)
y <- rep(1, 100)
plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
lines(x[c(15,35)], y[c(15,35)], col="blue",lwd=3)
points(x,y, cex = 0.25)
points(x[25],y[25],col="blue", cex = 0.5, pch=4)
text(x[c(15,35)], y[c(15,35)], c("[","]"))
```

Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to $\sqrt{.10} \approx .316$:

```{r, echo=FALSE, fig.width=7, fig.height=3.5, out.width="50%",}
rafalib::mypar(1,2)
tmp <- expand.grid(1:10, 1:10)
x <- tmp[,1]
y <- tmp[,2]
plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
polygon(c(x[25]-0.5, x[25]-0.5, x[25]+0.5, x[25]+0.5),
        c(y[25]-0.5, y[25]+0.5, y[25]+0.5, y[25]-0.5), col="blue")
points(x,y, cex = 0.25)
points(x[25],y[25], cex = 0.5, pch=4)

plot(x,y, xlab="",ylab="", cex=0.25, yaxt="n", xaxt="n",type="n")
polygon(c(x[25]-sqrt(10)/2, x[25]-sqrt(10)/2, x[25]+sqrt(10)/2, x[25]+sqrt(10)/2),
        c(y[25]-sqrt(10)/2, y[25]+sqrt(10)/2, y[25]+sqrt(10)/2, y[25]-sqrt(10)/2),
        col="blue")
points(x,y, cex = 0.25)
points(x[25],y[25], cex = 0.5, pch=4)
```

Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is  $\sqrt[3]{.10} \approx 0.464$.
In general, to include 10% of the data in a case with $p$ dimensions, we need an interval with each side of size $\sqrt[p]{.10}$ of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.

```{r, message=FALSE, message=FALSE}
library(tidyverse)
p <- 1:100
qplot(p, .1^(1/p), ylim = c(0,1))
```

By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.

Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.

### CART motivation

To motivate this section, we will use a new dataset 
that includes the breakdown of the composition of olive oil into 8 fatty acids:

```{r}
library(tidyverse)
library(dslabs)
data("olive")
names(olive)
```

For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.

```{r}
table(olive$region)
```

We remove the `area` column because we won't use it as a predictor.

```{r}
olive <- select(olive, -area)
```

Let's very quickly try to predict the region using kNN:

```{r, warning=FALSE, message=FALSE}
library(caret)
fit <- train(region ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 15, 2)), 
             data = olive)
ggplot(fit)
```

We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.

```{r, fig.height=3, fig.width=6}
olive %>% gather(fatty_acid, percentage, -region) %>%
  ggplot(aes(region, percentage, fill = region)) +
  geom_boxplot() +
  facet_wrap(~fatty_acid, scales = "free", ncol = 4) +
  theme(axis.text.x = element_blank(), legend.position="bottom")
```

This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.

```{r}
olive %>% 
  ggplot(aes(eicosenoic, linoleic, color = region)) + 
  geom_point() +
  geom_vline(xintercept = 0.065, lty = 2) + 
  geom_segment(x = -0.2, y = 10.54, xend = 0.065, yend = 10.54, 
               color = "black", lty = 2)
```

In Section \@ref(predictor-space) we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, 
we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than $10.535$, predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4.5, out.width="50%"}
library(caret)
library(rpart)
rafalib::mypar()
train_rpart <- train(region ~ ., method = "rpart", data = olive)

plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

Decision trees like this are often used in practice. For example, to decide on a person's risk of poor outcome after having a heart attack, doctors use the following:

```{r, echo=FALSE, out.width="50%"}
# source https://www.researchgate.net/profile/Douglas_Walton/publication/228297479/figure/fig1/AS:301828123185152@1448972840090/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png
knitr::include_graphics(file.path(img_path,"Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png"))
```

(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184^[https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&mirid=1&type=2].)

A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_.
Regression and decision trees operate by predicting an outcome variable $Y$ by partitioning the predictors.


### Regression trees

When the outcome is continuous, we call the method a _regression_ tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation $f(x) = \mbox{E}(Y | X = x)$ with $Y$ the poll margin and $x$ the day. 

```{r}
data("polls_2008")
qplot(day, margin, data = polls_2008)
```

The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. A mathematical way to describe this is to say that we are partitioning the predictor space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$, and then for any predictor $x$ that falls within region $R_j$, estimate $f(x)$ with the average of the training observations $y_i$ for which the associated predictor $x_i$ is also in $R_j$.

But how do we decide on the partition  $R_1, R_2, \ldots, R_J$ and how do we choose $J$? Here is where the algorithm gets a bit complicated.

Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later.

Once we select a partition $\mathbf{x}$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$:

$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$

In our current example we only have one predictor, so we will always choose $j=1$, but in general this will not be the case. Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning, we compute predictors by taking the average of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 

But how do we pick $j$ and $s$? Basically we find the pair that minimizes the residual sum of square (RSS):
$$
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
$$


This is then applied recursively to the new regions $R_1$ and $R_2$. We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 

Let's take a look at what this algorithm does on the 2008 presidential election poll data. We will use the `rpart` function in the __rpart__ package.

```{r}
library(rpart)
fit <- rpart(margin ~ ., data = polls_2008)
```

Here, there is only one predictor. Thus we do not have to decide which predictor $j$ to split by, we simply have to decide what value $s$ we use to split. We can visually see where the splits were made:


```{r, eval=FALSE}
plot(fit, margin = 0.1)
text(fit, cex = 0.75)
```

```{r, fig.height=5, out.width="60%", echo=FALSE}
rafalib::mypar()
plot(fit, margin = 0.1)
text(fit, cex = 0.75)
```

The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate $\hat{f}(x)$ looks like this:

```{r}
polls_2008 %>% 
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col="red")
```

Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made. 

First we need to define the term _complexity parameter_ (cp).  Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes. 

However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the `rpart` function is `minsplit` and the default is 20. The `rpart` implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is `minbucket` and defaults to `round(minsplit/3)`. 

As expected, if we set `cp = 0` and `minsplit = 2`, then our prediction is as flexible as possible and our predictor is our original data:

```{r}
fit <- rpart(margin ~ ., data = polls_2008, 
             control = rpart.control(cp = 0, minsplit = 2))
polls_2008 %>% 
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col="red")
```

Intuitively we know that this is not a good approach as it will generally result in over-training. These `cp`, `minsplit`, and `minbucket`, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility. 

So how do we pick these parameters? We can use cross validation, described in Chapter \@ref(cross-validation), just like with any tuning parameter. Here is an example of using cross validation to chose cp.

```{r}
library(caret)
train_rpart <- train(margin ~ ., 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = polls_2008)
ggplot(train_rpart)
```

To see the resulting tree, we access the `finalModel` and plot it:


```{r, eval=FALSE}
plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

```{r, fig.height=5, out.width="80%", echo=FALSE}
rafalib::mypar()
plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

And because we only have one predictor, we can actually plot $\hat{f}(x)$:

```{r}
polls_2008 %>% 
  mutate(y_hat = predict(train_rpart)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col="red")
```

Note that if we already have a tree and want to apply a higher cp value, we can use the `prune` function. We call this _pruning_ a tree because we are snipping off partitions that do not meet a `cp` criterion. We previously created a tree that used a `cp = 0` and saved it to `fit`. We can prune it like this:

```{r}
pruned_fit <- prune(fit, cp = 0.01)
```


### Classification (decision) trees

Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome. 

The first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can't take the average of categories).

The second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the _Gini Index_ and _Entropy_.   

In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as 

$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above. 

_Entropy_ is a very similar quantity, defined as

$$
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
$$

Let us look at how a classification tree performs on the digits example we examined before:

```{r, echo=FALSE}
library(dslabs)
data("mnist_27")
```

```{r, echo=FALSE}
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
  geom_raster(show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5),color="black")
}
```

We can use this code to run the algorithm and plot the resulting tree:
```{r}
train_rpart <- train(y ~ .,
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
                     data = mnist_27$train)
plot(train_rpart)
```

The accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods:

```{r}
y_hat <- predict(train_rpart, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall["Accuracy"]
```

The plot of the estimated conditional probability shows us the limitations of classification trees:

```{r, echo=FALSE, out.width="100%", warning=FALSE, message=FALSE}
library(gridExtra)
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rpart, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("Decision Tree")

grid.arrange(p2, p1, nrow=1)
```

Note that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity. 

Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes and don't require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.

## Random forests

Random forests are a **very popular** machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. The specific steps are as follows.

1\. Build $B$ decision trees using the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2\. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3\. For continuous outcomes, form a final prediction with the average $\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j$. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

1\. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
2\. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to 
the 2008 polls data.  We will use the `randomForest` function in the __randomForest__ package:

```{r, message=FALSE, warning=FALSE}
library(randomForest)
fit <- randomForest(margin~., data = polls_2008) 
```

Note that if we apply the function `plot` to the resulting object, stored in `fit`, we see how the error rate of our algorithm changes as we add trees. 

```{r, eval=FALSE}
rafalib::mypar()
plot(fit)
```

```{r, echo=FALSE}
rafalib::mypar()
plot(fit)
```

We can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.

The resulting estimate for this random forest can be seen like this:

```{r}
polls_2008 %>%
  mutate(y_hat = predict(fit, newdata = polls_2008)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_line(aes(day, y_hat), col="red")
```

Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of $b$ and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.

```{r, echo=FALSE, out.width="100%"}
library(rafalib)
set.seed(1)
ntrees <- 50
XLIM <- range(polls_2008$day)
YLIM <- range(polls_2008$margin)

if(!file.exists(file.path(img_path,"rf.gif"))){
  sum <- rep(0,nrow(polls_2008))
  res <- vector("list", ntrees)
  animation::saveGIF({
    for(i in 0:ntrees){
      mypar(1,1)
      if(i==0){
        with(polls_2008, plot(day, margin, pch = 1, main = "Data", xlim=XLIM,
                              ylim=YLIM,
                              xlab = "Days", ylab="Obama - McCain"))
      } else{
        ind <- sort(sample(1:nrow(polls_2008), replace = TRUE))
        tmp <- polls_2008[ind,]
        fit <- rpart(margin~day, data = tmp)
        pred <- predict(fit, newdata = tmp)
        res[[i]] <- tibble(day = tmp$day, margin=pred)
        pred <- predict(fit, newdata = polls_2008)
        sum <- sum+pred
        avg <- sum/i
        with(tmp, plot(day,margin, pch=1, xlim=XLIM, ylim=YLIM, type="n",
                       xlab = "Days", ylab="Obama - McCain",
                       main=ifelse(i==1, paste(i, "tree"),paste(i, "trees"))))
        for(j in 1:i){
          with(res[[j]], lines(day, margin, type="s", col="grey", lty=2))
        }
        with(tmp, points(day,margin, pch=1))
        with(res[[i]], lines(day, margin, type="s",col="azure4",lwd=2))
        lines(polls_2008$day, avg, lwd=3, col="blue")
      }
    }
    for(i in 1:5){
      mypar(1,1)
      with(polls_2008, plot(day, margin, pch = 1, main="Final", xlim=XLIM, ylim=YLIM,
                            xlab = "Days", ylab="Obama - McCain"))
      lines(polls_2008$day, avg, lwd=3, col="blue")
    }
  }, movie.name = "images/rf.gif", ani.loop=0, ani.delay =50)
}  

if(knitr::is_html_output()){
  knitr::include_graphics(file.path(img_path,"rf.gif"))
} else {
  sum <- rep(0,nrow(polls_2008))
  res <- vector("list", ntrees)

  mypar(2,3)
  show <- c(1, 5, 25, 50) 
  for(i in 0:ntrees){
      if(i==0){
        with(polls_2008, plot(day, margin, pch = 1, main = "Data", xlim=XLIM,
                              ylim=YLIM,
                              xlab = "Days", ylab="Obama - McCain"))
      } else{
        ind <- sort(sample(1:nrow(polls_2008), replace = TRUE))
        tmp <- polls_2008[ind,]
        fit <- rpart(margin~day, data = tmp)
        pred <- predict(fit, newdata = tmp)
        res[[i]] <- tibble(day = tmp$day, margin=pred)
        pred <- predict(fit, newdata = polls_2008)
        sum <- sum+pred
        avg <- sum/i
        if(i %in% show){
          with(tmp, plot(day,margin, pch=1, xlim=XLIM, ylim=YLIM, type="n",
                         xlab = "Days", ylab="Obama - McCain",
                         main=ifelse(i==1, paste(i, "tree"),paste(i, "trees"))))
          for(j in 1:i){
            with(res[[j]], lines(day, margin, type="s", col="grey", lty=2))
          }
          with(tmp, points(day,margin, pch=1))
          with(res[[i]], lines(day, margin, type="s",col="azure4",lwd=2))
          lines(polls_2008$day, avg, lwd=3, col="blue")
        }
      }
  }
  with(polls_2008, plot(day, margin, pch = 1, main="Final", xlim=XLIM, ylim=YLIM,
                        xlab = "Days", ylab="Obama - McCain"))
  lines(polls_2008$day, avg, lwd=3, col="blue")
}
```


Here is the random forest fit for our digits example based on two predictors:

```{r}
library(randomForest)
train_rf <- randomForest(y ~ ., data=mnist_27$train)

confusionMatrix(predict(train_rf, mnist_27$test),
                mnist_27$test$y)$overall["Accuracy"]
```

Here is what the conditional probabilities look like:

```{r, echo = FALSE, out.width="100%"}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rf, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("Random Forest")

grid.arrange(p2, p1, nrow=1)
```

Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the __caret__ package to optimize over the minimum node size. Because, this is not one of the parameters that the __caret__ package optimizes by default we will write our own code:

```{r, cache=TRUE}
nodesize <- seq(1, 51, 10)
acc <- sapply(nodesize, function(ns){
  train(y ~ ., method = "rf", data = mnist_27$train,
               tuneGrid = data.frame(mtry = 2),
               nodesize = ns)$results$Accuracy
})
qplot(nodesize, acc)
```

We can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data.

```{r}
train_rf_2 <- randomForest(y ~ ., data=mnist_27$train,
                           nodesize = nodesize[which.max(acc)])

confusionMatrix(predict(train_rf_2, mnist_27$test),
                mnist_27$test$y)$overall["Accuracy"]
```

The selected model improves accuracy and provides a smoother estimate.

```{r, echo=FALSE, out.width="100%"}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rf_2, newdata = mnist_27$true_p, type="prob")[,2]) +
  ggtitle("Random Forest")

grid.arrange(p2, p1, nrow=1)
```


Note that we can avoid writing our own code by using other random forest implementations as described in the __caret__ manual^[http://topepo.github.io/caret/available-models.html].


Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine _variable importance_.
To define _variable importance_ we count how often a predictor is used in the individual trees. You can learn more about _variable importance_ in an advanced machine learning book^[https://web.stanford.edu/~hastie/Papers/ESLII.pdf]. The __caret__ package includes the function `varImp` that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.


