# Classification {#classification}

$$
\newcommand{\lik}{\operatorname{Lik}}
\newcommand{\Lik}{\operatorname{Lik}}
$$
Many research questions have binary (yes/no or success/failure) responses: 

- a. Are students with poor grades more likely to binge drink?
- b. Is exposure to a particular chemical associated with a cancer diagnosis?
 
**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), Yes ($Y=1$) or No ($Y=0$), etc. Thus, examples (a) and (b) above would be considered to have binary responses (Does a student binge drink?  Was a patient diagnosed with cancer?). Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success $p$ based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure.

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapter \@ref(regression) and \@ref(multiple-regression).  Second, we will consider classification and regression trees (CART).  Finally, we will discuss random forests.

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \@ref(tidyverse-package) that loading the **tidyverse** package by running `library(tidyverse)` loads the following commonly used data science packages all at once:

* **ggplot2** for data visualization
* **dplyr** for data wrangling
* **tidyr** for converting data to "tidy" format
* **readr** for importing spreadsheet data into R
* As well as the more advanced **purrr**, **tibble**, **stringr**, and **forcats** packages

If needed, read Section \@ref(packages) for information on how to install and load R packages. 

```{r, eval=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(fivethirtyeight)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(fivethirtyeight)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text:
library(knitr)
library(kableExtra)
library(gridExtra)
```
   
## Logistic regression

### What is logistic regression?

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. 

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

where the observed values $Y_i \sim$ Bernoulli with $p=p_i$ for a given set of predictors $X$.

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *logistic function*, is $\left(\frac{e^x}{e^x + 1}\right)$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.

### House elections: exploratory data analysis

What affects whether a Democrat or Republican wins a race in the U.S. House of Representatives? This is an example of a binary response: either a Democrat wins (and a Republican loses) or a Republican wins (and a Democrat loses).^[It is rare that third party candidates mount serious bids in U.S. House elections, so it isn't a much of an oversimplication to think of the variable as binary.] In this section, we are going to consider several models predicting Democratic victory in House races.  First, we will consider a single categorical variable as a predictor: the region that a district lies in (Midwest, Northeast, South, or West).  Second, we will consider a single continuous variable (year).  Finally, we fill fit a model that contains an interaction of the two.

The data on House election results from 1976 to 2018 can be found in the `house_results` data frame in **politicaldata** package.  We'll create a version of this data frame called `house_ch13` that creates a new column `dem_win` that notes for each state if the Democratic candidate in a congressional district received more votes than the other candidates.  We'll also join it with the `state_info` data frame in the **fivethirtyeight** package to add the `region` of each state.

```{r}
library(politicaldata)

house_ch13 <- house_results %>%
  
  # Create dem_win variable
  
  mutate(dem = ifelse(is.na(dem), 0, dem),
         other = ifelse(is.na(other), 0, other),
         rep = ifelse(is.na(rep), 0, rep),
         dem_win = ifelse(dem > rep & dem > other, 1, 0)) %>%
  
  # Rename to join with state_info
  
  rename(state_abbrev = state_abb) %>%
  left_join(state_info) %>%
  select(region, state, district, year, dem_win)
```

Recall the three common steps in an exploratory data analysis we saw in Subsection \@ref(model1EDA):

1. Looking at the raw data values.
1. Computing summary statistics.
1. Creating data visualizations.

Let's first look at the raw data values by either looking at `house_ch13` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(house_ch13)
```

Let's also display a random sample of 5 rows of the 9,557 rows corresponding to different district-years. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval = FALSE}
house_ch13 %>% 
  sample_n(size = 5)
```
```{r, echo = FALSE}
house_ch13 %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "A random sample of 5 out of the 4,201 district-years",
    booktabs = TRUE,
    linesep = ""
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Now that we’ve looked at the raw values in our `house_ch13` data frame and got a sense of the data, let’s compute summary statistics. As we've done in our exploratory data analyses before, let’s use the `skim()` function from the `skimr` package, being sure to only select() the variables of interest in our model:

```{r}
house_ch13 %>% 
  select(dem_win, region, year) %>% 
  skim()
```

Observe that we have no missing data, that we have 9,557 observations, and that the mean of `dem_win` is 0.54, indicating that Democrats won 54% of the House elections in this period (1976--2018).

Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations.

For our categorical variable, we'll look at histograms of `dem_win` faceted by `region`:

```{r}
house_ch13 %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) +
  theme_minimal()
```

Wait!  That doesn't tell us very much, because our outcome variable only takes two values, 0 and 1.  Let's instead `group_by(district)` and `summarize()` to get a better sense of the distributions:

```{r}
house_ch13 %>%
  group_by(region, district) %>%
  summarize(dem_win = mean(dem_win)) %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) + 
  theme_minimal()
```

This is much more informative!  We can see that the Midwest is highly bimodal, with many districts either electing Democrats for every year in this period or for none.  The Northeast and West have many districts that always elect Democrats but few that never do.  The South is the only region with a peak in the middle, indicating that there are many districts in the South that elected Democrats for about half the time during 1976-2018.

What happens if we create a scatterplot of our outcome variable `dem_win` and a continuous predictor, `year`?

```{r}
house_ch13 %>%
  ggplot(aes(x = year, y = dem_win)) +
  geom_point() +
  labs(x = "Year", y = "Democratic Victory") +
  geom_smooth(method = "lm", se = FALSE)
```

This is completely incomprehensible!  When dealing with binary data, it is more helpful to construct an *empirical logit* plot instead of a regular scatterplot.  The steps for constructing such a plot are as follows:

1. `group_by` your continuous variable.
1. `summarize` the percentage of successes in your outcome variable.
1. Calculate the *empirical logit* for each group, using the logit function: $log\left(\frac{p}{1 - p}\right)$
1. Plot the results.

First, since we will use it frequently, let's define a function `logit()` that performs the logit transformation:

```{r}
logit <- function(p) log(p / (1 - p))
```

Next, let's look at the empirical logit plot:

```{r}
house_ch13 %>%
  group_by(year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = logit(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  theme_classic()
```

Now we see that after the logit transformation, there is roughly a linear relationship between our outcome variable and our explanatory variable `year`.  This means that a logistic regression model makes sense.  Some of the most visually apparent outliers will be familiar to students of American politics: 1994 (the "Republican Revolution"), 2008 (Obama's first election), and 2018. Yet in general it appears that over time the Democrats have performed worse in House elections.

We can follow the same steps to look at this relationship within Census regions (Midwest, Northeast, South and West):
 
```{r}
house_ch13 %>%
  group_by(region, year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = logit(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  facet_wrap(~ region) +
  theme_classic()
```

We see roughly linear relationships after the logit transformation within-region as well, although the relationship looks more linear in the Midwest and South than in the Northeast and West.  We see that the Democratic Party's overall decline in House races is driven by the South and to a lesser extent the Midwest; Democratic performance has on average improved in the Northeast and West.  The sharp negative slope in the South will not be surprising if one is familiar with the collapse of the "Solid South."

### One categorical explanatory variable

Let's start our modeling by predicting `dem_win` with a single categorical explanatory variable, `region`.  As we'll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression.  In fact, we'll follow the same basic steps:

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `house_region_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_region_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

Note that the key difference is that instead of using `lm()`, we are now using `glm()`.  `glm()` operates very similarly to `lm()`, but it has an additional argument: `family`.  To run a logistic regression, we use `family = binomial`.

```{r, eval=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial data = house_ch13)
house_region_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch13)
```
```{r, echo=FALSE}
tidy(house_region_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Recall that in the linear regression context, we interpreted the coefficients as follows:  the intercept represented the mean for the omitted category, while the other coefficients all represented offsets from that value.  We can't use that intepretation here.  Recall our logistic regression model equation:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

Note that there's a familiar term for what we are taking the log of on the left hand side:

\[\textrm{Odds} = \frac{\textrm{probability of success}}{\textrm{probability of failure}}=
\frac{p}{1-p}.\] 

For example, there's a 25\% chance of flipping two heads in a row when flipping fair coins.  In odds terms, the odds of flipping two heads in a row are $\frac{0.25}{0.75} = \frac{1}{3}$. These are conventionally phrased in words in terms of the odds *against* success, such as "three to one odds against flipping two heads" or just "three to one odds."

Thus, our equation can also be read as:

\[
\log(\textrm{Odds})=\beta_0+\beta_1X 
\]

Let's say that $X$ is our categorical variable `region`, with the baseline category being the Midwest.  Based on this model, the log odds of a Democratic victory when $X = \textrm{Midwest}$ is:
\[
\log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) =\beta_0 = -0.076,
\]
and the log odds when $X = \textrm{South}$ is:
\[
\log\left(\frac{p_{South}}{1-p_{South}}\right)=\beta_0+\beta_{1,South} = -0.076 + 0.056 = -0.020.
\]

We can see that $\beta_{1,South}$ is the difference between the log odds of success when $X = \textrm{South}$ versus $X = \textrm{Midwest}$.  Using rules of logs:
\[
\begin{aligned}
\beta_{1,South} &= (\beta_0 + \beta_{1,South}) - \beta_0 \\ &=
\log\left(\frac{p_{South}}{1-p_{South}}\right) - \log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) \\ &=
\log\left(\frac{p_{South}/(1-p_{South})}{p_{Midwest}/{(1-p_{Midwest})}}\right) \\ &=
\log\left(\frac{\textrm{Odds}(South)}{\textrm{Odds}(Midwest)}\right)
\end{aligned}
\]

Since $e^{\log(x)} = x$, $e^{\beta_{1,South}}$ is the ratio of the odds of success when $X = \textrm{South}$ compared to $X = \textrm{Midwest}$. In general, __exponentiated coefficients in a logistic regression are odds ratios__. A general interpretation of an odds ratio is the odds of success for group A compared to the odds of success for group B---how many times greater the odds of success are in group A compared to group B.

Note that the logistic regression model can also be re-written in a __probability form__:

\[
p_X=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
\]

Now that we understand that expontentiated coefficients in logistic regressions are odds ratios, we can use the argument `exponentiate = TRUE` in the `tidy()` function to exponentiate the coefficients for us:

```{r, eval=FALSE}
house_region_model %>%
  tidy(conf.int = TRUE,
       exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
tidy(house_region_model,
     conf.int = TRUE,
     exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table, exponentiated coefficients",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

This allows us to see that the odds of a Democratic victory in the Northeast are nearly twice as high as the odds of a Democratic victory in the Midwest, the reference category.  The odds of a Democratic victory in the West are about 1.4 times higher than the odds of a Democratic victory in the Midwest.  The odds of a Democratic victory in the South are similar to the odds of a Democratic victory in the Midwest.

### Observed/fitted values and residuals

We have previously defined the following three concepts for a linear regression:

1. Observed values $y$, or the observed value of the outcome variable
1. Fitted values $\widehat{y}$, or the value on the regression line for a given $x$ value
1. Residuals $y - \widehat{y}$, or the error between the observed value and the fitted value

We obtained these values and other values using the `augment()` function from the **broom** package. Recall too that we used the `.se.fit` column to construct confidence intervals.  We'll see here how we can apply these same concepts to logistic regression.

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model) %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

The syntax is the same, but the interpretation has to change, since the `.fitted`, `conf.low`, and `conf.high` columns are all on the logit scale.  While we could try to interpret these values, `augment()` has the argument `type.predict = "response"` that allow us to present the results in terms of **predicted probabilities**:

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment(type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model,
                             type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Now each of the `.fitted` values is a *predicted probability* of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability. 
You may be wondering how to interpret the residuals.  The residuals reported by `augment()` for a logistic regression are called *deviance residuals*.  A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.

However, you can also have `augment()` report residuals as differences between the observed outcome and the predicted probabilities by using `type.residuals = "response"`:

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Now, the `.resid` value is the difference between the actual outcome (`dem_win`) and the predicted probability.

### One numerical explanatory variable

We'll now predict `dem_win` with a single numerical explanatory variable, `year`. 

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `house_year_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_year_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.  Recall that setting `exponentiate = TRUE` exponentiates the logistic regression coefficients, which can help with interpretation. 

```{r, eval=FALSE}
house_year_model <- glm(dem_win ~ year, family = binomial data = house_ch13)
house_year_model %>%
  tidy(conf.int = TRUE,
       exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
house_year_model <- glm(dem_win ~ year, family = binomial, data = house_ch13)
```
```{r, echo=FALSE}
tidy(house_year_model,
     conf.int = TRUE,
     exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

How do we interpret the coefficients in this model?  Since the `year` coefficient is less than 1, that means that each additional year *reduces* the odds of a Democratic victory.  In fact, each additional year is associated with a 17% reduction in the odds of a Democratic victory.

Note that the very high number for the "intercept" suggests that the odds of a Democratic victory in year 0 are arbitrarily high -- that is, the *probability* of a Democratic victory in year 0 is essentially 100%.  But of course, there was no House of Representatives in year 0 (furthermore, there is no year 0)!  This is a good lesson in making sure to know the limits of one's models: just because something makes *mathematical* sense as an output of your model, doesn't mean that it makes real-world sense for the phenomenon you are studying.

While `house_region_model` and `house_year_model` both tell us something interesting, we could learn more with an *interaction model* that includes both of our predictors.

### One numerical and one categorical explanatory variable

We'll now predict `dem_win` with a two variable, `region` and `year`, as well as the interaction between the two 

1. We first "fit" the logistic regression model using the `glm(y ~ x1 * x2, family, data)` function and save it in `house_interact_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_interact_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.  Recall that setting `exponentiate = TRUE` exponentiates the logistic regression coefficients, which can help with interpretation. 

```{r, eval=FALSE}
house_interact_model <- glm(dem_win ~ region * year, family = binomial data = house_ch13)
house_interact_model %>%
  tidy(conf.int = TRUE,
       exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```
```{r, echo=FALSE}
house_interact_model <- glm(dem_win ~ region * year, family = binomial, data = house_ch13)
```
```{r, echo=FALSE}
tidy(house_interact_model,
     conf.int = TRUE,
     exponentiate = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  knitr::kable(
    digits = 3,
    caption = "Logistic regression table",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

Now we can see how the effect of `year` varies by `region`.  While the passage of time is associated with more Democratic victories in the Northeast and the West, which we can see from the exponentiated coefficients on `regionNortheast:year` and `regionWest:year` being greater than 1, `year` is associated with declining Democratic fortunes in the Midwest (`year`) and the South (`regionSouth:year`).

Looking at predicted probabilities can also put this model in perspective.  Let's use `augment()` to generate the predictions.  Remember that `type.predict = "response"` and `type.residuals = "response"` put the fitted values and the residuals on the probability scale.

```{r, eval=FALSE}
regression_points <- house_interact_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_interact_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

We can also use `augment` to make predictions for years that aren't in our data.  What would our model predict for the 2020 elections?  We use the `newdata` argument in `augment()` to make these predictions.

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit)
```

These can easily be plotted using `ggplot()`:

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  ggplot(aes(y = .fitted,
             ymin = conf.low,
             ymax = conf.high,
             x = region)) +
  geom_point() +
  geom_errorbar() +
  labs(y = "Predicted probability of a Democratic victory (2020)",
       x = "") +
  theme_classic()
```

### Fitting many models using `map()`

While it is interesting to see how Democrats perform by region over time, it would also be interesting to see how each state has changed in its partisan voting from 1976--2018.  Have any seen particularly large increases (or decreases) in the probability of a Democratic candidate winning?

The code to do this is very similar to the code we used for the gubernatorial forecasts in Chapter \@ref(regression) and the Seattle house prices in Chapter \@ref(multiple-regression).  However, we will use `glm()` instead of `lm()`, and we will use the `exponentiate = TRUE` option for `tidy()`.

First, we'll filter to the states that have at least 50 district-years in the dataset. Next, let's use `map()` to learn about these districts:

```{r}
infreq_states <- house_ch13 %>%
  count(state) %>%
  filter(n < 50) %>%
  pull(state)

house_state_model <- house_ch13 %>%
  filter(! state %in%  infreq_states) %>%
  group_by(state) %>%
  nest() %>%
  mutate(mod = map(data, ~ glm(dem_win ~ year, family = binomial, data = .)),
         reg_results = map(mod, ~ tidy(., conf.int = TRUE, exponentiate = TRUE)),
         year_coef = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(estimate)),
         year_low = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(conf.low)),
         year_high = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(conf.high)))

glimpse(house_state_model)
```

The easiest way to see the results of these models is to plot the exponentiated coefficients:

```{r}
house_state_model %>%
  ungroup() %>%
  mutate(state = fct_reorder(state, year_coef)) %>%
  ggplot(aes(x = state, y = year_coef, ymin = year_low, ymax = year_high)) +
  geom_point() +
  geom_errorbar() +
  theme_minimal() +
  labs(x = "",
       y = "Exponentiated coefficients of \"year\"",
       title = "Predicting Democratic victories in the U.S. House over time by state",
       subtitle = "Logistic regression coefficients for year plotted by state") +
  coord_flip()
```

Consistent with the account we saw when looking at the effect of `year` overall, there are more states where the odds of a Democratic victory have been decreasing by year than ones where they have been increasing. 

### Linear regression vs. logistic regression

Here's a useful summary of the differences between linear regression and logistic regression:

\[
\underline{\textrm{Response}} \\
\mathbf{Linear\ Regression:}\textrm{ numeric} \\
\mathbf{Logistic\ Regression:}\textrm{ binary} \\
\textrm{ } \\
\underline{\textrm{Model Fitting}} \\
\mathbf{Linear\ Regression:}\ \mu=\beta_0+\beta_1x \textrm{ using }\texttt{lm()}\\
\mathbf{Logistic\ Regression:}\ \log\left(\frac{p}{1-p}\right)=\beta_0+\beta_1x \text{ using }\texttt{glm()}\\
\textrm{ } \\
\underline{\textrm{EDA}} \\
\mathbf{Linear\ Regression:}\textrm{ plot $X$ vs. $Y$; add line} \\
\mathbf{Logistic\ Regression:}\textrm{ find $\log(\textrm{odds})$ for several subgroups; plot vs. $X$} \\
\textrm{ } \\
\underline{\textrm{Interpreting Coefficients}} \\
\mathbf{Linear\ Regression:}\ \beta_1=\textrm{ change in }\mu_y\textrm{ for unit change in $X$} \\
\mathbf{Logistic\ Regression:}\ e^{\beta_1}=\textrm{ percent change in odds for unit change in $X$} 
\]

### Professional models

So far, we have been fitting linear regressions and logistic regressions using `lm()` and `glm()`.  While these functions are well-known and easy to use, what if we wanted to fit another model?  We'd have to learn a new function, which may have new syntax.  It'd be much easier if we could use the same syntax for every model we fit.

A collection of packages that helps address this issue is called **tidymodels**: 

```{r, message = FALSE}
library(tidymodels)
```

**tidymodels** includes many packages, but we'll start by showing how to use **parsnip** to fit a logistic regression. 

First, in the **tidymodels** workflow, we have to save the *model specification*.  We do that using two functions: `logistic_reg()` and `set_engine()`.

```{r}
logistic_mod <- logistic_reg() %>%
  set_engine("glm")
```

`logistic_reg()` says that we want to fit a logistic regression, and `set_engine("glm")` specifies that we want to do it using `glm()`.  Behind the scenes, **parsnip** uses many other packages to fit its models, but by unifying the syntax, it means that you don't have to memorize how a lot of different functions work.

Note that our new object, `logistic_mod`, doesn't contain our data or a formula.  In order actually to fit our model, we need to feed `logistic_mod` to a function called `fit()`.  `fit()` is the general purpose function in **parsnip** for fitting any model specification.  It takes as its first argument the model specification, but otherwise it operates similarly to `lm()` and `glm()`:

```{r}
logistic_fit <- fit(logistic_mod,
                    factor(dem_win) ~ region * year,
                    data = house_ch13)
```

(We have to wrap `dem_win` in `factor()`, because `fit()` is more careful than `glm()` in requiring that classification models actually have categorical outcomes.)

One we have fit the model, how can we use it?  The `glm` object is still stored in `logistic_fit$fit`, so we can access that and use `tidy()`, just like we did before:

```{r}
logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

As you can see, this generates the same results as when we used `glm()` directly.

We'll use **tidymodels** when introducing CART and random forests in this chapter and machine learning in the next chapter.  While the models will change, the basic code structure will be very similar to how we fit the logistic regression above.

## Classification and regression trees (CART)

```{r, echo=FALSE}
img_path <- "images"
```

### What is CART?

We have learned how to fit models for binary responses using logistic regression.  However, logistic regression is just one of many methods we can use to model binary responses.  CART is another approach, which we'll learn about in this section.  In the next section, we'll learn about random forests.

A **tree** is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_. Decision trees predict an outcome variable $Y$ by *partitioning* the predictors.

Decision trees like this are often used in practice. For example, to decide on a person's risk of poor outcome after having a heart attack, doctors use the following:

```{r, echo=FALSE, out.width="50%"}
# source https://www.researchgate.net/profile/Douglas_Walton/publication/228297479/figure/fig1/AS:301828123185152@1448972840090/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png
knitr::include_graphics(file.path(img_path,"Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png"))
```

(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184^[https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&mirid=1&type=2].)

Here, the binary outcome is whether a patient is "High Risk" or "Low Risk."  We have three predictors: minimum systolic blood pressure over the initial 24-hour period, age, and presence of sinus tachycardia.  The tree presents a series of yes or no questions that allow us to use the predictors to classify a patient's risk level.

**Classification trees**, or decision trees, are used in prediction problems where the outcome is categorical.  (When the outcome is numerical, they are called **regression trees**; hence the acronym **CART**, standing for Classification and Regression Trees.)  The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. A mathematical way to describe this is to say that we are partitioning the *predictor space* into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$, and then for any predictor $x$ that falls within region $R_j$, we estimate $f(x)$ with the class that is the most common among the data within the partition for which the associated predictor $x_i$ is also in $R_j$.

But how do we decide on which partitions to make  ($R_1, R_2, \ldots, R_J$) and how do we choose $J$, the total number of partitions? Here is where the algorithm gets a bit complicated.

Classification trees create partitions recursively. We start the algorithm with one partition, the entire predictor space (i.e., every observation is classified as 0 or 1). But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. (We will describe how we decide when to stop later.)

Once we select a partition $\mathbf{x}$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$ (or if $x_j$ falls into a particular category $s$, if the predictor $j$ is categorical):

$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$

Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 

But how do we pick the predictor $j$ and the value $s$? One of the more popular ways for categorical data is the _Gini Index_.

In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as 

$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since $\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0$ for all $k$.

Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 

But when do we stop partitioning?  Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data.  However, our model may therefore perform worse when exposed to new data (this problem is called *overfitting*).  To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ ($c_p$). The measure of fit must improve by a factor of $c_p$ for the new partition to be added. Large values of $c_p$ will therefore force the algorithm to stop earlier which results in fewer nodes.

Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.

### One categorical explanatory variable

To create classification trees, we'll use the `decision_tree()` model specification and the `"rpart"` engine.  The syntax is very similar to when we used `logistic_reg()`.  Note that our binary response variable has to be a factor, just like with `logistic_reg()`. 

```{r}
tree_mod <- decision_tree() %>%
  set_engine("rpart",
             model = TRUE) %>%
  set_mode("classification")
```

(Note that we added the argument `model = TRUE` to `set_engine()`.  This saves the model frame, which we will need to avoid a warning when we plot the trees later.)

The function `set_mode()` wasn't necessary when we did logistic regression.  Here it clarifies that we want a *classification* tree rather than a *regression* tree.

Now that we have the object `tree_mod`, we can use `fit()` in the **parsnip** package.  We'll start by predicting `dem_win` with `region`:

```{r}
house_region_tree <- fit(tree_mod,
                         factor(dem_win) ~ region, 
                         data = house_ch13)
```

See how when using **tidymodels**, this is exactly the same as how we would fit a logistic regression, but with our model specification saved in `tree_mod` rather than the model specification we saved in `logistic_fit`.

What was the result of our tree?

```{r}
house_region_tree
```

It's not especially helpful to look at the results of a tree as text.  In order to visualize the tree, we'll use the `prp()` function in the **rpart.plot** package.  Remember that the model object is stored in `house_region_tree$fit`.

```{r, message = FALSE}
library(rpart.plot)

house_region_tree$fit %>%
  prp(extra = 6, varlen = 0, faclen = 0)
```

The arguments `varlen = 0` and `faclen = 0` ensure that the full variable names and factor levels are printed.  The argument `extra = 6` shows the proportion of "yes" outcomes within a given partition. Since we'll be using these same arguments throughout the chapter, we'll create a new function that calls `prp()` but with these options as defaults:

```{r}
prp_ch13 <- function(x, ...) prp(x, extra = 6, varlen = 0, faclen = 0, ...)

house_region_tree$fit %>%
  prp_ch13()
```

How do we interpret this tree?  Here we have two partitions, based on whether an observation is in the Midwest or South or not in the Midwest or South (i.e., in the Northeast or West).  If an observation is in the Midwest or South, the tree classifies the observation as a 0, a Democratic loss. The "0.49" means that 49% of the observations in this node were Democratic wins. If an observation is not in the Midwest or South, the algorithm classifies the observation as a 1, a Democratic win; 60% of the observations in this node were Democratic wins.

As you can see, the algorithm is very simple when you have one categorical explanatory variable: it just classifies every observation based on the most common response per category.  Take a look at the following table:

```{r, echo = FALSE}
house_ch13 %>%
  group_by(region) %>%
  summarize(dem_win = mean(dem_win) * 100) %>%
  kable(digits = 1,
        col.names = c("Region", "Democratic Win Percentage"))
```

The two partitions just divide the observations by region into a) those regions where Democrats won a majority of the elections in the data and b) those where they lost a majority of the elections.

### One numerical explanatory variable

Once we have created our model specification `tree_mod`, it is easy to use it to fit new models with different formulae and data.  We can use the same approach to create a classification tree predicting `dem_win` with `year`:

```{r}
house_year_tree <- fit(tree_mod,
                       factor(dem_win) ~ year,
                       data = house_ch13)

house_year_tree$fit %>%
  prp_ch13()
```

Now, the algorithm creates cutpoints in the `year` variable in order to classify observations.  This tree will classify observations before 1993 (the 1992 election cycle or earlier, since House elections only occur in even-numbered years) or between 2005 and 2009 (the 2006 and 2008 cycles) as Democratic victories and all other observations as Democratic losses.

Eagle-eyed observers will notice that this is simply classifying every year based on whether the Democrats won a majority in that year, with one exception --- observations in 2018 are predicted to be Democratic losses.  Why is that?  Recall that the algorithm uses a *complexity parameter*, $c_p$, to avoid overfitting.  The default value of $c_p$ in `rpart()` (the engine we are using) is 0.01; setting it to 0 allows one to see the maximum number of partitions that the algorithm will do.

How can we change $c_p$?  We don't need to create an entirely new model specification.  Rather, we can use the `update()` function to change $c_p$ while leaving everything else about `tree_mod` the same.  All we need to do is specify the parameter we want to change, which here is called `cost_complexity`:

```{r}
house_year_tree_0 <- fit(update(tree_mod, cost_complexity = 0),
                         factor(dem_win) ~ year,
                         data = house_ch13)
```

We can then look at the new tree and see what changed:

```{r}
house_year_tree_0$fit %>%
  prp_ch13()
```

Now there is an additional partition that classifies observations where `year` $\geq 2017$ as 1.

<!-- AR: leaving this out because prune() uses cp, not cost_complexity, which seems needlessly confusing. -->

<!-- Note that if we already have a tree and want to apply a higher $c_p$ value, we can use the `prune` function. We call this _pruning_ a tree because we are snipping off partitions that do not meet a `cp` criterion. We  created a tree that used `cost_complexity = 0` and saved it to `house_year_cart_0`. We can prune it like this: -->

<!-- ```{r} -->
<!-- pruned_cart <- house_year_tree_0$fit %>% -->
<!--   prune(cp = 0.01) -->

<!-- pruned_cart %>% -->
<!--   prp_ch13() -->
<!-- ``` -->

<!-- which gives us back our original tree. -->

<!-- AR: Leaving the following discussion as a comment because caret::train() is
much, much faster than tune_grid().  As a practical matter, I'd actually use
train() rather than tidymodels here. -->

<!-- But which tree is better?  We can answer that question using the `train()` function in the **caret** package. The basic idea is that we want to test out various values of `cp` and select the model that performs best.  The `train()` function decides which model is best by calculating the accuracy on 25 bootstrapped samples of our data. -->

<!-- In order to tell `train` how to behave, we need to give it some arguments: -->

<!-- 1. The formula for our model.  (Here, `factor(dem_win) ~ year`.) -->
<!-- 1. The `method` we want to use. (Here, `method = "rpart"`.) -->
<!-- 1. The values of `cp` we want to test, using the argument `tuneGrid`. To test values from 0.0 to 0.1, going up by increments of 0.005, we would use the argument `tuneGrid = data.frame(cp = seq(0.0, 0.1, by = 0.005))`. -->
<!-- 1. Our data (`data = house_ch13`). -->

But which tree is better?  We can answer that question using the `tune_grid()` function in the **tune** package, included as part of **tidymodels**. The basic idea is that we want to test out various values of $c_p$. and select the model that performs best.  We will provide code on how to decide which model is best by calculating the accuracy on 25 bootstrapped samples of our data.  (We wouldn't want to calculate the accuracy on our full data, because a lower value of $c_p$ will always perform best on any particular dataset, but the resulting model may not perform well on out-of-sample observations; this is an example of a general phenomenon called *overfitting*.)

First, we need to update the model specification to note that we are tuning the `cost_complexity` parameter.  We can do that using the `update()` function and setting `cost_complexity = tune()`:

```{r}
tree_mod_tune <- tree_mod %>%
  update(cost_complexity = tune())
```

Next, we need to create our bootstrapped data.  We can do that using **tidymodels** with the `bootstraps()` function in the **rsample** package.  We will first call `set.seed()` in order to make sure our results are replicable, given the random nature of the bootstrap process.

```{r}
set.seed(12345)
house_boots <- bootstraps(house_ch13)
```

By default, the `bootstraps()` function generates 25 samples.  You can modify this with the `times` argument.

Next, we must set the values of `cost_complexity` that we will test.  We can select random values using the `grid_random()` function; by default, it selects 5, although you can adjust that with the `size` option.  We'll save the result in `cost_grid`.

```{r}
cost_grid <- grid_random(cost_complexity())
cost_grid
```

Now we have the elements necessary to run `tune_grid`.  The key arguments are our formula (here, `factor(dem_win) ~ year`), the `model` (`tree_mod_tune`), the `resamples` (`house_boots`), and the grid (`cost_grid`).  We'll save the result as `tree_res`:

```{r}
tree_res <- tune_grid(factor(dem_win) ~ year,
                      model = tree_mod_tune,
                      resamples = house_boots,
                      grid = cost_grid)
```

Now we need to know what value of `cost_complexity` performed the best.  For this, we use the function `select_best()`. We need to supply an argument called `metric` to `select_best()`; for this example, we'll choose `"accuracy"`, which is just the percentage of observations correctly classified.

```{r}
best_cost <- select_best(tree_res, metric = "accuracy")
best_cost
```

We can see from the output that `r best_cost` performed best on our bootstrap samples. We can visualize these results using `autoplot()`:

```{r}
autoplot(tree_res, metric = "accuracy")
```

All we have left to do is to `update()` our model specification based on `best_cost` and fit the results:

```{r}
house_year_tree_best <- fit(update(tree_mod, cost_complexity = best_cost),
                       factor(dem_win) ~ year,
                       data = house_ch13)
```

Let's look at the tree, saved in `house_year_tree_best$fit`:

```{r}
house_year_tree_best$fit %>%
  prp_ch13()
```

With the value of `cost_complexity` chosen by `tune_grid()`, we retain the partition based on `year < 2017`.

### Multiple explanatory variables

What if we wanted to predict `dem_win` based on `region` and `year`?  The process is similar to what we've seen before.  We'll fit a new model called `region_year_tree` using our model specification `tree_mod`.

```{r}
region_year_tree <- fit(tree_mod,
                        factor(dem_win) ~ region + year,
                        data = house_ch13)

region_year_tree$fit %>%
  prp_ch13()
```

This tree classifies observations in the Northeast and West as a Democratic win and observations in the Midwest and South as a Democratic win before 1993 and a Democratic loss afterward. 

In the section on logistic regression, we used `map()` to fit many models to visualize the effect of `year` by `state`.  When fitting classification trees, it is easy just to add `state` as a predictor and allow the complexity parameter to determine whether we ought to partition further states by `year`.  In fact, unlike with `lm()` and `glm()`, we can include both `region` and `state` in the model, and the algorithm will decide whether it wants to use the additional information provided by `state` or just create partitions based on `region`.  Let's take a look:

```{r}
state_year_tree <- fit(tree_mod,
                       factor(dem_win) ~ region + state + year,
                       data = house_ch13)

state_year_tree$fit %>%
  prp_ch13()
```

Unfortunately, this is very difficult to read, since `prp()` tries to plot all the state names on one line.  The [documentation][http://www.milbo.org/doc/prp.pdf] gives example code on how to wrap the factor labels across multiple lines, which we will adapt:

```{r}
split.fun <- function(x, labs, digits, varlen, faclen) {
  
  # Replace commas with spaces (needed for strwrap)
  
  labs <- gsub(",", " ", labs)
  
  for(i in 1:length(labs)) {
    
    # Split labs[i] into multiple lines
    
    labs[i] <- paste(strwrap(labs[i], width = 50), collapse = "\n")
  }
  
  labs
}
```

Now we need to apply `split.fun` to `prp()`; we'll also set `faclen = 2` in order to abbreviate the state names:

```{r}
state_year_tree$fit %>%
  prp(split.fun = split.fun,
      faclen = 2)
```

This experience shows a tradeoff when working with decision trees: as they get more complex, they become more accurate but also harder to read.

## Random forests

### What are random forests?

Random forests are a **very popular** machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. The specific steps are as follows.

<!-- AR: do we explain training set/test set before this? -->

1\. Build $B$ decision trees using the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2\. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3\. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

1\. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
2\. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

### Fitting random forests

We will demonstrate by fitting a random forest to the House elections data, predicting `dem_win` with `region`, `state`, and `year`.  We will use the `rand_forest()` function to create our model specification, setting the engine to `"randomForest"` and the mode to `"classification"`.

```{r, message=FALSE, warning=FALSE}
forest_mod <- rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification")

house_forest <- fit(forest_mod,
                    factor(dem_win) ~ region + state + year,
                    data = house_ch13)

house_forest
```

We see under "OOB estimate of error rate" that this model has an error rate of 36% (or, looking at it the other way, an accuracy of 64%). We can see how the error rate of our algorithm changes as we add trees by looking at `house_forest$fit$err.rate[, "OOB"]`.  By default, `randomForest()` (the engine we specified) grows 500 trees.

```{r}
tibble(`Error rate` = house_forest$fit$err.rate[, "OOB"],
       Trees = 1:500) %>%
  ggplot(aes(x = Trees, y = `Error rate`)) +
  geom_line() +
  theme_classic()
```

We can see that in this case, the accuracy improves as we add more trees until about 300 trees where accuracy stabilizes.

Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability---we don't get anything like the coefficients from a logistic regression or the single tree from CART.

<!-- AR: Cutting this; not sure how to do this in tidymodels -->

<!-- An approach that helps with interpretability is to examine _variable importance_. To define _variable importance_ we count how often a predictor is used in the individual trees. You can learn more about _variable importance_ in an advanced machine learning book.^[https://web.stanford.edu/~hastie/Papers/ESLII.pdf] The __caret__ package includes the function `varImp` that extracts variable importance from any model in which the calculation is implemented.  -->

<!-- ```{r} -->
<!-- caret::varImp(house_forest) -->
<!-- ``` -->

<!-- Thus, we can see that the state an observation is in plays the biggest role in our forest, followed by year and finally by region. -->

## Comparing the three approaches

```{r, warning = FALSE}

house_logistic <- fit(logistic_mod,
                      factor(dem_win) ~ state * year,
                      data = house_ch13)

logistic_preds <- predict(house_logistic, new_data = house_ch13)
tree_preds <- predict(state_year_tree, new_data = house_ch13)
forest_preds <- predict(house_forest, new_data = house_ch13)

mean(logistic_preds$.pred_class == house_ch13$dem_win)
mean(tree_preds$.pred_class == house_ch13$dem_win)
mean(forest_preds$.pred_class == house_ch13$dem_win)

# How well did they predict 2018?

house_logistic_2016 <- fit(logistic_mod,
                           factor(dem_win) ~ state * year,
                           data = house_ch13 %>% filter(year != 2018))

house_tree_2016 <- fit(tree_mod,
                       factor(dem_win) ~ state + year,
                       data = house_ch13 %>% filter(year != 2018))

house_forest_2016 <- fit(forest_mod,
                         factor(dem_win) ~ state + year,
                         data = house_ch13 %>% filter(year != 2018))

house_2018 <- house_ch13 %>% filter(year == 2018)

logistic_preds <- predict(house_logistic_2016, new_data = house_2018)
tree_preds <- predict(house_tree_2016, new_data = house_2018)
forest_preds <- predict(house_forest_2016, new_data = house_2018)

mean(logistic_preds$.pred_class == house_2018$dem_win)
mean(tree_preds$.pred_class == house_2018$dem_win)
mean(forest_preds$.pred_class == house_2018$dem_win)
```
