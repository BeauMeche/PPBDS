[
["index.html", "Preceptor’s Primer for Bayesian Data Science Cover", " Preceptor’s Primer for Bayesian Data Science David Kane January 27, 2020 Cover "],
["forward.html", "Forward", " Forward The world confronts us. Make decisions we must. "],
["warning.html", "Warning", " Warning This book is not the book you are looking for. First, the book is for students in Gov 1005: Data, a course offered in the Government Department at Harvard University. Everything about the book is designed to make the experience of those students better. Some of the material here may be useful to students outside of this class, but I don’t really care if it is. Second, the book changes all the time. It is as up-to-date as possible. Third, I am highly opinionated about what matters and what does not. It is unlikely that you share my views. "],
["license.html", "License", " License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements This work builds on the contributions of many people in the R and Open Source communities. In particular, I would like to acknowledge extensive material taken from Introduction to Data Science: Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry, ModernDive: Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim, STAT 545: Data wrangling, exploration, and analysis with R by Jenny Bryan, Intro Stat with Randomization and Simulation by David M. Diez, Christopher D. Barr and Mine Cetinkaya-Rundel, Think Bayes: Bayesian Statistics Made Simple by Allen B. Downey, R for Data Science by Garrett Grolemund and Hadley Wickham, and Broadening Your Statistical Horizons: Generalized Linear Models and Multilevel Models by Julie Legler and Paul Roback. Alboukadel Kassambara and others kindly allowed for the re-use and/or modification of their work. Thanks to contributions from Harvard students and colleagues: Nicholas Dow, Albert Rivero, Celine Vendler and Sophia Zheng. "],
["dedication.html", "Dedication", " Dedication And what is romantic, Kay — And what is love? Need we ask anyone to tell us these things? "],
["1-getting-started.html", "Chapter 1 Getting Started 1.1 What are R and RStudio? 1.2 How do I code in R? 1.3 What are R packages? 1.4 Explore your first datasets 1.5 Conclusion", " Chapter 1 Getting Started Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? We’ll introduce these concepts in the upcoming Sections 1.1-1.3. If you are already somewhat familiar with these concepts, feel free to skip to Section 1.4 where we’ll introduce our first dataset: all domestic flights departing one of the three main New York City (NYC) airports in 2013. 1.1 What are R and RStudio? Throughout this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure 1.1. FIGURE 1.1: Analogy of difference between R and RStudio. More precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. 1.1.1 Installing R and RStudio You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio. You must do this first: Download and install R by going to https://cloud.r-project.org/. If you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link. If you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of November 25, 2019 was R-3.6.1. If you are a Linux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup. You must do this second: Download and install RStudio at https://www.rstudio.com/products/rstudio/download/. Scroll down to “Installers for Supported Platforms” near the bottom of the page. Click on the download link corresponding to your computer’s operating system. 1.1.2 Using R via RStudio Recall our car analogy from earlier. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs (also called applications) you can open. We’ll always work in RStudio and not in the R application. Figure 1.2 shows what icon you should be clicking on your computer. FIGURE 1.2: Icons of R versus RStudio on your computer. After you open RStudio, you should see something similar to Figure 1.3. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.) FIGURE 1.3: RStudio interface to R. Note the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serves. 1.2 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself, “OK. Now how do I use R?”. The first thing to note is that unlike other statistical software programs like Excel, SPSS, or Minitab that provide point-and-click interfaces, R is an interpreted language. This means you have to type in commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. 1.2.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to memorize all these concepts and terminology right now, we’ll guide you so that you’ll “learn by doing.” To help you learn, we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, through deliberate practice with R and lots of repetition. Basics: Console pane: where you enter in commands. Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in R. We’ll show you how to assign values to objects and how to display the contents of objects. Data types: integers, doubles/numerics, logicals, and characters. Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either TRUE or FALSE while characters are text such as “cabbage”, “Hamilton”, “The Wire is the greatest TV show ever”, and “This ramen is delicious.” Note that characters are often denoted with the quotation marks around them. Vectors: a series of values. These are created using the c() function, where c() stands for “combine” or “concatenate.” For example, c(6, 11, 13, 31, 90, 92) creates a six element series of positive integer values . Factors: categorical data are commonly represented in R as factors. Categorical data can also be represented as strings. We’ll study this difference as we progress through the book. Data frames: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section 1.4. Modern data frames are called tibbles. Conditionals: Testing for equality in R using == (and not =, which is typically used for assignment). For example, 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). For example, 4 + 2 &gt;= 3 will return TRUE, but 3 + 5 &lt;= 1 will return FALSE. Logical operators: &amp; representing “and” as well as | representing “or.” For example, (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn’t seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don’t pass in different values for from and to to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result 2 3 4 5, as we would expect. We’ll work with functions a lot throughout this book and you’ll get lots of practice in understanding their behaviors. To further assist you in understanding when a function is mentioned in the book, we’ll also include the () after them as we did with seq() above. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is a minimal list of programming concepts and terminology you need to know before getting started. We feel that you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice. 1.2.2 Errors, warnings, and messages One thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 1.3.3 if you see Error in ggplot(...) : could not find function &quot;ggplot&quot;, it means that the ggplot() function is not accessible because the package that contains the function, ggplot2, was not loaded with library(ggplot2). You cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. If you create a scatterplot based on a dataset where two of the rows of data have missing entries, you will see this warning: Warning: Removed 2 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 1.3.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter 5. These are helpful diagnostic messages. They don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages() as discussed in Subsection 1.3.1. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather: If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine and keep on going! 1.2.3 Tips on learning to code Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. Such frustrations are common and it is normal to feel discouraged as you learn. However, just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn and improve. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone is “smart,” but really people spent a lot of time and energy designing them to appear “smart.” In reality, you have to tell a computer everything it needs to do. Furthermore, the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. We call this the “copy, paste, and tweak” approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the “copy, paste, and tweak” approach as training wheels for learning to ride a bike. After getting comfortable, you won’t need them anymore. The best way to learn to code is by doing: Rather than learning to code for its own sake, we find that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in and that is important to you. Practice is key: Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. Write R code every day. 1.3 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package (Wickham, Chang, et al. 2019) for data visualization in Chapter 2 and the dplyr package (Wickham, François, et al. 2019) for data wrangling in Chapter 4. A good analogy for R packages is they are like apps you can download onto a mobile phone: FIGURE 1.4: Analogy of R versus R packages. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app. Open the app: After you’ve installed Instagram, you need to open it. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s perform these two steps for the ggplot2 package for data visualization. 1.3.1 Package installation Let’s install the ggplot2 package. Type install.packages(&quot;ggplot2&quot;) in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package. Much like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps. 1.3.2 Package loading Recall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the console pane. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key. library(ggplot2) If after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads ... Error in library(ggplot2) : there is no package called ‘ggplot2’ ... it means that you didn’t successfully install it. This is an example of an “error message” we discussed in Subsection 1.2.2. If you get this error message, go back to Subsection 1.3.1 on R package installation and make sure to install the ggplot2 package before proceeding. 1.3.3 Package use One very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function This is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out. However, you’ll remember with practice and after some time it will become second nature for you. 1.4 Explore your first datasets Let’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format. This is probably the most common way data are collected and saved in many fields. Remember from Subsection 1.2.1 that these “spreadsheet”-type datasets are called data frames in R. We’ll focus on working with data saved as data frames throughout this book. Again, “tibble” is the more modern term for “data frame,” but we will use both interchangeably. Let’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 1.3 for information on how to install and load R packages if you haven’t already. library(nycflights13) library(dplyr) library(knitr) At the beginning of all subsequent chapters in this book, we’ll always have a list of packages that you should have installed and loaded in order to work with that chapter’s R code. 1.4.1 nycflights13 package Many of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect of many people’s lives. If you look at the Departures flight information board at an airport, you will frequently see that some flights are delayed for a variety of reasons. Are there ways that we can understand the reasons that cause flight delays? We’re going to analyze data related to all domestic flights departing from one of New York City’s three main airports in 2013: Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia Airport (LGA). We’ll access this data using the nycflights13 R package, which contains five datasets saved in five tibbles: flights: Information on all 336,776 flights. airlines: A table matching airline names and their two-letter International Air Transport Association (IATA) airline codes (also known as carrier codes) for 16 airline companies. For example, “DL” is the two-letter code for Delta. planes: Information about each of the 3,322 physical aircraft used. weather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughly corresponding to the \\(365 \\times 24 \\times 3 = 26,280\\) possible hourly measurements one can observe at three locations over the course of a year. airports: Names, codes, and locations of the 1,458 domestic destinations. 1.4.2 flights data frame We’ll begin by exploring the flights data frame and get an idea of its structure. Run the following code in your console, either by typing it or by cutting-and-pasting it. It displays the contents of the flights data frame in your console. Note that depending on the size of your monitor, the output may vary slightly. flights # A tibble: 336,776 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 7 2013 1 1 555 600 -5 913 854 8 2013 1 1 557 600 -3 709 723 9 2013 1 1 557 600 -3 838 846 10 2013 1 1 558 600 -2 753 745 # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Let’s unpack this output: A tibble: 336,776 x 19: A tibble is a specific kind of data frame in R. This particular data frame has 336,776 rows corresponding to different observations. Here, each observation is a flight. 19 columns corresponding to 19 variables describing each observation. year, month, day, dep_time, sched_dep_time, dep_delay, and arr_time are the different columns, in other words, the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 flights. R is only showing the first 10 rows, because if it showed all 336,776 rows, it would overwhelm your screen. ... with 336,766 more rows, and 11 more variables: indicating to us that 336,766 more rows of data and 11 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well, but it does give a nice preview. Let’s look at some different ways to explore data frames. 1.4.3 Exploring data frames There are many ways to get a feel for the data contained in a data frame such as flights. We present three functions that take as their “argument” (their input) the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function, which brings up RStudio’s built-in data viewer. Using the glimpse() function, which is included in the dplyr package. Using the kable() function, which is included in the knitr package. Using the $ “extraction operator,” which is used to view a single variable/column in a data frame. 1. View(): Run View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights). By running View(flights), we can explore the different variables listed in the columns. Observe that there are many different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical. Note that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the observational unit. The observational unit in this example is an individual flight departing from New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables. We’ll talk more about observational units in Subsection 1.4.4 on identification and measurement variables. 2. glimpse(): The second way we’ll cover to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package by running library(dplyr). This function provides us with an alternative perspective for exploring a data frame than the View() function: glimpse(flights) Observations: 336,776 Variables: 19 $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, … $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558,… $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600,… $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -… $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849… $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851… $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -… $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;, … $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, … $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N39… $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;… $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IAD&quot;… $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, … $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733,… $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, … $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, … $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 … Observe that glimpse() will give you the first few entries of each variable in a row after the variable name. In addition, the data type (see Subsection 1.2.1) of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. “Doubles” take up twice the size to store on a computer compared to integers. In contrast, chr refers to “character”, which is computer terminology for text data. In most forms, text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is another data type: dttm. These types of variables represent date and time combinations. However, we won’t work with dates and times in this book; we leave this topic for other data science books like Introduction to Data Science by Tiffany-Anne Timbers, Melissa Lee, and Trevor Campbell or R for Data Science (Grolemund and Wickham 2017). 3. kable(): The final way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in the console: airlines kable(airlines) At first glance, it may not appear that there is much difference in the outputs. However, when using tools for producing reproducible reports such as R Markdown, the latter code produces output that is much more legible and reader-friendly. You’ll see us use this reader-friendly style in many places in the book when we want to print a data frame as a nice table. 4. $ operator Lastly, the $ operator allows us to extract and then explore a single variable within a data frame. For example, run the following in your console airlines$name We used the $ operator to extract only the name variable and return it as a vector of length 16. We’ll only be occasionally exploring data frames using the $ operator, instead favoring the View() and glimpse() functions. 1.4.4 Identification and measurement variables There is a subtle difference between the kinds of variables that you will encounter in data frames. There are identification variables and measurement variables. For example, let’s explore the airports data frame by showing the output of glimpse(airports): glimpse(airports) Observations: 1,458 Variables: 8 $ faa &lt;chr&gt; &quot;04G&quot;, &quot;06A&quot;, &quot;06C&quot;, &quot;06N&quot;, &quot;09J&quot;, &quot;0A9&quot;, &quot;0G6&quot;, &quot;0G7&quot;, &quot;0P2&quot;, … $ name &lt;chr&gt; &quot;Lansdowne Airport&quot;, &quot;Moton Field Municipal Airport&quot;, &quot;Schaumbu… $ lat &lt;dbl&gt; 41.1, 32.5, 42.0, 41.4, 31.1, 36.4, 41.5, 42.9, 39.8, 48.1, 39.… $ lon &lt;dbl&gt; -80.6, -85.7, -88.1, -74.4, -81.4, -82.2, -84.5, -76.8, -76.6, … $ alt &lt;dbl&gt; 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409, 875, 1… $ tz &lt;dbl&gt; -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, -5, -5,… $ dst &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;, &quot;A&quot;, &quot;U&quot;, &quot;A&quot;… $ tzone &lt;chr&gt; &quot;America/New_York&quot;, &quot;America/Chicago&quot;, &quot;America/Chicago&quot;, &quot;Amer… The variables faa and name are what we will call identification variables, variables that uniquely identify each observational unit. In this case, the identification variables uniquely identify airports. Such variables are mainly used in practice to uniquely identify each row in a data frame. faa gives the unique code provided by the FAA for that airport, while the name variable gives the longer official name of the airport. The remaining variables (lat, lon, alt, tz, dst, tzone) are often called measurement or characteristic variables: variables that describe properties of each observational unit. For example, lat and long describe the latitude and longitude of each airport. Furthermore, sometimes a single variable might not be enough to uniquely identify each observational unit: combinations of variables might be needed. While it is not an absolute rule, for organizational purposes it is considered good practice to have your identification variables in the leftmost columns of your data frame. 1.4.5 Help files Another nice feature of R are help files, which provide documentation for various functions and datasets. You can bring up help files by adding a ? before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation if it exists. For example, let’s look at the help file for the flights data frame. ?flights The help file should pop up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away. 1.5 Conclusion We’ve given you what we feel is a minimally viable set of tools to explore data in R. Does this chapter contain everything you need to know? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful! As we said earlier, the best way to add to your toolbox is to get into RStudio and run and write code as much as possible. 1.5.1 Additional resources If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out the short book, Getting Used to R, RStudio, and R Markdown (Ismay and Kennedy 2016). It includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R. FIGURE 1.5: Preview of Getting Used to R, RStudio, and R Markdown. References "],
["2-viz.html", "Chapter 2 Visualization 2.1 The grammar of graphics 2.2 Scatterplots 2.3 Linegraphs 2.4 Histograms 2.5 Facets 2.6 Boxplots 2.7 Barplots 2.8 Conclusion", " Chapter 2 Visualization We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn’t initially obtain from just looking at the raw data values. We’ll use the ggplot2 package, as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as the grammar of graphics (Wilkinson 2005), developed by Leland Wilkinson. At their most basic, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way to explore the patterns in data, such as the presence of outliers, distributions of individual variables, and relationships between groups of variables. Graphics are designed to emphasize the findings and insights you want your audience to understand. This does, however, require a balancing act. On the one hand, you want to highlight as many interesting findings as possible. On the other hand, you don’t want to include so much information that it overwhelms your audience. As we will see, plots also help us to identify patterns and outliers in our data. We’ll see that a common extension of these ideas is to compare the distribution of one numerical variable, such as what are the center and spread of the values, as we go across the levels of a different categorical variable. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 1.3 for information on how to install and load R packages. library(nycflights13) library(ggplot2) library(dplyr) 2.1 The grammar of graphics We start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. Think of how we construct and form sentences in English by combining different elements, like nouns, verbs, articles, subjects, objects, etc. We can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “the grammar of graphics” defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau. 2.1.1 Components of the grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the dataset containing the variables of interest. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example. 2.1.2 Gapminder data In February 2006, a Swedish physician and data advocate named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for data on 142 countries in 2007, let’s consider only a few countries in Table 2.1 as a peak into the data. TABLE 2.1: Gapminder 2007 Data: First 3 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. Note that “Americas” includes countries in both North and South America and that Antarctica is excluded. Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 2.1, which plots this for all 142 of the data’s countries. FIGURE 2.1: Life expectancy over GDP per capita in 2007. Let’s view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. We’ll see shortly that data corresponds to the particular data frame where our data is saved and that “data variables” correspond to particular columns in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. We can also use lines, bars, and other geometric objects. Let’s summarize the three essential components of the grammar in Table 2.2. TABLE 2.2: Summary of the grammar of graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 2.1.3 Other components There are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. faceting breaks up a plot into several plots split by the values of another variable (Section 2.5) position adjustments for barplots (Section 2.7) scales that both convert data units to physical units the computer can display. For example, apply a log-transformation on one of the axes to focus on multiplicative rather than additive changes. draw a legend and/or axes, which provide an inverse mapping to make it possible to read the original data values from the graph. coordinate system for x/y values: typically cartesian, but can also be map or polar. statistical transformations: this includes smoothing, binning values into a histogram, or no transformation at all (known as the &quot;identity&quot; transformation). Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 2.1.4 ggplot2 package In this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R (Wickham, Chang, et al. 2019). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum: The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 2.5). Let’s now put the theory of the grammar of graphics into practice. 2.2 Scatterplots Scatterplots, also called bivariate plots, allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics we presented in Section 2.1. Specifically, we will visualize the relationship between the following two numerical variables in the flights data frame included in the nycflights13 package: dep_delay: departure delay on the horizontal “x” axis and arr_delay: arrival delay on the vertical “y” axis for Alaska Airlines flights leaving NYC in 2013. This requires paring down the data from all 336,776 flights that left NYC in 2013, to only the 714 Alaska Airlines flights that left NYC in 2013. We do this so our scatterplot will involve a manageable 714 points, and not an overwhelmingly large number like 336,776. To achieve this, we’ll take the flights data frame, filter the rows so that only the 714 rows corresponding to Alaska Airlines flights are kept, and save this in a new data frame called alaska_flights using the &lt;- assignment operator: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) For now, we suggest you don’t worry if you don’t fully understand this code. We’ll see later in Chapter 4 on data wrangling that this code uses the dplyr package for data wrangling to achieve our goal: it takes the flights data frame and filters it to only return the rows where carrier is equal to &quot;AS&quot;, Alaska Airlines’ carrier code. Recall from Section 1.2 that testing for equality is specified with == and not =. Convince yourself that this code achieves what it is supposed to by exploring the resulting data frame by running View(alaska_flights). You’ll see that it has 714 rows, consisting of only 714 Alaska Airlines flights. 2.2.1 Scatterplots via geom_point Let’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced in Section 2.1. Let’s take a look at the code and break it down piece-by-piece. ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() Within the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs): The data as the alaska_flights data frame via data = alaska_flights. The aesthetic mapping by setting mapping = aes(x = dep_delay, y = arr_delay). Specifically, the variable dep_delay maps to the x position aesthetic, while the variable arr_delay maps to the y position. We then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and the graphic shown in Figure 2.2. Warning: Removed 5 rows containing missing values (geom_point). FIGURE 2.2: Arrival delays versus departure delays for Alaska Airlines flights from NYC in 2013. Let’s first unpack the graphic in Figure 2.2. Observe that a positive relationship exists between dep_delay and arr_delay: as departure delays increase, arrival delays tend to also increase. Observe also the large mass of points clustered near (0, 0), the point indicating flights that neither departed nor arrived late. Let’s turn our attention to the warning message. R is alerting us to the fact that five rows were ignored due to them being missing. For these 5 rows, either the value for dep_delay or arr_delay or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Before we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code. To stress the importance of adding the layer specifying the geometric object, consider Figure 2.3 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful! ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) FIGURE 2.3: A plot with no layers. 2.2.2 Overplotting The large mass of points near (0, 0) in Figure 2.2 can cause some confusion since it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by Adjusting the transparency of the points or Adding a little random “jitter”, or random “nudges”, to each of the points. Method 1: Changing the transparency The first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1. Note how the following code is identical to the code in Section 2.2 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point() function: ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point(alpha = 0.2) FIGURE 2.4: Arrival vs. departure delays scatterplot with alpha = 0.2. The key feature to note in Figure 2.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.2)). Method 2: Jittering the points The second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 2.5, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right). FIGURE 2.5: Regular and jittered scatterplot. In the left-hand regular scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right-hand jittered scatterplot, it is now plainly evident that this plot involves four points since each point is given a random “nudge.” Keep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. To create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created the scatterplot with overplotting in Subsection 2.2.1, but with geom_point() replaced with geom_jitter(). ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_jitter(width = 30, height = 30) FIGURE 2.6: Arrival versus departure delays jittered scatterplot. In order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in minutes. How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points. As can be seen in the resulting Figure 2.6, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make. 2.2.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots. 2.3 Linegraphs Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Let’s illustrate linegraphs using another dataset in the nycflights13 package: the weather data frame. Let’s explore the weather data frame by running View(weather) and glimpse(weather). Furthermore let’s read the associated help file by running ?weather to bring up the help file. Observe that there is a variable called temp of hourly temperature recordings in Fahrenheit at weather stations near all three major airports in New York City: Newark (origin code EWR), John F. Kennedy International (JFK), and LaGuardia (LGA). However, instead of considering hourly temperatures for all days in 2013 for all three airports, for simplicity let’s only consider hourly temperatures at Newark airport for the first 15 days in January. Recall in Section 2.2, we used the filter() function to only choose the subset of rows of flights corresponding to Alaska Airlines flights. We similarly use filter() here, but by using the &amp; operator we only choose the subset of rows of weather where the origin is &quot;EWR&quot;, the month is January, and the day is between 1 and 15. Recall we performed a similar task in Section 2.2 when creating the alaska_flights data frame of only Alaska Airlines flights, a topic we’ll explore more in Chapter 4 on data wrangling. early_january_weather &lt;- weather %&gt;% filter(origin == &quot;EWR&quot; &amp; month == 1 &amp; day &lt;= 15) 2.3.1 Linegraphs via geom_line Let’s create a time series plot of the hourly temperatures saved in the early_january_weather data frame by using geom_line() to create a linegraph, instead of using geom_point() like we used previously to create scatterplots: ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() FIGURE 2.7: Hourly temperature in Newark for January 1-15, 2013. Much as with the ggplot() code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2, let’s break down this code piece-by-piece in terms of the grammar of graphics: Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments: The data to be the early_january_weather data frame by setting data = early_january_weather. The aesthetic mapping by setting mapping = aes(x = time_hour, y = temp). Specifically, the variable time_hour maps to the x position aesthetic, while the variable temp maps to the y position aesthetic. We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case, the geometric object is a line set by specifying geom_line(). 2.3.2 Summary Linegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time. 2.4 Histograms Let’s consider the temp variable in the weather data frame once again, but unlike with the linegraphs in Section 2.3, let’s say we don’t care about its relationship with time, but rather we only care about how the values of temp distribute. In other words: What are the smallest and largest values? What is the “center” or “most typical” value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable temp is to plot them on a horizontal line as we do in Figure 2.8: FIGURE 2.8: Plot of hourly temperature recordings from NYC in 2013. This gives us a general idea of how the values of temp distribute: observe that temperatures vary from around 11°F (-11°C) up to 100°F (38°C). Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between say 50°F and 55°F. What is commonly produced instead of Figure 2.8 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Let’s drill-down on an example of a histogram, shown in Figure 2.9. FIGURE 2.9: Example histogram. Let’s focus only on temperatures between 30°F (-1°C) and 60°F (15°C) for now. Observe that there are three bins of equal width between 30°F and 60°F. Thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since: The bin for the 30-40°F range has a height of around 5000. In other words, around 5000 of the hourly temperature recordings are between 30°F and 40°F. The bin for the 40-50°F range has a height of around 4300. In other words, around 4300 of the hourly temperature recordings are between 40°F and 50°F. The bin for the 50-60°F range has a height of around 3500. In other words, around 3500 of the hourly temperature recordings are between 50°F and 60°F. All nine bins spanning 10°F to 100°F on the x-axis have this interpretation. 2.4.1 Histograms via geom_histogram Let’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 2.10 as well as warning messages. We’ll discuss the warning messages first. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Warning: Removed 1 rows containing non-finite values (stat_bin). FIGURE 2.10: Histogram of hourly temperatures at three NYC airports. The first message is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change the number of bins to another value than the default. The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2: that because one row has a missing NA value for temp, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Now let’s unpack the resulting histogram in Figure 2.10. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = &quot;white&quot; argument to geom_histogram() and ignore the warning about setting the number of bins to a better value: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;) FIGURE 2.11: Histogram of hourly temperatures at three NYC airports with white borders. We now have an easier time associating ranges of temperatures to each of the bins in Figure 2.11. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = &quot;steelblue&quot;: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) If you’re curious, run colors() to see all 657 possible choice of colors in R! 2.4.2 Adjusting the bins Observe in Figure 2.11 that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or 3.125°F, which is not a very easily interpretable range to work with. Let’s improve this by adjusting the number of bins in our histogram in one of two ways: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(bins = 40, color = &quot;white&quot;) Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) We compare both resulting histograms side-by-side in Figure 2.12. FIGURE 2.12: Setting histogram bins in two ways. 2.4.3 Summary Histograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 2.5 Facets Let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. For example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Figure 2.9 differed in each month. We could “split” this histogram by the 12 possible months in a given year. In other words, we would plot histograms of temp for each month separately. We do this by adding facet_wrap(~ month) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'month' not found if you don’t include it here. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month) FIGURE 2.13: Faceted histogram of hourly temperatures by month. We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an nrow = 4 argument to facet_wrap(~ month) ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month, nrow = 4) FIGURE 2.14: Faceted histogram with 4 instead of 3 rows. Observe in both Figures 2.13 and 2.14 that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter. 2.6 Boxplots While faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves this same goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable (see Appendix 7.5). To keep things simple for now, let’s only consider the 2141 hourly temperature recordings for the month of November, each represented as a jittered point in Figure 2.15. FIGURE 2.15: November temperatures represented as jittered points. These 2141 observations have the following five-number summary: Minimum: 21°F First quartile (25th percentile): 36°F Median (second quartile, 50th percentile): 45°F Third quartile (75th percentile): 52°F Maximum: 71°F In the leftmost plot of Figure 2.16, let’s mark these 5 values with dashed horizontal lines on top of the 2141 points. In the middle plot of Figure 2.16 let’s add the boxplot. In the rightmost plot of Figure 2.16, let’s remove the points and the dashed horizontal lines for clarity’s sake. FIGURE 2.16: Building up a boxplot of November temperatures. What the boxplot does is visually summarize the 2141 points by cutting the 2141 temperature recordings into quartiles at the dashed lines, where each quartile contains roughly 2141 \\(\\div\\) 4 \\(\\approx\\) 535 observations. Thus 25% of points fall below the bottom edge of the box, which is the first quartile of 36°F. In other words, 25% of observations were below 36°F. 25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 45°F. Thus, 25% of observations were between 36°F and 45°F and 50% of observations were below 45°F. 25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 52°F. It follows that 25% of observations were between 45°F and 52°F and 75% of observations were below 52°F. 25% of points fall above the top edge of the box. In other words, 25% of observations were above 52°F. The middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 52 - 36 = 16°F. The interquartile range is a measure of a numerical variable’s spread. Furthermore, in the rightmost plot of Figure 2.16, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21°F and 71°F, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case of the November temperatures, no more than 1.5 \\(\\times\\) 16°F = 24°F from either end of the box. Any observed values outside this range get marked with points called outliers, which we’ll see in the next section. 2.6.1 Boxplots via geom_boxplot Let’s now create a side-by-side boxplot of hourly temperatures split by the 12 months as we did previously with the faceted histograms. We do this by mapping the month variable to the x-position aesthetic, the temp variable to the y-position aesthetic, and by adding a geom_boxplot() layer: ggplot(data = weather, mapping = aes(x = month, y = temp)) + geom_boxplot() FIGURE 2.17: Invalid boxplot specification. Warning messages: 1: Continuous x aesthetic -- did you forget aes(group=...)? 2: Removed 1 rows containing non-finite values (stat_boxplot). Observe in Figure 2.17 that this plot does not provide information about temperature separated by month. The first warning message clues us in as to why. It is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as NA missing. We can convert the numerical variable month into a factor categorical variable by using the factor() function. So after applying factor(month), month goes from having numerical values just the 1, 2, …, and 12 to having an associated ordering. With this ordering, ggplot() now knows how to work with this variable to produce the needed plot. ggplot(data = weather, mapping = aes(x = factor(month), y = temp)) + geom_boxplot() FIGURE 2.18: Side-by-side boxplot of temperature split by month. The resulting Figure 2.18 shows 12 separate “box and whiskers” plots similar to the rightmost plot of Figure 2.16 of only November temperatures. Thus the different boxplots are shown “side-by-side.” The “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile. The height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability. The “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability. The dots representing values falling outside the whiskers are called outliers. These can be thought of as anomalous (“out-of-the-ordinary”) values. It is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month. 2.6.2 Summary Side-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes. To study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points. 2.7 Barplots Both histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts). One complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually… # A tibble: 5 x 1 fruit &lt;chr&gt; 1 apple 2 apple 3 orange 4 apple 5 orange … fruits_counted has a variable count which represent the “pre-counted” values of each fruit. # A tibble: 2 x 2 fruit number &lt;chr&gt; &lt;dbl&gt; 1 apple 3 2 orange 2 Depending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore. 2.7.1 Barplots via geom_bar or geom_col Let’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer: ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() FIGURE 2.19: Barplot when counts are not pre-counted. However, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() FIGURE 2.20: Barplot when counts are pre-counted. Compare the barplots in Figures 2.19 and 2.20. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize Is not pre-counted in your data frame, we use geom_bar(). Is pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Let’s now go back to the flights data frame in the nycflights13 package and visualize the distribution of the categorical variable carrier. In other words, let’s visualize the number of domestic flights out of New York City each airline company flew in 2013. Recall from Subsection 1.4.3 when you first explored the flights data frame, you saw that each row corresponds to a flight. In other words, the flights data frame is more like the fruits data frame than the fruits_counted data frame because the flights have not been pre-counted by carrier. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable carrier gets mapped to the x-position. As a difference though, histograms have bars that touch whereas bar graphs have white space between the bars going from left to right. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() FIGURE 2.21: Number of flights departing NYC in 2013 by airline using geom_bar(). Observe in Figure 2.21 that United Airlines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart NYC in 2013. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each carrier was pre-counted as in Table 2.3. TABLE 2.3: Number of flights pre-counted for each carrier carrier number 9E 18460 AA 32729 AS 714 B6 54635 DL 48110 EV 54173 F9 685 FL 3260 HA 342 MQ 26397 OO 32 UA 58665 US 20536 VX 5162 WN 12275 YV 601 In order to create a barplot visualizing the distribution of the categorical variable carrier in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = carrier. The resulting barplot would be identical to Figure 2.21. 2.7.2 Must avoid pie charts! One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another. Let’s examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure 2.21, but this time we will use a pie chart in Figure 2.22. Try to answer the following questions: How much larger is the portion of the pie for ExpressJet Airlines (EV) compared to US Airways (US)? What is the third largest carrier in terms of departing flights? How many carriers have fewer flights than United Airlines (UA)? FIGURE 2.22: The dreaded pie chart. While it is quite difficult to answer these questions when looking at the pie chart in Figure 2.22, we can much more easily answer these questions using the barchart in Figure 2.21. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons must be made by comparing angles. 2.7.3 Two categorical variables Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of outgoing domestic flights from NYC by carrier as well as origin. In other words, the number of flights for each carrier and origin combination. For example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure 2.21: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() We can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar() FIGURE 2.23: Stacked barplot of flight amount by carrier and origin. Figure 2.23 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each origin airport between the carriers. Before we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Subsection 2.4.1: we set the outline of the bars to white by setting color = &quot;white&quot; and the colors of the bars to blue steel by setting fill = &quot;steelblue&quot;. Observe in Figure 2.24 that mapping origin to color and not fill yields grey bars with different colored outlines. ggplot(data = flights, mapping = aes(x = carrier, color = origin)) + geom_bar() FIGURE 2.24: Stacked barplot with color aesthetic used instead of fill. Second, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make: ggplot(data = flights, mapping = aes(x = carrier), fill = origin) + geom_bar() An alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 2.25. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = &quot;dodge&quot; argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar(position = &quot;dodge&quot;) FIGURE 2.25: Side-by-side barplot comparing number of flights by carrier and origin. Note the width of the bars for AS, F9, FL, HA and YV is different than the others. We can make one tweak to the position argument to get them to be the same size in terms of width as the other bars by using the more robust position_dodge() function. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar(position = position_dodge(preserve = &quot;single&quot;)) FIGURE 2.26: Side-by-side barplot comparing number of flights by carrier and origin (with formatting tweak). Lastly, another type of barplot is a faceted barplot. Recall in Section 2.5 we visualized the distribution of hourly temperatures at the 3 NYC airports split by month using facets. We apply the same principle to our barplot visualizing the frequency of carrier split by origin: instead of mapping origin to fill we include it as the variable to create small multiples of the plot across the levels of origin. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() + facet_wrap(~ origin, ncol = 1) FIGURE 2.27: Faceted barplot comparing the number of flights by carrier and origin. 2.7.4 Summary Barplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice. 2.8 Conclusion 2.8.1 Summary table Table 2.4 summarizing the differences among these graphs. Using them, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package. TABLE 2.4: Summary of Five Named Graphs Named graph Shows Geometric object Notes 1 Scatterplot Relationship between 2 numerical variables geom_point() 2 Linegraph Relationship between 2 numerical variables geom_line() Used when there is a sequential order to x-variable, e.g., time 3 Histogram Distribution of 1 numerical variable geom_histogram() Facetted histograms show the distribution of 1 numerical variable split by the values of another variable 4 Boxplot Distribution of 1 numerical variable split by the values of another variable geom_boxplot() 5 Barplot Distribution of 1 categorical variable geom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted Stacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables 2.8.2 Function argument specification Let’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code: # Segment 1: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() # Segment 2: ggplot(flights, aes(x = carrier)) + geom_bar() You’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() code will be like the second segment: with the data = and mapping = explicit naming of the argument omitted with the default ordering of arguments respected. We’ll do this for brevity’s sake; it’s common to see this style when reviewing other R users’ code. 2.8.3 Additional resources If you want to further unlock the power of the ggplot2 package for data visualization, we suggest that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter. In particular, it presents many more than the 5 geometric objects we covered in this chapter while providing quick and easy to read visual descriptions. For all the geometric objects, it also lists all the possible aesthetic attributes one can tweak. In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Visualization with ggplot2.” You can see a preview in the figure below. FIGURE 2.28: Data Visualization with ggplot2 cheatsheet. 2.8.4 What’s to come Recall in Figure 2.2 in Section 2.2 we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights. This necessitated paring down the flights data frame to a new data frame alaska_flights consisting of only carrier == AS flights first: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() Furthermore recall in Figure 2.7 in Section 2.3 we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This necessitated paring down the weather data frame to a new data frame early_january_weather consisting of hourly temperature recordings only for origin == &quot;EWR&quot;, month == 1, and day less than or equal to 15 first: early_january_weather &lt;- weather %&gt;% filter(origin == &quot;EWR&quot; &amp; month == 1 &amp; day &lt;= 15) ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() These two code segments were a preview of Chapter 4 on data wrangling using the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, these two code segments used the filter() function to create new data frames (alaska_flights and early_january_weather) by choosing only a subset of rows of existing data frames (flights and weather). In the next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %&gt;% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 4 on data wrangling! References "],
["3-productivity.html", "Chapter 3 Productivity 3.1 Set Up 3.2 Organizing with Unix 3.3 Unix commands 3.4 Git and GitHub 3.5 R", " Chapter 3 Productivity This chapter is meant to be skimmed. Only read those portions which apply to your computer set up and which you do not already understand. Do not use point-and-click approaches for data analysis. Use scripting languages, such as R, since they are more flexible and facilitate reproducibility. Similarly, do not use point-and-click approaches to organizing files and document preparation. We will learn freely available tools that the vast majority of professionals use every day. Three general guiding principles that motivate what we learn here are 1) be systematic when organizing your filesystem, 2) automate when possible, and 3) minimize the use of the mouse. As you become more proficient at coding, you will find that 1) you want to minimize the time you spend remembering what you called a file or where you put it, 2) if you find yourself repeating the same task over and over, there is probably a way to automate, and 3) anytime your fingers leave the keyboard, it results in loss of productivity. A data analysis project is not always a dataset and a script. A typical data analysis challenge may involve several parts, each involving several data files, including files containing the scripts we use to analyze data. Keeping all this organized can be challenging. We will learn to use the Unix shell for managing files and directories on your computer. Using Unix will permit you to use the keyboard, rather than the mouse, when creating folders, moving from directory to directory, and renaming, deleting, or moving files. We also provide specific suggestions on how to keep the filesystem organized. The data analysis process is also iterative and adaptive. As a result, we are constantly editing our scripts and reports. In this chapter, we introduce you to the version control system Git, which is a powerful tool for keeping track of these changes. We also introduce you to GitHub1, a service that permits you to host and share your code. We will demonstrate how you can use this service to facilitate collaborations. Keep in mind that another positive benefit of using GitHub is that you can easily showcase your work to potential employers. Finally, we learn to write reports in R markdown, which permits you to incorporate text and code into a single document. We will demonstrate how, using the knitr package, we can write reproducible and aesthetically pleasing reports by running the analysis and generating the report simultaneously. We will put all this together using the powerful integrated desktop environment RStudio2. 3.1 Set Up Before getting started, we need to make sure you have access to a terminal and that Git is installed. The terminal is integrated into Mac and Linux systems, but Windows users will have to install an emulator. There are many emulator options available, but here we show how to install Git Bash because it can be done as part of the Windows Git installation. Because of the differences in Mac and Windows, the sections in this chapter are divided accordingly. 3.1.1 Accessing the terminal on a Mac The terminal is our window into the Unix world. On a Mac you can access a terminal by opening the application in the Utilities folder: You can also use the Spotlight feature on the Mac by typing command-spacebar, then type Terminal. Yet another way to access the terminal is from RStudio. In the Console pane you should see a Terminal tab. If you click on this tab you will open a terminal window. 3.1.2 Installing Git on the Mac Start by opening a terminal as described in the previous section. Once you start the terminal, you will see a console like this: You might have Git installed already. One way to check is by asking for the version by typing: git --version If you get a version number back, it is already installed. If not, you will get the following message: and you will be asked if you want to install it. You should click Install: This will take you through the installation process: Once installed, you can check for the version again and it should show you something like this: Congratulations. You have installed Git on your Mac. 3.1.3 Installing Git and Git Bash on Windows There are several pieces of software that will permit you to perform Unix commands on Windows. We will be using Git Bash as it interfaces with RStudio and it is automatically installed when we install Git for Windows. Start by searching for Git for Windows on your browser and clicking on the link from git-scm.com. This will take you to the Download Git page from which you can download the most recent maintained build: You can then accept to run the installer and agree to the license: In one of the installation steps, you will be asked to pick the default editor for Git. Unless you are already a vi or vim user, we recommend against selecting vim which might be the default. If you do not recognize an editor you are familiar with among the options given, we recommend that you select nano as your default editor for Git since it is the easiest to learn: The next installation decision is actually an important one. This installation process installs Git Bash. We recommend that you select Git and optional Unix tools from the Windows Command Prompt as this will permit you to learn Unix from within RStudio. However, if you do this, some commands that run on your Windows command line will stop working. If you do not use your Windows command line, then this should not be a problem. Also, most, if not all, of these Windows command lines have a Unix equivalent that you will be able to use now. You can now continue selecting the default options. You have now installed Git on Windows. 3.1.4 Accessing the terminal on Windows Now that Git Bash is installed, we can access the terminal either through RStudio or by opening Git Bash directly. We recommend that Windows users always using the terminal via RStudio. To access the terminal through RStudio, we need to change a preference so that Git Bash becomes the default Unix shell in RStudio. In RStudio, go to preferences (under the File pull down menu), then select Terminal, then select Git Bash: To check that you in fact are using Git Bash in RStudio, you can open a New Terminal in RStudio: It should look something like this: 3.2 Organizing with Unix Unix is the operating system of choice in data science. We will introduce you to the Unix way of thinking using an example: how to keep a data analysis project organized. We will learn some of the most commonly used commands along the way. When searching for Unix resources, keep in mind that other terms used to describe what we will learn here are Linux, the shell and the command line. Basically, what we are learning is a series of commands and a way of thinking that facilitates the organization of files without using the mouse. To serve as motivation, we are going to start constructing a directory using Unix tools and RStudio. 3.2.1 Naming convention Before you start organizing projects with Unix you want to pick a name convention that you will use to systematically name your files and directories. This will help you find files and know what is in them. In general you want to name your files in a way that is related to their contents and specifies how they relate to other files. The Smithsonian Data Management Best Practices3 has “five precepts of file naming and organization” and they are: Have a distinctive, human-readable name that gives an indication of the content. Follow a consistent pattern that is machine-friendly. Organize files into directories (when necessary) that follow a consistent pattern. Avoid repetition of semantic elements among file and directory names. Have a file extension that matches the file format (no changing extensions!) Follow The Tidyverse Style Guide4. 3.2.2 The terminal Instead of clicking, dragging, and dropping to organize our files and folders, we will be typing Unix commands into the terminal. The way we do this is similar to how we type commands into the R console, but instead of generating plots and statistical summaries, we will be organizing files on our system. Once you have a terminal open, you can start typing commands. You should see a blinking cursor at the spot where what you type will show up. This position is called the command line. Once you type something and hit enter on Windows or return on the Mac, Unix will try to execute this command. If you want to try out an example, type this command into your command line: echo &quot;hello world&quot; Executing this line should print out hello world, then return back to the command line. Notice that you can’t use the mouse to move around in the terminal. You have to use the keyboard. To go back to a command you previously typed, you can use the up arrow. Note that above we included a chunk of code showing Unix commands in the same way we have previously shown R commands. We will make sure to distinguish when the command is meant for R and when it is meant for Unix. 3.2.3 The filesystem We refer to all the files, folders, and programs on your computer as the filesystem. Keep in mind that folders and programs are also files, but this is a technicality we rarely think about and ignore in this book. 3.2.4 Directories and subdirectories Think of your filesystem as a series of nested folders, each containing files, folders, and executables. Here is a visual representation of the structure we are describing: In Unix, we refer to folders as directories. Directories that are inside other directories are often referred to as subdirectories. So, for example, in the figure above, the directory docs has two subdirectories: reports and resumes, and docs is a subdirectory of home. 3.2.5 The home directory The home directory is where all your stuff is kept, as opposed to the system files that come with your computer, which are kept elsewhere. In the figure above, the directory called home represents your home directory, but that is rarely the name used. On your system, the name of your home directory is likely the same as your username on that system. Below are an example on Windows and Mac showing a home directory, in this case, named rafa: Now, look back at the figure showing a filesystem. Suppose you are using a point-and-click system and you want to remove the file cv.tex. Imagine that on your screen you can see the home directory. To erase this file, you would double click on the home directory, then docs, then resumes, and then drag cv.tex to the trash. Here you are experiencing the hierarchical nature of the system: cv.tex is a file inside the resumes directory, which is a subdirectory inside the docs directory, which is a subdirectory of the home directory. Now suppose you can’t see your home directory on your screen. You would somehow need to make it appear on your screen. One way to do this is to navigate from what is called the root directory all the way to your home directory. Any filesystem will have what is called a root directory, which is the directory that contains all directories. The home directory shown in the figure above will usually be two or more levels from the root. On Windows, you will have a structure like this: while on the Mac, it will be like this: Note for Windows Users: The typical R installation will make your Documents directory your home directory in R. This will likely be different from your home directory in Git Bash. Generally, when we discuss home directories, we refer to the Unix home directory which for Windows, in this book, is the Git Bash Unix directory. 3.2.6 Working directory The concept of a current location is part of the point-and-click experience: at any given moment we are in a folder and see the content of that folder. As you search for a file, as we did above, you are experiencing the concept of a current location: once you double click on a directory, you change locations and are now in that folder, as opposed to the folder you were in before. In Unix, we don’t have the same visual cues, but the concept of a current location is indispensable. We refer to this as the working directory. Each terminal window you have open has a working directory associated with it. How do we know what is our working directory? To answer this, we learn our first Unix command: pwd, which stands for print working directory. This command returns the working directory. Open a terminal and type: pwd We do not show the result of running this command because it will be quite different on your system compared to others. If you open a terminal and type pwd as your first command, you should see something like /Users/yourusername on a Mac or something like /c/Users/yourusername on Windows. The character string returned by calling pwd represents your working directory. When we first open a terminal, it will start in our home directory so in this case the working directory is the home directory. Notice that the forward slashes / in the strings above separate directories. So, for example, the location /c/Users/rafa implies that our working directory is called rafa and it is a subdirectory of Users, which is a subdirectory of c, which is a subdirectory of the root directory. The root directory is therefore represented by just a forward slash: /. 3.2.7 Paths We refer to the string returned by pwd as the full path of the working directory. The name comes from the fact that this string spells out the path you need to follow to get to the directory in question from the root directory. Every directory has a full path. Later, we will learn about relative paths, which tell us how to get to a directory from the working directory. In Unix, we use the shorthand ~ as a nickname for your home directory. So, for example, if docs is a directory in your home directory, the full path for docs can be written like this ~/docs. Most terminals will show the path to your working directory right on the command line. If you are using default settings and open a terminal on the Mac, you will see that right at the command line you have something like computername:~ username with ~ representing your working directory, which in this example is the home directory ~. The same is true for the Git Bash terminal where you will see something like username@computername MINGW64 ~, with the working directory at the end. When we change directories, we will see this change on both Macs and Windows. 3.3 Unix commands We will now learn a series of Unix commands that will permit us to prepare a directory for a data science project. We also provide examples of commands that, if you type into your terminal, will return an error. This is because we are assuming the filesystem in the earlier diagram. Your filesystem is different. In the next section, we will provide examples that you can type in. 3.3.1 ls: Listing directory content In a point-and-click system, we know what is in a directory because we see it. In the terminal, we do not see the icons. Instead, we use the command ls to list the directory content. To see the content of your home directory, open a terminal and type: ls We will see more examples soon. 3.3.2 mkdir and rmdir: make and remove a directory When we are preparing for a data science project, we will need to create directories. In Unix, we can do this with the command mkdir, which stands for make directory. Because you will soon be working on several projects, we highly recommend creating a directory called projects in your home directory. You can try this particular example on your system. Open a terminal and type: mkdir projects If you do this correctly, nothing will happen: no news is good news. If the directory already exists, you will get an error message and the existing directory will remain untouched. To confirm that you created these directories, you can list the directories: ls You should see the directories we just created listed. Perhaps you can also see many other directories that come pre-installed on your computer. For illustrative purposes, let’s make a few more directories. You can list more than one directory name like this: mkdir docs teaching You can check to see if the three directories were created: ls If you made a mistake and need to remove the directory, you can use the command rmdir to remove it. mkdir junk rmdir junk This will remove the directory as long as it is empty. If it is not empty, you will get an error message and the directory will remain untouched. To remove directories that are not empty, we will learn about the command rm later. 3.3.3 cd: navigating the filesystem by changing directories Next we want to create directories inside directories that we have already created. We also want to avoid pointing and clicking our way through the filesystem. We explain how to do this in Unix, using the command line. Suppose we open a terminal and our working directory is our home directory. We want to change our working directory to projects. We do this using the cd command, which stands for change directory: cd projects To check that the working directory changed, we can use a command we previously learned to see our location: pwd Our working directory should now be ~/projects. Note that on your computer the home directory ~ will be spelled out to something like /c/Users/yourusername). Important Pro Tip: In Unix you can auto-complete by hitting tab. This means that we can type cd d then hit tab. Unix will either auto-complete if docs is the only directory/file starting with d or show you the options. Try it out! Using Unix without auto-complete will make it unbearable. When using cd, we can either type a full path, which will start with / or ~, or a relative path. In the example above, in which we typed cd projects, we used a relative path. If the path you type does not start with / or ~, Unix will assume you are typing a relative path, meaning that it will look for the directory in your current working directory. So something like this will give you an error: cd Users because there is no Users directory in your working directory. Now suppose we want to move back to the directory in which projects is a subdirectory, referred to as the parent directory. We could use the full path of the parent directory, but Unix provides a shortcut for this: the parent directory of the working directory is represented with two dots: .., so to move back we simply type: cd .. You should now be back in your home directory which you can confirm using pwd. Because we can use full paths with cd, the following command: cd ~ will always take us back to the home directory, no matter where we are in the filesystem. The working directory also has a nickname, which is a single ., so if you type cd . you will not move. Although this particular use of . is not useful, this nickname does come in handy sometimes. The reasons are not relevant for this section, but you should still be aware of this fact. In summary, we have learned that when using cd we either stay put, move to a new directory using the desired directory name, or move back to the parent directory using ... When typing directory names, we can concatenate directories with the forward-slashes. So if we want a command that takes us to the projects directory no matter where we are in the filesystem, we can type: cd ~/projects which is equivalent to writing the entire path out. For example, in Windows we would write something like cd /c/Users/yourusername/projects The last two commands are equivalent and in both cases we are typing the full path. When typing out the path of the directory we want, either full or relative, we can concatenate directories with the forward-slashes. We already saw that we can move to the projects directory regardless of where we are by typing the full path like this: cd ~/projects We can also concatenate directory names for relative paths. For instance, if we want to move back to the parent directory of the parent directory of the working directory, we can type: cd ../.. Here are a couple of final tips related to the cd command. First, you can go back to whatever directory you just left by typing: cd - This can be useful if you type a very long path and then realize you want to go back to where you were, and that too has a very long path. Second, if you just type: cd you will be returned to your home directory. 3.3.4 Some examples Let’s explore some examples of using cd. To help visualize, we will show the graphical representation of our filesystem vertically: Suppose our working directory is ~/projects and we want to move to figs in project-1. Here it is convenient to use relative paths: cd project-1/figs Now suppose our working directory is ~/projects and we want to move to reports in docs, how can we do this? One way is to use relative paths: cd ../docs/reports Another is to use the full path: cd ~/docs/reports If you are trying this out on your system, remember to use auto-complete. Let’s examine one more example. Suppose we are in ~/projects/project-1/figs and want to change to ~/projects/project-2. Again, there are two ways. With relative paths: cd ../../proejct-2 and with full paths: cd ~/projects/project-2 3.3.5 More Unix commands 3.3.5.1 mv: moving files In a point-and-click system, we move files from one directory to another by dragging and dropping. In Unix, we use the mv command. Warning: mv will not ask “are you sure?” if your move results in overwriting a file. Now that you know how to use full and relative paths, using mv is relatively straightforward. The general form is: mv path-to-file path-to-destination-directory For example, if we want to move the file cv.tex from resumes to reports, you could use the full paths like this: mv ~/docs/resumes/cv.tex ~/docs/reports/ You can also use relative paths. So you could do this: cd ~/docs/resumes mv cv.tex ../reports/ or this: cd ~/docs/reports/ mv ../cv.tex ./ Notice that in the last one we used the working directory shortcut . to give a relative path as the destination directory. We can also use mv to change the name of a file. To do this, instead of the second argument being the destination directory, it also includes a filename. So, for example, to change the name from cv.tex to resume.tex, we simply type: cd ~/docs/resumes mv cv.tex resume.tex We can also combine the move and a rename. For example: cd ~/docs/resumes mv cv.tex ../reports/resume.tex And we can move entire directories. To move the resumes directory into reports, we do as follows: mv ~/docs/resumes ~/docs/reports/ It is important to add the last / to make it clear you do not want to rename the resumes directory to reports, but rather move it into the reports directory. 3.3.5.2 cp: copying files The command cp behaves similar to mv except instead of moving, we copy the file, meaning that the original file stays untouched. So in all the mv examples above, you can switch mv to cp and they will copy instead of move with one exception: we can’t copy entire directories without learning about arguments, which we do later. 3.3.5.3 rm: removing files In point-and-click systems, we remove files by dragging and dropping them into the trash or using a special click on the mouse. In Unix, we use the rm command. Warning: Unlike throwing files into the trash, rm is permanent. Be careful! The general way it works is as follows: rm filename You can actually list files as well like this: rm filename-1 filename-2 filename-3 You can use full or relative paths. To remove directories, you will have to learn about arguments, which we do later. 3.3.5.4 less: looking at a file Often you want to quickly look at the content of a file. If this file is a text file, the quickest way to do is by using the command less. To look a the file cv.tex, you do this: cd ~/docs/resumes less cv.tex To exit the viewer, you type q. If the files are long, you can use the arrow keys to move up and down. There are many other keyboard commands you can use within less to, for example, search or jump pages. You will learn more about this in a later section. If you are wondering why the command is called less, it is because the original was called more, as in “show me more of this file”. The second version was called less because of the saying “less is more”. 3.3.6 Advanced Unix Most Unix implementations include a large number of powerful tools and utilities. We have just learned the very basics here. We recommend that you use Unix as your main file management tool. It will take time to become comfortable with it, but as you struggle, you will find yourself learning just by looking up solutions on the internet. In this section, we superficially cover slightly more advanced topics. The main purpose of the section is to make you aware of what is available rather than explain everything in detail. 3.3.6.1 Arguments Most Unix commands can be run with arguments. Arguments are typically defined by using a dash - or two dashes -- (depending on the command) followed by a letter or a word. An example of an argument is the -r behind rm. The r stands for recursive and the result is that files and directories are removed recursively, which means that if you type: rm -r directory-name all files, subdirectories, files in subdirectories, subdirectories in subdirectories, and so on, will be removed. This is equivalent to throwing a folder in the trash, except you can’t recover it. Once you remove it, it is deleted for good. Often, when you are removing directories, you will encounter files that are protected. In such cases, you can use the argument -f which stands for force. You can also combine arguments. For instance, to remove a directory regardless of protected files, you type: rm -rf directory-name Remember that once you remove there is no going back, so use this command very carefully. A command that is often called with argument is ls. Here are some examples: ls -a The a stands for all. This argument makes ls show you all files in the directory, including hidden files. In Unix, all files starting with a . are hidden. Many applications create hidden directories to store important information without getting in the way of your work. An example is git (which we cover in depth later). Once you initialize a directory as a git directory with git init, a hidden directory called .git is created. Another hidden file is the .gitignore file. Another example of using an argument is: ls -l The l stands for long and the result is that more information about the files is shown. It is often useful to see files in chronological order. For that we use: ls -t and to reverse the order of how files are shown you can use: ls -r We can combine all these arguments to show more information for all files in reverse chronological order: ls -lart Each command has a different set of arguments. In the next section, we learn how to find out what they each do. 3.3.6.2 Pipes The help pages are typically long and if you type the commands above to see the help, it scrolls all the way to the end. It would be useful if we could save the help to a file and then use less to see it. The pipe, written like this |, does something similar. It pipes the results of a command to the command after the pipe. This is similar to the pipe %&gt;% that we use in R. To get more help we thus can type: man ls | less or in Git Bash: ls --help | less This is also useful when listing files with many files. We can type: ls -lart | less 3.3.6.3 Wild cards Some of the most powerful aspects of Unix are the wild cards. Suppose we want to remove all the temporary html files produced while trouble shooting for a project. Imagine there are dozens of files. It would be quite painful to remove them one by one. In Unix, we can actually write an expression that means all the files that end in .html. To do this we type wild card: *. As discussed in the data wrangling part of this book, this character means any number of any combination of characters. Specifically, to list all html files, we would type: ls *.html To remove all html files in a directory, we would type: rm *.html The other useful wild card is the ? symbol. This means any single character. So if all the files we want to erase have the form file-001.html with the numbers going from 1 to 999, we can type: rm file-???.html This will only remove files with that format. We can combine wild cards. For example, to remove all files with the name file-001 regardless of suffix, we can type: rm file-001.* Warning: Combining rm with the * wild card can be dangerous. There are combinations of these commands that will erase your entire filesystem without asking “are you sure?”. Make sure you understand how it works before using this wild card with the rm command. 3.3.6.4 Executables In Unix, all programs are files. They are called executables. So ls, mv and git are all files. But where are these program files? You can find out using the command which: which git That directory is probably full of program files. The directory /usr/bin usually holds many program files. If you type: ls /usr/bin in your terminal, you will see several executable files. There are other directories that usually hold program files. The Application directory in the Mac or Program Files directory in Windows are examples. When you type ls, Unix knows to run a program which is an executable that is stored in some other directory. So how does Unix know where to find it? This information is included in the environmental variable $PATH. If you type: echo $PATH you will see a list of directories separated by :. The directory /usr/bin is probably one of the first ones on the list. Unix looks for program files in those directories in that order. Although we don’t teach it here, you can actually create executables yourself. However, if you put it in your working directory and this directory is not on the path, you can’t run it just by typing the command. You get around this by typing the full path. So if your command is called my-ls, you can type: ./my-ls Once you have mastered the basics of Unix, you should consider learning to write your own executables as they can help alleviate repetitive work. 3.3.6.5 Permissions and file types If you type: ls -l At the beginning, you will see a series of symbols like this -rw-r--r--. This string indicates the type of file: regular file -, directory d, or executable x. This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute if the file is executable? This is more advanced than what we cover here, but you can learn much more in a Unix reference book. 3.3.7 File manipulation in R We can also perform file management from within R. The key functions to learn about can be seen by looking at the help file for ?files. These are the base commands, however. A more modern approach is to use the fs package — where fs stands for filesytem — for all file manipulation in R. 3.4 Git and GitHub Here we provide some details on Git and GitHub. However, we are only scratching the surface. To learn more about this topic, read Happy Git and GitHub for the useR. There are three main reasons to use Git and GitHub. Sharing: Even if we do not take advantage of the advanced and powerful version control functionality, we can still use Git and GitHub to share our code. We have already shown how we can do this with RStudio. Collaborating: Once you set up a central repo, you can have multiple people make changes to code and keep versions synched. GitHub provides a free service for centralized repos. GitHub also has a special utility, called a pull request, that can be used by anybody to suggest changes to your code. You can easily either accept or deny the request. Version control: The version control capabilities of Git permit us to keep track of changes we make to our code. We can also revert back to previous versions of files. Git also permits us to create branches in which we can test out ideas, then decide if we merge the new branch with the original. Here we focus on the sharing aspects of Git and GitHub and refer the reader to the links above to learn more about this powerful tool. 3.4.1 GitHub accounts After installing git5, the first step is to get a GitHub account. Basic GitHub accounts are free. To do this, go to GitHub where you will see a box in which you can sign up. You want to pick a name carefully. It should be short, easy to remember and to spell, somehow related to your name, and professional. This last one is important since you might be sending potential employers a link to your GitHub account. In the example below, I am sacrificing on the ease of spelling to incorporate my name. Your initials and last name are usually a good choice. If you have a very common name, then this may have to be taken into account. A simple solution would be to add numbers or spell out part of your name. The account I use for my research, rafalab, is the same one I use for my webpage6 and Twitter7, which makes it easy to remember for those that follow my work. Once you have a GitHub account, you are ready to connect Git and RStudio to this account. A first step is to let Git know who we are. This will make it easier to connect with GitHub. We start by opening a terminal window in RStudio (remember you can get one through Tools in the menu bar). Now we use the git config command to tell Git who we are. We will type the following two commands in our terminal window: git config --global user.name &quot;Your Name&quot; git config --global user.mail &quot;your@email.com&quot; You need to use the email account that you used to open your GitHub account. The RStudio session should look something like this: You start by going to the Global Options, selecting Git/SVN, and then you enter a path for the Git executable we just installed. On the Windows default installation, this will be C:/Program File/Git/bin/git.exe, but you should find it by browsing your system as this can change from system to system. Now to avoid entering our GitHub password every time we try to access our repository, we will create what is called an SSH RSA Key. RStudio can do this for us automatically if we click on the Create RSA Key button: You can follow the default instructions as shown below: Git, RStudio and GitHub should now be able to connect and we are ready to create a first GitHub code repository. 3.4.2 GitHub repositories You are now ready to create a GitHub repository (repo). The general idea is that you will have at least two copies of your code: one on your computer and one on GitHub. If you add collaborators to this project, then each will have a copy on their computer. The GitHub copy is usually considered the master copy that each collaborator syncs to. Git will help you keep all the different copies synced. As mentioned, one of the advantages of keeping code on a GitHub repository is that you can easily share it with potential employers interested in seeing examples of your work. Because many data science companies use version control systems, like Git, to collaborate on projects, they might also be impressed that you already know at least the basics. The first step in creating a repo for your code is to initialize on GitHub. Because you already created an account, you will have a page on GitHub with the URL http://github.com/username. To create a repo, first log in to your account by clicking the Sign In button on https://github.com. You might already be signed in, in which case the Sign In button will not show up. If signing in, you will have to enter your username and password. We recommend you set up your browser to remember this to avoid typing it in each time. Once on your account, you can click on Repositories and then click on New to create a new repo: You will then want to choose a good descriptive name for the project. In the future, you might have dozens of repos so keep that in mind when choosing a name. Here we will use homework-0. We recommend you make the repo public. If you want to keep it private, you will have to pay a monthly charge. You now have your first repo on GitHub. The next step will be to clone it on your computer and start editing and syncing using Git. To do this, it is convenient to copy the link provided by GitHub specifically to connect to this repo, using Git as shown below. We will later need to copy and paste this so make sure to remember this step. 3.4.3 Overview of Git The main actions in Git are to: pull changes from the remote repo, in this case the GitHub repo add files, or as we say in the Git lingo stage files commit changes to the local repo push changes to the remote repo, in our case the GitHub repo To effectively permit version control and collaboration in Git, files move across four different areas: But how does it all get started? There are two ways: we can clone an existing repo or initialize one. We will explore cloning first. 3.4.3.1 Clone We are going to clone an existing Upstream Repository. You can see it on GitHub here: https://github.com/rairizarry/murders. By visiting this page, you can see multiple files and directories. This is the Upstream Repository. By clicking the green clone button, we can copy the repo’s URL https://github.com/rairizarry/murders.git. But what does clone mean? Rather than download all these files to your computer, we are going to actually copy the entire Git structure, which means we will add the files and directories to each of the three local stages: Working Directory, Staging Area, and Local Repository. When you clone, all three are exactly the same to start. You can quickly see an example of this by doing the following. Open a terminal and type: pwd mkdir git-example cd git-example git clone https://github.com/rairizarry/murders.git cd murders You now have cloned a GitHub repo and have a working Git directory, with all the files, on your system. ls The Working Directory is the same as your Unix working directory. When you edit files using an editor such as RStudio, you change the files in this area and only in this area. Git can tell you how these files relate to the versions of the files in other areas with the command git status: If you check the status now, you will see that nothing has changed and you get the following message: git status Now we are going to make changes to these files. Eventually, we want these new versions of the files to be tracked and synched with the upstream repo. But we don’t want to keep track of every little change: we don’t want to sync until we are sure these versions are final enough to share. For this reason, edits in the staging area are not kept by the version control system. To demonstrate, we add a file to the staging area with the git add command. Below we create a file using the Unix echo command just as an example (in reality you would use RStudio): echo &quot;test&quot; &gt;&gt; new-file.txt We are also adding a temporary file that we do not want to track at all: echo &quot;temporary&quot; &gt;&gt; tmp.txt Now we can stage the file we eventually want to add to our repository: git add new-file.txt Notice what the status says now: git status Because new-file.txt is staged, the current version of the file will get added to the local repository next time we commit, which we do as follows: git commit -m &quot;adding a new file&quot; We have now changed the local repo, which you can confirm using git status However, if we edit that file again, it changes only in the working directory. To add to the local repo, we need to stage it and commit the changes that are added to the local repo: echo &quot;adding a line&quot; &gt;&gt; new-file.txt git add new-file.txt git commit -m &quot;adding a new line to new-file&quot; Note that this step is often unnecessary in our uses of Git. We can skip the staging part if we add the file name to the commit command like this: echo &quot;adding a second line&quot; &gt;&gt; new-file.txt git commit -m &quot;minor change to new-file&quot; new-file.txt We can keep track of all the changes we have made with: git log new-file.txt To keep everything synced, the final step is to push the changes to the upstream repo. This is done with the git push command like this: git push However, in this particular example, you will not be able to do this because you do not have permission to edit the upstream repo. If this was your repo, you could. If this is a collaborative project, the upstream repo may change and become different than our version. To update our local repository to be like the upstream repo, we use the command fetch: git fetch And then to make these copies to the staging and working directory areas, we use the command: git merge However, we often just want to change both with one command. For this, we use: git pull We will learn in Section 3.4.4 how RStudio has buttons to do all this. The details provided here should help you understand what happens in the background. 3.4.4 Using Git and GitHub in RStudio While command line Git is a powerful and flexible tool, it can be somewhat daunting when we are getting started. RStudio provides a graphical interface that facilitates the use of Git in the context of a data analysis project. We describe how to use this RStudio feature to do this here. Now we are ready to start an RStudio project that uses version control and stores the code on a GitHub repo. To do this, we start a project but, instead of New Directory, we will select Version Control and then we will select Git as our version control system: The repository URL is the link you used to clone. In Section 3.4.2, we used https://github.com/username/homework-0.git as an example. In the project directory name, you need to put the name of the folder that was generated, which in our example will be the name of the repo homework-0. This will create a folder called homework-0 on your local system. Once you do this, the project is created and it is aware of the connection to a GitHub repo. You will see on the top right corner the name and type of project as well as a new tab on the upper right pane titled Git. If you select this tab, it will show you the files on your project with some icons that give you information about these files and their relationship to the repo. In the example below, we already added a file to the folder, called code.R which you can see in the editing pane. We now need to pay attention to the Git pane. It is important to know that your local files and the GitHub repo will not be synced automatically. As described in Section 3.4.3, you have to sync using git push when you are ready. We show you can do this through RStudio rather than the terminal below. Before we start working on a collaborative project, usually the first thing we do is pull in the changes from the remote repo, in our case the one on GitHub. However, for the example shown here, since we are starting with an empty repo and we are the only ones making changes, we don’t need to start by pulling. In RStudio, the status of the file as it relates to the remote and local repos are represented in the status symbols with colors. A yellow square means that Git knows nothing about this file. To sync with the GitHub repo, we need to add the file, then commit the change to our local Git repo, then push the change to the GitHub repo. Right now, the file is just on our computer. To add the file using RStudio, we click the Stage box. You will see that the status icon now changes to a green A. Note: we are only adding the code.R file. We don’t necessarily need to add all the files in our local repo to the GitHub repo, only the ones we want to keep track of or the ones we want to share. If our work is producing files of a certain type that we do not want to keep track of, we can add the suffix that defines these files to the .gitignore file. More details on using .gitignore are included here: https://git-scm.com/docs/gitignore. These files will stop appearing in your RStudio Git pane. For the example shown here, we will only be adding code.R. But, in general, for an RStudio project, we recommend adding both the .gitignore and .Rproj files. Now we are ready to commit the file to our local repo. In RStudio, we can use the Commit button. This will open a new dialog window. With Git, whenever we commit a change, we are required to enter a comment describing the changes being committed. In this case, we will simply describe that we are adding a new script. In this dialog box, RStudio also gives you a summary of what you are changing to the GitHub repo. In this case, because it is a new file, the entire file is highlighted as green, which highlights the changes. Once we hit the commit button, we should see a message from Git with a summary of the changes that were committed. Now we are ready to push these changes to the GitHub repo. We can do this by clicking on the Push button on the top right corner: We now see a message from Git letting us know that the push has succeeded. In the pop-up window we no longer see the code.R file. This is because no new changes have been performed since we last pushed. We can exit this pop-up window now and continue working on our code. If we now visit our repo on the web, we will see that it matches our local copy. Congratulations, you have successfully shared code on a GitHub repository! 3.5 R The final product of a data analysis project is often a report. Many scientific publications can be thought of as a final report of a data analysis. The same is true for news articles based on data, an analysis report for your company, or lecture notes for a class on how to analyze data. The reports are often on paper or in a PDF that includes a textual description of the findings along with some figures and tables resulting from the analysis. Imagine that after you finish the analysis and the report, you are told that you were given the wrong dataset, you are sent a new one and you are asked to run the same analysis with this new dataset. Or what if you realize that a mistake was made and need to re-examine the code, fix the error, and re-run the analysis? Or imagine that someone you are training wants to see the code and be able to reproduce the results to learn about your approach? Situations like the ones just described are actually quite common for a data scientist. Here, we describe how you can keep your data science projects organized with RStudio so that re-running an analysis is straight-forward. We then demonstrate how to generate reproducible reports with R markdown and the knitR package in a way that will greatly help with recreating reports with minimal work. This is possible due to the fact that R markdown documents permit code and textual descriptions to be combined into the same document, and the figures and tables produced by the code are automatically added to the document. 3.5.1 RStudio projects RStudio provides a way to keep all the components of a data analysis project organized into one folder and to keep track of information about this project, such as the Git status of files, in one file. In Section 3.4.4 we demonstrate how RStudio facilitates the use of Git and GitHub through RStudio projects. In this section we quickly demonstrate how to start a new a project and some recommendations on how to keep these organized. RStudio projects also permit you to have several RStudio sessions open and keep track of which is which. To start a project, click on File and then New Project. Often we have already created a folder to save the work, as we did in Section ?? and we select Existing Directory. Here we show an example in which we have not yet created a folder and select the New Directory option. Then, for a data analysis project, you usually select the New Project option: Now you will have to decide on the location of the folder that will be associated with your project, as well as the name of the folder. When choosing a folder name, just like with file names, make sure it is a meaningful name that will help you remember what the project is about. As with files, we recommend using lower case letters, no spaces, and hyphens to separate words. We will call the folder for this project my-first-project. This will then generate a Rproj file called my-first-project.Rproj in the folder associated with the project. We will see how this is useful a few lines below. You will be given options on where this folder should be on your filesystem. In this example, we will place it in our home folder, but this is generally not good practice. As we described in Section ?? in the Unix chapter, you want to organize your filesystem following a hierarchical approach and with a folder called projects where you keep a folder for each project. When you start using RStudio with a project, you will see the project name in the upper left corner. This will remind you what project this particular RStudio session belongs to. When you open an RStudio session with no project, it will say Project: (None). When working on a project, all files will be saved and searched for in the folder associated with the project. Below, we show an example of a script that we wrote and saved with the name code.R. Because we used a meaningful name for the project, we can be a bit less informative when we name the files. Although we do not do it here, you can have several scripts open at once. You simply need to click File, then New File and pick the type of file you want to edit. One of the main advantages of using Projects is that after closing RStudio, if we wish to continue where we left off on the project, we simply double click or open the file saved when we first created the RStudio project. In this case, the file is called my-first-project.Rproj. If we open this file, RStudio will start up and open the scripts we were editing. Another advantage is that if you click on two or more different Rproj files, you start new RStudio and R sessions for each. 3.5.2 R markdown R markdown is a format for literate programming documents. It is based on markdown, a markup language that is widely used to generate html pages. You can learn more about markdown here: https://www.markdowntutorial.com/. Literate programming weaves instructions, documentation, and detailed comments in between machine executable code, producing a document that describes the program that is best for human understanding (Knuth 1984). Unlike a word processor, such as Microsoft Word, where what you see is what you get, with R markdown, you need to compile the document into the final report. The R markdown document looks different than the final product. This seems like a disadvantage at first, but it is not because, for example, instead of producing plots and inserting them one by one into the word processing document, the plots are automatically added. In RStudio, you can start an R markdown document by clicking on File, New File, the R Markdown. You will then be asked to enter a title and author for your document. We are going to prepare a report on gun murders so we will give it an appropriate name. You can also decide what format you would like the final report to be in: HTML, PDF, or Microsoft Word. Later, we can easily change this, but here we select html as it is the preferred format for debugging purposes: This will generate a template file: As a convention, we use the Rmd suffix for these files. Once you gain experience with R Markdown, you will be able to do this without the template and can simply start from a blank template. In the template, you will see several things to note. 3.5.2.1 The header At the top you see: --- title: &quot;Report on Gun Murders&quot; author: &quot;Rafael Irizarry&quot; date: &quot;April 16, 2018&quot; output: html_document --- The things between the --- is the header. We actually don’t need a header, but it is often useful. You can define many other things in the header than what is included in the template. We don’t discuss those here, but much information is available online. The one parameter that we will highlight is output. By changing this to, say, pdf_document, we can control the type of output that is produced when we compile. 3.5.2.2 R code chunks In various places in the document, we see something like this: ```{r} summary(pressure) ``` These are the code chunks. When you compile the document, the R code inside the chunk, in this case summary(pressure), will be evaluated and the result included in that position in the final document. To add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows. This applies to plots as well; the plot will be placed in that position. We can write something like this: ```{r} plot(pressure) ``` By default, the code will show up as well. To avoid having the code show up, you can use an argument. To avoid this, you can use the argument echo=FALSE. For example: ```{r echo=FALSE} summary(pressure) ``` We recommend getting into the habit of adding a label to the R code chunks. This will be very useful when debugging, among other situations. You do this by adding a descriptive word like this: ```{r pressure-summary} summary(pressure) ``` 3.5.2.3 Global options One of the R chunks contains a complex looking call: ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` We will not cover this here, but as you become more experienced with R Markdown, you will learn the advantages of setting global options for the compilation process. 3.5.2.4 knitR We use the knitR package to compile R markdown documents. The specific function used to compile is the knit function, which takes a filename as input. RStudio provides a button that makes it easier to compile the document. For the screenshot below, we have edited the document so that a report on gun murders is produced. You can see the file here: https://raw.githubusercontent.com/rairizarry/murders/master/report.Rmd. You can now click on the Knit button: The first time you click on the Knit button, a dialog box may appear asking you to install packages you need. Once you have installed the packages, clicking the Knit will compile your R markdown file and the resulting document will pop up: This produces an html document which you can see in your working directory. To view it, open a terminal and list the files. You can open the file in a browser and use this to present your analysis. You can also produce a PDF or Microsoft document by changing: output: html_document to output: pdf_document or output: word_document. We can also produce documents that render on GitHub using output: github_document. This will produce a markdown file, with suffix md, that renders in GitHub. Because we have uploaded these files to GitHub, you can click on the md file and you will see the report as a webpage: This is a convenient way to share your reports. 3.5.2.5 More on R markdown There is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including: RStudio’s tutorial: https://rmarkdown.rstudio.com The cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf The knitR book: https://yihui.name/knitr/ 3.5.3 Help for R 3.5.3.1 Searching for Help with R Google is your friend. Searching for help, especially when you are not sure what you are looking for, is more art than science. Adding certain strings to the search query — like “R”, “tidyverse”, or “ggplot” — often helps. Some locations are high quality (anything to do with RStudio or tidyverse) while others are less good than they initially appear (sthda.com, r-statistics.co, rdocumentation.org). 3.5.3.2 Reproducible Examples The best way to get help from other people is to create a reproducible example — a so-called “reprex” — which shows an example of the problem you are having. Install the reprex package: install.packages(&quot;reprex&quot;) Load the library: library(reprex) The code for your reproducible example needs to exist somewhere before you can use it to create your reprex. It is tempting to just grab it from the current file in which you are working, usually either a .R or .Rmd. Instead, I recommend creating a new file, dedicated to the example. Call it whatever.R. This makes everything easier, not least because some of the code you need, like the calls to library will often be many lines away, in your original file, from the code which is generating your error. Creating a stand-alone file helps to make the problem clear in your own head. Example: library(tidyverse) mtcars %&gt;% mutate(new_var = mpg + cly) Running this code produces an error because I have misspelled cyl as cly. Try it! To create a reprex: Highlight the code. Press Ctrl-C (or the equivalent) to copy the code to the clipboard. Type reprex() at the R prompt. The reproducible example now exists in two places. First, you can see it in the Viewer pane in RStudio. Second, it is on your clipboard. Next step is to go to RStudio Community (or whatever location at which you want to ask your question), create a new post, and paste in the reprex from your clipboard. Of course, you may first have to create an account. Also, your post will need a title, associated text and perhaps other information like tags. The key point is that using a reproducible example makes it much likelier that someone will answer your question. Here is advice about using your own data in making a reprex. Datapasta may also be relevant. http://github.com↩ https://www.rstudio.com/↩ https://library.si.edu/sites/default/files/tutorial/pdf/filenamingorganizing20180227.pdf↩ https://style.tidyverse.org/↩ https://rafalab.github.io/dsbook/accessing-the-terminal-and-installing-git.html↩ http://rafalab.org↩ http://twitter.com/rafalab↩ "],
["4-wrangling.html", "Chapter 4 Wrangling 4.1 The pipe operator: %&gt;% 4.2 filter rows 4.3 summarize variables 4.4 group_by rows 4.5 mutate existing variables 4.6 arrange and sort rows 4.7 Factors 4.8 Character Vectors 4.9 Combining Data 4.10 Other Verbs 4.11 Conclusion", " Chapter 4 Wrangling So far in our journey, we’ve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 1, and how to create data visualizations using the ggplot2 package in Chapter 2. In particular we studied what we term the “five named graphs” (5NG): scatterplots via geom_point() linegraphs via geom_line() boxplots via geom_boxplot() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the grammar of graphics, which maps variables in a data frame to the aesthetic attributes of one of the 5 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 2.1. Recall however that for two of our visualizations, we first needed to transform/modify existing data frames a little. For example, recall the scatterplot in Figure 2.2 of departure and arrival delays only for Alaska Airlines flights. In order to create this visualization, we first needed to pare down the flights data frame to an alaska_flights data frame consisting of only carrier == &quot;AS&quot; flights. Thus, alaska_flights will have fewer rows than flights. We did this using the filter() function: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) In this chapter, we’ll extend this example and we’ll introduce a series of functions from the dplyr package for data wrangling that will allow you to take a data frame and “wrangle” it (transform it) to suit your needs. Such functions include: filter() a data frame’s existing rows to only pick out a subset of them. For example, the alaska_flights data frame. summarize() one or more of its columns/variables with a summary statistic. Examples of summary statistics include the median and interquartile range of temperatures as we saw in Section 2.6 on boxplots. group_by() its rows. In other words, assign different rows to be part of the same group. We can then combine group_by() with summarize() to report summary statistics for each group separately. For example, say you don’t want a single overall average departure delay dep_delay for all three origin airports combined, but rather three separate average departure delays, one computed for each of the three origin airports. mutate() its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius. arrange() its rows. For example, sort the rows of weather in ascending or descending order of temp. join() it with another data frame by matching along a “key” variable. In other words, merge these two data frames together. Notice how we used computer_code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling has intuitively verb-named functions that are easy to remember. There is a further benefit to learning to use the dplyr package for data wrangling: its similarity to the database querying language SQL (pronounced “sequel” or spelled out as “S”, “Q”, “L”). SQL (which stands for “Structured Query Language”) is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn dplyr, you can learn SQL easily. We’ll talk more about their similarities in Subsection 4.9.7. Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 1.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(nycflights13) 4.1 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Let’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) This code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example, the flights data frame we explored in Section 1.4. The sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the six data wrangling verb-named functions we listed in the introduction to this chapter. For example, the filter(carrier == &quot;AS&quot;) function and argument specified we previewed earlier. The result will be the transformed/modified data frame that you want. In our example, we’ll save the result in a new data frame by using the &lt;- assignment operator with the name alaska_flights via alaska_flights &lt;-. alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) Much like when adding layers to a ggplot() using the + sign, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator %&gt;%. Furthermore, much like how the + sign has to come at the end of lines when constructing plots, the pipe operator %&gt;% has to come at the end of lines as well. Keep in mind, there are many more advanced data wrangling functions than just the six listed in the introduction to this chapter; you’ll see some examples of these in Section 4.10. However, just with these six verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book. 4.2 filter rows FIGURE 4.1: Diagram of filter() rows operation. The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria. We begin by focusing only on flights from New York City to Portland, Oregon. The dest destination code (or airport code) for Portland, Oregon is &quot;PDX&quot;. Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only flights heading to Portland are chosen here: portland_flights &lt;- flights %&gt;% filter(dest == &quot;PDX&quot;) View(portland_flights) Note the order of the code. First, take the flights data frame flights then filter() the data frame so that only those where the dest equals &quot;PDX&quot; are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(dest = &quot;PDX&quot;) will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other operators beyond just the == operator that tests for equality: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.” Furthermore, you can combine multiple criteria using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter flights for all rows that departed from JFK and were heading to Burlington, Vermont (&quot;BTV&quot;) or Seattle, Washington (&quot;SEA&quot;) and departed in the months of October, November, or December. Run the following: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot; &amp; (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) &amp; month &gt;= 10) View(btv_sea_flights_fall) Note that even though colloquially speaking one might say “all flights leaving Burlington, Vermont and Seattle, Washington,” in terms of computer operations, we really mean “all flights leaving Burlington, Vermont or leaving Seattle, Washington.” For a given row in the data, dest can be &quot;BTV&quot;, or &quot;SEA&quot;, or something else, but not both &quot;BTV&quot; and &quot;SEA&quot; at the same time. Furthermore, note the careful use of parentheses around dest == &quot;BTV&quot; | dest == &quot;SEA&quot;. We can often skip the use of &amp; and just separate our conditions with a comma. The previous code will return the identical output btv_sea_flights_fall as the following code: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot;, (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;), month &gt;= 10) View(btv_sea_flights_fall) Let’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to flights that didn’t go to Burlington, VT or Seattle, WA. not_BTV_SEA &lt;- flights %&gt;% filter(!(dest == &quot;BTV&quot; | dest == &quot;SEA&quot;)) View(not_BTV_SEA) Again, note the careful use of parentheses around the (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;). If we didn’t use parentheses as follows: flights %&gt;% filter(!dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) We would be returning all flights not headed to &quot;BTV&quot; or those headed to &quot;SEA&quot;, which is an entirely different resulting data frame. Now say we have a larger number of airports we want to filter for, say &quot;SEA&quot;, &quot;SFO&quot;, &quot;PDX&quot;, &quot;BTV&quot;, and &quot;BDL&quot;. We could continue to use the | (or) operator: many_airports &lt;- flights %&gt;% filter(dest == &quot;SEA&quot; | dest == &quot;SFO&quot; | dest == &quot;PDX&quot; | dest == &quot;BTV&quot; | dest == &quot;BDL&quot;) but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection 1.2.1 that the c() function “combines” or “concatenates” values into a single vector of values. many_airports &lt;- flights %&gt;% filter(dest %in% c(&quot;SEA&quot;, &quot;SFO&quot;, &quot;PDX&quot;, &quot;BTV&quot;, &quot;BDL&quot;)) View(many_airports) What this code is doing is filtering flights for all flights where dest is in the vector of airports c(&quot;BTV&quot;, &quot;SEA&quot;, &quot;PDX&quot;, &quot;SFO&quot;, &quot;BDL&quot;). Both outputs of many_airports are the same, but as you can see the latter takes much less energy to code. The %in% operator is useful for looking for matches commonly in one vector/variable compared to another. As a final note, we recommend that filter() should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. 4.3 summarize variables The next common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation. See Appendix 7.5 for a glossary of such summary statistics. Let’s calculate two summary statistics of the temp temperature variable in the weather data frame: the mean and standard deviation (recall from Section 1.4 that the weather data frame is included in the nycflights13 package). To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value, as illustrated in Figure 4.2. FIGURE 4.2: Diagram illustrating a summary function in R. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. Note you can also use the British English spelling of summarise(). As shown in Figure 4.3, the summarize() function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. FIGURE 4.3: Diagram of summarize() rows. We’ll save the results in a new data frame called summary_temp that will have two columns/variables: the mean and the std_dev: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp), std_dev = sd(temp)) summary_temp # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA Why are the values returned NA? As we saw in Subsection 2.2.1 when creating the scatterplot of departure and arrival delays for alaska_flights, NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult to do so? Perhaps there was an erroneous value that someone entered that has been corrected to read as missing? You’ll often encounter issues with missing values when working with real data. Going back to our summary_temp output, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values. The code that follows computes the mean and standard deviation of all non-missing values of temp: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_temp # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 55.3 17.8 Notice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function. However, one needs to be cautious whenever ignoring missing values as we’ve just done. There are possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis. What are other summary functions we can use inside the summarize() verb to compute summary statistics? As seen in the diagram in Figure 4.2, you can use any function in R that takes many values and returns just one. Here are just a few: mean(): the average sd(): the standard deviation, which is a measure of spread min() and max(): the minimum and maximum values, respectively IQR(): interquartile range sum(): the total amount when adding multiple numbers n(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered in Section 4.4. 4.4 group_by rows FIGURE 4.4: Diagram of group_by() and summarize(). Say instead of a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately. In other words, we would like to compute the mean temperature split by month. We can do this by “grouping” temperature observations by the values of another variable, in this case by the 12 values of the variable month. Run the following code: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 x 3 month mean std_dev &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 10.2 2 2 34.3 6.98 3 3 39.9 6.25 4 4 51.7 8.79 5 5 61.8 9.68 6 6 72.2 7.55 7 7 80.1 7.12 8 8 74.5 5.19 9 9 67.4 8.47 10 10 60.1 8.85 11 11 45.0 10.4 12 12 38.4 9.98 This code is identical to the previous code that created summary_temp, but with an extra group_by(month) added before the summarize(). Grouping the weather dataset by month and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year. It is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the diamonds data frame included in the ggplot2 package. Run this code: diamonds # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Observe that the first line of the output reads # A tibble: 53,940 x 10. This is an example of meta-data, in this case the number of observations/rows and variables/columns in diamonds. The actual data itself are the subsequent table of values. Now let’s pipe the diamonds data frame into group_by(cut): diamonds %&gt;% group_by(cut) # A tibble: 53,940 x 10 # Groups: cut [5] carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Observe that now there is additional meta-data: # Groups: cut [5] indicating that the grouping structure meta-data has been set based on the 5 possible levels of the categorical variable cut: &quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, and &quot;Ideal&quot;. On the other hand, observe that the data has not changed: it is still a table of 53,940 \\(\\times\\) 10 values. Only by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed. diamonds %&gt;% group_by(cut) %&gt;% summarize(avg_price = mean(price)) # A tibble: 5 x 2 cut avg_price &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good 3982. 4 Premium 4584. 5 Ideal 3458. If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function: diamonds %&gt;% group_by(cut) %&gt;% ungroup() # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Observe how the # Groups: cut [5] meta-data is no longer present. Let’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many flights departed each of the three airports in New York City: by_origin &lt;- flights %&gt;% group_by(origin) %&gt;% summarize(count = n()) by_origin # A tibble: 3 x 2 origin count &lt;chr&gt; &lt;int&gt; 1 EWR 120835 2 JFK 111279 3 LGA 104662 We see that Newark (&quot;EWR&quot;) had the most flights departing in 2013 followed by &quot;JFK&quot; and lastly by LaGuardia (&quot;LGA&quot;). Note there is a subtle but important difference between sum() and n(); while sum() returns the sum of a numerical variable, n() returns a count of the number of rows/observations. 4.4.1 Grouping by more than one variable You are not limited to grouping by one variable. Say you want to know the number of flights leaving each of the three New York City airports for each month. We can also group by a second variable month using group_by(origin, month): by_origin_monthly &lt;- flights %&gt;% group_by(origin, month) %&gt;% summarize(count = n()) by_origin_monthly # A tibble: 36 x 3 # Groups: origin [3] origin month count &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 EWR 1 9893 2 EWR 2 9107 3 EWR 3 10420 4 EWR 4 10531 5 EWR 5 10592 6 EWR 6 10175 7 EWR 7 10475 8 EWR 8 10359 9 EWR 9 9550 10 EWR 10 10104 # … with 26 more rows Observe that there are 36 rows to by_origin_monthly because there are 12 months for 3 airports (EWR, JFK, and LGA). Why do we group_by(origin, month) and not group_by(origin) and then group_by(month)? Let’s investigate: by_origin_monthly_incorrect &lt;- flights %&gt;% group_by(origin) %&gt;% group_by(month) %&gt;% summarize(count = n()) by_origin_monthly_incorrect # A tibble: 12 x 2 month count &lt;int&gt; &lt;int&gt; 1 1 27004 2 2 24951 3 3 28834 4 4 28330 5 5 28796 6 6 28243 7 7 29425 8 8 29327 9 9 27574 10 10 28889 11 11 27268 12 12 28135 What happened here is that the second group_by(month) overwrote the grouping structure meta-data of the earlier group_by(origin), so that in the end we are only grouping by month. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names. 4.5 mutate existing variables FIGURE 4.5: Diagram of mutate() columns. Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (°C) instead of degrees Fahrenheit (°F). The formula to convert temperatures from °F to °C is \\[ \\text{temp in C} = \\frac{\\text{temp in F} - 32}{1.8} \\] We can apply this formula to the temp variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones. weather &lt;- weather %&gt;% mutate(temp_in_C = (temp - 32) / 1.8) In this code, we mutate() the weather data frame by creating a new variable temp_in_C = (temp - 32) / 1.8 and then overwrite the original weather data frame. Why did we overwrite the data frame weather, instead of assigning the result to a new data frame like weather_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable temp, but instead created a new variable called temp_in_C? Because if we did this, we would have erased the original information contained in temp of temperatures in Fahrenheit that may still be valuable to us. Let’s now compute monthly average temperatures in both °F and °C using the group_by() and summarize() code we saw in Section 4.4: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), mean_temp_in_C = mean(temp_in_C, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 x 3 month mean_temp_in_F mean_temp_in_C &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 2.02 2 2 34.3 1.26 3 3 39.9 4.38 4 4 51.7 11.0 5 5 61.8 16.6 6 6 72.2 22.3 7 7 80.1 26.7 8 8 74.5 23.6 9 9 67.4 19.7 10 10 60.1 15.6 11 11 45.0 7.22 12 12 38.4 3.58 Let’s consider another example. Passengers are often frustrated when their flight departs late, but aren’t as annoyed if, in the end, pilots can make up some time during the flight. This is known in the airline industry as gain, and we will create this variable using the mutate() function: flights &lt;- flights %&gt;% mutate(gain = dep_delay - arr_delay) Let’s take a look at only the dep_delay, arr_delay, and the resulting gain variables for the first 5 rows in our updated flights data frame in Table 4.1. TABLE 4.1: First five rows of departure/arrival delay and gain variables dep_delay arr_delay gain 2 11 -9 4 20 -16 2 33 -31 -1 -18 17 -6 -25 19 The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its “gained time in the air” is a loss of 9 minutes, hence its gain is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (dep_delay of -1) but arrived 18 minutes early (arr_delay of -18), so its “gained time in the air” is \\(-1 - (-18) = -1 + 18 = 17\\) minutes, hence its gain is +17. Let’s look at some summary statistics of the gain variable by considering multiple summary functions at once in the same summarize() code: gain_summary &lt;- flights %&gt;% summarize( min = min(gain, na.rm = TRUE), q1 = quantile(gain, 0.25, na.rm = TRUE), median = quantile(gain, 0.5, na.rm = TRUE), q3 = quantile(gain, 0.75, na.rm = TRUE), max = max(gain, na.rm = TRUE), mean = mean(gain, na.rm = TRUE), sd = sd(gain, na.rm = TRUE), missing = sum(is.na(gain)) ) gain_summary # A tibble: 1 x 8 min q1 median q3 max mean sd missing &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 -196 -3 7 17 109 5.66 18.0 9430 We see for example that the average gain is +5 minutes, while the largest is +109 minutes! However, this code would take some time to type out in practice. We’ll see later on in Subsection 11.1.1 that there is a much more succinct way to compute a variety of common summary statistics: using the skim() function from the skimr package. Recall from Section 2.4 that since gain is a numerical variable, we can visualize its distribution using a histogram. ggplot(data = flights, mapping = aes(x = gain)) + geom_histogram(color = &quot;white&quot;, bins = 20) FIGURE 4.6: Histogram of gain variable. The resulting histogram in Figure 4.6 provides a different perspective on the gain variable than the summary statistics we computed earlier. For example, note that most values of gain are right around 0. To close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created. As an example, consider the mutate() code Hadley Wickham and Garrett Grolemund show in Chapter 5 of R for Data Science (Grolemund and Wickham 2017): flights &lt;- flights %&gt;% mutate( gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) 4.6 arrange and sort rows One of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable. Suppose we are interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013: freq_dest &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) freq_dest # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ABQ 254 2 ACK 265 3 ALB 439 4 ANC 8 5 ATL 17215 6 AUS 2439 7 AVL 275 8 BDL 443 9 BGR 375 10 BHM 297 # … with 95 more rows Observe that by default the rows of the resulting freq_dest data frame are sorted in alphabetical order of destination. Say instead we would like to see the same data, but sorted from the most to the least number of flights (num_flights) instead: freq_dest %&gt;% arrange(num_flights) # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 LEX 1 2 LGA 1 3 ANC 8 4 SBN 10 5 HDN 15 6 MTJ 15 7 EYW 17 8 PSP 19 9 JAC 25 10 BZN 36 # … with 95 more rows This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so: freq_dest %&gt;% arrange(desc(num_flights)) # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ORD 17283 2 ATL 17215 3 LAX 16174 4 BOS 15508 5 MCO 14082 6 CLT 14064 7 SFO 13331 8 FLL 12055 9 MIA 11728 10 DCA 9705 # … with 95 more rows 4.7 Factors We’ve spent a lot of time working with big, beautiful data frames, like the Gapminder data. But we also need to manage the individual variables housed within. Factors are the variable type that useRs love to hate. It is how we store truly categorical information in R. The values a factor can take on are called the levels. For example, the levels of the factor continent in Gapminder are are “Africa”, “Americas”, etc. and this is what’s usually presented to your eyeballs by R. In general, the levels are friendly human-readable character strings, like “male/female” and “control/treated”. But never ever ever forget that, under the hood, R is really storing integer codes 1, 2, 3, etc. This [Janus][wiki-janus]-like nature of factors means they are rich with booby traps for the unsuspecting but they are a necessary evil. I recommend you learn how to be the boss of your factors. The pros far outweigh the cons. Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. The worst kind of factor is the stealth factor. The variable that you think of as character, but that is actually a factor (numeric!!). This is a classic R gotcha. Check your variable types explicitly when things seem weird. It happens to the best of us. Where do stealth factors come from? Base R has a burning desire to turn character information into factor. The happens most commonly at data import via read.table() and friends. But data.frame() and other functions are also eager to convert character to factor. To shut this down, use stringsAsFactors = FALSE in read.table() and data.frame() or – even better – use the tidyverse! For data import, use readr::read_csv(), readr::read_tsv(), etc. For data frame creation, use tibble::tibble(). And so on. Good articles about how the factor fiasco came to be: [stringsAsFactors: An unauthorized biography][bio-strings-as-factors] by Roger Peng [stringsAsFactors = &lt;sigh&gt;][blog-strings-as-factors] by Thomas Lumley 4.7.1 The forcats package [forcats][forcats-web] is a core package in the tidyverse. It is installed via install.packages(&quot;tidyverse&quot;), and loaded with library(tidyverse). You can also install via install.packages(&quot;forcats&quot;)and load it yourself separately as needed via library(forcats). Main functions start with fct_. There really is no coherent family of base functions that forcats replaces – that’s why it’s such a welcome addition. Load tidyverse (which include forcats) and gapminder. library(tidyverse) library(gapminder) Get to know your factor before you start touching it! It’s polite. Let’s use gapminder$continent as our example. str(gapminder$continent) Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... levels(gapminder$continent) [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) [1] 5 class(gapminder$continent) [1] &quot;factor&quot; To get a frequency table as a tibble, from a tibble, use dplyr::count(). To get similar from a free-range factor, use forcats::fct_count(). gapminder %&gt;% count(continent) # A tibble: 5 x 2 continent n &lt;fct&gt; &lt;int&gt; 1 Africa 624 2 Americas 300 3 Asia 396 4 Europe 360 5 Oceania 24 fct_count(gapminder$continent) # A tibble: 5 x 2 f n &lt;fct&gt; &lt;int&gt; 1 Africa 624 2 Americas 300 3 Asia 396 4 Europe 360 5 Oceania 24 4.7.2 Dropping unused levels Just because you drop all the rows corresponding to a specific factor level, the levels of the factor itself do not change. Sometimes all these unused levels can come back to haunt you later, e.g., in figure legends. Watch what happens to the levels of country (= nothing) when we filter Gapminder to a handful of countries. nlevels(gapminder$country) [1] 142 h_countries &lt;- c(&quot;Egypt&quot;, &quot;Haiti&quot;, &quot;Romania&quot;, &quot;Thailand&quot;, &quot;Venezuela&quot;) h_gap &lt;- gapminder %&gt;% filter(country %in% h_countries) nlevels(h_gap$country) [1] 142 Even though h_gap only has data for a handful of countries, we are still schlepping around all 142 levels from the original gapminder tibble. How can you get rid of them? The base function droplevels() operates on all the factors in a data frame or on a single factor. The function forcats::fct_drop() operates on a factor. h_gap_dropped &lt;- h_gap %&gt;% droplevels() nlevels(h_gap_dropped$country) [1] 5 ## use forcats::fct_drop() on a free-range factor h_gap$country %&gt;% fct_drop() %&gt;% levels() [1] &quot;Egypt&quot; &quot;Haiti&quot; &quot;Romania&quot; &quot;Thailand&quot; &quot;Venezuela&quot; Exercise: Filter the gapminder data down to rows where population is less than a quarter of a million, i.e. 250,000. Get rid of the unused factor levels for country and continent in different ways, such as: droplevels() fct_drop() inside mutate() fct_dopr() with mutate_at() or mutate_if() 4.7.3 Change order of the levels, principled By default, factor levels are ordered alphabetically. Which might as well be random, when you think about it! It is preferable to order the levels according to some principle: Frequency. Make the most common level the first and so on. Another variable. Order factor levels according to a summary statistic for another variable. Example: order Gapminder countries by life expectancy. First, let’s order continent by frequency, forwards and backwards. This is often a great idea for tables and figures, esp. frequency barplots. ## default order is alphabetical gapminder$continent %&gt;% levels() [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; ## order by frequency gapminder$continent %&gt;% fct_infreq() %&gt;% levels() [1] &quot;Africa&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Americas&quot; &quot;Oceania&quot; ## backwards! gapminder$continent %&gt;% fct_infreq() %&gt;% fct_rev() %&gt;% levels() [1] &quot;Oceania&quot; &quot;Americas&quot; &quot;Europe&quot; &quot;Asia&quot; &quot;Africa&quot; These two barcharts of frequency by continent differ only in the order of the continents. Which do you prefer? Now we order country by another variable, forwards and backwards. This other variable is usually quantitative and you will order the factor according to a grouped summary. The factor is the grouping variable and the default summarizing function is median() but you can specify something else. ## order countries by median life expectancy fct_reorder(gapminder$country, gapminder$lifeExp) %&gt;% levels() %&gt;% head() [1] &quot;Sierra Leone&quot; &quot;Guinea-Bissau&quot; &quot;Afghanistan&quot; &quot;Angola&quot; [5] &quot;Somalia&quot; &quot;Guinea&quot; ## order accoring to minimum life exp instead of median fct_reorder(gapminder$country, gapminder$lifeExp, min) %&gt;% levels() %&gt;% head() [1] &quot;Rwanda&quot; &quot;Afghanistan&quot; &quot;Gambia&quot; &quot;Angola&quot; &quot;Sierra Leone&quot; [6] &quot;Cambodia&quot; ## backwards! fct_reorder(gapminder$country, gapminder$lifeExp, .desc = TRUE) %&gt;% levels() %&gt;% head() [1] &quot;Iceland&quot; &quot;Japan&quot; &quot;Sweden&quot; &quot;Switzerland&quot; &quot;Netherlands&quot; [6] &quot;Norway&quot; Example of why we reorder factor levels: often makes plots much better! When a factor is mapped to x or y, it should almost always be reordered by the quantitative variable you are mapping to the other one. Compare the interpretability of these two plots of life expectancy in Asian countries in 2007. The only difference is the order of the country factor. Which one do you find easier to learn from? gap_asia_2007 &lt;- gapminder %&gt;% filter(year == 2007, continent == &quot;Asia&quot;) ggplot(gap_asia_2007, aes(x = lifeExp, y = country)) + geom_point() ggplot(gap_asia_2007, aes(x = lifeExp, y = fct_reorder(country, lifeExp))) + geom_point() Use fct_reorder2() when you have a line chart of a quantitative x against another quantitative y and your factor provides the color. This way the legend appears in some order as the data! Contrast the legend on the left with the one on the right. h_countries &lt;- c(&quot;Egypt&quot;, &quot;Haiti&quot;, &quot;Romania&quot;, &quot;Thailand&quot;, &quot;Venezuela&quot;) h_gap &lt;- gapminder %&gt;% filter(country %in% h_countries) %&gt;% droplevels() ggplot(h_gap, aes(x = year, y = lifeExp, color = country)) + geom_line() ggplot(h_gap, aes(x = year, y = lifeExp, color = fct_reorder2(country, year, lifeExp))) + geom_line() + labs(color = &quot;country&quot;) 4.7.4 Change order of the levels, “because I said so” Sometimes you just want to hoist one or more levels to the front. Why? Because I said so. This resembles what we do when we move variables to the front with dplyr::select(special_var, everything()). h_gap$country %&gt;% levels() [1] &quot;Egypt&quot; &quot;Haiti&quot; &quot;Romania&quot; &quot;Thailand&quot; &quot;Venezuela&quot; h_gap$country %&gt;% fct_relevel(&quot;Romania&quot;, &quot;Haiti&quot;) %&gt;% levels() [1] &quot;Romania&quot; &quot;Haiti&quot; &quot;Egypt&quot; &quot;Thailand&quot; &quot;Venezuela&quot; This might be useful if you are preparing a report for, say, the Romanian government. The reason for always putting Romania first has nothing to do with the data, it is important for external reasons and you need a way to express this. 4.7.5 Recode the levels Sometimes you have better ideas about what certain levels should be. This is called recoding. i_gap &lt;- gapminder %&gt;% filter(country %in% c(&quot;United States&quot;, &quot;Sweden&quot;, &quot;Australia&quot;)) %&gt;% droplevels() i_gap$country %&gt;% levels() [1] &quot;Australia&quot; &quot;Sweden&quot; &quot;United States&quot; i_gap$country %&gt;% fct_recode(&quot;USA&quot; = &quot;United States&quot;, &quot;Oz&quot; = &quot;Australia&quot;) %&gt;% levels() [1] &quot;Oz&quot; &quot;Sweden&quot; &quot;USA&quot; 4.7.6 Grow a factor Let’s create two data frames, each with data from two countries, dropping unused factor levels. df1 &lt;- gapminder %&gt;% filter(country %in% c(&quot;United States&quot;, &quot;Mexico&quot;), year &gt; 2000) %&gt;% droplevels() df2 &lt;- gapminder %&gt;% filter(country %in% c(&quot;France&quot;, &quot;Germany&quot;), year &gt; 2000) %&gt;% droplevels() The country factors in df1 and df2 have different levels. levels(df1$country) [1] &quot;Mexico&quot; &quot;United States&quot; levels(df2$country) [1] &quot;France&quot; &quot;Germany&quot; Can you just combine them? c(df1$country, df2$country) [1] 1 1 2 2 1 1 2 2 Umm, no. That is wrong on many levels! Use fct_c() to do this. fct_c(df1$country, df2$country) [1] Mexico Mexico United States United States France [6] France Germany Germany Levels: Mexico United States France Germany 4.8 Character Vectors We’ve spent a lot of time working with big, beautiful data frames. That are clean and wholesome, like the Gapminder data. But real life will be much nastier. You will bring data into R from the outside world and discover there are problems. You might think: how hard can it be to deal with character data? And the answer is: it can be very hard! [Stack Exchange outage][stackexchange-outage] [Regexes to validate/match email addresses][email-regex] [Fixing an Atom bug][fix-atom-bug] Here we discuss common remedial tasks for cleaning and transforming character data, also known as “strings”. A data frame or tibble will consist of one or more atomic vectors of a certain class. This lesson deals with things you can do with vectors of class character. Here are some resources: 4.8.1 Manipulating character vectors [stringr package][stringr-web]. A core package in the tidyverse. It is installed via install.packages(&quot;tidyverse&quot;) and also loaded via library(tidyverse). Of course, you can also install or load it individually. Main functions start with str_. Auto-complete is your friend. Replacements for base functions re: string manipulation and regular expressions (see below). Main advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task. [tidyr package][tidyr-web]. Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract(). Base functions: nchar(), strsplit(), substr(), paste(), paste0(). The [glue package][glue-web] is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package. 4.8.2 Regular expressions resources A God-awful and powerful language for expressing patterns to match in text or for search-and-replace. Frequently described as “write only”, because regular expressions are easier to write than to read/understand. And they are not particularly easy to write. We again prefer the [stringr package][stringr-cran] over base functions. Why? Wraps [stringi][stringi-cran], which is a great place to look if stringr isn’t powerful enough. Standardized on [ICU regular expressions][icu-regex], so you can stop toggling perl = TRUE/FALSE at random. Results come back in a form that is much friendlier for downstream work. The [Strings chapter][r4ds-strings] of [R for Data Science][r4ds] (???) is a great resource. Older STAT 545 lessons on regular expressions have some excellent content. This lesson draws on them, but makes more rigorous use of stringr and uses example data that is easier to support long-term. 2014 Intro to regular expressions by TA Gloria Li (Appendix ??). 2015 Regular expressions and character data in R by TA Kieran Samuk (Appendix ??). RStudio Cheat Sheet on [Regular Expressions in R][rstudio-regex-cheatsheet]. Regex testers: [regex101.com][regex101] [regexr.com][regexr] [rex R package][rex-github]: make regular expression from human readable expressions. Base functions: grep() and friends. 4.8.3 Character encoding resources [Strings subsection of data import chapter][r4ds-readr-strings] in [R for Data Science][r4ds] (???). Screeds on the Minimum Everyone Needs to Know about encoding: [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)][unicode-no-excuses] [What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text][programmers-encoding] Chapter ?? - I’ve translated this blog post [Guide to fixing encoding problems in Ruby][encoding-probs-ruby] into R as the first step to developing a lesson. 4.8.4 Character vectors that live in a data frame Certain operations are facilitated by tidyr. These are described below. For a general discussion of how to work on variables that live in a data frame, see Vectors versus tibbles (Appendix ??). Load the tidyverse, which includes stringr library(tidyverse) 4.8.5 Regex-free string manipulation with stringr and tidyr Basic string manipulation tasks: Study a single character vector How long are the strings? Presence/absence of a literal string Operate on a single character vector Keep/discard elements that contain a literal string Split into two or more character vectors using a fixed delimiter Snip out pieces of the strings based on character position Collapse into a single string Operate on two or more character vectors Glue them together element-wise to get a new character vector. fruit, words, and sentences are character vectors that ship with stringr for practicing. 4.8.6 Detect or filter on a target string Determine presence/absence of a literal string with str_detect(). Spoiler: later we see str_detect() also detects regular expressions. Which fruits actually use the word “fruit”? str_detect(fruit, pattern = &quot;fruit&quot;) [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [25] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [37] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [73] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE What’s the easiest way to get the actual fruits that match? Use str_subset() to keep only the matching elements. Note we are storing this new vector my_fruit to use in later examples! (my_fruit &lt;- str_subset(fruit, pattern = &quot;fruit&quot;)) [1] &quot;breadfruit&quot; &quot;dragonfruit&quot; &quot;grapefruit&quot; &quot;jackfruit&quot; &quot;kiwi fruit&quot; [6] &quot;passionfruit&quot; &quot;star fruit&quot; &quot;ugli fruit&quot; 4.8.7 String splitting by delimiter Use stringr::str_split() to split strings on a delimiter. Some of our fruits are compound words, like “grapefruit”, but some have two words, like “ugli fruit”. Here we split on a single space &quot; &quot;, but show use of a regular expression later. str_split(my_fruit, pattern = &quot; &quot;) [[1]] [1] &quot;breadfruit&quot; [[2]] [1] &quot;dragonfruit&quot; [[3]] [1] &quot;grapefruit&quot; [[4]] [1] &quot;jackfruit&quot; [[5]] [1] &quot;kiwi&quot; &quot;fruit&quot; [[6]] [1] &quot;passionfruit&quot; [[7]] [1] &quot;star&quot; &quot;fruit&quot; [[8]] [1] &quot;ugli&quot; &quot;fruit&quot; It’s bummer that we get a list back. But it must be so! In full generality, split strings must return list, because who knows how many pieces there will be? If you are willing to commit to the number of pieces, you can use str_split_fixed() and get a character matrix. You’re welcome! str_split_fixed(my_fruit, pattern = &quot; &quot;, n = 2) [,1] [,2] [1,] &quot;breadfruit&quot; &quot;&quot; [2,] &quot;dragonfruit&quot; &quot;&quot; [3,] &quot;grapefruit&quot; &quot;&quot; [4,] &quot;jackfruit&quot; &quot;&quot; [5,] &quot;kiwi&quot; &quot;fruit&quot; [6,] &quot;passionfruit&quot; &quot;&quot; [7,] &quot;star&quot; &quot;fruit&quot; [8,] &quot;ugli&quot; &quot;fruit&quot; If the to-be-split variable lives in a data frame, tidyr::separate() will split it into 2 or more variables. my_fruit_df &lt;- tibble(my_fruit) my_fruit_df %&gt;% separate(my_fruit, into = c(&quot;pre&quot;, &quot;post&quot;), sep = &quot; &quot;) # A tibble: 8 x 2 pre post &lt;chr&gt; &lt;chr&gt; 1 breadfruit &lt;NA&gt; 2 dragonfruit &lt;NA&gt; 3 grapefruit &lt;NA&gt; 4 jackfruit &lt;NA&gt; 5 kiwi fruit 6 passionfruit &lt;NA&gt; 7 star fruit 8 ugli fruit 4.8.8 Substring extraction (and replacement) by position Count characters in your strings with str_length(). Note this is different from the length of the character vector itself. length(my_fruit) [1] 8 str_length(my_fruit) [1] 10 11 10 9 10 12 10 10 You can snip out substrings based on character position with str_sub(). head(fruit) %&gt;% str_sub(1, 3) [1] &quot;app&quot; &quot;apr&quot; &quot;avo&quot; &quot;ban&quot; &quot;bel&quot; &quot;bil&quot; The start and end arguments are vectorised. Example: a sliding 3-character window. tibble(fruit) %&gt;% head() %&gt;% mutate(snip = str_sub(fruit, 1:6, 3:8)) # A tibble: 6 x 2 fruit snip &lt;chr&gt; &lt;chr&gt; 1 apple &quot;app&quot; 2 apricot &quot;pri&quot; 3 avocado &quot;oca&quot; 4 banana &quot;ana&quot; 5 bell pepper &quot; pe&quot; 6 bilberry &quot;rry&quot; Finally, str_sub() also works for assignment, i.e. on the left hand side of &lt;-. (x &lt;- head(fruit, 3)) [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; str_sub(x, 1, 3) &lt;- &quot;AAA&quot; x [1] &quot;AAAle&quot; &quot;AAAicot&quot; &quot;AAAcado&quot; 4.8.9 Collapse a vector You can collapse a character vector of length n &gt; 1 to a single string with str_c(), which also has other uses (see the next section). head(fruit) %&gt;% str_c(collapse = &quot;, &quot;) [1] &quot;apple, apricot, avocado, banana, bell pepper, bilberry&quot; 4.8.10 Create a character vector by catenating multiple vectors If you have two or more character vectors of the same length, you can glue them together element-wise, to get a new vector of that length. Here are some … awful smoothie flavors? str_c(fruit[1:4], fruit[5:8], sep = &quot; &amp; &quot;) [1] &quot;apple &amp; bell pepper&quot; &quot;apricot &amp; bilberry&quot; &quot;avocado &amp; blackberry&quot; [4] &quot;banana &amp; blackcurrant&quot; Element-wise catenation can be combined with collapsing. str_c(fruit[1:4], fruit[5:8], sep = &quot; &amp; &quot;, collapse = &quot;, &quot;) [1] &quot;apple &amp; bell pepper, apricot &amp; bilberry, avocado &amp; blackberry, banana &amp; blackcurrant&quot; If the to-be-combined vectors are variables in a data frame, you can use tidyr::unite() to make a single new variable from them. fruit_df &lt;- tibble( fruit1 = fruit[1:4], fruit2 = fruit[5:8] ) fruit_df %&gt;% unite(&quot;flavor_combo&quot;, fruit1, fruit2, sep = &quot; &amp; &quot;) # A tibble: 4 x 1 flavor_combo &lt;chr&gt; 1 apple &amp; bell pepper 2 apricot &amp; bilberry 3 avocado &amp; blackberry 4 banana &amp; blackcurrant 4.8.11 Substring replacement You can replace a pattern with str_replace(). Here we use an explicit string-to-replace, but later we revisit with a regular expression. str_replace(my_fruit, pattern = &quot;fruit&quot;, replacement = &quot;THINGY&quot;) [1] &quot;breadTHINGY&quot; &quot;dragonTHINGY&quot; &quot;grapeTHINGY&quot; &quot;jackTHINGY&quot; [5] &quot;kiwi THINGY&quot; &quot;passionTHINGY&quot; &quot;star THINGY&quot; &quot;ugli THINGY&quot; A special case that comes up a lot is replacing NA, for which there is str_replace_na(). melons &lt;- str_subset(fruit, pattern = &quot;melon&quot;) melons[2] &lt;- NA melons [1] &quot;canary melon&quot; NA &quot;watermelon&quot; str_replace_na(melons, &quot;UNKNOWN MELON&quot;) [1] &quot;canary melon&quot; &quot;UNKNOWN MELON&quot; &quot;watermelon&quot; If the NA-afflicted variable lives in a data frame, you can use tidyr::replace_na(). tibble(melons) %&gt;% replace_na(replace = list(melons = &quot;UNKNOWN MELON&quot;)) # A tibble: 3 x 1 melons &lt;chr&gt; 1 canary melon 2 UNKNOWN MELON 3 watermelon And that concludes our treatment of regex-free manipulations of character data! 4.8.12 Regular expressions with stringr FIGURE 4.7: From @ThePracticalDev The country names in the gapminder dataset are convenient for examples. Load it now and store the 142 unique country names to the object countries. library(gapminder) countries &lt;- levels(gapminder$country) 4.8.13 Characters with special meaning Frequently your string tasks cannot be expressed in terms of a fixed string, but can be described in terms of a pattern. Regular expressions, aka “regexes”, are the standard way to specify these patterns. In regexes, specific characters and constructs take on special meaning in order to match multiple strings. The first metacharacter is the period ., which stands for any single character, except a newline (which by the way, is represented by \\n). The regex a.b will match all countries that have an a, followed by any single character, followed by b. Yes, regexes are case sensitive, i.e. “Italy” does not match. str_subset(countries, pattern = &quot;i.a&quot;) [1] &quot;Argentina&quot; &quot;Bosnia and Herzegovina&quot; [3] &quot;Burkina Faso&quot; &quot;Central African Republic&quot; [5] &quot;China&quot; &quot;Costa Rica&quot; [7] &quot;Dominican Republic&quot; &quot;Hong Kong, China&quot; [9] &quot;Jamaica&quot; &quot;Mauritania&quot; [11] &quot;Nicaragua&quot; &quot;South Africa&quot; [13] &quot;Swaziland&quot; &quot;Taiwan&quot; [15] &quot;Thailand&quot; &quot;Trinidad and Tobago&quot; Notice that i.a matches “ina”, “ica”, “ita”, and more. Anchors can be included to express where the expression must occur within the string. The ^ indicates the beginning of string and $ indicates the end. Note how the regex i.a$ matches many fewer countries than i.a alone. Likewise, more elements of my_fruit match d than ^d, which requires “d” at string start. str_subset(countries, pattern = &quot;i.a$&quot;) [1] &quot;Argentina&quot; &quot;Bosnia and Herzegovina&quot; &quot;China&quot; [4] &quot;Costa Rica&quot; &quot;Hong Kong, China&quot; &quot;Jamaica&quot; [7] &quot;South Africa&quot; str_subset(my_fruit, pattern = &quot;d&quot;) [1] &quot;breadfruit&quot; &quot;dragonfruit&quot; str_subset(my_fruit, pattern = &quot;^d&quot;) [1] &quot;dragonfruit&quot; The metacharacter \\b indicates a word boundary and \\B indicates NOT a word boundary. This is our first encounter with something called “escaping” and right now I just want you at accept that we need to prepend a second backslash to use these sequences in regexes in R. We’ll come back to this tedious point later. str_subset(fruit, pattern = &quot;melon&quot;) [1] &quot;canary melon&quot; &quot;rock melon&quot; &quot;watermelon&quot; str_subset(fruit, pattern = &quot;\\\\bmelon&quot;) [1] &quot;canary melon&quot; &quot;rock melon&quot; str_subset(fruit, pattern = &quot;\\\\Bmelon&quot;) [1] &quot;watermelon&quot; 4.8.14 Character classes Characters can be specified via classes. You can make them explicitly “by hand” or use some pre-existing ones. Character classes are usually given inside square brackets, [] but a few come up so often that we have a metacharacter for them, such as \\d for a single digit. Here we match ia at the end of the country name, preceded by one of the characters in the class. Or, in the negated class, preceded by anything but one of those characters. ## make a class &quot;by hand&quot; str_subset(countries, pattern = &quot;[nls]ia$&quot;) [1] &quot;Albania&quot; &quot;Australia&quot; &quot;Indonesia&quot; &quot;Malaysia&quot; &quot;Mauritania&quot; [6] &quot;Mongolia&quot; &quot;Romania&quot; &quot;Slovenia&quot; &quot;Somalia&quot; &quot;Tanzania&quot; [11] &quot;Tunisia&quot; ## use ^ to negate the class str_subset(countries, pattern = &quot;[^nls]ia$&quot;) [1] &quot;Algeria&quot; &quot;Austria&quot; &quot;Bolivia&quot; &quot;Bulgaria&quot; &quot;Cambodia&quot; [6] &quot;Colombia&quot; &quot;Croatia&quot; &quot;Ethiopia&quot; &quot;Gambia&quot; &quot;India&quot; [11] &quot;Liberia&quot; &quot;Namibia&quot; &quot;Nigeria&quot; &quot;Saudi Arabia&quot; &quot;Serbia&quot; [16] &quot;Syria&quot; &quot;Zambia&quot; Here we revisit splitting my_fruit with two more general ways to match whitespace: the \\s metacharacter and the POSIX class [:space:]. Notice that we must prepend an extra backslash \\ to escape \\s and the POSIX class has to be surrounded by two sets of square brackets. ## remember this? # str_split_fixed(fruit, pattern = &quot; &quot;, n = 2) ## alternatives str_split_fixed(my_fruit, pattern = &quot;\\\\s&quot;, n = 2) [,1] [,2] [1,] &quot;breadfruit&quot; &quot;&quot; [2,] &quot;dragonfruit&quot; &quot;&quot; [3,] &quot;grapefruit&quot; &quot;&quot; [4,] &quot;jackfruit&quot; &quot;&quot; [5,] &quot;kiwi&quot; &quot;fruit&quot; [6,] &quot;passionfruit&quot; &quot;&quot; [7,] &quot;star&quot; &quot;fruit&quot; [8,] &quot;ugli&quot; &quot;fruit&quot; str_split_fixed(my_fruit, pattern = &quot;[[:space:]]&quot;, n = 2) [,1] [,2] [1,] &quot;breadfruit&quot; &quot;&quot; [2,] &quot;dragonfruit&quot; &quot;&quot; [3,] &quot;grapefruit&quot; &quot;&quot; [4,] &quot;jackfruit&quot; &quot;&quot; [5,] &quot;kiwi&quot; &quot;fruit&quot; [6,] &quot;passionfruit&quot; &quot;&quot; [7,] &quot;star&quot; &quot;fruit&quot; [8,] &quot;ugli&quot; &quot;fruit&quot; Let’s see the country names that contain punctuation. str_subset(countries, &quot;[[:punct:]]&quot;) [1] &quot;Congo, Dem. Rep.&quot; &quot;Congo, Rep.&quot; &quot;Cote d&#39;Ivoire&quot; &quot;Guinea-Bissau&quot; [5] &quot;Hong Kong, China&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; &quot;Yemen, Rep.&quot; 4.8.15 Quantifiers You can decorate characters (and other constructs, like metacharacters and classes) with information about how many characters they are allowed to match. quantifier meaning quantifier meaning * 0 or more {n} exactly n + 1 or more {n,} at least n ? 0 or 1 {,m} at most m {n,m} between n and m, inclusive Explore these by inspecting matches for l followed by e, allowing for various numbers of characters in between. l.*e will match strings with 0 or more characters in between, i.e. any string with an l eventually followed by an e. This is the most inclusive regex for this example, so we store the result as matches to use as a baseline for comparison. (matches &lt;- str_subset(fruit, pattern = &quot;l.*e&quot;)) [1] &quot;apple&quot; &quot;bell pepper&quot; &quot;bilberry&quot; [4] &quot;blackberry&quot; &quot;blood orange&quot; &quot;blueberry&quot; [7] &quot;cantaloupe&quot; &quot;chili pepper&quot; &quot;clementine&quot; [10] &quot;cloudberry&quot; &quot;elderberry&quot; &quot;huckleberry&quot; [13] &quot;lemon&quot; &quot;lime&quot; &quot;lychee&quot; [16] &quot;mulberry&quot; &quot;olive&quot; &quot;pineapple&quot; [19] &quot;purple mangosteen&quot; &quot;salal berry&quot; Change the quantifier from * to + to require at least one intervening character. The strings that no longer match: all have a literal le with no preceding l and no following e. list(match = intersect(matches, str_subset(fruit, pattern = &quot;l.+e&quot;)), no_match = setdiff(matches, str_subset(fruit, pattern = &quot;l.+e&quot;))) $match [1] &quot;bell pepper&quot; &quot;bilberry&quot; &quot;blackberry&quot; [4] &quot;blood orange&quot; &quot;blueberry&quot; &quot;cantaloupe&quot; [7] &quot;chili pepper&quot; &quot;clementine&quot; &quot;cloudberry&quot; [10] &quot;elderberry&quot; &quot;huckleberry&quot; &quot;lime&quot; [13] &quot;lychee&quot; &quot;mulberry&quot; &quot;olive&quot; [16] &quot;purple mangosteen&quot; &quot;salal berry&quot; $no_match [1] &quot;apple&quot; &quot;lemon&quot; &quot;pineapple&quot; Change the quantifier from * to ? to require at most one intervening character. In the strings that no longer match, the shortest gap between l and following e is at least two characters. list(match = intersect(matches, str_subset(fruit, pattern = &quot;l.?e&quot;)), no_match = setdiff(matches, str_subset(fruit, pattern = &quot;l.?e&quot;))) $match [1] &quot;apple&quot; &quot;bilberry&quot; &quot;blueberry&quot; [4] &quot;clementine&quot; &quot;elderberry&quot; &quot;huckleberry&quot; [7] &quot;lemon&quot; &quot;mulberry&quot; &quot;pineapple&quot; [10] &quot;purple mangosteen&quot; $no_match [1] &quot;bell pepper&quot; &quot;blackberry&quot; &quot;blood orange&quot; &quot;cantaloupe&quot; &quot;chili pepper&quot; [6] &quot;cloudberry&quot; &quot;lime&quot; &quot;lychee&quot; &quot;olive&quot; &quot;salal berry&quot; Finally, we remove the quantifier and allow for no intervening characters. The strings that no longer match lack a literal le. list(match = intersect(matches, str_subset(fruit, pattern = &quot;le&quot;)), no_match = setdiff(matches, str_subset(fruit, pattern = &quot;le&quot;))) $match [1] &quot;apple&quot; &quot;clementine&quot; &quot;huckleberry&quot; [4] &quot;lemon&quot; &quot;pineapple&quot; &quot;purple mangosteen&quot; $no_match [1] &quot;bell pepper&quot; &quot;bilberry&quot; &quot;blackberry&quot; &quot;blood orange&quot; &quot;blueberry&quot; [6] &quot;cantaloupe&quot; &quot;chili pepper&quot; &quot;cloudberry&quot; &quot;elderberry&quot; &quot;lime&quot; [11] &quot;lychee&quot; &quot;mulberry&quot; &quot;olive&quot; &quot;salal berry&quot; 4.8.16 Escaping You’ve probably caught on by now that there are certain characters with special meaning in regexes, including $ * + . ? [ ] ^ { } | ( ) \\. What if you really need the plus sign to be a literal plus sign and not a regex quantifier? You will need to escape it by prepending a backslash. But wait … there’s more! Before a regex is interpreted as a regular expression, it is also interpreted by R as a string. And backslash is used to escape there as well. So, in the end, you need to preprend two backslashes in order to match a literal plus sign in a regex. This will be more clear with examples! 4.8.16.1 Escapes in plain old strings Here is routine, non-regex use of backslash \\ escapes in plain vanilla R strings. We intentionally use cat() instead of print() here. To escape quotes inside quotes: cat(&quot;Do you use \\&quot;airquotes\\&quot; much?&quot;) Do you use &quot;airquotes&quot; much? Sidebar: eliminating the need for these escapes is exactly why people use double quotes inside single quotes and vice versa. To insert newline (\\n) or tab (\\t): cat(&quot;before the newline\\nafter the newline&quot;) before the newline after the newline cat(&quot;before the tab\\tafter the tab&quot;) before the tab after the tab 4.8.16.2 Escapes in regular expressions Examples of using escapes in regexes to match characters that would otherwise have a special interpretation. We know several gapminder country names contain a period. How do we isolate them? Although it’s tempting, this command str_subset(countries, pattern = &quot;.&quot;) won’t work! ## cheating using a POSIX class ;) str_subset(countries, pattern = &quot;[[:punct:]]&quot;) [1] &quot;Congo, Dem. Rep.&quot; &quot;Congo, Rep.&quot; &quot;Cote d&#39;Ivoire&quot; &quot;Guinea-Bissau&quot; [5] &quot;Hong Kong, China&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; &quot;Yemen, Rep.&quot; ## using two backslashes to escape the period str_subset(countries, pattern = &quot;\\\\.&quot;) [1] &quot;Congo, Dem. Rep.&quot; &quot;Congo, Rep.&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; [5] &quot;Yemen, Rep.&quot; A last example that matches an actual square bracket. (x &lt;- c(&quot;whatever&quot;, &quot;X is distributed U[0,1]&quot;)) [1] &quot;whatever&quot; &quot;X is distributed U[0,1]&quot; str_subset(x, pattern = &quot;\\\\[&quot;) [1] &quot;X is distributed U[0,1]&quot; 4.8.17 Groups and backreferences Your first use of regex is likely to be simple matching: detecting or isolating strings that match a pattern. But soon you will want to use regexes to transform the strings in character vectors. That means you need a way to address specific parts of the matching strings and to operate on them. You can use parentheses inside regexes to define groups and you can refer to those groups later with backreferences. For now, this lesson will refer you to other place to read up on this: STAT 545 2014 Intro to regular expressions by TA Gloria Li (Appendix ??). The [Strings chapter][r4ds-strings] of [R for Data Science][r4ds] (???). 4.9 Combining Data There are many ways to bring data together. Bind - This is basically smashing rocks tibbles together. You can smash things together row-wise (“row binding”) or column-wise (“column binding”). Why do I characterize this as rock-smashing? They’re often fairly crude operations, with lots of responsibility falling on the analyst for making sure that the whole enterprise even makes sense. When row binding, you need to consider the variables in the two tibbles. Do the same variables exist in each? Are they of the same type? Different approaches for row binding have different combinations of flexibility vs rigidity around these matters. When column binding, the onus is entirely on the analyst to make sure that the rows are aligned. I would avoid column binding whenever possible. If you can introduce new variables through any other, safer means, do so! By safer, I mean: use a mechanism where the row alignment is correct by definition. A proper join is the gold standard. In addition to joins, functions like dplyr::mutate() and tidyr::separate() can be very useful for forcing yourself to work inside the constraint of a tibble. Join - Here you designate a variable (or a combination of variables) as a key. A row in one data frame gets matched with a row in another data frame because they have the same key. You can then bring information from variables in a secondary data frame into a primary data frame based on this key-based lookup. That description is incredibly oversimplified, but that’s the basic idea. A variety of row- and column-wise operations fit into this framework, which implies there are many different flavors of join. The concepts and vocabulary around joins come from the database world. The relevant functions in dplyr follow this convention and all mention join. The most relevant base R function is merge(). Let’s explore each type of operation with a few examples. 4.9.1 Bind 4.9.1.1 Row binding We used word count data from the Lord of the Rings trilogy to explore the concept of tidy data. That kicked off with a quiet, successful row bind. Let’s revisit that. Here’s what a perfect row bind of three (untidy!) data frames looks like. fship &lt;- tribble( ~Film, ~Race, ~Female, ~Male, &quot;The Fellowship Of The Ring&quot;, &quot;Elf&quot;, 1229, 971, &quot;The Fellowship Of The Ring&quot;, &quot;Hobbit&quot;, 14, 3644, &quot;The Fellowship Of The Ring&quot;, &quot;Man&quot;, 0, 1995 ) rking &lt;- tribble( ~Film, ~Race, ~Female, ~Male, &quot;The Return Of The King&quot;, &quot;Elf&quot;, 183, 510, &quot;The Return Of The King&quot;, &quot;Hobbit&quot;, 2, 2673, &quot;The Return Of The King&quot;, &quot;Man&quot;, 268, 2459 ) ttow &lt;- tribble( ~Film, ~Race, ~Female, ~Male, &quot;The Two Towers&quot;, &quot;Elf&quot;, 331, 513, &quot;The Two Towers&quot;, &quot;Hobbit&quot;, 0, 2463, &quot;The Two Towers&quot;, &quot;Man&quot;, 401, 3589 ) (lotr_untidy &lt;- bind_rows(fship, ttow, rking)) # A tibble: 9 x 4 Film Race Female Male &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 The Fellowship Of The Ring Elf 1229 971 2 The Fellowship Of The Ring Hobbit 14 3644 3 The Fellowship Of The Ring Man 0 1995 4 The Two Towers Elf 331 513 5 The Two Towers Hobbit 0 2463 6 The Two Towers Man 401 3589 7 The Return Of The King Elf 183 510 8 The Return Of The King Hobbit 2 2673 9 The Return Of The King Man 268 2459 dplyr::bind_rows() works like a charm with these very row-bindable data frames! So does base rbind() (try it!). But what if one of the data frames is somehow missing a variable? Let’s mangle one and find out. ttow_no_Female &lt;- ttow %&gt;% mutate(Female = NULL) bind_rows(fship, ttow_no_Female, rking) # A tibble: 9 x 4 Film Race Female Male &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 The Fellowship Of The Ring Elf 1229 971 2 The Fellowship Of The Ring Hobbit 14 3644 3 The Fellowship Of The Ring Man 0 1995 4 The Two Towers Elf NA 513 5 The Two Towers Hobbit NA 2463 6 The Two Towers Man NA 3589 7 The Return Of The King Elf 183 510 8 The Return Of The King Hobbit 2 2673 9 The Return Of The King Man 268 2459 rbind(fship, ttow_no_Female, rking) Error in rbind(deparse.level, ...): numbers of columns of arguments do not match We see that dplyr::bind_rows() does the row bind and puts NA in for the missing values caused by the lack of Female data from The Two Towers. Base rbind() refuses to row bind in this situation. I invite you to experiment with other realistic, challenging scenarios, e.g.: Change the order of variables. Does row binding match variables by name or position? Row bind data frames where the variable x is of one type in one data frame and another type in the other. Try combinations that you think should work and some that should not. What actually happens? Row bind data frames in which the factor x has different levels in one data frame and different levels in the other. What happens? In conclusion, row binding usually works when it should (especially with dplyr::bind_rows()) and usually doesn’t when it shouldn’t. The biggest risk is being aggravated. 4.9.1.2 Column binding Column binding is much more dangerous because it often “works” when it should not. It’s your job to the rows are aligned and it’s all too easy to screw this up. The data in gapminder was originally excavated from 3 messy Excel spreadsheets: one each for life expectancy, population, and GDP per capital. Let’s relive some of the data wrangling joy and show a column bind gone wrong. I create 3 separate data frames, do some evil row sorting, then column bind. There are no errors. The result gapminder_garbage sort of looks OK. Univariate summary statistics and exploratory plots will look OK. But I’ve created complete nonsense! library(gapminder) life_exp &lt;- gapminder %&gt;% select(country, year, lifeExp) pop &lt;- gapminder %&gt;% arrange(year) %&gt;% select(pop) gdp_percap &lt;- gapminder %&gt;% arrange(pop) %&gt;% select(gdpPercap) (gapminder_garbage &lt;- bind_cols(life_exp, pop, gdp_percap)) # A tibble: 1,704 x 5 country year lifeExp pop gdpPercap &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan 1952 28.8 8425333 880. 2 Afghanistan 1957 30.3 1282697 861. 3 Afghanistan 1962 32.0 9279525 2670. 4 Afghanistan 1967 34.0 4232095 1072. 5 Afghanistan 1972 36.1 17876956 1385. 6 Afghanistan 1977 38.4 8691212 2865. 7 Afghanistan 1982 39.9 6927772 1533. 8 Afghanistan 1987 40.8 120447 1738. 9 Afghanistan 1992 41.7 46886859 3021. 10 Afghanistan 1997 41.8 8730405 1890. # … with 1,694 more rows summary(gapminder$lifeExp) Min. 1st Qu. Median Mean 3rd Qu. Max. 23.6 48.2 60.7 59.5 70.8 82.6 summary(gapminder_garbage$lifeExp) Min. 1st Qu. Median Mean 3rd Qu. Max. 23.6 48.2 60.7 59.5 70.8 82.6 range(gapminder$gdpPercap) [1] 241 113523 range(gapminder_garbage$gdpPercap) [1] 241 113523 One last cautionary tale about column binding. This one requires the use of cbind() and it’s why the tidyverse is generally unwilling to recycle when combining things of different length. I create a tibble with most of the gapminder columns. I create another with the remainder, but filtered down to just one country. I am able to cbind() these objects! Why? Because the 12 rows for Canada divide evenly into the 1704 rows of gapminder. Note that dplyr::bind_cols() refuses to column bind here. gapminder_mostly &lt;- gapminder %&gt;% select(-pop, -gdpPercap) gapminder_leftovers_filtered &lt;- gapminder %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(pop, gdpPercap) gapminder_nonsense &lt;- cbind(gapminder_mostly, gapminder_leftovers_filtered) head(gapminder_nonsense, 14) country continent year lifeExp pop gdpPercap 1 Afghanistan Asia 1952 28.8 14785584 11367 2 Afghanistan Asia 1957 30.3 17010154 12490 3 Afghanistan Asia 1962 32.0 18985849 13462 4 Afghanistan Asia 1967 34.0 20819767 16077 5 Afghanistan Asia 1972 36.1 22284500 18971 6 Afghanistan Asia 1977 38.4 23796400 22091 7 Afghanistan Asia 1982 39.9 25201900 22899 8 Afghanistan Asia 1987 40.8 26549700 26627 9 Afghanistan Asia 1992 41.7 28523502 26343 10 Afghanistan Asia 1997 41.8 30305843 28955 11 Afghanistan Asia 2002 42.1 31902268 33329 12 Afghanistan Asia 2007 43.8 33390141 36319 13 Albania Europe 1952 55.2 14785584 11367 14 Albania Europe 1957 59.3 17010154 12490 This data frame isn’t obviously wrong, but it is wrong. See how the Canada’s population and GDP per capita repeat for each country? Bottom line: Row bind when you need to, but inspect the results re: coercion. Column bind only if you must and be extremely paranoid. 4.9.2 Joins in dplyr Visit Chapter ?? to see concrete examples of all the joins implemented in dplyr, based on comic characters and publishers. The most recent release of gapminder includes a new data frame, country_codes, with country names and ISO codes. Therefore you can also use it to practice joins. gapminder %&gt;% select(country, continent) %&gt;% group_by(country) %&gt;% slice(1) %&gt;% left_join(country_codes) Joining, by = &quot;country&quot; # A tibble: 142 x 4 # Groups: country [142] country continent iso_alpha iso_num &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; 1 Afghanistan Asia AFG 4 2 Albania Europe ALB 8 3 Algeria Africa DZA 12 4 Angola Africa AGO 24 5 Argentina Americas ARG 32 6 Australia Oceania AUS 36 7 Austria Europe AUT 40 8 Bahrain Asia BHR 48 9 Bangladesh Asia BGD 50 10 Belgium Europe BEL 56 # … with 132 more rows 4.9.3 Joining Join (a.k.a. merge) two tables: dplyr join cheatsheet with comic characters and publishers. Other great places to read about joins: The dplyr vignette on [Two-table verbs][dplyr-vignette-two-table]. The [Relational data chapter][r4ds-relational-data] in [R for Data Science][r4ds] (???). Excellent diagrams. 4.9.3.1 The data Working with two small data frames: superheroes and publishers. library(tidyverse) ## dplyr provides the join functions superheroes &lt;- tibble::tribble( ~name, ~alignment, ~gender, ~publisher, &quot;Magneto&quot;, &quot;bad&quot;, &quot;male&quot;, &quot;Marvel&quot;, &quot;Storm&quot;, &quot;good&quot;, &quot;female&quot;, &quot;Marvel&quot;, &quot;Mystique&quot;, &quot;bad&quot;, &quot;female&quot;, &quot;Marvel&quot;, &quot;Batman&quot;, &quot;good&quot;, &quot;male&quot;, &quot;DC&quot;, &quot;Joker&quot;, &quot;bad&quot;, &quot;male&quot;, &quot;DC&quot;, &quot;Catwoman&quot;, &quot;bad&quot;, &quot;female&quot;, &quot;DC&quot;, &quot;Hellboy&quot;, &quot;good&quot;, &quot;male&quot;, &quot;Dark Horse Comics&quot; ) publishers &lt;- tibble::tribble( ~publisher, ~yr_founded, &quot;DC&quot;, 1934L, &quot;Marvel&quot;, 1939L, &quot;Image&quot;, 1992L ) Sorry, cheat sheet does not illustrate “multiple match” situations terribly well. Sub-plot: watch the row and variable order of the join results for a healthy reminder of why it’s dangerous to rely on any of that in an analysis. 4.9.3.2 inner_join(superheroes, publishers) inner_join(x, y): Return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. This is a mutating join. (ijsp &lt;- inner_join(superheroes, publishers)) Joining, by = &quot;publisher&quot; # A tibble: 6 x 5 name alignment gender publisher yr_founded &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Magneto bad male Marvel 1939 2 Storm good female Marvel 1939 3 Mystique bad female Marvel 1939 4 Batman good male DC 1934 5 Joker bad male DC 1934 6 Catwoman bad female DC 1934 We lose Hellboy in the join because, although he appears in x = superheroes, his publisher Dark Horse Comics does not appear in y = publishers. The join result has all variables from x = superheroes plus yr_founded, from y. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #camhpegnvd .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #camhpegnvd .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #camhpegnvd .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #camhpegnvd .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #camhpegnvd .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #camhpegnvd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #camhpegnvd .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #camhpegnvd .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #camhpegnvd .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #camhpegnvd .gt_sep_right { border-right: 5px solid #FFFFFF; } #camhpegnvd .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #camhpegnvd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #camhpegnvd .gt_striped { background-color: #8080800D; } #camhpegnvd .gt_from_md > :first-child { margin-top: 0; } #camhpegnvd .gt_from_md > :last-child { margin-bottom: 0; } #camhpegnvd .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #camhpegnvd .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #camhpegnvd .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #camhpegnvd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #camhpegnvd .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #camhpegnvd .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #camhpegnvd .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #camhpegnvd .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #camhpegnvd .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #camhpegnvd .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #camhpegnvd .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #camhpegnvd .gt_center { text-align: center; } #camhpegnvd .gt_left { text-align: left; } #camhpegnvd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #camhpegnvd .gt_font_normal { font-weight: normal; } #camhpegnvd .gt_font_bold { font-weight: bold; } #camhpegnvd .gt_font_italic { font-style: italic; } #camhpegnvd .gt_super { font-size: 65%; } #camhpegnvd .gt_footnote_marks { font-style: italic; font-size: 65%; } inner_join(x = superheroes, y = publishers) name alignment gender publisher yr_founded Magneto bad male Marvel 1939 Storm good female Marvel 1939 Mystique bad female Marvel 1939 Batman good male DC 1934 Joker bad male DC 1934 Catwoman bad female DC 1934 4.9.3.3 semi_join(superheroes, publishers) semi_join(x, y): Return all rows from x where there are matching values in y, keeping just columns from x. A semi join differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x. This is a filtering join. (sjsp &lt;- semi_join(superheroes, publishers)) Joining, by = &quot;publisher&quot; # A tibble: 6 x 4 name alignment gender publisher &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Magneto bad male Marvel 2 Storm good female Marvel 3 Mystique bad female Marvel 4 Batman good male DC 5 Joker bad male DC 6 Catwoman bad female DC We get a similar result as with inner_join() but the join result contains only the variables originally found in x = superheroes. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qcljwltoaf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #qcljwltoaf .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #qcljwltoaf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qcljwltoaf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #qcljwltoaf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #qcljwltoaf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #qcljwltoaf .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #qcljwltoaf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qcljwltoaf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qcljwltoaf .gt_sep_right { border-right: 5px solid #FFFFFF; } #qcljwltoaf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #qcljwltoaf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #qcljwltoaf .gt_striped { background-color: #8080800D; } #qcljwltoaf .gt_from_md > :first-child { margin-top: 0; } #qcljwltoaf .gt_from_md > :last-child { margin-bottom: 0; } #qcljwltoaf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qcljwltoaf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qcljwltoaf .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #qcljwltoaf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #qcljwltoaf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qcljwltoaf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qcljwltoaf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #qcljwltoaf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #qcljwltoaf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #qcljwltoaf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #qcljwltoaf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #qcljwltoaf .gt_center { text-align: center; } #qcljwltoaf .gt_left { text-align: left; } #qcljwltoaf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qcljwltoaf .gt_font_normal { font-weight: normal; } #qcljwltoaf .gt_font_bold { font-weight: bold; } #qcljwltoaf .gt_font_italic { font-style: italic; } #qcljwltoaf .gt_super { font-size: 65%; } #qcljwltoaf .gt_footnote_marks { font-style: italic; font-size: 65%; } semi_join(x = superheroes, y = publishers) name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC 4.9.3.4 left_join(superheroes, publishers) left_join(x, y): Return all rows from x, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. This is a mutating join. (ljsp &lt;- left_join(superheroes, publishers)) Joining, by = &quot;publisher&quot; # A tibble: 7 x 5 name alignment gender publisher yr_founded &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Magneto bad male Marvel 1939 2 Storm good female Marvel 1939 3 Mystique bad female Marvel 1939 4 Batman good male DC 1934 5 Joker bad male DC 1934 6 Catwoman bad female DC 1934 7 Hellboy good male Dark Horse Comics NA We basically get x = superheroes back, but with the addition of variable yr_founded, which is unique to y = publishers. Hellboy, whose publisher does not appear in y = publishers, has an NA for yr_founded. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rdqezhpcpv .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #rdqezhpcpv .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #rdqezhpcpv .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rdqezhpcpv .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #rdqezhpcpv .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #rdqezhpcpv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #rdqezhpcpv .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #rdqezhpcpv .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rdqezhpcpv .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rdqezhpcpv .gt_sep_right { border-right: 5px solid #FFFFFF; } #rdqezhpcpv .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #rdqezhpcpv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #rdqezhpcpv .gt_striped { background-color: #8080800D; } #rdqezhpcpv .gt_from_md > :first-child { margin-top: 0; } #rdqezhpcpv .gt_from_md > :last-child { margin-bottom: 0; } #rdqezhpcpv .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rdqezhpcpv .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rdqezhpcpv .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #rdqezhpcpv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #rdqezhpcpv .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rdqezhpcpv .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rdqezhpcpv .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #rdqezhpcpv .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #rdqezhpcpv .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #rdqezhpcpv .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #rdqezhpcpv .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #rdqezhpcpv .gt_center { text-align: center; } #rdqezhpcpv .gt_left { text-align: left; } #rdqezhpcpv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rdqezhpcpv .gt_font_normal { font-weight: normal; } #rdqezhpcpv .gt_font_bold { font-weight: bold; } #rdqezhpcpv .gt_font_italic { font-style: italic; } #rdqezhpcpv .gt_super { font-size: 65%; } #rdqezhpcpv .gt_footnote_marks { font-style: italic; font-size: 65%; } left_join(x = superheroes, y = publishers) name alignment gender publisher yr_founded Magneto bad male Marvel 1939 Storm good female Marvel 1939 Mystique bad female Marvel 1939 Batman good male DC 1934 Joker bad male DC 1934 Catwoman bad female DC 1934 Hellboy good male Dark Horse Comics NA 4.9.3.5 anti_join(superheroes, publishers) anti_join(x, y): Return all rows from x where there are not matching values in y, keeping just columns from x. This is a filtering join. (ajsp &lt;- anti_join(superheroes, publishers)) Joining, by = &quot;publisher&quot; # A tibble: 1 x 4 name alignment gender publisher &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Hellboy good male Dark Horse Comics We keep only Hellboy now (and do not get yr_founded). html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ubdkzzhsuq .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #ubdkzzhsuq .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #ubdkzzhsuq .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ubdkzzhsuq .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #ubdkzzhsuq .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #ubdkzzhsuq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #ubdkzzhsuq .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #ubdkzzhsuq .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ubdkzzhsuq .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubdkzzhsuq .gt_sep_right { border-right: 5px solid #FFFFFF; } #ubdkzzhsuq .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #ubdkzzhsuq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #ubdkzzhsuq .gt_striped { background-color: #8080800D; } #ubdkzzhsuq .gt_from_md > :first-child { margin-top: 0; } #ubdkzzhsuq .gt_from_md > :last-child { margin-bottom: 0; } #ubdkzzhsuq .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ubdkzzhsuq .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ubdkzzhsuq .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #ubdkzzhsuq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #ubdkzzhsuq .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ubdkzzhsuq .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ubdkzzhsuq .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #ubdkzzhsuq .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #ubdkzzhsuq .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #ubdkzzhsuq .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #ubdkzzhsuq .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #ubdkzzhsuq .gt_center { text-align: center; } #ubdkzzhsuq .gt_left { text-align: left; } #ubdkzzhsuq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ubdkzzhsuq .gt_font_normal { font-weight: normal; } #ubdkzzhsuq .gt_font_bold { font-weight: bold; } #ubdkzzhsuq .gt_font_italic { font-style: italic; } #ubdkzzhsuq .gt_super { font-size: 65%; } #ubdkzzhsuq .gt_footnote_marks { font-style: italic; font-size: 65%; } anti_join(x = superheroes, y = publishers) name alignment gender publisher Hellboy good male Dark Horse Comics 4.9.3.6 inner_join(publishers, superheroes) inner_join(x, y): Return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. This is a mutating join. (ijps &lt;- inner_join(publishers, superheroes)) Joining, by = &quot;publisher&quot; # A tibble: 6 x 5 publisher yr_founded name alignment gender &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 DC 1934 Batman good male 2 DC 1934 Joker bad male 3 DC 1934 Catwoman bad female 4 Marvel 1939 Magneto bad male 5 Marvel 1939 Storm good female 6 Marvel 1939 Mystique bad female In a way, this does illustrate multiple matches, if you think about it from the x = publishers direction. Every publisher that has a match in y = superheroes appears multiple times in the result, once for each match. In fact, we’re getting the same result as with inner_join(superheroes, publishers), up to variable order (which you should also never rely on in an analysis). html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xbmpcyevuq .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #xbmpcyevuq .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #xbmpcyevuq .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xbmpcyevuq .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #xbmpcyevuq .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #xbmpcyevuq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #xbmpcyevuq .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #xbmpcyevuq .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xbmpcyevuq .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xbmpcyevuq .gt_sep_right { border-right: 5px solid #FFFFFF; } #xbmpcyevuq .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #xbmpcyevuq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #xbmpcyevuq .gt_striped { background-color: #8080800D; } #xbmpcyevuq .gt_from_md > :first-child { margin-top: 0; } #xbmpcyevuq .gt_from_md > :last-child { margin-bottom: 0; } #xbmpcyevuq .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xbmpcyevuq .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xbmpcyevuq .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #xbmpcyevuq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #xbmpcyevuq .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xbmpcyevuq .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xbmpcyevuq .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #xbmpcyevuq .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #xbmpcyevuq .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #xbmpcyevuq .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #xbmpcyevuq .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #xbmpcyevuq .gt_center { text-align: center; } #xbmpcyevuq .gt_left { text-align: left; } #xbmpcyevuq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xbmpcyevuq .gt_font_normal { font-weight: normal; } #xbmpcyevuq .gt_font_bold { font-weight: bold; } #xbmpcyevuq .gt_font_italic { font-style: italic; } #xbmpcyevuq .gt_super { font-size: 65%; } #xbmpcyevuq .gt_footnote_marks { font-style: italic; font-size: 65%; } inner_join(x = publishers, y = superheroes) publisher yr_founded name alignment gender DC 1934 Batman good male DC 1934 Joker bad male DC 1934 Catwoman bad female Marvel 1939 Magneto bad male Marvel 1939 Storm good female Marvel 1939 Mystique bad female 4.9.3.7 semi_join(publishers, superheroes) semi_join(x, y): Return all rows from x where there are matching values in y, keeping just columns from x. A semi join differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x. This is a filtering join. (sjps &lt;- semi_join(x = publishers, y = superheroes)) Joining, by = &quot;publisher&quot; # A tibble: 2 x 2 publisher yr_founded &lt;chr&gt; &lt;int&gt; 1 DC 1934 2 Marvel 1939 Now the effects of switching the x and y roles is more clear. The result resembles x = publishers, but the publisher Image is lost, because there are no observations where publisher == &quot;Image&quot; in y = superheroes. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ysmfekwiwl .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #ysmfekwiwl .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #ysmfekwiwl .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ysmfekwiwl .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #ysmfekwiwl .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #ysmfekwiwl .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #ysmfekwiwl .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #ysmfekwiwl .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ysmfekwiwl .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ysmfekwiwl .gt_sep_right { border-right: 5px solid #FFFFFF; } #ysmfekwiwl .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #ysmfekwiwl .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #ysmfekwiwl .gt_striped { background-color: #8080800D; } #ysmfekwiwl .gt_from_md > :first-child { margin-top: 0; } #ysmfekwiwl .gt_from_md > :last-child { margin-bottom: 0; } #ysmfekwiwl .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ysmfekwiwl .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ysmfekwiwl .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #ysmfekwiwl .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #ysmfekwiwl .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ysmfekwiwl .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ysmfekwiwl .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #ysmfekwiwl .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #ysmfekwiwl .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #ysmfekwiwl .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #ysmfekwiwl .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #ysmfekwiwl .gt_center { text-align: center; } #ysmfekwiwl .gt_left { text-align: left; } #ysmfekwiwl .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ysmfekwiwl .gt_font_normal { font-weight: normal; } #ysmfekwiwl .gt_font_bold { font-weight: bold; } #ysmfekwiwl .gt_font_italic { font-style: italic; } #ysmfekwiwl .gt_super { font-size: 65%; } #ysmfekwiwl .gt_footnote_marks { font-style: italic; font-size: 65%; } semi_join(x = publishers, y = superheroes) publisher yr_founded DC 1934 Marvel 1939 4.9.3.8 left_join(publishers, superheroes) left_join(x, y): Return all rows from x, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. This is a mutating join. (ljps &lt;- left_join(publishers, superheroes)) Joining, by = &quot;publisher&quot; # A tibble: 7 x 5 publisher yr_founded name alignment gender &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 DC 1934 Batman good male 2 DC 1934 Joker bad male 3 DC 1934 Catwoman bad female 4 Marvel 1939 Magneto bad male 5 Marvel 1939 Storm good female 6 Marvel 1939 Mystique bad female 7 Image 1992 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; We get a similar result as with inner_join() but the publisher Image survives in the join, even though no superheroes from Image appear in y = superheroes. As a result, Image has NAs for name, alignment, and gender. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #awycpkgpsy .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #awycpkgpsy .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #awycpkgpsy .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #awycpkgpsy .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #awycpkgpsy .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #awycpkgpsy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #awycpkgpsy .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #awycpkgpsy .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #awycpkgpsy .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #awycpkgpsy .gt_sep_right { border-right: 5px solid #FFFFFF; } #awycpkgpsy .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #awycpkgpsy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #awycpkgpsy .gt_striped { background-color: #8080800D; } #awycpkgpsy .gt_from_md > :first-child { margin-top: 0; } #awycpkgpsy .gt_from_md > :last-child { margin-bottom: 0; } #awycpkgpsy .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #awycpkgpsy .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #awycpkgpsy .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #awycpkgpsy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #awycpkgpsy .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #awycpkgpsy .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #awycpkgpsy .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #awycpkgpsy .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #awycpkgpsy .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #awycpkgpsy .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #awycpkgpsy .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #awycpkgpsy .gt_center { text-align: center; } #awycpkgpsy .gt_left { text-align: left; } #awycpkgpsy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #awycpkgpsy .gt_font_normal { font-weight: normal; } #awycpkgpsy .gt_font_bold { font-weight: bold; } #awycpkgpsy .gt_font_italic { font-style: italic; } #awycpkgpsy .gt_super { font-size: 65%; } #awycpkgpsy .gt_footnote_marks { font-style: italic; font-size: 65%; } left_join(x = publishers, y = superheroes) publisher yr_founded name alignment gender DC 1934 Batman good male DC 1934 Joker bad male DC 1934 Catwoman bad female Marvel 1939 Magneto bad male Marvel 1939 Storm good female Marvel 1939 Mystique bad female Image 1992 NA NA NA 4.9.3.9 anti_join(publishers, superheroes) anti_join(x, y): Return all rows from x where there are not matching values in y, keeping just columns from x. This is a filtering join. (ajps &lt;- anti_join(publishers, superheroes)) Joining, by = &quot;publisher&quot; # A tibble: 1 x 2 publisher yr_founded &lt;chr&gt; &lt;int&gt; 1 Image 1992 We keep only publisher Image now (and the variables found in x = publishers). html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zjelhkdshm .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #zjelhkdshm .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #zjelhkdshm .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zjelhkdshm .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #zjelhkdshm .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #zjelhkdshm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #zjelhkdshm .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #zjelhkdshm .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zjelhkdshm .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zjelhkdshm .gt_sep_right { border-right: 5px solid #FFFFFF; } #zjelhkdshm .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #zjelhkdshm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #zjelhkdshm .gt_striped { background-color: #8080800D; } #zjelhkdshm .gt_from_md > :first-child { margin-top: 0; } #zjelhkdshm .gt_from_md > :last-child { margin-bottom: 0; } #zjelhkdshm .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zjelhkdshm .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zjelhkdshm .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #zjelhkdshm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #zjelhkdshm .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zjelhkdshm .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zjelhkdshm .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #zjelhkdshm .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #zjelhkdshm .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #zjelhkdshm .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #zjelhkdshm .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #zjelhkdshm .gt_center { text-align: center; } #zjelhkdshm .gt_left { text-align: left; } #zjelhkdshm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zjelhkdshm .gt_font_normal { font-weight: normal; } #zjelhkdshm .gt_font_bold { font-weight: bold; } #zjelhkdshm .gt_font_italic { font-style: italic; } #zjelhkdshm .gt_super { font-size: 65%; } #zjelhkdshm .gt_footnote_marks { font-style: italic; font-size: 65%; } anti_join(x = publishers, y = superheroes) publisher yr_founded Image 1992 4.9.3.10 full_join(superheroes, publishers) full_join(x, y): Return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing. This is a mutating join. (fjsp &lt;- full_join(superheroes, publishers)) Joining, by = &quot;publisher&quot; # A tibble: 8 x 5 name alignment gender publisher yr_founded &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Magneto bad male Marvel 1939 2 Storm good female Marvel 1939 3 Mystique bad female Marvel 1939 4 Batman good male DC 1934 5 Joker bad male DC 1934 6 Catwoman bad female DC 1934 7 Hellboy good male Dark Horse Comics NA 8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Image 1992 We get all rows of x = superheroes plus a new row from y = publishers, containing the publisher Image. We get all variables from x = superheroes AND all variables from y = publishers. Any row that derives solely from one table or the other carries NAs in the variables found only in the other table. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #eamrjpowlu .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #edc7fc; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #eamrjpowlu .gt_heading { background-color: #edc7fc; /* heading.background.color */ border-bottom-color: #edc7fc; } #eamrjpowlu .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #edc7fc; border-bottom-width: 0; } #eamrjpowlu .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #edc7fc; border-top-width: 0; } #eamrjpowlu .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #eamrjpowlu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #eamrjpowlu .gt_col_heading { color: #333333; background-color: #edc7fc; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #eamrjpowlu .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #eamrjpowlu .gt_sep_right { border-right: 5px solid #edc7fc; } #eamrjpowlu .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #edc7fc; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #eamrjpowlu .gt_striped { background-color: #8080800D; } #eamrjpowlu .gt_from_md > :first-child { margin-top: 0; } #eamrjpowlu .gt_from_md > :last-child { margin-bottom: 0; } #eamrjpowlu .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #eamrjpowlu .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #eamrjpowlu .gt_summary_row { color: #333333; background-color: #edc7fc; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #eamrjpowlu .gt_grand_summary_row { color: #333333; background-color: #edc7fc; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #eamrjpowlu .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #eamrjpowlu .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #eamrjpowlu .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #eamrjpowlu .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #eamrjpowlu .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #eamrjpowlu .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #eamrjpowlu .gt_center { text-align: center; } #eamrjpowlu .gt_left { text-align: left; } #eamrjpowlu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #eamrjpowlu .gt_font_normal { font-weight: normal; } #eamrjpowlu .gt_font_bold { font-weight: bold; } #eamrjpowlu .gt_font_italic { font-style: italic; } #eamrjpowlu .gt_super { font-size: 65%; } #eamrjpowlu .gt_footnote_marks { font-style: italic; font-size: 65%; } superheroes name alignment gender publisher Magneto bad male Marvel Storm good female Marvel Mystique bad female Marvel Batman good male DC Joker bad male DC Catwoman bad female DC Hellboy good male Dark Horse Comics html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nfljxnmvdf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #cce6f6; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #nfljxnmvdf .gt_heading { background-color: #cce6f6; /* heading.background.color */ border-bottom-color: #cce6f6; } #nfljxnmvdf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #cce6f6; border-bottom-width: 0; } #nfljxnmvdf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #cce6f6; border-top-width: 0; } #nfljxnmvdf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #nfljxnmvdf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #nfljxnmvdf .gt_col_heading { color: #333333; background-color: #cce6f6; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #nfljxnmvdf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfljxnmvdf .gt_sep_right { border-right: 5px solid #cce6f6; } #nfljxnmvdf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #cce6f6; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #nfljxnmvdf .gt_striped { background-color: #8080800D; } #nfljxnmvdf .gt_from_md > :first-child { margin-top: 0; } #nfljxnmvdf .gt_from_md > :last-child { margin-bottom: 0; } #nfljxnmvdf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfljxnmvdf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nfljxnmvdf .gt_summary_row { color: #333333; background-color: #cce6f6; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #nfljxnmvdf .gt_grand_summary_row { color: #333333; background-color: #cce6f6; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #nfljxnmvdf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfljxnmvdf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #nfljxnmvdf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #nfljxnmvdf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #nfljxnmvdf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #nfljxnmvdf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #nfljxnmvdf .gt_center { text-align: center; } #nfljxnmvdf .gt_left { text-align: left; } #nfljxnmvdf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfljxnmvdf .gt_font_normal { font-weight: normal; } #nfljxnmvdf .gt_font_bold { font-weight: bold; } #nfljxnmvdf .gt_font_italic { font-style: italic; } #nfljxnmvdf .gt_super { font-size: 65%; } #nfljxnmvdf .gt_footnote_marks { font-style: italic; font-size: 65%; } publishers publisher yr_founded DC 1934 Marvel 1939 Image 1992 html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #iozkpyzctf .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 80%; background-color: #FFFFFF; /* table.background.color */ width: 100%; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #iozkpyzctf .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; } #iozkpyzctf .gt_title { color: #333333; font-size: 90%; /* heading.title.font.size */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #iozkpyzctf .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; border-top-width: 0; } #iozkpyzctf .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #iozkpyzctf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #iozkpyzctf .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 90%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #iozkpyzctf .gt_columns_top_border { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #iozkpyzctf .gt_columns_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #iozkpyzctf .gt_sep_right { border-right: 5px solid #FFFFFF; } #iozkpyzctf .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #iozkpyzctf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 16px; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #iozkpyzctf .gt_striped { background-color: #8080800D; } #iozkpyzctf .gt_from_md > :first-child { margin-top: 0; } #iozkpyzctf .gt_from_md > :last-child { margin-bottom: 0; } #iozkpyzctf .gt_row { padding: 8px; /* row.padding */ margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #iozkpyzctf .gt_stub { border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #iozkpyzctf .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ padding: 8px; /* summary_row.padding */ text-transform: inherit; /* summary_row.text_transform */ } #iozkpyzctf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ padding: 8px; /* grand_summary_row.padding */ text-transform: inherit; /* grand_summary_row.text_transform */ } #iozkpyzctf .gt_first_summary_row { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #iozkpyzctf .gt_first_grand_summary_row { border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #iozkpyzctf .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #iozkpyzctf .gt_footnotes { border-top-style: solid; /* footnotes.border.top.style */ border-top-width: 2px; /* footnotes.border.top.width */ border-top-color: #D3D3D3; /* footnotes.border.top.color */ } #iozkpyzctf .gt_footnote { font-size: 90%; /* footnote.font.size */ margin: 0px; padding: 4px; /* footnote.padding */ } #iozkpyzctf .gt_sourcenotes { border-top-style: solid; /* sourcenotes.border.top.style */ border-top-width: 2px; /* sourcenotes.border.top.width */ border-top-color: #D3D3D3; /* sourcenotes.border.top.color */ } #iozkpyzctf .gt_sourcenote { font-size: 90%; /* sourcenote.font.size */ padding: 4px; /* sourcenote.padding */ } #iozkpyzctf .gt_center { text-align: center; } #iozkpyzctf .gt_left { text-align: left; } #iozkpyzctf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #iozkpyzctf .gt_font_normal { font-weight: normal; } #iozkpyzctf .gt_font_bold { font-weight: bold; } #iozkpyzctf .gt_font_italic { font-style: italic; } #iozkpyzctf .gt_super { font-size: 65%; } #iozkpyzctf .gt_footnote_marks { font-style: italic; font-size: 65%; } full_join(x = superheroes, y = publishers) name alignment gender publisher yr_founded Magneto bad male Marvel 1939 Storm good female Marvel 1939 Mystique bad female Marvel 1939 Batman good male DC 1934 Joker bad male DC 1934 Catwoman bad female DC 1934 Hellboy good male Dark Horse Comics NA NA NA NA Image 1992 4.9.3.11 join data frames “Joining” or “merging” two different datasets is tricky stuff. Let’s go through some more examples while reviewing the basic concepts. In the flights data frame, the variable carrier lists the carrier code for the different flights. While the corresponding airline names for &quot;UA&quot; and &quot;AA&quot; might be somewhat easy to guess (United and American Airlines), what airlines have codes &quot;VX&quot;, &quot;HA&quot;, and &quot;B6&quot;? This information is provided in a separate data frame airlines. View(airlines) We see that in airports, carrier is the carrier code, while name is the full name of the airline company. Using this table, we can see that &quot;VX&quot;, &quot;HA&quot;, and &quot;B6&quot; correspond to Virgin America, Hawaiian Airlines, and JetBlue, respectively. However, wouldn’t it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by “joining” the flights and airlines data frames. Note that the values in the variable carrier in the flights data frame match the values in the variable carrier in the airlines data frame. In this case, we can use the variable carrier as a key variable to match the rows of the two data frames. Key variables are almost always identification variables that uniquely identify the observational units as we saw in Subsection 1.4.4. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett (Grolemund and Wickham 2017) created the diagram shown in Figure 4.8 to help us understand how the different data frames in the nycflights13 package are linked by various key variables: FIGURE 4.8: Data relationships in nycflights13 from R for Data Science. 4.9.4 Matching “key” variable names In both the flights and airlines data frames, the key variable we want to join/merge/match the rows by has the same name: carrier. Let’s use the inner_join() function to join the two data frames, where the rows will be matched by the variable carrier, and then compare the resulting data frames: flights_joined &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(flights) View(flights_joined) Observe that the flights and flights_joined data frames are identical except that flights_joined has an additional variable name. The values of name correspond to the airline companies’ names as indicated in the airlines data frame. A visual representation of the inner_join() is shown in Figure 4.9 (Grolemund and Wickham 2017). There are other types of joins available (such as left_join(), right_join(), outer_join(), and anti_join()), but the inner_join() will solve nearly all of the problems you’ll encounter in this book. FIGURE 4.9: Diagram of inner join from R for Data Science. 4.9.5 Different “key” variable names Say instead you are interested in the destinations of all domestic flights departing NYC in 2013, and you ask yourself questions like: “What cities are these airports in?”, or “Is &quot;ORD&quot; Orlando?”, or “Where is &quot;FLL&quot;?”. The airports data frame contains the airport codes for each airport: View(airports) However, if you look at both the airports and flights data frames, you’ll find that the airport codes are in variables that have different names. In airports the airport code is in faa, whereas in flights the airport codes are in origin and dest. This fact is further highlighted in the visual representation of the relationships between these data frames in Figure 4.8. In order to join these two data frames by airport code, our inner_join() operation will use the by = c(&quot;dest&quot; = &quot;faa&quot;) argument with modified code syntax allowing us to join two data frames where the key variable has a different name: flights_with_airport_names &lt;- flights %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) View(flights_with_airport_names) Let’s construct the chain of pipe operators %&gt;% that computes the number of flights from NYC to each destination, but also includes information about each destination airport: named_dests &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) %&gt;% arrange(desc(num_flights)) %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% rename(airport_name = name) named_dests # A tibble: 101 x 9 dest num_flights airport_name lat lon alt tz dst tzone &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 ORD 17283 Chicago Ohare Intl 42.0 -87.9 668 -6 A America… 2 ATL 17215 Hartsfield Jackson… 33.6 -84.4 1026 -5 A America… 3 LAX 16174 Los Angeles Intl 33.9 -118. 126 -8 A America… 4 BOS 15508 General Edward Law… 42.4 -71.0 19 -5 A America… 5 MCO 14082 Orlando Intl 28.4 -81.3 96 -5 A America… 6 CLT 14064 Charlotte Douglas … 35.2 -80.9 748 -5 A America… 7 SFO 13331 San Francisco Intl 37.6 -122. 13 -8 A America… 8 FLL 12055 Fort Lauderdale Ho… 26.1 -80.2 9 -5 A America… 9 MIA 11728 Miami Intl 25.8 -80.3 8 -5 A America… 10 DCA 9705 Ronald Reagan Wash… 38.9 -77.0 15 -5 A America… # … with 91 more rows In case you didn’t know, &quot;ORD&quot; is the airport code of Chicago O’Hare airport and &quot;FLL&quot; is the main airport in Fort Lauderdale, Florida, which can be seen in the airport_name variable. 4.9.6 Multiple “key” variables Say instead we want to join two data frames by multiple key variables. For example, in Figure 4.8, we see that in order to join the flights and weather data frames, we need more than one key variable: year, month, day, hour, and origin. This is because the combination of these 5 variables act to uniquely identify each observational unit in the weather data frame: hourly weather recordings at each of the 3 NYC airports. We achieve this by specifying a vector of key variables to join by using the c() function. Recall from Subsection 1.2.1 that c() is short for “combine” or “concatenate.” flights_weather_joined &lt;- flights %&gt;% inner_join(weather, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;)) View(flights_weather_joined) 4.9.7 Normal forms The data frames included in the nycflights13 package are in a form that minimizes redundancy of data. For example, the flights data frame only saves the carrier code of the airline company; it does not include the actual name of the airline. For example, the first row of flights has carrier equal to UA, but it does not include the airline name of “United Air Lines Inc.” The names of the airline companies are included in the name variable of the airlines data frame. In order to have the airline company name included in flights, we could join these two data frames as follows: joined_flights &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(joined_flights) We are capable of performing this join because each of the data frames have keys in common to relate one to another: the carrier variable in both the flights and airlines data frames. The key variable(s) that we base our joins on are often identification variables as we mentioned previously. This is an important property of what’s known as normal forms of data. The process of decomposing data frames into less redundant tables without losing information is called normalization. More information is available on Wikipedia. Both dplyr and SQL we mentioned in the introduction of this chapter use such normal forms. Given that they share such commonalities, once you learn either of these two tools, you can learn the other very easily. 4.10 Other Verbs Here are some other useful data wrangling verbs: select() only a subset of variables/columns. rename() variables/columns to have new names. Return only the top_n() values of a variable. slice() and pull() specific rows and columns. 4.10.1 select variables FIGURE 4.10: Diagram of select() columns. We’ve seen that the flights data frame in the nycflights13 package contains 19 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(flights) However, say you only need two of these 19 variables, say carrier and flight. You can select() these two variables: flights %&gt;% select(carrier, flight) This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we select() only a smaller number of variables as is shown in Figure 4.10, it will make viewing the dataset in RStudio’s spreadsheet viewer more digestible. Let’s say instead you want to drop, or de-select, certain variables. For example, consider the variable year in the flights data frame. This variable isn’t quite a “variable” because it is always 2013 and hence doesn’t change. Say you want to remove this variable from the data frame. We can deselect year by using the - sign: flights_no_year &lt;- flights %&gt;% select(-year) Another way of selecting columns/variables is by specifying a range of columns: flight_arr_times &lt;- flights %&gt;% select(month:day, arr_time:sched_arr_time) flight_arr_times This will select() all columns between month and day, as well as between arr_time and sched_arr_time, and drop the rest. The select() function can also be used to reorder columns when used with the everything() helper function. For example, suppose we want the hour, minute, and time_hour variables to appear immediately after the year, month, and day variables, while not discarding the rest of the variables. In the following code, everything() will pick up all remaining variables: flights_reorder &lt;- flights %&gt;% select(year, month, day, hour, minute, time_hour, everything()) glimpse(flights_reorder) Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/columns that match those conditions. As examples, flights %&gt;% select(starts_with(&quot;a&quot;)) flights %&gt;% select(ends_with(&quot;delay&quot;)) flights %&gt;% select(contains(&quot;time&quot;)) 4.10.2 rename variables Another useful function is rename(), which as you may have guessed changes the name of variables. Suppose we want to only focus on dep_time and arr_time and change dep_time and arr_time to be departure_time and arrival_time instead in the flights_time data frame: flights_time_new &lt;- flights %&gt;% select(dep_time, arr_time) %&gt;% rename(departure_time = dep_time, arrival_time = arr_time) glimpse(flights_time_new) Note that in this case we used a single = sign within the rename(). For example, departure_time = dep_time renames the dep_time variable to have the new name departure_time. This is because we are not testing for equality like we would using ==. Instead we want to assign a new variable departure_time to have the same values as dep_time and then delete the variable dep_time. Note that new dplyr users often forget that the new variable name comes before the equal sign. 4.10.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top 10 destination airports using the example from Subsection 4.9.5. Observe that we set the number of values to return to n = 10 and wt = num_flights to indicate that we want the rows corresponding to the top 10 values of num_flights. See the help file for top_n() by running ?top_n for more information. named_dests %&gt;% top_n(n = 10, wt = num_flights) Let’s further arrange() these results in descending order of num_flights: named_dests %&gt;% top_n(n = 10, wt = num_flights) %&gt;% arrange(desc(num_flights)) 4.10.4 slice and pull and [] Using slice() gives us specific rows from the flights tibble: slice(flights, 2:5) # A tibble: 4 x 22 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 533 529 4 850 830 2 2013 1 1 542 540 2 923 850 3 2013 1 1 544 545 -1 1004 1022 4 2013 1 1 554 600 -6 812 837 # … with 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, gain &lt;dbl&gt;, hours &lt;dbl&gt;, # gain_per_hour &lt;dbl&gt; pull() grabs out a variable as a vector, rather than leaving it within a tibble, as select() does: slice(flights, 2:5) %&gt;% pull(dep_time) [1] 533 542 544 554 This is often handy when you want to feed the data into a function, like mean() which requires a vector as input: slice(flights, 2:5) %&gt;% pull(dep_time) %&gt;% mean() [1] 543 The most common way to subset vectors is to use the “bracket” operator []. Example: flights$dep_time[2:5] [1] 533 542 544 554 4.11 Conclusion 4.11.1 Summary table Let’s recap our data wrangling verbs in Table 4.2. Using these verbs and the pipe %&gt;% operator from Section 4.1, you’ll be able to write easily legible code to perform almost all the data wrangling and data transformation necessary for the rest of this book. TABLE 4.2: Summary of data wrangling verbs Verb Data wrangling operation filter() Pick out a subset of rows summarize() Summarize many values to one using a summary statistic function like mean(), median(), etc. group_by() Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data mutate() Create new variables by mutating existing ones arrange() Arrange rows of a data variable in ascending (default) or descending order inner_join() Join/merge two data frames, matching rows by a key variable 4.11.2 Additional resources If you want to further unlock the power of the dplyr package for data wrangling, we suggest that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular more intermediate level and advanced data wrangling functions, while providing quick and easy-to-read visual descriptions. In fact, many of the diagrams illustrating data wrangling operations in this chapter, such as Figure 4.1 on filter(), originate from this cheatsheet. In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Transformation with dplyr.” You can see a preview in the figure below. FIGURE 4.11: Data Transformation with dplyr cheatsheet. On top of the data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling, check out Chapter 5 of R for Data Science (Grolemund and Wickham 2017). References "],
["5-tidy.html", "Chapter 5 Tidy 5.1 Importing data 5.2 Web scraping 5.3 “Tidy” data 5.4 Case study: Democracy in Guatemala 5.5 tidyverse package 5.6 Conclusion", " Chapter 5 Tidy In Subsection 1.2.1, we introduced the concept of a data frame in R: a rectangular spreadsheet-like representation of data where the rows correspond to observations and the columns correspond to variables describing each observation. In Section 1.4, we started exploring our first data frame: the flights data frame included in the nycflights13 package. In Chapter 2, we created visualizations based on the data included in flights and other data frames such as weather. In Chapter 4, we learned how to take existing data frames and transform/modify them to suit our ends. Let’s extend some of these ideas by discussing a type of data formatting called “tidy” data. You will see that having data stored in “tidy” format is about more than just what the everyday definition of the term “tidy” might suggest: having your data “neatly organized.” Instead, we define the term “tidy” as it’s used by data scientists who use R, outlining a set of rules by which data is saved. Knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter 2 and data wrangling in Chapter 4. This is because all the data used were already in “tidy” format. In this chapter, we’ll now see that this format is essential to using the tools we covered up until now. Furthermore, it will also be useful for all subsequent chapters in this book when we cover regression and statistical inference. First, however, we’ll show you how to import spreadsheet data in R. Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 1.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(readr) library(tidyr) library(nycflights13) library(fivethirtyeight) 5.1 Importing data Up to this point, we’ve almost entirely used data stored inside of an R package. Say instead you have your own data saved on your computer or somewhere online. How can you analyze this data in R? Spreadsheet data is often saved in one of the following three formats: First, a Comma Separated Values .csv file. You can think of a .csv file as a bare-bones spreadsheet where: Each line in the file corresponds to one row of data/one observation. Values for each line are separated with commas. In other words, the values of different variables are separated by commas in each row. The first line is often, but not always, a header row indicating the names of the columns/variables. Second, an Excel .xlsx spreadsheet file. This format is based on Microsoft’s proprietary Excel software. As opposed to bare-bones .csv files, .xlsx Excel files contain a lot of meta-data (data about data). Recall we saw a previous example of meta-data in Section 4.4 when adding “group structure” meta-data to a data frame by using the group_by() verb. Some examples of Excel spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros. Third, a Google Sheets file, which is a “cloud” or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values .csv and Excel .xlsx formats. To import Google Sheets data in R use the googlesheets4 package. Let’s import a Comma Separated Values .csv file that exists on the internet. The .csv file dem_score.csv contains ratings of the level of democracy in different countries spanning 1952 to 1992 and is accessible at https://moderndive.com/data/dem_score.csv. Let’s use the read_csv() function from the readr (Wickham, Hester, and Francois 2018) package to read it off the web, import it into R, and save it in a data frame called dem_score. library(readr) dem_score &lt;- read_csv(&quot;https://moderndive.com/data/dem_score.csv&quot;) dem_score # A tibble: 96 x 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Albania -9 -9 -9 -9 -9 -9 -9 -9 5 2 Argentina -9 -1 -1 -9 -9 -9 -8 8 7 3 Armenia -9 -7 -7 -7 -7 -7 -7 -7 7 4 Australia 10 10 10 10 10 10 10 10 10 5 Austria 10 10 10 10 10 10 10 10 10 6 Azerbaijan -9 -7 -7 -7 -7 -7 -7 -7 1 7 Belarus -9 -7 -7 -7 -7 -7 -7 -7 7 8 Belgium 10 10 10 10 10 10 10 10 10 9 Bhutan -10 -10 -10 -10 -10 -10 -10 -10 -10 10 Bolivia -4 -3 -3 -4 -7 -7 8 9 9 # … with 86 more rows In this dem_score data frame, the minimum value of -10 corresponds to a highly autocratic nation, whereas a value of 10 corresponds to a highly democratic nation. Note also that backticks surround the different variable names. Variable names in R by default are not allowed to start with a number nor include spaces, but we can get around this fact by surrounding the column name with backticks. We’ll revisit the dem_score data frame in a case study in the upcoming Section 5.4. Note that the read_csv() function included in the readr package is different than the read.csv() function that comes installed with R. While the difference in the names might seem trivial (an _ instead of a .), the read_csv() function is, in our opinion, easier to use since it can more easily read data off the web and generally imports data at a much faster speed. Furthermore, the read_csv() function included in the readr saves data frames as tibbles by default. 5.2 Web scraping The data we need to answer a question is not always in a spreadsheet ready for us to read. For example, the US murders dataset we used in the R Basics chapter originally comes from this Wikipedia page: url &lt;- paste0(&quot;https://en.wikipedia.org/w/index.php?title=&quot;, &quot;Gun_violence_in_the_United_States_by_state&quot;, &quot;&amp;direction=prev&amp;oldid=810166167&quot;) You can see the data table when you visit the webpage: To get this data, we need to do some web scraping. Web scraping, or web harvesting, is the term we use to describe the process of extracting data from a website. The reason we can do this is because the information used by a browser to render webpages is received as a text file from a server. The text is code written in hyper text markup language (HTML). Every browser has a way to show the html source code for a page, each one different. On Chrome, you can use Control-U on a PC and command+alt+U on a Mac. You will see something like this: 5.2.1 HTML Because this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data: &lt;table class=&quot;wikitable sortable&quot;&gt; &lt;tr&gt; &lt;th&gt;State&lt;/th&gt; &lt;th&gt;&lt;a href=&quot;/wiki/List_of_U.S._states_and_territories_by_population&quot; title=&quot;List of U.S. states and territories by population&quot;&gt;Population&lt;/a&gt;&lt;br /&gt; &lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=&quot;cite_ref-1&quot; class=&quot;reference&quot;&gt; &lt;a href=&quot;#cite_note-1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt; &lt;th&gt;Murders and Nonnegligent &lt;p&gt;Manslaughter&lt;br /&gt; &lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=&quot;cite_ref-2&quot; class=&quot;reference&quot;&gt; &lt;a href=&quot;#cite_note-2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt;Murder and Nonnegligent &lt;p&gt;Manslaughter Rate&lt;br /&gt; &lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt; &lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/wiki/Alabama&quot; title=&quot;Alabama&quot;&gt;Alabama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;4,853,875&lt;/td&gt; &lt;td&gt;348&lt;/td&gt; &lt;td&gt;7.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/wiki/Alaska&quot; title=&quot;Alaska&quot;&gt;Alaska&lt;/a&gt;&lt;/td&gt; &lt;td&gt;737,709&lt;/td&gt; &lt;td&gt;59&lt;/td&gt; &lt;td&gt;8.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; You can actually see the data, except data values are surrounded by html code such as &lt;td&gt;. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS). We say more about this in Section 5.2.3. Although we provide tools that make it possible to scrape data without knowing HTML, as a data scientist it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. 5.2.2 The rvest package The tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple: library(tidyverse) library(rvest) h &lt;- read_html(url) Note that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is: class(h) [1] &quot;xml_document&quot; &quot;xml_node&quot; The rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents. Now, how do we extract the table from the object h? If we print h, we don’t really see much: h {html_document} &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... We can see all the code that defines the downloaded webpage using the html_text function like this: html_text(h) We don’t show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=&quot;wikitable sortable&quot;&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use: tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) Now, instead of the entire webpage, we just have the html code for the tables in the page: tab {xml_nodeset (2)} [1] &lt;table class=&quot;wikitable sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n ... [2] &lt;table class=&quot;nowraplinks hlist mw-collapsible mw-collapsed navbox-inner&quot; ... The table we are interested is the first one: tab[[1]] {html_node} &lt;table class=&quot;wikitable sortable&quot;&gt; [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=&quot;/wiki/List_of_U.S._states ... This is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames: tab &lt;- tab[[1]] %&gt;% html_table class(tab) [1] &quot;data.frame&quot; We are now much closer to having a usable data table: tab &lt;- tab %&gt;% setNames(c(&quot;state&quot;, &quot;population&quot;, &quot;total&quot;, &quot;murder_rate&quot;)) head(tab) state population total murder_rate 1 Alabama 4,853,875 348 7.2 2 Alaska 737,709 59 8.0 3 Arizona 6,817,565 309 4.5 4 Arkansas 2,977,853 181 6.1 5 California 38,993,940 1,861 4.8 6 Colorado 5,448,819 176 3.2 We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites. 5.2.3 CSS selectors The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more. If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible. SelectorGadget8 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s vignette9 and other tutorials based on the vignette10.11 5.2.4 JSON Sharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format: Attaching package: &#39;jsonlite&#39; The following object is masked from &#39;package:purrr&#39;: flatten [ { &quot;name&quot;: &quot;Miguel&quot;, &quot;student_id&quot;: 1, &quot;exam_1&quot;: 85, &quot;exam_2&quot;: 86 }, { &quot;name&quot;: &quot;Sofia&quot;, &quot;student_id&quot;: 2, &quot;exam_1&quot;: 94, &quot;exam_2&quot;: 93 }, { &quot;name&quot;: &quot;Aya&quot;, &quot;student_id&quot;: 3, &quot;exam_1&quot;: 87, &quot;exam_2&quot;: 88 }, { &quot;name&quot;: &quot;Cheng&quot;, &quot;student_id&quot;: 4, &quot;exam_1&quot;: 90, &quot;exam_2&quot;: 91 } ] The file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example: library(jsonlite) citi_bike &lt;- fromJSON(&quot;http://citibikenyc.com/stations/json&quot;) This downloads a list. The first argument tells you when you downloaded it: citi_bike$executionTime [1] &quot;2020-01-27 05:16:43 PM&quot; and the second is a data table: citi_bike$stationBeanList %&gt;% as_tibble() # A tibble: 935 x 18 id stationName availableDocks totalDocks latitude longitude statusValue &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 304 Broadway &amp;… 12 33 40.7 -74.0 In Service 2 359 E 47 St &amp; … 44 64 40.8 -74.0 In Service 3 367 E 53 St &amp; … 27 34 40.8 -74.0 In Service 4 402 Broadway &amp;… 23 39 40.7 -74.0 In Service 5 3255 8 Ave &amp; W … 6 19 40.8 -74.0 In Service 6 3443 W 52 St &amp; … 31 41 40.8 -74.0 In Service 7 72 W 52 St &amp; … 31 55 40.8 -74.0 In Service 8 79 Franklin S… 7 33 40.7 -74.0 In Service 9 82 St James P… 0 27 40.7 -74.0 In Service 10 83 Atlantic A… 5 62 40.7 -74.0 In Service # … with 925 more rows, and 11 more variables: statusKey &lt;int&gt;, # availableBikes &lt;int&gt;, stAddress1 &lt;chr&gt;, stAddress2 &lt;chr&gt;, city &lt;chr&gt;, # postalCode &lt;chr&gt;, location &lt;chr&gt;, altitude &lt;chr&gt;, testStation &lt;lgl&gt;, # lastCommunicationTime &lt;chr&gt;, landMark &lt;chr&gt; You can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converging data into tables. For more flexibility, we recommend rjson. 5.3 “Tidy” data Let’s now switch gears and learn about the concept of “tidy” data format with a motivating example from the fivethirtyeight package. The fivethirtyeight package (Kim, Ismay, and Chunn 2019) provides access to the datasets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all 127 datasets included in the fivethirtyeight package, check out the package webpage by going to: https://fivethirtyeight-r.netlify.com/articles/fivethirtyeight.html. Let’s focus our attention on the drinks data frame and look at its first 5 rows: # A tibble: 5 x 5 country beer_servings spirit_servings wine_servings total_litres_of_pure_a… &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanist… 0 0 0 0 2 Albania 89 132 54 4.9 3 Algeria 25 0 14 0.7 4 Andorra 245 138 312 12.4 5 Angola 217 57 45 5.9 After reading the help file by running ?drinks, you’ll see that drinks is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”. Let’s apply some of the data wrangling verbs we learned in Chapter 4 on the drinks data frame: filter() the drinks data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, then select() all columns except total_litres_of_pure_alcohol by using the - sign, then rename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine, respectively. and save the resulting data frame in drinks_smaller: drinks_smaller &lt;- drinks %&gt;% filter(country %in% c(&quot;USA&quot;, &quot;China&quot;, &quot;Italy&quot;, &quot;Saudi Arabia&quot;)) %&gt;% select(-total_litres_of_pure_alcohol) %&gt;% rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings) drinks_smaller # A tibble: 4 x 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 Let’s now ask ourselves a question: “Using the drinks_smaller data frame, how would we create the side-by-side barplot in Figure 5.1?”. Recall we saw barplots displaying two categorical variables in Subsection 2.7.3. FIGURE 5.1: Comparing alcohol consumption in 4 countries. Let’s break down the grammar of graphics we introduced in Section 2.1: The categorical variable country with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the x-position of the bars. The numerical variable servings would have to be mapped to the y-position of the bars (the height of the bars). The categorical variable type with three levels (beer, spirit, wine) would have to be mapped to the fill color of the bars. Observe, however, that drinks_smaller has three separate variables beer, spirit, and wine. In order to use the ggplot() function to recreate the barplot in Figure 5.1. However, we need a single variable type with three possible values: beer, spirit, and wine. We could then map this type variable to the fill aesthetic of our plot. In other words, to recreate the barplot in Figure 5.1, our data frame would have to look like this: drinks_smaller_tidy # A tibble: 12 x 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 Italy beer 85 3 Saudi Arabia beer 0 4 USA beer 249 5 China spirit 192 6 Italy spirit 42 7 Saudi Arabia spirit 5 8 USA spirit 158 9 China wine 8 10 Italy wine 237 11 Saudi Arabia wine 0 12 USA wine 84 Observe that while drinks_smaller and drinks_smaller_tidy are both rectangular in shape and contain the same 12 numerical values (3 alcohol types by 4 countries), they are formatted differently. drinks_smaller is formatted in what’s known as “wide” format, whereas drinks_smaller_tidy is formatted in what’s known as “long/narrow” format. In the context of doing data science in R, long/narrow format is also known as “tidy” format. In order to use the ggplot2 and dplyr packages for data visualization and data wrangling, your input data frames must be in “tidy” format. Thus, all non-“tidy” data must be converted to “tidy” format first. Before we convert non-“tidy” data frames like drinks_smaller to “tidy” data frames like drinks_smaller_tidy, let’s define “tidy” data. 5.3.1 Definition of “tidy” data You have surely heard the word “tidy” in your life: “Tidy up your room!” “Write your homework in a tidy way so it is easier to provide feedback.” Marie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo. “I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant What does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data (Wickham 2014) shown also in Figure 5.2: A dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes. “Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. FIGURE 5.2: Tidy data graphic from R for Data Science. For example, say you have the following table of stock prices in Table 5.1: TABLE 5.1: Stock prices (non-tidy format) Date Boeing stock price Amazon stock price Google stock price 2009-01-01 $173.55 $174.90 $174.34 2009-01-02 $172.61 $171.42 $170.04 Although the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In “tidy” data format, each variable should be its own column, as shown in Table 5.2. Notice that both tables present the same information, but in different formats. TABLE 5.2: Stock prices (tidy format) Date Stock Name Stock Price 2009-01-01 Boeing $173.55 2009-01-01 Amazon $174.90 2009-01-01 Google $174.34 2009-01-02 Boeing $172.61 2009-01-02 Amazon $171.42 2009-01-02 Google $170.04 Now we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table 5.3. TABLE 5.3: Example of tidy data Date Boeing Price Weather 2009-01-01 $173.55 Sunny 2009-01-02 $172.61 Overcast In this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data in Table 5.1, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day. 5.3.2 Converting to “tidy” data In this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore, for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-“tidy”) format and you would like to use the ggplot2 or dplyr packages, you will first have to convert it to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package (Wickham and Henry 2019). Going back to our drinks_smaller data frame from earlier: drinks_smaller # A tibble: 4 x 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 We convert it to “tidy” format by using the pivot_longer() function from the tidyr package as follows: drinks_smaller_tidy &lt;- drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = -country) drinks_smaller_tidy # A tibble: 12 x 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 China spirit 192 3 China wine 8 4 Italy beer 85 5 Italy spirit 42 6 Italy wine 237 7 Saudi Arabia beer 0 8 Saudi Arabia spirit 5 9 Saudi Arabia wine 0 10 USA beer 249 11 USA spirit 158 12 USA wine 84 We set the arguments to pivot_longer() as follows: names_to here corresponds to the name of the variable in the new “tidy”/long data frame that will contain the column names of the original data. Observe how we set names_to = &quot;type&quot;. In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here. values_to here is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = &quot;servings&quot; since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument. The third argument cols is the columns in the drinks_smaller data frame you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it. The third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = c(beer, spirit, wine)) Note that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller data frame, we could also do the following for the cols argument: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = beer:wine) With our drinks_smaller_tidy “tidy” formatted data frame, we can now produce the barplot you saw in Figure 5.1 using geom_col(). This is done in Figure 5.3. Recall from Section 2.7 on barplots that we use geom_col() and not geom_bar(), since we would like to map the “pre-counted” servings variable to the y-aesthetic of the bars. ggplot(drinks_smaller_tidy, aes(x = country, y = servings, fill = type)) + geom_col(position = &quot;dodge&quot;) FIGURE 5.3: Comparing alcohol consumption in 4 countries using geom_col(). Converting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different datasets. For example, run ?pivot_longer and look at the examples in the bottom of the help file. We’ll show another example of using pivot_longer() to convert a “wide” formatted data frame to “tidy” format in Section 5.4. If however you want to convert a “tidy” data frame to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples. You can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform. 5.3.3 nycflights13 package Recall the nycflights13 package we introduced in Section 1.4 with data about all domestic flights departing from New York City in 2013. Let’s revisit the flights data frame by running View(flights). We saw that flights has a rectangular shape, with each of its 336,776 rows corresponding to a flight and each of its 22 columns corresponding to different characteristics/measurements of each flight. This satisfied the first two criteria of the definition of “tidy” data from Subsection 5.3.1: that “Each variable forms a column” and “Each observation forms a row.” But what about the third property of “tidy” data that “Each type of observational unit forms a table”? Recall that we saw in Subsection 1.4.3 that the observational unit for the flights data frame is an individual flight. In other words, the rows of the flights data frame refer to characteristics/measurements of individual flights. Also included in the nycflights13 package are other data frames with their rows representing different observational units (Wickham 2019a): airlines: translation between two letter IATA carrier codes and airline company names (16 in total). The observational unit is an airline company. planes: aircraft information about each of 3,322 planes used, i.e., the observational unit is an aircraft. weather: hourly meteorological data (about 8,705 observations) for each of the three NYC airports, i.e., the observational unit is an hourly measurement of weather at one of the three airports. airports: airport names and locations. The observational unit is an airport. The organization of the information into these five data frames follows the third “tidy” data property: observations corresponding to the same observational unit should be saved in the same table, i.e., data frame. You could think of this property as the old English expression: “birds of a feather flock together.” 5.4 Case study: Democracy in Guatemala In this section, we’ll show you another example of how to convert a data frame that isn’t in “tidy” format (“wide” format) to a data frame that is in “tidy” format (“long/narrow” format). We’ll do this using the pivot_longer() function from the tidyr package again. Furthermore, we’ll make use of functions from the ggplot2 and dplyr packages to produce a time-series plot showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Recall that we saw time-series plots in Section 2.3 on creating linegraphs using geom_line(). Let’s use the dem_score data frame we imported in Section 5.1, but focus on only data corresponding to Guatemala. guat_dem &lt;- dem_score %&gt;% filter(country == &quot;Guatemala&quot;) guat_dem # A tibble: 1 x 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Guatemala 2 -6 -5 3 1 -3 -7 3 3 Let’s lay out the grammar of graphics we saw in Section 2.1. First we know we need to set data = guat_dem and use a geom_line() layer, but what is the aesthetic mapping of variables? We’d like to see how the democracy score has changed over the years, so we need to map: year to the x-position aesthetic and democracy_score to the y-position aesthetic Now we are stuck in a predicament, much like with our drinks_smaller example in Section 5.3. We see that we have a variable named country, but its only value is &quot;Guatemala&quot;. We have other variables denoted by different year values. Unfortunately, the guat_dem data frame is not “tidy” and hence is not in the appropriate format to apply the grammar of graphics, and thus we cannot use the ggplot2 package just yet. We need to take the values of the columns corresponding to years in guat_dem and convert them into a new “names” variable called year. Furthermore, we need to take the democracy score values in the inside of the data frame and turn them into a new “values” variable called democracy_score. Our resulting data frame will have three columns: country, year, and democracy_score. Recall that the pivot_longer() function in the tidyr package does this for us: guat_dem_tidy &lt;- guat_dem %&gt;% pivot_longer(names_to = &quot;year&quot;, values_to = &quot;democracy_score&quot;, cols = -country, names_ptypes = list(year = integer())) guat_dem_tidy # A tibble: 9 x 3 country year democracy_score &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 Guatemala 1952 2 2 Guatemala 1957 -6 3 Guatemala 1962 -5 4 Guatemala 1967 3 5 Guatemala 1972 1 6 Guatemala 1977 -3 7 Guatemala 1982 -7 8 Guatemala 1987 3 9 Guatemala 1992 3 We set the arguments to pivot_longer() as follows: names_to is the name of the variable in the new “tidy” data frame that will contain the column names of the original data. Observe how we set names_to = &quot;year&quot;. In the resulting guat_dem_tidy, the column year contains the years where Guatemala’s democracy scores were measured. values_to is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = &quot;democracy_score&quot;. In the resulting guat_dem_tidy the column democracy_score contains the 1 \\(\\times\\) 9 = 9 democracy scores as numeric values. The third argument is the columns you either want to or don’t want to “tidy.” Observe how we set this to cols = -country indicating that we don’t want to “tidy” the country variable in guat_dem and rather only variables 1952 through 1992. The last argument of names_ptypes tells R what type of variable year should be set to. Without specifying that it is an integer as we’ve done here, pivot_longer() will set it to be a character value by default. We can now create the time-series plot in Figure 5.4 to visualize how democracy scores in Guatemala have changed from 1952 to 1992 using a geom_line(). Furthermore, we’ll use the labs() function in the ggplot2 package to add informative labels to all the aes()thetic attributes of our plot, in this case the x and y positions. ggplot(guat_dem_tidy, aes(x = year, y = democracy_score)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Democracy Score&quot;) FIGURE 5.4: Democracy scores in Guatemala 1952-1992. Note that if we forgot to include the names_ptypes argument specifying that year was not of character format, we would have gotten an error here since geom_line() wouldn’t have known how to sort the character values in year in the right order. 5.5 tidyverse package Notice at the beginning of the chapter we loaded the following four packages, which are among four of the most frequently used R packages for data science: library(ggplot2) library(dplyr) library(readr) library(tidyr) Recall that ggplot2 is for data visualization, dplyr is for data wrangling, readr is for importing spreadsheet data into R, and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you. After installing the tidyverse package as you would a normal package as seen in Section 1.3, running: library(tidyverse) would be the same as running: library(ggplot2) library(dplyr) library(readr) library(tidyr) library(purrr) library(tibble) library(stringr) library(forcats) The purrr, tibble, stringr, and forcats are left for a more advanced book; check out R for Data Science to learn about these packages. For the remainder of this book, we’ll start every chapter by running library(tidyverse), instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames are in “tidy” format. This standardization of input and output data frames makes transitions between different functions in the different packages as seamless as possible. For more information, check out the tidyverse.org webpage for the package. 5.6 Conclusion 5.6.1 Additional resources If you want to learn more about using the readr and tidyr package, we suggest that you check out RStudio’s “Data Import Cheat Sheet.” In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Browse Cheatsheets” -&gt; Scroll down the page to the “Data Import Cheat Sheet.” The first page of this cheatsheet has information on using the readr package to import data, while the second page has information on using the tidyr package to “tidy” data. You can see a preview of both cheatsheets in the figures below. FIGURE 5.5: Data Import cheatsheet (first page): readr package. FIGURE 5.6: Data Import cheatsheet (second page): tidyr package. References "],
["6-functions.html", "Chapter 6 Functions 6.1 Part 1 6.2 Part 2 6.3 Part 3 6.4 List columns and map_* functions 6.5 Resources", " Chapter 6 Functions My goal here is to reveal the process a long-time useR employs for writing functions. I also want to illustrate why the process is the way it is. Merely looking at the finished product, e.g. source code for R packages, can be extremely deceiving. Reality is generally much uglier … but more interesting! Why are we covering this now, smack in the middle of data aggregation? Powerful machines like dplyr, purrr, and the built-in “apply” family of functions, are ready and waiting to apply your purpose-built functions to various bits of your data. If you can express your analytical wishes in a function, these tools will give you great power. 6.1 Part 1 As usual, load gapminder. library(gapminder) str(gapminder) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1704 obs. of 6 variables: $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... $ year : int 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... $ lifeExp : num 28.8 30.3 32 34 36.1 ... $ pop : int 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... $ gdpPercap: num 779 821 853 836 740 ... Say you’ve got a numeric vector, and you want to compute the difference between its max and min. lifeExp or pop or gdpPercap are great examples of a typical input. You can imagine wanting to get this statistic after we slice up the Gapminder data by year, country, continent, or combinations thereof. 6.1.1 Get something that works First, develop some working code for interactive use, using a representative input. I’ll use Gapminder’s life expectancy variable. R functions that will be useful: min(), max(), range(). (Read their documentation: [here][rdocs-extremes] and [here][rdocs-range]) ## get to know the functions mentioned above min(gapminder$lifeExp) [1] 23.6 max(gapminder$lifeExp) [1] 82.6 range(gapminder$lifeExp) [1] 23.6 82.6 ## some natural solutions max(gapminder$lifeExp) - min(gapminder$lifeExp) [1] 59 with(gapminder, max(lifeExp) - min(lifeExp)) [1] 59 range(gapminder$lifeExp)[2] - range(gapminder$lifeExp)[1] [1] 59 with(gapminder, range(lifeExp)[2] - range(lifeExp)[1]) [1] 59 diff(range(gapminder$lifeExp)) [1] 59 Internalize this “answer” because our informal testing relies on you noticing departures from this. 6.1.2 Skateboard &gt;&gt; perfectly formed rear-view mirror This image [widely attributed to the Spotify development team][min-viable-product] conveys an important point. FIGURE 6.1: From Your ultimate guide to Minimum Viable Product (+great examples) Build that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps the spirits high. This is related to the valuable [Telescope Rule][telescope-rule]: It is faster to make a four-inch mirror then a six-inch mirror than to make a six-inch mirror. 6.1.3 Turn the working interactive code into a function Add NO new functionality! Just write your very first R function. max_minus_min &lt;- function(x) max(x) - min(x) max_minus_min(gapminder$lifeExp) [1] 59 Check that you’re getting the same answer as you did with your interactive code. Test it eyeball-o-metrically at this point. 6.1.4 Test your function 6.1.4.1 Test on new inputs Pick some new artificial inputs where you know (at least approximately) what your function should return. max_minus_min(1:10) [1] 9 max_minus_min(runif(1000)) [1] 0.999 I know that 10 minus 1 is 9. I know that random uniform [0, 1] variates will be between 0 and 1. Therefore max - min should be less than 1. If I take LOTS of them, max - min should be pretty close to 1. It is intentional that I tested on integer input as well as floating point. Likewise, I like to use valid-but-random data for this sort of check. 6.1.4.2 Test on real data but different real data Back to the real world now. Two other quantitative variables are lying around: gdpPercap and pop. Let’s have a go. max_minus_min(gapminder$gdpPercap) [1] 113282 max_minus_min(gapminder$pop) [1] 1318623085 Either check these results “by hand” or apply the “does that even make sense?” test. 6.1.4.3 Test on weird stuff Now we try to break our function. Don’t get truly diabolical (yet). Just make the kind of mistakes you can imagine making at 2am when, 3 years from now, you rediscover this useful function you wrote. Give your function inputs it’s not expecting. max_minus_min(gapminder) ## hey sometimes things &quot;just work&quot; on data.frames! Error in FUN(X[[i]], ...): only defined on a data frame with all numeric variables max_minus_min(gapminder$country) ## factors are kind of like integer vectors, no? Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;max&#39; not meaningful for factors max_minus_min(&quot;eggplants are purple&quot;) ## i have no excuse for this one Error in max(x) - min(x): non-numeric argument to binary operator How happy are you with those error messages? You must imagine that some entire script has failed and that you were hoping to just source() it without re-reading it. If a colleague or future you encountered these errors, do you run screaming from the room? How hard is it to pinpoint the usage problem? 6.1.4.4 I will scare you now Here are some great examples STAT545 students devised during class where the function should break but it does not. max_minus_min(gapminder[c(&#39;lifeExp&#39;, &#39;gdpPercap&#39;, &#39;pop&#39;)]) [1] 1318683072 max_minus_min(c(TRUE, TRUE, FALSE, TRUE, TRUE)) [1] 1 In both cases, R’s eagerness to make sense of our requests is unfortunately successful. In the first case, a data.frame containing just the quantitative variables is eventually coerced into numeric vector. We can compute max minus min, even though it makes absolutely no sense at all. In the second case, a logical vector is converted to zeroes and ones, which might merit an error or at least a warning. 6.1.5 Check the validity of arguments For functions that will be used again – which is not all of them! – it is good to check the validity of arguments. This implements a rule from [the Unix philosophy][unix-philosophy]: Rule of Repair: When you must fail, fail noisily and as soon as possible. 6.1.5.1 stop if not stopifnot() is the entry level solution. I use it here to make sure the input x is a numeric vector. mmm &lt;- function(x) { stopifnot(is.numeric(x)) max(x) - min(x) } mmm(gapminder) Error in mmm(gapminder): is.numeric(x) is not TRUE mmm(gapminder$country) Error in mmm(gapminder$country): is.numeric(x) is not TRUE mmm(&quot;eggplants are purple&quot;) Error in mmm(&quot;eggplants are purple&quot;): is.numeric(x) is not TRUE mmm(gapminder[c(&#39;lifeExp&#39;, &#39;gdpPercap&#39;, &#39;pop&#39;)]) Error in mmm(gapminder[c(&quot;lifeExp&quot;, &quot;gdpPercap&quot;, &quot;pop&quot;)]): is.numeric(x) is not TRUE mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE)) Error in mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE)): is.numeric(x) is not TRUE And we see that it catches all of the self-inflicted damage we would like to avoid. 6.1.5.2 if then stop stopifnot() doesn’t provide a very good error message. The next approach is very widely used. Put your validity check inside an if() statement and call stop() yourself, with a custom error message, in the body. mmm2 &lt;- function(x) { if(!is.numeric(x)) { stop(&#39;I am so sorry, but this function only works for numeric input!\\n&#39;, &#39;You have provided an object of class: &#39;, class(x)[1]) } max(x) - min(x) } mmm2(gapminder) Error in mmm2(gapminder): I am so sorry, but this function only works for numeric input! You have provided an object of class: tbl_df In addition to a gratuitous apology, the error raised also contains two more pieces of helpful info: Which function threw the error. Hints on how to fix things: expected class of input vs actual class. If it is easy to do so, I highly recommend this template: “you gave me THIS, but I need THAT”. The tidyverse style guide has a very useful chapter on how to construct error messages. 6.1.5.3 Sidebar: non-programming uses for assertions Another good use of this pattern is to leave checks behind in data analytical scripts. Consider our repetitive use of Gapminder in this course. Every time we load it, we inspect it, hoping to see the usual stuff. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run ad nauseam on auto-pilot or non-interactively. 6.1.6 Wrap-up and what’s next? Here’s the function we’ve written so far: mmm2 function(x) { if(!is.numeric(x)) { stop(&#39;I am so sorry, but this function only works for numeric input!\\n&#39;, &#39;You have provided an object of class: &#39;, class(x)[1]) } max(x) - min(x) } What we’ve accomplished: We’ve written our first function. We are checking the validity of its input, argument x. We’ve done a good amount of informal testing. 6.2 Part 2 6.2.1 Resources Packages for runtime assertions (the last 3 seem to be under more active development than assertthat): assertthat on [CRAN][assertthat-cran] and [GitHub][assertthat-github] - the Hadleyverse option ensurer on [CRAN][ensurer-cran] and [GitHub][ensurer-github] - general purpose, pipe-friendly assertr on [CRAN][assertr-cran] and [GitHub][assertr-github] - explicitly data pipeline oriented assertive on [CRAN][assertive-cran] and [Bitbucket][assertive-bitbucket] - rich set of built-in functions Hadley Wickham’s book [Advanced R][adv-r] (???): Section on [defensive programming][adv-r-defensive-programming] 6.2.2 Where were we? Where are we going? In part 1, we wrote our first R function to compute the difference between the max and min of a numeric vector. We checked the validity of the function’s only argument and, informally, we verified that it worked pretty well. In this part, we generalize this function, learn more technical details about R functions, and set default values for some arguments. ##E Load the Gapminder data As usual, load gapminder. library(gapminder) 6.2.3 Restore our max minus min function Let’s keep our previous function around as a baseline. mmm &lt;- function(x) { stopifnot(is.numeric(x)) max(x) - min(x) } 6.2.4 Generalize our function to other quantiles The max and the min are special cases of a quantile. Here are other special cases you may have heard of: median = 0.5 quantile 1st quartile = 0.25 quantile 3rd quartile = 0.75 quantile If you’re familiar with [box plots][wiki-boxplot], the rectangle typically runs from the 1st quartile to the 3rd quartile, with a line at the median. If \\(q\\) is the \\(p\\)-th quantile of a set of \\(n\\) observations, what does that mean? Approximately \\(pn\\) of the observations are less than \\(q\\) and \\((1 - p)n\\) are greater than \\(q\\). Yeah, you need to worry about rounding to an integer and less/greater than or equal to, but these details aren’t critical here. Let’s generalize our function to take the difference between any two quantiles. We can still consider the max and min, if we like, but we’re not limited to that. 6.2.5 Get something that works, again The eventual inputs to our new function will be the data x and two probabilities. First, play around with the quantile() function. Convince yourself you know how to use it, for example, by cross-checking your results with other built-in functions. quantile(gapminder$lifeExp) 0% 25% 50% 75% 100% 23.6 48.2 60.7 70.8 82.6 quantile(gapminder$lifeExp, probs = 0.5) 50% 60.7 median(gapminder$lifeExp) [1] 60.7 quantile(gapminder$lifeExp, probs = c(0.25, 0.75)) 25% 75% 48.2 70.8 boxplot(gapminder$lifeExp, plot = FALSE)$stats [,1] [1,] 23.6 [2,] 48.2 [3,] 60.7 [4,] 70.8 [5,] 82.6 Now write a code snippet that takes the difference between two quantiles. the_probs &lt;- c(0.25, 0.75) the_quantiles &lt;- quantile(gapminder$lifeExp, probs = the_probs) max(the_quantiles) - min(the_quantiles) [1] 22.6 6.2.6 Turn the working interactive code into a function, again I’ll use qdiff as the base of our function’s name. I copy the overall structure from our previous “max minus min” work but replace the guts of the function with the more general code we just developed. qdiff1 &lt;- function(x, probs) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } qdiff1(gapminder$lifeExp, probs = c(0.25, 0.75)) [1] 22.6 IQR(gapminder$lifeExp) # hey, we&#39;ve reinvented IQR [1] 22.6 qdiff1(gapminder$lifeExp, probs = c(0, 1)) [1] 59 mmm(gapminder$lifeExp) [1] 59 Again we do some informal tests against familiar results and external implementations. 6.2.7 Argument names: freedom and conventions I want you to understand the importance of argument names. I can name my arguments almost anything I like. Proof: qdiff2 &lt;- function(zeus, hera) { stopifnot(is.numeric(zeus)) the_quantiles &lt;- quantile(x = zeus, probs = hera) max(the_quantiles) - min(the_quantiles) } qdiff2(zeus = gapminder$lifeExp, hera = 0:1) [1] 59 While I can name my arguments after Greek gods, it’s usually a bad idea. Take all opportunities to make things more self-explanatory via meaningful names. If you are going to pass the arguments of your function as arguments of a built-in function, consider copying the argument names. Unless you have a good reason to do your own thing (some argument names are bad!), be consistent with the existing function. Again, the reason is to reduce your cognitive load. This is what I’ve been doing all along and now you know why: qdiff1 function(x, probs) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } &lt;bytecode: 0x7f8ce4eb7cd0&gt; We took this detour so you could see there is no structural relationship between my arguments (x and probs) and those of quantile() (also x and probs). The similarity or equivalence of the names accomplishes nothing as far as R is concerned; it is solely for the benefit of humans reading, writing, and using the code. Which is very important! 6.2.8 What a function returns By this point, I expect someone will have asked about the last line in my function’s body. Look above for a reminder of the function’s definition. By default, a function returns the result of the last line of the body. I am just letting that happen with the line max(the_quantiles) - min(the_quantiles). However, there is an explicit function for this: return(). I could just as easily make this the last line of my function’s body: return(max(the_quantiles) - min(the_quantiles)) You absolutely must use return() if you want to return early based on some condition, i.e. before execution gets to the last line of the body. Otherwise, you can decide your own conventions about when you use return() and when you don’t. 6.2.9 Default values: freedom to NOT specify the arguments What happens if we call our function but neglect to specify the probabilities? qdiff1(gapminder$lifeExp) Error in quantile(x = x, probs = probs): argument &quot;probs&quot; is missing, with no default Oops! At the moment, this causes a fatal error. It can be nice to provide some reasonable default values for certain arguments. In our case, it would be crazy to specify a default value for the primary input x, but very kind to specify a default for probs. We started by focusing on the max and the min, so I think those make reasonable defaults. Here’s how to specify that in a function definition. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } Again we check how the function works, in old examples and new, specifying the probs argument and not. qdiff3(gapminder$lifeExp) [1] 59 mmm(gapminder$lifeExp) [1] 59 qdiff3(gapminder$lifeExp, c(0.1, 0.9)) [1] 33.6 6.2.10 Check the validity of arguments, again Exercise: upgrade our argument validity checks in light of the new argument probs. ## problems identified during class ## we&#39;re not checking that probs is numeric ## we&#39;re not checking that probs is length 2 ## we&#39;re not checking that probs are in [0,1] 6.2.11 Wrap-up and what’s next? Here’s the function we’ve written so far: qdiff3 function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } &lt;bytecode: 0x7f8ce8a51e30&gt; What we’ve accomplished: We’ve generalized our first function to take a difference between arbitrary quantiles. We’ve specified default values for the probabilities that set the quantiles. 6.3 Part 3 6.3.1 Resources Hadley Wickham’s book [Advanced R][adv-r] (???): Section on [function arguments][adv-r-fxn-args] Section on [return values][adv-r-return-values] 6.3.2 Where were we? Where are we going? In part 2 we generalized our first R function so it could take the difference between any two quantiles of a numeric vector. We also set default values for the underlying probabilities, so that, by default, we compute the max minus the min. In this part, we tackle NAs, the special argument ... and formal testing. 6.3.3 Load the Gapminder data As usual, load gapminder. library(gapminder) 6.3.4 Restore our max minus min function Let’s keep our previous function around as a baseline. qdiff3 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs) max(the_quantiles) - min(the_quantiles) } 6.3.5 Be proactive about NAs I am being gentle by letting you practice with the Gapminder data. In real life, missing data will make your life a living hell. If you are lucky, it will be properly indicated by the special value NA, but don’t hold your breath. Many built-in R functions have an na.rm = argument through which you can specify how you want to handle NAs. Typically the default value is na.rm = FALSE and typical default behavior is to either let NAs propagate or to raise an error. Let’s see how quantile() handles NAs: z &lt;- gapminder$lifeExp z[3] &lt;- NA quantile(gapminder$lifeExp) 0% 25% 50% 75% 100% 23.6 48.2 60.7 70.8 82.6 quantile(z) Error in quantile.default(z): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE quantile(z, na.rm = TRUE) 0% 25% 50% 75% 100% 23.6 48.2 60.8 70.8 82.6 So quantile() simply will not operate in the presence of NAs unless na.rm = TRUE. How shall we modify our function? If we wanted to hardwire na.rm = TRUE, we could. Focus on our call to quantile() inside our function definition. qdiff4 &lt;- function(x, probs = c(0, 1)) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = TRUE) max(the_quantiles) - min(the_quantiles) } qdiff4(gapminder$lifeExp) [1] 59 qdiff4(z) [1] 59 This works but it is dangerous to invert the default behavior of a well-known built-in function and to provide the user with no way to override this. We could add an na.rm = argument to our own function. We might even enforce our preferred default – but at least we’re giving the user a way to control the behavior around NAs. qdiff5 &lt;- function(x, probs = c(0, 1), na.rm = TRUE) { stopifnot(is.numeric(x)) the_quantiles &lt;- quantile(x, probs, na.rm = na.rm) max(the_quantiles) - min(the_quantiles) } qdiff5(gapminder$lifeExp) [1] 59 qdiff5(z) [1] 59 qdiff5(z, na.rm = FALSE) Error in quantile.default(x, probs, na.rm = na.rm): missing values and NaN&#39;s not allowed if &#39;na.rm&#39; is FALSE 6.3.6 The useful but mysterious ... argument You probably could have lived a long and happy life without knowing there are at least 9 different algorithms for computing quantiles. [Go read about the type argument][rdocs-quantile] of quantile(). TLDR: If a quantile is not unambiguously equal to an observed data point, you must somehow average two data points. You can weight this average different ways, depending on the rest of the data, and type = controls this. Let’s say we want to give the user of our function the ability to specify how the quantiles are computed, but we want to accomplish with as little fuss as possible. In fact, we don’t even want to clutter our function’s interface with this! This calls for the very special ... argument. In English, this set of three dots is frequently called an “ellipsis”. qdiff6 &lt;- function(x, probs = c(0, 1), na.rm = TRUE, ...) { the_quantiles &lt;- quantile(x = x, probs = probs, na.rm = na.rm, ...) max(the_quantiles) - min(the_quantiles) } The practical significance of the type = argument is virtually nonexistent, so we can’t demo with the Gapminder data. Thanks to [@wrathematics][twitter-wrathematics], here’s a small example where we can (barely) detect a difference due to type. set.seed(1234) z &lt;- rnorm(10) quantile(z, type = 1) 0% 25% 50% 75% 100% -2.346 -0.890 -0.564 0.429 1.084 quantile(z, type = 4) 0% 25% 50% 75% 100% -2.346 -1.049 -0.564 0.353 1.084 all.equal(quantile(z, type = 1), quantile(z, type = 4)) [1] &quot;Mean relative difference: 0.178&quot; Now we can call our function, requesting that quantiles be computed in different ways. qdiff6(z, probs = c(0.25, 0.75), type = 1) [1] 1.32 qdiff6(z, probs = c(0.25, 0.75), type = 4) [1] 1.4 While the difference may be subtle, it’s there. Marvel at the fact that we have passed type = 1 through to quantile() even though it was not a formal argument of our own function. The special argument ... is very useful when you want the ability to pass arbitrary arguments down to another function, but without constantly expanding the formal arguments to your function. This leaves you with a less cluttered function definition and gives you future flexibility to specify these arguments only when you need to. You will also encounter the ... argument in many built-in functions – read up on [c()][rdocs-c] or [list()][rdocs-list] – and now you have a better sense of what it means. It is not a breezy “and so on and so forth.” There are also downsides to ..., so use it with intention. In a package, you will have to work harder to create truly informative documentation for your user. Also, the quiet, absorbent properties of ... mean it can sometimes silently swallow other named arguments, when the user has a typo in the name. Depending on whether or how this fails, it can be a little tricky to find out what went wrong. The ellipsis package provides tools that help package developers use ... more safely. The in-progress tidyverse principles guide provides further guidance on the design of functions that take ... in Data, dots, details. 6.3.7 Use testthat for formal unit tests Until now, we’ve relied on informal tests of our evolving function. If you are going to use a function a lot, especially if it is part of a package, it is wise to use formal unit tests. The [testthat][testthat-web] package ([CRAN][testthat-cran]; [GitHub][testthat-github]) provides excellent facilities for this, with a distinct emphasis on automated unit testing of entire packages. However, we can take it out for a test drive even with our one measly function. We will construct a test with test_that() and, within it, we put one or more expectations that check actual against expected results. You simply harden your informal, interactive tests into formal unit tests. Here are some examples of tests and indicative expectations. library(testthat) test_that(&#39;invalid args are detected&#39;, { expect_error(qdiff6(&quot;eggplants are purple&quot;)) expect_error(qdiff6(iris)) }) test_that(&#39;NA handling works&#39;, { expect_error(qdiff6(c(1:5, NA), na.rm = FALSE)) expect_equal(qdiff6(c(1:5, NA)), 4) }) No news is good news! Let’s see what test failure would look like. Let’s revert to a version of our function that does no NA handling, then test for proper NA handling. We can watch it fail. qdiff_no_NA &lt;- function(x, probs = c(0, 1)) { the_quantiles &lt;- quantile(x = x, probs = probs) max(the_quantiles) - min(the_quantiles) } test_that(&#39;NA handling works&#39;, { expect_that(qdiff_no_NA(c(1:5, NA)), equals(4)) }) Similar to the advice to use assertions in data analytical scripts, I recommend you use unit tests to monitor the behavior of functions you (or others) will use often. If your tests cover the function’s important behavior, then you can edit the internals freely. You’ll rest easy in the knowledge that, if you broke anything important, the tests will fail and alert you to the problem. A function that is important enough for unit tests probably also belongs in a package, where there are obvious mechanisms for running the tests as part of overall package checks. 6.4 List columns and map_* functions We are now going to learn about list columns and map_* functions, powerful tools that we will be using throughout the book.12 6.4.1 What are list columns? A list column is a column of your data which is a list rather than an atomic vector. Atomic vectors are familiar to us; each element of the vector has one value, and thus if an atomic vector is a column in your dataset, each observation gets a single value. Lists, however, can contain vectors as elements. The best way to understand this is by creating a list column yourself and inspecting the results. str() is a helpful function for looking at the contents of a tibble with list columns: library(tidyverse) tibble(new_col = list(1:2, 3:5)) %&gt;% str() Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2 obs. of 1 variable: $ new_col:List of 2 ..$ : int 1 2 ..$ : int 3 4 5 Here we see that our tibble has one column (new_col) and one observation, which is a list with two elements. The first element in the list is a vector of the integers 1 and 2 and the second element is a vector of the integers 3, 4, and 5. The tibble only has one row because there is only one value for new_col. Note that this is a case where it is crucial to use tibble(), not data.frame()! If we had used data.frame() in the last example, it wouldn’t have worked: data.frame(new_col = list(1:2, 3:5)) %&gt;% str() Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 2, 3 Lists are very flexible. For example, each element of a list can have its own data type: tibble(new_col = list(1:2, c(&quot;Alice&quot;, &quot;Bob&quot;))) %&gt;% str() Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2 obs. of 1 variable: $ new_col:List of 2 ..$ : int 1 2 ..$ : chr &quot;Alice&quot; &quot;Bob&quot; Now the first element consists of the integers 1 and 2 while the second is a character vector containing “Alice” and “Bob”. We wrapped “Alice” and “Bob” in c() in order to make it clear that this is a vector. If we hadn’t done that, we would have three elements in our list: the vector with 1 and 2, “Alice”, and “Bob”: tibble(new_col = list(1:2, &quot;Alice&quot;, &quot;Bob&quot;)) %&gt;% str() Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 1 variable: $ new_col:List of 3 ..$ : int 1 2 ..$ : chr &quot;Alice&quot; ..$ : chr &quot;Bob&quot; 6.4.2 Creating list columns with mutate() Any function that returns multiple values can be used to create a list column. For example, consider the following tibble: tibble(col_1 = c(&quot;Alice and Bob&quot;, &quot;Carol and Dan&quot;)) col_1 is a character vector with two observations: “Alice and Bob” and “Carol and Dan”. The tibble has two rows. It may be more useful to present these as vectors of names, without the annoying “and” in between. That’s exactly what we can do with str_split(): tibble(col_1 = c(&quot;Alice and Bob&quot;, &quot;Carol and Dan&quot;)) %&gt;% mutate(col_2 = str_split(col_1, &quot; and &quot;)) %&gt;% str() Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2 obs. of 2 variables: $ col_1: chr &quot;Alice and Bob&quot; &quot;Carol and Dan&quot; $ col_2:List of 2 ..$ : chr &quot;Alice&quot; &quot;Bob&quot; ..$ : chr &quot;Carol&quot; &quot;Dan&quot; After using str_split() within mutate(), we have created a new column, and that new column is a list column. This is often how we will go about creating list columns. Let’s practice with the gapminder dataset. How could we add a column to the dataset that included the quantiles of the lifeExp variable? gapminder %&gt;% group_by(year) %&gt;% mutate(lifeExpQuantile = list(quantile(lifeExp))) # A tibble: 1,704 x 7 # Groups: year [12] country continent year lifeExp pop gdpPercap lifeExpQuantile &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;list&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. &lt;dbl [5]&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. &lt;dbl [5]&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. &lt;dbl [5]&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. &lt;dbl [5]&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. &lt;dbl [5]&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. &lt;dbl [5]&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. &lt;dbl [5]&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. &lt;dbl [5]&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. &lt;dbl [5]&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. &lt;dbl [5]&gt; # … with 1,694 more rows Note: str_split() was a particularly easy function for using with mutate() and summarize() because it returns a list. You can check this by running typeof() on the output of str_split(): e.g., str_split(&quot;Alice and Bob&quot;, &quot; and &quot;) %&gt;% typeof(). If a function returns multiple values as a vector, like quantile() does, you can’t use it directly in mutate() or summarize(), but you can wrap list() around it in order to get the same behavior. Or let’s say that we wanted 1) to subset the dataset to the most recent year, 2) group by continent, and 3) get a summary() of the gdpPercap variable by continent: gapminder %&gt;% filter(year == max(year)) %&gt;% group_by(continent) %&gt;% summarize(gdpSummary = list(summary(gdpPercap))) # A tibble: 5 x 2 continent gdpSummary &lt;fct&gt; &lt;list&gt; 1 Africa &lt;smmryDfl&gt; 2 Americas &lt;smmryDfl&gt; 3 Asia &lt;smmryDfl&gt; 4 Europe &lt;smmryDfl&gt; 5 Oceania &lt;smmryDfl&gt; 6.4.3 map_* functions What can we do with a list column? That is tricky! Lists are hard to work with. But the most natural thing to do with a list is to feed it to a map_* function. What are these functions? Let’s say that you want to add 1 to row of a variable in a tibble, storing the result in x. You could accomplish this with mutate: tibble(start = c(3, 2, 6)) %&gt;% mutate(new = start + 1) # A tibble: 3 x 2 start new &lt;dbl&gt; &lt;dbl&gt; 1 3 4 2 2 3 3 6 7 We could instead create a function that adds 1 to a number. Then, we can use map_dbl() to apply this to our input variable. map_dbl() — pronounced “map double” — comes from the purrr package, which you will have loaded if you have loaded tidyverse. map_dbl() is just one member of a family of map_* functions which applies the same function to every row in a tibble. The “dbl” suffix indicates that the function returns a “double,” meaning a numeric value. increment &lt;- function(x) return(x + 1) tibble(start = c(3, 2, 6)) %&gt;% mutate(new = map_dbl(start, increment)) # A tibble: 3 x 2 start new &lt;dbl&gt; &lt;dbl&gt; 1 3 4 2 2 3 3 6 7 What happened is that map_dbl() took the function increment() and applied it to each element of start. The syntax can get even simpler than this if we use anonymous functions. The following code snippet shows two ways to use anonymous functions. The first one, using base R, creates a function within map_dbl() using function(). The second, using the purrr package (from where we get map_dbl()), starts with the ~ operator and then uses . to represent the current element: tibble(start = c(3, 2, 6)) %&gt;% mutate(new = map_dbl(start, function(x) x + 1)) # A tibble: 3 x 2 start new &lt;dbl&gt; &lt;dbl&gt; 1 3 4 2 2 3 3 6 7 tibble(start = c(3, 2, 6)) %&gt;% mutate(new = map_dbl(start, ~ . + 1)) # A tibble: 3 x 2 start new &lt;dbl&gt; &lt;dbl&gt; 1 3 4 2 2 3 3 6 7 The latter shorthand is very convenient once you get used to it. Note that we are piping start directly into map_dbl(), which we can do because like other tidyverse functions, map_dbl() takes its data as the first argument. We called these map_* functions (plural) before. If you know the expected output of your function, you can specify that kind of vector: map(): list map_lgl(): logical map_int(): integer map_dbl(): double (numeric) map_chr(): character map_df(): data frame So, since our above code produces numeric output, we use map_dbl() instead of map():13 6.4.4 Using map_* functions to create list columns Now that we understand map_* functions, we can also use them to create list columns. We’ll use the weather dataset in the nycflights13 package. First, let’s wrangle the data so each observation is a day rather than an hour. We’ll create a list column temps_F that consists of all the temperatures recorded that day at a particular origin. weather %&gt;% group_by(origin, year, month, day) %&gt;% summarize(temps_F = list(c(temp))) Now that we have a list column, we can use it as the input to map(), outputting another list column. Let’s say we wanted a new list column, temps_C, which records the temperature in Celsius: weather %&gt;% group_by(origin, year, month, day) %&gt;% summarize(temps_F = list(c(temp))) %&gt;% mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9)) Note that we took the list column temps_F and, by applying an anonymous function to it with map(), created another list column temps_C. This is a very common process. It is similar to taking a tibble and piping it into a dplyr function (such as mutate()) which gives you a new tibble that you can work with. You can also use map_* functions to take a list column as an input and return an atomic vector – a column with a single value per observation – as an output. For instance, let’s say we now wanted the mean of the recorded temperatures per day in Celsius: weather %&gt;% group_by(origin, year, month, day) %&gt;% summarize(temps_F = list(c(temp))) %&gt;% mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9), mean_C = map_dbl(temps_C, mean, na.rm = TRUE)) Here, we also see that the map_* functions have the ... argument, which allows na.rm = TRUE to be passed along to mean(). What if we wanted to know the proportion of temperatures recorded per day per airport that were below freezing? weather %&gt;% group_by(origin, year, month, day) %&gt;% summarize(temps_F = list(c(temp))) %&gt;% mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9), is_freezing = map(temps_C, ~ . &lt; 0), prop_freezing = map_dbl(is_freezing, mean, na.rm = TRUE)) See how we chained the map_* functions: temps_F was used as the input to map() to create temps_C temps_C was used as the input to map() to create is_freezing is_freezing was used as the input to map_dbl() to create prop_freezing Or let’s say we wanted to know the top 5 temperatures recorded at each airport each day: weather %&gt;% group_by(origin, year, month, day) %&gt;% summarize(temps_F = list(c(temp))) %&gt;% mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9), sorted_C = map(temps_C, ~ sort(., decreasing = TRUE)), top5_C = map(temps_C, ~ .[1:5])) 6.4.5 Practice with map_* functions and list columns Let’s practice map_* functions and list columns! We will give step by step instructions; we recommend that you follow along so that you understand how each part works. We’ll also be introducing some important conditional functions (ifelse(), any(), all(), and case_when()). Write a function called roll_dice(), which will throw a pair of dice as many times as the user specifies. For clarity, we will begin by creating an intermediate function add_dice() which throws n dice and adds the results: add_dice &lt;- function(n = 1) { stopifnot(is.numeric(n)) sum(sample(1:6, n, replace = TRUE)) } Next, we will create roll_dice(), which calls add_dice(n = 2) as many times as the user specifies: roll_dice &lt;- function(n = 1) { stopifnot(is.numeric(n)) map_int(rep(2, n), add_dice) } rep() is a useful input to map_* when you want to call a function with the same input multiple times. rep(2, n) creates a vector of length n where every element is 2. We use that as the input to map_int() because we want the input to add_dice() to be 2 every time (we are always throwing a pair of dice) and we want to perform the operation n times, chosen by the user. Create a tibble named x with one variable: throws. throws is a list column, each element of which is three throws of the dice pair, i.e., the result of calling roll_dice(n = 3). Thus, our tibble will have ten rows and two columns. x &lt;- tibble(throws = map(rep(3, 10), roll_dice)) Add a variable to x called first_seven which is TRUE if the first roll in throws is a 7. library(magrittr) # The magrittr package allows us to use %&lt;&gt;%, the compound assignment pipe # operator, which (as its name suggests) both assigns and pipes x %&lt;&gt;% mutate(first_seven = map_lgl(throws, ~ ifelse(.[[1]] == 7, TRUE, FALSE))) We see here that [[1]] is how we extract the first element of a list, just like [1] extracts the first element of an atomic vector. ifelse() takes as its first argument a condition; if it is TRUE it returns the second argument (here TRUE) and if not the third (here FALSE). Add a variable to x called a_winner which is TRUE if at least one of the three throws is a 7 or an 11 and is FALSE otherwise. x %&lt;&gt;% mutate(a_winner = map_lgl(throws, ~ ifelse(any(c(7, 11) %in% .), TRUE, FALSE))) Here we use any(): any() checks if any element in its input is TRUE. So, let’s say a particular throw was 4, 6, and 7: c(7, 11) %in% c(4, 6, 7) returns the vector TRUE FALSE (since 7 is in the vector but 11 isn’t); then, any(c(7, 11) %in% c(4, 6, 7)) returns TRUE because one of the conditions is TRUE. Run str() on x and show the results. str(x) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 10 obs. of 3 variables: $ throws :List of 10 ..$ : int 12 10 9 ..$ : int 7 9 7 ..$ : int 8 7 7 ..$ : int 4 10 5 ..$ : int 7 7 9 ..$ : int 7 6 3 ..$ : int 4 8 4 ..$ : int 9 3 11 ..$ : int 4 5 7 ..$ : int 10 5 8 $ first_seven: logi FALSE TRUE FALSE FALSE TRUE TRUE ... $ a_winner : logi FALSE TRUE TRUE FALSE TRUE TRUE ... Calculate how “surpised” you should be if someone rolls three winners in a row. First, create a tibble with 10,000 rows. Include a throws list column with three throws of our dice, just as in part a). Second, create a column called perfection which is TRUE if all three of the throws are either 7 or 11. surprised &lt;- tibble(throws = map(rep(3, 10000), roll_dice)) %&gt;% mutate(perfection = map_lgl(throws, ~ ifelse(all(. %in% c(7, 11)), TRUE, FALSE))) surprised %&gt;% pull(perfection) %&gt;% mean() [1] 0.0112 Here, we use all(), which has the same structure as any(), but checks whether all the elements in the input are TRUE. Approximately 1.1% of three rolls of a pair of fair dice are all equal to either 7 or 11. Your friend proposes the following bet. You will roll a pair of fair dice 10 times. Side A gets the second highest of the first 4 rolls. Side B gets 1 plus the the median of the remaining 6 rolls. Which side is more likely to win? What’s the chance of a tie? bet &lt;- tibble(throws = map(rep(10, 10000), roll_dice)) %&gt;% mutate(A = map_dbl(throws, ~ sort(.[1:4])[3]), B = map_dbl(throws, ~ 1 + median(.[5:10])), winner = case_when(A &gt; B ~ &quot;A&quot;, B &gt; A ~ &quot;B&quot;, TRUE ~ &quot;tie&quot;)) case_when(sum(bet$winner == &quot;A&quot;) &gt; sum(bet$winner == &quot;B&quot;) ~ &quot;A&quot;, sum(bet$winner == &quot;B&quot;) &gt; sum(bet$winner == &quot;A&quot;) ~ &quot;B&quot;, TRUE ~ &quot;Same # Victories&quot;) [1] &quot;B&quot; sum(bet$winner == &quot;tie&quot;)/length(bet$winner) [1] 0.11 You can think of case_when() as a generalized ifelse(). The syntax is a little more complicated. Each expression is followed by ~ and what should be returned if that expression is TRUE. The final expression, TRUE, is always true, and thus is the residual category if none of the above expression are TRUE. Thus, the second case_when() in our above code will print “A” if A is the winner in more replications than B, “B” if B is the winner more than A, and “Same # Victories” otherwise. Side B is more likely to win. The chance of a tie is approximately 11%. 6.5 Resources Hadley Wickham’s book [Advanced R][adv-r] (???) Section on [function arguments][adv-r-fxn-args] Unit testing with testthat On [CRAN][testthat-cran], development on [GitHub][testthat-github], main [webpage][testthat-web] Wickham and Bryan’s [R Packages][r-pkgs2] book (???) Testing chapter Wickham’s [testthat: Get Started with Testing][testthat-article] article in The R Journal (???). Maybe this is completely superseded by the newer chapter above? Be aware that parts could be out of date, but I recall it was a helpful read. If I were going to give a lecture on these topics, it would look like this one.↩ You may wonder why we aren’t using map_int() instead. We could, but we’d have to write the code a little differently. R uses L to mark integers, so we’d have to say map_int(~ . + 1L).↩ "],
["7-probability.html", "Chapter 7 Probability 7.1 Defining probability 7.2 Conditional probability 7.3 Random variables 7.4 Statistical Background 7.5 Basic statistical terms 7.6 Normal distribution 7.7 log10 transformations", " Chapter 7 Probability Probability forms a foundation for statistics. You might already be familiar with many aspects of probability, however, formalization of the concepts is new for most. This chapter aims to introduce probability on familiar terms using processes most people have seen before. 7.1 Defining probability 7.1.1 Intro Questions Q1: A “die”, the singular of dice, is a cube with six faces numbered 1, 2, 3, 4, 5, and 6. What is the chance of getting 1 when rolling a die S1: If the die is fair, then the chance of a 1 is as good as the chance of any other number. Since there are six outcomes, the chance must be 1-in-6 or, equivalently, \\(1/6\\). Q2: What is the chance of getting a 1 or 2 in the next roll S2: 1 and 2 constitute two of the six equally likely possible outcomes, so the chance of getting one of these two outcomes must be \\(2/6 = 1/3\\). Q3: What is the chance of getting either 1, 2, 3, 4, 5, or 6 on the next roll S3: 100%. The outcome must be one of these numbers. Q4: What is the chance of not rolling a 2 S4: Since the chance of rolling a 2 is \\(1/6\\) or \\(16.\\bar{6}\\%\\), the chance of not rolling a 2 must be \\(100\\% - 16.\\bar{6}\\%=83.\\bar{3}\\%\\) or \\(5/6\\). Alternatively, we could have noticed that not rolling a 2 is the same as getting a 1, 3, 4, 5, or 6, which makes up five of the six equally likely outcomes and has probability \\(5/6\\). Q5: Consider rolling two dice. If \\(1/6^{th}\\) of the time the first die is a 1 and \\(1/6^{th}\\) of those times the second die is a 1, what is the chance of getting two 1s S: If \\(16.\\bar{6}\\)% of the time the first die is a 1 and \\(1/6^{th}\\) of those times the second die is also a 1, then the chance that both dice are 1 is \\((1/6)\\times (1/6)\\) or \\(1/36\\). 7.1.2 Probability We use probability to build tools to describe and understand apparent randomness. We often frame probability in terms of a random process giving rise to an . Roll a die \\(\\rightarrow\\) 1, 2, 3, 4, 5, or 6 Flip a coin \\(\\rightarrow\\) H or T Rolling a die or flipping a coin is a seemingly random process and each gives rise to an outcome. Def of Probability The probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times. Probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). It may also be displayed as a percentage between 0% and 100%. Probability can be illustrated by rolling a die many times. Let \\(\\hat{p}_n\\) be the proportion of outcomes that are 1 after the first \\(n\\) rolls. As the number of rolls increases, \\(\\hat{p}_n\\) will converge to the probability of rolling a 1, \\(p = 1/6\\). Figure 1.1 shows this convergence for 100,000 die rolls. The tendency of \\(\\hat{p}_n\\) to stabilize around \\(p\\) is described by the Law of Large Numbers. The fraction of die rolls that are 1 at each stage in a simulation. The proportion tends to get closer to the probability \\(1/6 \\approx 0.167\\) as the number of rolls increases. Def of Law of Large Numbers As more observations are collected, the proportion \\(\\hat{p}_n\\) of occurrences with a particular outcome converges to the probability \\(p\\) of that outcome. Occasionally the proportion will veer off from the probability and appear to defy the Law of Large Numbers, as \\(\\hat{p}_n\\) does many times in Figure 1.1. However, these deviations become smaller as the number of rolls increases. Above we write \\(p\\) as the probability of rolling a 1. We can also write this probability as \\[\\begin{aligned} P(\\text{rolling a 1})\\end{aligned}\\] As we become more comfortable with this notation, we will abbreviate it further. For instance, if it is clear that the process is “rolling a die”, we could abbreviate \\(P(\\)rolling a 1\\()\\) as \\(P(\\)1\\()\\). Guided Practice Random processes include rolling a die and flipping a coin. Think of another random process. Describe all the possible outcomes of that process. For instance, rolling a die is a random process with potential outcomes 1, 2, ..., 6.14 What we think of as random processes are not necessarily random, but they may just be too difficult to understand exactly. The fourth example in the footnote solution to Guided Practice suggests a roommate’s behavior is a random process. However, even if a roommate’s behavior is not truly random, modeling her behavior as a random process can still be useful. Tip It can be helpful to model a process as random even if it is not truly random. 7.1.3 Disjoint or mutually exclusive outcomes Two outcomes are called disjoint or mutually exclusive if they cannot both happen. For instance, if we roll a die, the outcomes 1 and 2 are disjoint since they cannot both occur. On the other hand, the outcomes 1 and “rolling an odd number” are not disjoint since both occur if the outcome of the roll is a 1. The terms disjoint and mutually exclusive are equivalent and interchangeable. Calculating the probability of disjoint outcomes is easy. When rolling a die, the outcomes 1 and 2 are disjoint, and we compute the probability that one of these outcomes will occur by adding their separate probabilities: \\[\\begin{aligned} P(\\text{1 or 2}) = P(\\text{1})+P(\\text{2}) = 1/6 + 1/6 = 1/3\\end{aligned}\\] What about the probability of rolling a 1, 2, 3, 4, 5, or 6 Here again, all of the outcomes are disjoint so we add the probabilities: \\[\\begin{aligned} &amp;&amp;P(\\text{1 or 2 or 3 or 4 or 5 or 6}) \\\\ &amp;&amp;\\quad= P(\\text{1})+P(\\text{2})+P(\\text{3})+P(\\text{4})+P(\\text{5})+P(\\text{6}) \\\\ &amp;&amp;\\quad= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.\\end{aligned}\\] The Addition Rule guarantees the accuracy of this approach when the outcomes are disjoint. Addition Rule of disjoint outcomes If \\(A_1\\) and \\(A_2\\) represent two disjoint outcomes, then the probability that one of them occurs is given by \\[\\begin{aligned} P(A_1\\text{ or } A_2) = P(A_1) + P(A_2)\\end{aligned}\\] If there are many disjoint outcomes \\(A_1\\), ..., \\(A_k\\), then the probability that one of these outcomes will occur is \\[\\begin{aligned} P(A_1) + P(A_2) + \\cdots + P(A_k)\\end{aligned}\\] Guided Practice We are interested in the probability of rolling a 1, 4, or 5. Explain why the outcomes 1, 4, and 5 are disjoint. Apply the Addition Rule for disjoint outcomes to determine \\(P(\\)1 or 4 or 5\\()\\).15 Statisticians rarely work with individual outcomes and instead consider or of outcomes. Let \\(A\\) represent the event where a die roll results in 1 or 2 and \\(B\\) represent the event that the die roll is a 4 or a 6. We write \\(A\\) as the set of outcomes \\(\\{\\)1, 2\\(\\}\\) and \\(B=\\{\\)4, 6\\(\\}\\). These sets are commonly called . Because \\(A\\) and \\(B\\) have no elements in common, they are disjoint events. \\(A\\) and \\(B\\) are represented in Figure 1.2. Three events, \\(A\\), \\(B\\), and \\(D\\), consist of outcomes from rolling a die. \\(A\\) and \\(B\\) are disjoint since they do not have any outcomes in common. The Addition Rule applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events \\(A\\) or \\(B\\) occurs is the sum of the separate probabilities: \\[\\begin{aligned} P(A\\text{ or }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3\\end{aligned}\\] Verify the probability of event \\(A\\), \\(P(A)\\), is \\(1/3\\) using the Addition Rule. Do the same for event \\(B\\).16 Using Figure 1.2 as a reference, what outcomes are represented by event \\(D\\) Are events \\(B\\) and \\(D\\) disjoint Are events \\(A\\) and \\(D\\) disjoint 17 In the Guided Practice above, you confirmed \\(B\\) and \\(D\\) from Figure 1.2 are disjoint. Compute the probability that either event \\(B\\) or event \\(D\\) occurs.18 7.1.4 Probabilities when events are not disjoint Let’s consider calculations for two events that are not disjoint in the context of a , represented in Table below. If you are unfamiliar with the cards in a regular deck, please see the footnote.19 Representations of the 52 unique cards in a deck. 2\\(\\clubsuit\\) 3\\(\\clubsuit\\) 4\\(\\clubsuit\\) 5\\(\\clubsuit\\) 6\\(\\clubsuit\\) 7\\(\\clubsuit\\) 8\\(\\clubsuit\\) 9\\(\\clubsuit\\) 10\\(\\clubsuit\\) J\\(\\clubsuit\\) Q\\(\\clubsuit\\) K\\(\\clubsuit\\) A\\(\\clubsuit\\) 2\\(\\diamondsuit\\) 3\\(\\diamondsuit\\) 4\\(\\diamondsuit\\) 5\\(\\diamondsuit\\) 6\\(\\diamondsuit\\) 7\\(\\diamondsuit\\) 8\\(\\diamondsuit\\) 9\\(\\diamondsuit\\) 10\\(\\diamondsuit\\) J\\(\\diamondsuit\\) Q\\(\\diamondsuit\\) K\\(\\diamondsuit\\) A\\(\\diamondsuit\\) 2\\(\\heartsuit\\) 3\\(\\heartsuit\\) 4\\(\\heartsuit\\) 5\\(\\heartsuit\\) 6\\(\\heartsuit\\) 7\\(\\heartsuit\\) 8\\(\\heartsuit\\) 9\\(\\heartsuit\\) 10\\(\\heartsuit\\) J\\(\\heartsuit\\) Q\\(\\heartsuit\\) K\\(\\heartsuit\\) A\\(\\heartsuit\\) 2\\(\\spadesuit\\) 3\\(\\spadesuit\\) 4\\(\\spadesuit\\) 5\\(\\spadesuit\\) 6\\(\\spadesuit\\) 7\\(\\spadesuit\\) 8\\(\\spadesuit\\) 9\\(\\spadesuit\\) 10\\(\\spadesuit\\) J\\(\\spadesuit\\) Q\\(\\spadesuit\\) K\\(\\spadesuit\\) A\\(\\spadesuit\\) Guided Practice What is the probability that a randomly selected card is a diamond What is the probability that a randomly selected card is a face card 20 Venn diagrams are useful when outcomes can be categorized as “in” or “out” for two or three variables, attributes, or random processes. The Venn diagram in Figure 1.3 uses a circle to represent diamonds and another to represent face cards. If a card is both a diamond and a face card, it falls into the intersection of the circles. If it is a diamond but not a face card, it will be in part of the left circle that is not in the right circle (and so on). The total number of cards that are diamonds is given by the total number of cards in the diamonds circle: \\(10+3=13\\). The probabilities are also shown (e.g. \\(10/52 = 0.1923\\)). A Venn diagram for diamonds and face cards. Guided Practice Using the Venn diagram, verify \\(P(\\)face card\\() = 12/52=3/13\\).21 Let \\(A\\) represent the event that a randomly selected card is a diamond and \\(B\\) represent the event that it is a face card. How do we compute \\(P(A\\) or \\(B)\\) Events \\(A\\) and \\(B\\) are not disjoint – the cards \\(J\\diamondsuit\\), \\(Q\\diamondsuit\\), and \\(K\\diamondsuit\\) fall into both categories – so we cannot use the Addition Rule for disjoint events. Instead we use the Venn diagram. We start by adding the probabilities of the two events: \\[\\begin{aligned} P(A) + P(B) = P({\\color{redcards}\\diamondsuit}) + P(\\text{face card}) = 12/52 + 13/52 \\label{overCountFaceDiamond}\\end{aligned}\\] However, the three cards that are in both events were counted twice, once in each probability. We must correct this double counting: \\[\\begin{aligned} P(A\\text{ or } B) &amp;=&amp;P(\\text{face card or }{\\color{redcards}\\diamondsuit}) \\notag \\\\ &amp;=&amp; P(\\text{face card}) + P({\\color{redcards}\\diamondsuit}) - P(\\text{face card and }{\\color{redcards}\\diamondsuit}) \\label{diamondFace} \\\\ &amp;=&amp; 12/52 + 13/52 - 3/52 \\notag \\\\ &amp;=&amp; 22/52 = 11/26 \\notag\\end{aligned}\\] The Equation above is an example of the General Addition Rule. General Addition Rule If \\(A\\) and \\(B\\) are any two events, disjoint or not, then the probability that at least one of them will occur is \\[\\begin{aligned} P(A\\text{ or }B) = P(A) + P(B) - P(A\\text{ and }B) \\label{generalAdditionRule}\\end{aligned}\\] where \\(P(A\\) and \\(B)\\) is the probability that both events occur. Tip When we write “or” in statistics, we mean “and/or” unless we explicitly state otherwise. Thus, \\(A\\) or \\(B\\) occurs means \\(A\\), \\(B\\), or both \\(A\\) and \\(B\\) occur. Guided Practice If \\(A\\) and \\(B\\) are disjoint, describe why this implies \\(P(A\\) and \\(B) = 0\\). Using part (a), verify that the General Addition Rule simplifies to the simpler Addition Rule for disjoint events if \\(A\\) and \\(B\\) are disjoint.22 In a given email data set with 3,921 emails, 367 were spam, 2,827 contained some small numbers but no big numbers, and 168 had both characteristics. Create a Venn diagram for this setup.23 Use your Venn diagram from (c) to determine the probability a randomly drawn email from the data set is spam and had small numbers (but not big numbers). What is the probability that the email had either of these attributes 24 7.1.5 Probability distributions A Probability distribution is a table of all disjoint outcomes and their associated probabilities. The table below shows the probability distribution for the sum of two dice. Probability distribution for the sum of two dice. Dice sum 2 3 4 5 6 7 8 9 10 11 12 Probability \\(\\frac{1}{36}\\) \\(\\frac{2}{36}\\) \\(\\frac{3}{36}\\) \\(\\frac{4}{36}\\) \\(\\frac{5}{36}\\) \\(\\frac{6}{36}\\) \\(\\frac{5}{36}\\) \\(\\frac{4}{36}\\) \\(\\frac{3}{36}\\) \\(\\frac{2}{36}\\) \\(\\frac{1}{36}\\) Rules for probability distributions A probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules: The outcomes listed must be disjoint. Each probability must be between 0 and 1. The probabilities must total 1. Guided Practice The table below suggests three distributions for household income in the United States. Only one is correct. Which one must it be? What is wrong with the other two 25 Proposed distributions of US household incomes. ncome range ($1000s) 0 -25 2 5-50 5 0-100 1 00+ \\(a\\) 0.18 0.39 0.33 0.16 \\(b\\) 0.38 -0.27 0.52 0.37 \\(c\\) 0.28 0.27 0.29 0.16 Probability distributions can also be summarized in a bar plot. For instance, the distribution of US household incomes is shown in Figure 1.4 as a bar plot.26 The probability distribution for the sum of two dice is shown in the table at the begining of the probability distributions section and plotted in Figure 1.5. The probability distribution of US household income. The probability distribution of the sum of two dice. In these bar plots, the bar heights represent the probabilities of outcomes. If the outcomes are numerical and discrete, it is usually (visually) convenient to make a bar plot that resembles a histogram, as in the case of the sum of two dice. Another example of plotting the bars at their respective locations is shown in Figure 1.11 on page . 7.1.6 Complement of an event Rolling a die produces a value in the set \\(\\{\\)1, 2, 3, 4, 5, 6\\(\\}\\). This set of all possible outcomes is called the sample space (\\(S\\)) for rolling a die. We often use the sample space to examine the scenario where an event does not occur. Let \\(D=\\{\\)2, 3\\(\\}\\) represent the event that the outcome of a die roll is 2 or 3. Then the complement of \\(D\\) represents all outcomes in our sample space that are not in \\(D\\), which is denoted by \\(D^c = \\{\\)1, 4, 5, 6\\(\\}\\). That is, \\(D^c\\) is the set of all possible outcomes not already included in \\(D\\). Figure 1.6 shows the relationship between \\(D\\), \\(D^c\\), and the sample space \\(S\\). Event \\(D=\\{\\)2, 3\\(\\}\\) and its complement, \\(D^c = \\{\\)1, 4, 5, 6\\(\\}\\). \\(S\\) represents the sample space, which is the set of all possible events. Guided Practice Compute \\(P(D^c) = P(\\)rolling a 1, 4, 5, or 6\\()\\). What is \\(P(D) + P(D^c)\\) 27 Events \\(A=\\{\\)1, 2\\(\\}\\) and \\(B=\\{\\)4, 6\\(\\}\\) are shown in Figure 1.2 on page . Guided Practice Write out what \\(A^c\\) and \\(B^c\\) represent. Compute \\(P(A^c)\\) and \\(P(B^c)\\). Compute \\(P(A)+P(A^c)\\) and \\(P(B)+P(B^c)\\).28 A complement of an event \\(A\\) is constructed to have two very important properties: (i) every possible outcome not in \\(A\\) is in \\(A^c\\), and (ii) \\(A\\) and \\(A^c\\) are disjoint. Property (i) implies \\[\\begin{aligned} P(A\\text{ or }A^c) = 1 \\label{complementSumTo1}\\end{aligned}\\] That is, if the outcome is not in \\(A\\), it must be represented in \\(A^c\\). We use the Addition Rule for disjoint events to apply Property (ii): \\[\\begin{aligned} P(A\\text{ or }A^c) = P(A) + P(A^c) \\label{complementDisjointEquation}\\end{aligned}\\] Combining the Equations above yields a very useful relationship between the probability of an event and its complement. Complement The complement of event \\(A\\) is denoted \\(A^c\\), and \\(A^c\\) represents all outcomes not in \\(A\\). \\(A\\) and \\(A^c\\) are mathematically related: \\[\\begin{aligned} \\label{complement} P(A) + P(A^c) = 1, \\quad\\text{i.e.}\\quad P(A) = 1-P(A^c)\\end{aligned}\\] In simple examples, computing \\(A\\) or \\(A^c\\) is feasible in a few steps. However, using the complement can save a lot of time as problems grow in complexity. Guided Practice Let \\(A\\) represent the event where we roll two dice and their total is less than 12. What does the event \\(A^c\\) represent Determine \\(P(A^c)\\) from the die sum table at the begining of the probability distribution section. Determine \\(P(A)\\).29 Consider again the probabilities from rolling two dice as in the table at the section on probability distributions. Find the following probabilities: The sum of the dice is not 6. The sum is at least 4, i.e. \\(\\{\\)4, 5, ..., 12\\(\\}\\). The sum is no more than 10. That is, determine the probability of the event \\(D=\\{\\)2, 3, ..., 10\\(\\}\\).30 7.1.7 Independence Just as variables and observations can be independent, random processes can be independent, too. Two processes are independent if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes – knowing the coin was heads does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent. The rolling two dice is a basic example of this. We want to determine the probability that both will be 1. Suppose one of the dice is red and the other white. If the outcome of the red die is a 1, it provides no information about the outcome of the white die. We first encountered this same question in in Q5 at the begining of this chapter, where we calculated the probability using the following reasoning: \\(1/6^{th}\\) of the time the red die is a 1, and \\(1/6^{th}\\) of those times the white die will also be 1. This is illustrated in Figure 1.7. Because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to get the final answer: \\((1/6)\\times(1/6)=1/36\\). This can be generalized to many independent processes. \\(1/6^{th}\\) of the time, the first roll is a 1. Then \\(1/6^{th}\\) of those times, the second roll will also be a 1. Example What if there was also a blue die independent of the other two? What is the probability of rolling the three dice and getting all 1s? If \\(1/36^{th}\\) of the time the white and red dice are both 1, then \\(1/6^{th}\\) of those times the blue die will also be 1, so multiply: \\[\\begin{aligned} P(white=\\text{1 and } red=\\text{1 and } blue=\\text{1}) &amp;= P(white=\\text{1})\\times P(red=\\text{1})\\times P(blue=\\text{1}) \\\\ &amp;= (1/6)\\times (1/6)\\times (1/6) = 1/216\\end{aligned}\\] The example above illustrate what is called the Multiplication Rule for independent processes. Multiplication Rule for independent processes If \\(A\\) and \\(B\\) represent events from two different and independent processes, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their separate probabilities: \\[\\begin{aligned} \\label{eqForIndependentEvents} P(A \\text{ and }B) = P(A) \\times P(B)\\end{aligned}\\] Similarly, if there are \\(k\\) events \\(A_1\\), ..., \\(A_k\\) from \\(k\\) independent processes, then the probability they all occur is \\[\\begin{aligned} P(A_1) \\times P(A_2)\\times \\cdots \\times P(A_k)\\end{aligned}\\] Guided Practice About 9% of people are left-handed. Suppose 2 people are selected at random from the U.S. population. Because the sample size of 2 is very small relative to the population, it is reasonable to assume these two people are independent. What is the probability that both are left-handed What is the probability that both are right-handed 31 Guided Practice Suppose 5 people are selected at random.32 What is the probability that all are right-handed What is the probability that all are left-handed What is the probability that not all of the people are right-handed Suppose the variables handedness and gender are independent, i.e. knowing someone’s gender provides no useful information about their handedness and vice-versa. Then we can compute whether a randomly selected person is right-handed and female33 using the Multiplication Rule: \\[\\begin{aligned} P(\\text{right-handed and female}) &amp;=&amp; P(\\text{right-handed}) \\times P(\\text{female}) \\\\ &amp;=&amp; 0.91 \\times 0.50 = 0.455\\end{aligned}\\] Guided Practice Three people are selected at random.34 What is the probability that the first person is male and right-handed What is the probability that the first two people are male and right-handed What is the probability that the third person is female and left-handed What is the probability that the first two people are male and right-handed and the third person is female and left-handed Sometimes we wonder if one outcome provides useful information about another outcome. The question we are asking is, are the occurrences of the two events independent We say that two events \\(A\\) and \\(B\\) are independent if they satisfy \\[P(A \\text{ and }B) = P(A) \\times P(B)\\] Example If we shuffle up a deck of cards and draw one, is the event that the card is a heart independent of the event that the card is an ace The probability the card is a heart is \\(1/4\\) and the probability that it is an ace is \\(1/13\\). The probability the card is the ace of hearts is \\(1/52\\). We check whether \\(P(A \\text{ and }B) = P(A) \\times P(B)\\) is satisfied: \\[\\begin{aligned} P({\\color{redcards}\\heartsuit})\\times P(\\text{ace}) = \\frac{1}{4}\\times \\frac{1}{13} = \\frac{1}{52} = P({\\color{redcards}\\heartsuit}\\text{ and ace})\\end{aligned}\\] Because the equation holds, the event that the card is a heart and the event that the card is an ace are independent events. 7.2 Conditional probability Are students more likely to use marijuana when their parents used drugs The data set contains a sample of 445 cases with two variables, students and parents, and is summarized in the table below.35 The variable is either uses or not, where a student is labeled as if she has recently used marijuana. The student variable takes the value used if at least one of the parents used drugs, including alcohol. Contingency table summarizing the data set. *used * *not * Tota l uses 12 5 9 4 21 9 not 8 5 14 1 22 6 Total 21 0 23 5 44 5 A Venn diagram using boxes for the data set. Example If at least one parent used drugs, what is the chance their child () uses We will estimate this probability using the data. Of the 210 cases in this data set where = used, 125 represent cases where = uses: \\[\\begin{aligned} P(\\text{student = uses given parents = used}) = \\frac{125}{210} = 0.60\\end{aligned}\\] Example A student is randomly selected from the study and she does not use drugs. What is the probability that at least one of her parents used If the student does not use drugs, then she is one of the 226 students in the second row. Of these 226 students, 85 had at least one parent who used drugs: \\[\\begin{aligned} P(\\text{parents = used given student = not}) = \\frac{85}{226} = 0.376\\end{aligned}\\] 7.2.1 Marginal and joint probabilities The Table below includes row and column totals for each variable separately in the data set. These totals represent marginal probabilities for the sample, which are the probabilities based on a single variable without conditioning on any other variables. For instance, a probability based solely on the variable is a marginal probability: \\[\\begin{aligned} P(\\text{student = uses}) = \\frac{219}{445} = 0.492\\end{aligned}\\] A probability of outcomes for two or more variables or processes is called a joint probability: \\[\\begin{aligned} P(\\text{student = uses and parents = not}) = \\frac{94}{445} = 0.21\\end{aligned}\\] It is common to substitute a comma for “and” in a joint probability, although either is acceptable. Probability table summarizing parental and student drug use. : used : not T otal : uses 0.28 0.21 0.49 : not 0.19 0.32 0.51 Total 0.47 0.53 1.00 Marginal and joint Probabilities If a probability is based on a single variable, it is a marginal probability. The probability of outcomes for two or more variables or processes is called a joint probability. We use table proportions to summarize joint probabilities for the sample. These proportions are computed by dividing each count in the first table by 445 to obtain the proportions in the second table. The joint probability distribution of the and variables is shown in the table below. A joint probability distribution for the data set. int outcome P robability = used, = uses 0.28 = used, = not 0.19 = not, = uses 0.21 = not, = not 0.32 Total 1.00 Guided Practice Verify that the table above represents a probability distribution: events are disjoint, all probabilities are non-negative, and the probabilities sum to 1.36 We can compute marginal probabilities using joint probabilities in simple cases. For example, the probability a random student from the study uses drugs is found by summing the outcomes from the table above where = uses: \\[\\begin{aligned} &amp;&amp;P(\\text{student = uses}) \\\\ &amp;&amp; \\quad = P(\\text{parents = used, student = uses}) + \\\\ &amp;&amp; \\quad \\quad \\quad \\quad P(\\text{parents = not, student = uses}) \\\\ &amp;&amp; \\quad = 0.28 + 0.21 = 0.49\\end{aligned}\\] 7.2.2 Defining conditional probability There is some connection between drug use of parents and of the student: drug use of one is associated with drug use of the other.37 In this section, we discuss how to use information about associations between two variables to improve probability estimation. The probability that a random student from the study uses drugs is 0.49. Could we update this probability if we knew that this student’s parents used drugs Absolutely. To do so, we limit our view to only those 210 cases where parents used drugs and look at the fraction where the student uses drugs: \\[\\begin{aligned} P(\\text{student = uses given parents = used}) = \\frac{125}{210} = 0.60\\end{aligned}\\] We call this a conditional probability because we computed the probability under a condition: = used. There are two parts to a conditional probability, the outcome of interest and the condition . It is useful to think of the condition as information we know to be true, and this information usually can be described as a known outcome or event. We separate the text inside our probability notation into the outcome of interest and the condition: \\[\\begin{aligned} &amp;&amp; P(\\text{student = uses given parents = used}) \\notag \\\\ &amp;&amp; = P(\\text{student = uses } | \\text{ parents = used}) = \\frac{125}{210} = 0.60 \\label{probStudentUsedIfParentsUsedInFormalNotation}\\end{aligned}\\] The vertical bar “\\(|\\)” is read as given. In the Equation above, we computed the probability a student uses based on the condition that at least one parent used as a fraction: \\[\\begin{aligned} &amp;&amp; P(\\text{student = uses } | \\text{ parents = used}) \\notag \\\\ &amp;&amp;\\quad = \\frac{\\text{#times student = uses and parents = used}}{\\text{#times parents = used}} \\label{ratioOfBothToRatioOfConditionalForParentsAndStudent} \\\\ &amp;&amp;\\quad = \\frac{125}{210} = 0.60 \\notag\\end{aligned}\\] We considered only those cases that met the condition, = used, and then we computed the ratio of those cases that satisfied our outcome of interest, the student uses. Counts are not always available for data, and instead only marginal and joint probabilities may be provided. For example, disease rates are commonly listed in percentages rather than in a count format. We would like to be able to compute conditional probabilities even when no counts are available, and we use the equation above as an example demonstrating this technique. We considered only those cases that satisfied the condition, = used. Of these cases, the conditional probability was the fraction who represented the outcome of interest, = uses. Suppose we were provided only the information in the second table of this section i.e. only probability data. Then if we took a sample of 1000 people, we would anticipate about 47% or \\(0.47\\times 1000 = 470\\) would meet our information criterion. Similarly, we would expect about 28% or \\(0.28\\times 1000 = 280\\) to meet both the information criterion and represent our outcome of interest. Thus, the conditional probability could be computed: \\[\\begin{aligned} P(\\text{student = uses } | \\text{ parents = used}) &amp;= \\frac{\\text{#(student = uses and parents = used)}}{\\text{#(parents = used)}} \\notag \\\\ &amp;= \\frac{280}{470} = \\frac{0.28}{0.47} = 0.60 \\label{stUserPUsedHypSampSize}\\end{aligned}\\] In In the equation above, we examine exactly the fraction of two probabilities, 0.28 and 0.47, which we can write as \\[\\begin{aligned} P(student = uses\\ \\text{and}\\ parents = used) \\quad\\text{and}\\quad P(parents = used).\\end{aligned}\\] The fraction of these probabilities represents our general formula for conditional probability. Conditional Probability The conditional probability of the outcome of interest \\(A\\) given condition \\(B\\) is computed as the following: \\[\\begin{aligned} P(A | B) = \\frac{P(A\\text{ and }B)}{P(B)} \\label{condProbEq}\\end{aligned}\\] Guided Practice Write out the following statement in conditional probability notation: &quot;The probability a random case has = not* if it is known that = *not**&quot;. Notice that the condition is now based on the student, not the parent. Determine the probability from part (a). The Probability table summarizing parental and student drug use. may be helpful.38 Determine the probability that one of the parents had used drugs if it is known the student does not use drugs. Using the answers from part (b) and (c), compute \\[\\begin{aligned} P(\\text{parents = used}|\\text{student = not}) + P(\\text{parents = not}|\\text{student = not})\\end{aligned}\\] Provide an intuitive argument to explain why the sum in (b) is 1.39 The data indicate that drug use of parents and children are associated. Does this mean the drug use of parents causes the drug use of the students 40 7.2.3 Smallpox in Boston, 1721 The data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in Boston.41 Doctors at the time believed that inoculation, which involves exposing a person to the disease in a controlled form, could reduce the likelihood of death. Each case represents one person with two variables: innoculated and result . The variable takes two levels: yes or no, indicating whether the person was inoculated or not. The variable has outcomes lived or died. These data are summarized in the tables below. Contingency table for the data set. ye s n o Total lived 23 8 513 6 5374 died 6 84 4 850 Total 24 4 598 0 6224 Table proportions for the data, computed by dividing each count by the table total, 6224. ye s n o Total lived 0.038 2 0.825 2 0.8634 died 0.001 0 0.135 6 0.1366 Total 0.039 2 0.960 8 1.0000 : Table proportions for the data, computed by dividing each count by the table total, 6224. Guided Practice Write out, in formal notation, the probability a randomly selected person who was not inoculated died from smallpox, and find this probability.42 Guided Practice Determine the probability that an inoculated person died from smallpox. How does this result compare with the result of the Guided Practice above 43 Guided Practice The people of Boston self-selected whether or not to be inoculated. (a) Is this study observational or was this an experiment Can we infer any causal connection using these data What are some potential confounding variables that might influence whether someone lived or died and also affect whether that person was inoculated 44 7.2.4 General multiplication rule Section 1.1.6 introduced the Multiplication Rule for independent processes. Here we provide the General Multiplication rule for events that might not be independent. General Multiplication Rule If \\(A\\) and \\(B\\) represent two outcomes or events, then \\[\\begin{aligned} P(A\\text{ and }B) = P(A | B)\\times P(B)\\end{aligned}\\] It is useful to think of \\(A\\) as the outcome of interest and \\(B\\) as the condition. This General Multiplication Rule is simply a rearrangement of the definition for conditional probability. Example Consider the smallpox data set. Suppose we are given only two pieces of information: 96.08% of residents were not inoculated, and 85.88% of the residents who were not inoculated ended up surviving. How could we compute the probability that a resident was not inoculated and lived We will compute our answer using the General Multiplication Rule and then verify it using the smallpox probability table. We want to determine \\[\\begin{aligned} P(\\text{result = lived and inoculated = no})\\end{aligned}\\] and we are given that \\[\\begin{aligned} P(\\text{result = lived }|\\text{ inoculated = no})=0.8588 \\\\ P(\\text{inoculated = no})=0.9608\\end{aligned}\\] Among the 96.08% of people who were not inoculated, 85.88% survived: \\[\\begin{aligned} P(\\text{result = lived and inoculated = no}) = 0.8588\\times 0.9608 = 0.8251\\end{aligned}\\] This is equivalent to the General Multiplication Rule. We can confirm this probability in the smallpox probability table at the intersection of no and lived (with a small rounding error). Guided Practice Use \\(P(\\) = yes\\() = 0.0392\\) and \\(P(\\) = lived \\(|\\) = yes\\() = 0.9754\\) to determine the probability that a person was both inoculated and lived.45 Guided Practice If 97.45% of the people who were inoculated lived, what proportion of inoculated people must have died 46 Sum of conditional probabilities Let \\(A_1\\), ..., \\(A_k\\) represent all the disjoint outcomes for a variable or process. Then if \\(B\\) is an event, possibly for another variable or process, we have: \\[\\begin{aligned} P(A_1|B)+\\cdots+P(A_k|B) = 1\\end{aligned}\\] The rule for complements also holds when an event and its complement are conditioned on the same information: \\[\\begin{aligned} P(A | B) = 1 - P(A^c | B)\\end{aligned}\\] Guided Practice Based on the probabilities computed above, does it appear that inoculation is effective at reducing the risk of death from smallpox 47 7.2.5 Independence considerations in conditional probability If two processes are independent, then knowing the outcome of one should provide no information about the other. We can show this is mathematically true using conditional probabilities. Guided Practice Let \\(X\\) and \\(Y\\) represent the outcomes of rolling two dice. What is the probability that the first die, \\(X\\), is 1 What is the probability that both \\(X\\) and \\(Y\\) are 1 Use the formula for conditional probability to compute \\(P(Y =\\) 1 \\(| X =\\) 1\\()\\). What is \\(P(Y=1)\\) Is this different from the answer from part (c) Explain.48 We can show in the Guided Practice (c) that the conditioning information has no influence by using the Multiplication Rule for independence processes: \\[\\begin{aligned} P(Y=\\text{1}|X=\\text{1}) &amp;=&amp; \\frac{P(Y=\\text{1 and }X=\\text{1})}{P(X=\\text{1})} \\\\ &amp;=&amp; \\frac{P(Y=\\text{1})\\times \\color{oiGB}P(X=\\text{1})}{\\color{oiGB}P(X=\\text{1})} \\\\ &amp;=&amp; P(Y=\\text{1}) \\\\\\end{aligned}\\] Guided Practice Ron is watching a roulette table in a casino and notices that the last five outcomes were black. He figures that the chances of getting black six times in a row is very small (about \\(1/64\\)) and puts his paycheck on red. What is wrong with his reasoning 49 7.2.6 Tree diagrams Tree diagrams are a tool to organize outcomes and probabilities around the structure of the data. They are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors. The data fit this description. We see the population as split by : yes and no. Following this split, survival rates were observed for each group. This structure is reflected in the *tree diagram shown in Figure 1.9. The first branch for is said to be the primary branch while the other branches are secondary** . A tree diagram of the data set. Tree diagrams are annotated with marginal and conditional probabilities, as shown in Figure 1.9. This tree diagram splits the smallpox data by innoculation into the yes and no groups with respective marginal probabilities 0.0392 and 0.9608. The secondary branches are conditioned on the first, so we assign conditional probabilities to these branches. For example, the top branch in Figure 1.9 is the probability that result = lived conditioned on the information that innoculated = yes. We may (and usually do) construct joint probabilities at the end of each branch in our tree by multiplying the numbers we come across as we move from left to right. These joint probabilities are computed using the General Multiplication Rule: \\[\\begin{aligned} &amp;&amp; P(\\text{inoculated = yes and result = lived}) \\\\ &amp;&amp;\\quad = P(\\text{inoculated = yes})\\times P(\\text{result = lived}|\\text{inoculated = yes}) \\\\ &amp;&amp;\\quad = 0.0392\\times 0.9754=0.0382\\end{aligned}\\] Example Consider the midterm and final for a statistics class. Suppose 13% of students earned an A on the midterm. Of those students who earned an A on the midterm, 47% received an A on the final, and 11% of the students who earned lower than an A on the midterm received an A on the final. You randomly pick up a final exam and notice the student received an A. What is the probability that this student earned an A on the midterm The end-goal is to find \\(P(\\text{midterm = A} | \\text{final = A})\\). To calculate this conditional probability, we need the following probabilities: \\[\\begin{aligned} P(\\text{midterm = A and final = A}) \\qquad\\text{and}\\qquad P(\\text{final = A})\\end{aligned}\\] However, this information is not provided, and it is not obvious how to calculate these probabilities. Since we aren’t sure how to proceed, it is useful to organize the information into a tree diagram, as shown in Figure 1.10. When constructing a tree diagram, variables provided with marginal probabilities are often used to create the tree’s primary branches; in this case, the marginal probabilities are provided for midterm grades. The final grades, which correspond to the conditional probabilities provided, will be shown on the secondary branches. A tree diagram describing the and variables. With the tree diagram constructed, we may compute the required probabilities: \\[\\begin{aligned} &amp;&amp;P(\\text{midterm = A and final = A}) = 0.0611 \\\\ &amp;&amp;P(\\text{final = A}) \\\\ &amp;&amp; \\quad= P(\\text{midterm = other and final = A}) + P(\\text{midterm = A and final = A}) \\\\ &amp;&amp; \\quad= 0.0611 + 0.0957 = 0.1568\\end{aligned}\\] The marginal probability, \\(P(\\) = A\\()\\), was calculated by adding up all the joint probabilities on the right side of the tree that correspond to = A. We may now finally take the ratio of the two probabilities: \\[\\begin{aligned} P(\\text{midterm = A} | \\text{final = A}) &amp;=&amp; \\frac{P(\\text{midterm = A and final = A})}{P(\\text{final = A})} \\\\ &amp;=&amp; \\frac{0.0611}{0.1568} = 0.3897\\end{aligned}\\] The probability the student also earned an A on the midterm is about 0.39. Guided Practice After an introductory statistics course, 78% of students can successfully construct tree diagrams. Of those who can construct tree diagrams, 97% passed, while only 57% of those students who could not construct tree diagrams passed. Organize this information into a tree diagram. What is the probability that a randomly selected student passed Compute the probability a student is able to construct a tree diagram if it is known that she passed.50 7.3 Random variables Example Two books are assigned for a statistics class: a textbook and its corresponding study guide. The university bookstore determined 20% of enrolled students do not buy either book, 55% buy the textbook only, and 25% buy both books, and these percentages are relatively constant from one term to another. If there are 100 students enrolled, how many books should the bookstore expect to sell to this class Guided Practice Around 20 students will not buy either book (0 books total), about 55 will buy one book (55 books total), and approximately 25 will buy two books (totaling 50 books for these 25 students). The bookstore should expect to sell about 105 books for this class. Example Would you be surprised if the bookstore sold slightly more or less than 105 books 51 The textbook costs $137 and the study guide $33. How much revenue should the bookstore expect from this class of 100 students About 55 students will just buy a textbook, providing revenue of \\[\\begin{aligned} \\$137 \\times 55 = \\$7,535\\end{aligned}\\] The roughly 25 students who buy both the textbook and the study guide would pay a total of \\[\\begin{aligned} (\\$137 + \\$33) \\times 25 = \\$170 \\times 25 = \\$4,250\\end{aligned}\\] Thus, the bookstore should expect to generate about \\(\\$7,535 + \\$4,250 = \\$11,785\\) from these 100 students for this one class. However, there might be some sampling variability so the actual amount may differ by a little bit. Probability distribution for the bookstore’s revenue from a single student. The distribution balances on a triangle representing the average revenue per student. Example What is the average revenue per student for this course The expected total revenue is $11,785, and there are 100 students. Therefore the expected revenue per student is \\(\\$11,785/100 = \\$117.85\\). 7.3.1 Expectation We call a variable or process with a numerical outcome a , and we usually represent this random variable with a capital letter such as \\(X\\), \\(Y\\), or \\(Z\\). The amount of money a single student will spend on her statistics books is a random variable, and we represent it by \\(X\\). Random Variable &gt; A random process or variable with a numerical outcome. The possible outcomes of \\(X\\) are labeled with a corresponding lower case letter \\(x\\) and subscripts. For example, we write \\(x_1=\\$0\\), \\(x_2=\\$137\\), and \\(x_3=\\$170\\), which occur with probabilities \\(0.20\\), \\(0.55\\), and \\(0.25\\). The distribution of \\(X\\) is summarized in Figure 1.11 and the table below The probability distribution for the random variable \\(X\\), representing the bookstore’s revenue from a single student. $ 1 2 3 Total \\(x_i\\) $0 $137 $170 – \\(P(X=x_i)\\) 0.20 0.55 0.25 1.00 We computed the average outcome of \\(X\\) as $117.85 in dealing with the revenue per student. We call this average the expected value of \\(X\\), denoted by \\(E(X)\\). The expected value of a random variable is computed by adding each outcome weighted by its probability: \\[\\begin{aligned} E(X) &amp;= 0 \\times P(X=0) + 137 \\times P(X=137) + 170 \\times P(X=170) \\\\ &amp;= 0 \\times 0.20 + 137 \\times 0.55 + 170 \\times 0.25 = 117.85\\end{aligned}\\] Expected value of a Discrete Random Variable If \\(X\\) takes outcomes \\(x_1\\), ..., \\(x_k\\) with probabilities \\(P(X=x_1)\\), ..., \\(P(X=x_k)\\), the expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[\\begin{aligned} E(X) &amp;= x_1\\times P(X=x_1) + \\cdots + x_k\\times P(X=x_k) \\notag \\\\ &amp;= \\sum_{i=1}^{k}x_iP(X=x_i)\\end{aligned}\\] The Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\). The expected value for a random variable represents the average outcome. For example, \\(E(X)=117.85\\) represents the average amount the bookstore expects to make from a single student, which we could also write as \\(\\mu=117.85\\). It is also possible to compute the expected value of a continuous random variable. However, it requires a little calculus and we save it for a later class.52 In physics, the expectation holds the same meaning as the center of gravity. The distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point. This is represented in Figures 1.11 and 1.12. The idea of a center of gravity also expands to continuous probability distributions. Figure 1.13 shows a continuous probability distribution balanced atop a wedge placed at the mean. A weight system representing the probability distribution for \\(X\\). The string holds the distribution at the mean to keep the system balanced. A continuous distribution can also be balanced at its mean. 7.3.2 Variability in random variables Suppose you ran the university bookstore. Besides how much revenue you expect to generate, you might also want to know the volatility (variability) in your revenue. The and can be used to describe the variability of a random variable. We first computed deviations from the mean (\\(x_i - \\mu\\)), squared those deviations, and took an average to get the variance. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation. This weighted sum of squared deviations equals the variance, and we calculate the standard deviation by taking the square root of the variance. General variance formula If \\(X\\) takes outcomes \\(x_1\\), ..., \\(x_k\\) with probabilities \\(P(X=x_1)\\), ..., \\(P(X=x_k)\\) and expected value \\(\\mu=E(X)\\), then the variance of \\(X\\), denoted by \\(Var(X)\\) or the symbol \\(\\sigma^2\\), is \\[\\begin{aligned} \\sigma^2 &amp;= (x_1-\\mu)^2\\times P(X=x_1) + \\cdots \\notag \\\\ &amp; \\qquad\\quad\\cdots+ (x_k-\\mu)^2\\times P(X=x_k) \\notag \\\\ &amp;= \\sum_{j=1}^{k} (x_j - \\mu)^2 P(X=x_j)\\end{aligned}\\] The standard deviation of \\(X\\), labeled \\(\\sigma\\), is the square root of the variance. Example Compute the expected value, variance, and standard deviation of \\(X\\), the revenue of a single statistics student for the bookstore. It is useful to construct a table that holds computations for each outcome separately, then add up the results. $ 1 2 3 Total \\(x_i\\) $0 $137 $170 \\(P(X=x_i)\\) 0.20 0.55 0.25 \\(x_i \\times P(X=x_i)\\) 0 75.35 42.50 117.85 Thus, the expected value is \\(\\mu=117.85\\), which we computed earlier. The variance can be constructed by extending this table: $ 1 2 3 Total \\(x_i\\) $0 $137 $170 \\(P(X=x_i)\\) 0.20 0.55 0.25 \\(x_i \\times P(X=x_i)\\) 0 75.35 42.50 117.85 \\(x_i - \\mu\\) -117.85 19.15 52.15 \\((x_i-\\mu)^2\\) 13888.62 366.72 2719.62 \\((x_i-\\mu)^2\\times P(X=x_i)\\) 2777.7 201.7 679.9 3659.3 The variance of \\(X\\) is \\(\\sigma^2 = 3659.3\\), which means the standard deviation is \\(\\sigma = \\sqrt{3659.3} = \\$60.49\\). Guided Practice The bookstore also offers a chemistry textbook for $159 and a book supplement for $41. From past experience, they know about 25% of chemistry students just buy the textbook while 60% buy both the textbook and supplement.53 What proportion of students don’t buy either book Assume no students buy the supplement without the textbook. Let \\(Y\\) represent the revenue from a single student. Write out the probability distribution of \\(Y\\), i.e. a table for each outcome and its associated probability. Compute the expected revenue from a single chemistry student. Find the standard deviation to describe the variability associated with the revenue from a single student. 7.3.3 Linear combinations of random variables So far, we have thought of each variable as being a complete story in and of itself. Sometimes it is more appropriate to use a combination of variables. For instance, the amount of time a person spends commuting to work each week can be broken down into several daily commutes. Similarly, the total gain or loss in a stock portfolio is the sum of the gains and losses in its components. Example John travels to work five days a week. We will use \\(X_1\\) to represent his travel time on Monday, \\(X_2\\) to represent his travel time on Tuesday, and so on. Write an equation using \\(X_1\\), ..., \\(X_5\\) that represents his travel time for the week, denoted by \\(W\\). His total weekly travel time is the sum of the five daily values: \\[W = X_1 + X_2 + X_3 + X_4 + X_5\\] Breaking the weekly travel time \\(W\\) into pieces provides a framework for understanding each source of randomness and is useful for modeling \\(W\\). Example It takes John an average of 18 minutes each day to commute to work. What would you expect his average commute time to be for the week We were told that the average (i.e. expected value) of the commute time is 18 minutes per day: \\(E(X_i) = 18\\). To get the expected time for the sum of the five days, we can add up the expected time for each individual day: \\[\\begin{aligned} E(W) &amp;= E(X_1 + X_2 + X_3 + X_4 + X_5) \\\\ &amp;= E(X_1) + E(X_2) + E(X_3) + E(X_4) + E(X_5) \\\\ &amp;= 18 + 18 + 18 + 18 + 18 = 90\\text{ minutes}\\end{aligned}\\] The expectation of the total time is equal to the sum of the expected individual times. More generally, the expectation of a sum of random variables is always the sum of the expectation for each random variable. Guided Practice Elena is selling a TV at a cash auction and also intends to buy a toaster oven in the auction. If \\(X\\) represents the profit for selling the TV and \\(Y\\) represents the cost of the toaster oven, write an equation that represents the net change in Elena’s cash.54 Based on past auctions, Elena figures she should expect to make about $175 on the TV and pay about $23 for the toaster oven. In total, how much should she expect to make or spend 55 Guided Practice Would you be surprised if John’s weekly commute wasn’t exactly 90 minutes or if Elena didn’t make exactly $152 Explain.56 Two important concepts concerning combinations of random variables have so far been introduced. First, a final value can sometimes be described as the sum of its parts in an equation. Second, intuition suggests that putting the individual average values into this equation gives the average value we would expect in total. This second point needs clarification – it is guaranteed to be true in what are called linear combinations of random variables. A Linear Combination of two random variables \\(X\\) and \\(Y\\) is a fancy phrase to describe a combination \\[aX + bY\\] where \\(a\\) and \\(b\\) are some fixed and known numbers. For John’s commute time, there were five random variables – one for each work day – and each random variable could be written as having a fixed coefficient of 1: \\[1X_1 + 1 X_2 + 1 X_3 + 1 X_4 + 1 X_5\\] For Elena’s net gain or loss, the \\(X\\) random variable had a coefficient of +1 and the \\(Y\\) random variable had a coefficient of -1. When considering the average of a linear combination of random variables, it is safe to plug in the mean of each random variable and then compute the final result. For a few examples of nonlinear combinations of random variables – cases where we cannot simply plug in the means – see the footnote.57 Linear combinations of random variables and the average result If \\(X\\) and \\(Y\\) are random variables, then a linear combination of the random variables is given by \\[\\begin{aligned} \\label{linComboOfRandomVariablesXAndY} aX + bY\\end{aligned}\\] where \\(a\\) and \\(b\\) are some fixed numbers. To compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result: \\[\\begin{aligned} a\\times E(X) + b\\times E(Y)\\end{aligned}\\] Recall that the expected value is the same as the mean, e.g. \\(E(X) = \\mu_X\\). Example Leonard has invested $6000 in Google Inc. (stock ticker: GOOG) and $2000 in Exxon Mobil Corp. (XOM). If \\(X\\) represents the change in Google’s stock next month and \\(Y\\) represents the change in Exxon Mobil stock next month, write an equation that describes how much money will be made or lost in Leonard’s stocks for the month. For simplicity, we will suppose \\(X\\) and \\(Y\\) are not in percents but are in decimal form (e.g. if Google’s stock increases 1%, then \\(X=0.01\\); or if it loses 1%, then \\(X=-0.01\\)). Then we can write an equation for Leonard’s gain as \\[\\begin{aligned} \\$6000\\times X + \\$2000\\times Y\\end{aligned}\\] If we plug in the change in the stock value for \\(X\\) and \\(Y\\), this equation gives the change in value of Leonard’s stock portfolio for the month. A positive value represents a gain, and a negative value represents a loss. Guided Practice Suppose Google and Exxon Mobil stocks have recently been rising 2.1% and 0.4% per month, respectively. Compute the expected change in Leonard’s stock portfolio for next month.58 Guided Practice You should have found that Leonard expects a positive gain. However, would you be surprised if he actually had a loss this month 59 7.3.4 Variability in linear combinations of random variables Quantifying the average outcome from a linear combination of random variables is helpful, but it is also important to have some sense of the uncertainty associated with the total outcome of that combination of random variables. We calculated the expected net gain or loss of Leonard’s stock portfolio was considered in Guided Practice. However, there was no quantitative discussion of the volatility of this portfolio. For instance, while the average monthly gain might be about $134 according to the data, that gain is not guaranteed. Figure 1.14 shows the monthly changes in a portfolio like Leonard’s during the 36 months from 2009 to 2011. The gains and losses vary widely, and quantifying these fluctuations is important when investing in stocks. The change in a portfolio like Leonard’s for the 36 months from 2009 to 2011, where $6000 is in Google’s stock and $2000 is in Exxon Mobil’s. Just as we have done in many previous cases, we use the variance and standard deviation to describe the uncertainty associated with Leonard’s monthly returns. To do so, the variances of each stock’s monthly return will be useful, and these are shown in the table below. The stocks’ returns are nearly independent. The mean, standard deviation, and variance of the GOOG and XOM stocks. These statistics were estimated from historical stock data, so notation used for sample statistics has been used. M ean (\\(\\bar{x}\\)) S tandard deviation (\\(s\\)) V ariance (\\(s^2\\)) GOOG 0.0210 0.0846 0.0072 XOM 0.0038 0.0519 0.0027 Here we use an equation from probability theory to describe the uncertainty of Leonard’s monthly returns; we leave the proof of this method to a dedicated probability course. The variance of a linear combination of random variables can be computed by plugging in the variances of the individual random variables and squaring the coefficients of the random variables: \\[\\begin{aligned} Var(aX + bY) = a^2\\times Var(X) + b^2\\times Var(Y)\\end{aligned}\\] It is important to note that this equality assumes the random variables are independent; if independence doesn’t hold, then more advanced methods are necessary. This equation can be used to compute the variance of Leonard’s monthly return: \\[\\begin{aligned} Var(6000\\times X + 2000\\times Y) &amp;= 6000^2\\times Var(X) + 2000^2\\times Var(Y) \\\\ &amp;= 36,000,000\\times 0.0072 + 4,000,000\\times 0.0027 \\\\ &amp;= 270,000\\end{aligned}\\] The standard deviation is computed as the square root of the variance: \\(\\sqrt{270,000} = \\$520\\). While an average monthly return of $134 on an $8000 investment is nothing to scoff at, the monthly returns are so volatile that Leonard should not expect this income to be very stable. Variability of linear combinations of random variables The variance of a linear combination of random variables may be computed by squaring the constants, substituting in the variances for the random variables, and computing the result: \\[\\begin{aligned} Var(aX + bY) = a^2\\times Var(X) + b^2\\times Var(Y)\\end{aligned}\\] This equation is valid as long as the random variables are independent of each other. The standard deviation of the linear combination may be found by taking the square root of the variance. Example Suppose John’s daily commute has a standard deviation of 4 minutes. What is the uncertainty in his total commute time for the week The expression for John’s commute time was \\[\\begin{aligned} X_1 + X_2 + X_3 + X_4 + X_5\\end{aligned}\\] Each coefficient is 1, and the variance of each day’s time is \\(4^2=16\\). Thus, the variance of the total weekly commute time is \\[\\begin{aligned} &amp;\\text{variance }= 1^2 \\times 16 + 1^2 \\times 16 + 1^2 \\times 16 + 1^2 \\times 16 + 1^2 \\times 16 = 5\\times 16 = 80 \\\\ &amp;\\text{standard deviation } = \\sqrt{\\text{variance}} = \\sqrt{80} = 8.94\\end{aligned}\\] The standard deviation for John’s weekly work commute time is about 9 minutes. Guided Practice The computation in the example above relied on an important assumption: the commute time for each day is independent of the time on other days of that week. Do you think this is valid Explain.60 Guided Practice Elena is selling a TV at a cash auction and also intends to buy a toaster oven in the auction. If \\(X\\) represents the profit for selling the TV and \\(Y\\) represents the cost of the toaster oven. Suppose these auctions are approximately independent and the variability in auction prices associated with the TV and toaster oven can be described using standard deviations of $25 and $8. Compute the standard deviation of Elena’s net gain.61 The negative coefficient for \\(Y\\) in the linear combination was eliminated when we squared the coefficients. This generally holds true: negatives in a linear combination will have no impact on the variability computed for a linear combination, but they do impact the expected value computations. 7.4 Statistical Background 7.5 Basic statistical terms Note that all the following statistical terms apply only to numerical variables, except the distribution which can exist for both numerical and categorical variables. 7.5.1 Mean The mean is the most commonly reported measure of center. It is commonly called the average though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[Mean = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] 7.5.2 Median The median is calculated by first sorting a variable’s data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two middle values. 7.5.3 Standard deviation We will next discuss the standard deviation (\\(sd\\)) of a variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far we expect a given data value will be from its mean: \\[sd = \\sqrt{\\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}}\\] 7.5.4 Five-number summary The five-number summary consists of five summary statistics: the minimum, the first quantile AKA 25th percentile, the second quantile AKA median or 50th percentile, the third quantile AKA 75th, and the maximum. The five-number summary of a variable is used when constructing boxplots, as seen in Section 2.6. The quantiles are calculated as first quantile (\\(Q_1\\)): the median of the first half of the sorted data third quantile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range (IQR) is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values are. The IQR corresponds to the length of the box in a boxplot. The median and the IQR are not influenced by the presence of outliers in the ways that the mean and standard deviation are. They are, thus, recommended for skewed datasets. We say in this case that the median and IQR are more robust to outliers. 7.5.5 Distribution The distribution of a variable shows how frequently different values of a variable occur. Looking at the visualization of a distribution can show where the values are centered, show how the values vary, and give some information about where a typical value might fall. It can also alert you to the presence of outliers. Recall from Chapter 2 that we can visualize the distribution of a numerical variable using binning in a histogram and that we can visualize the distribution of a categorical variable using a barplot. 7.5.6 Outliers Outliers correspond to values in the dataset that fall far outside the range of “ordinary” values. In the context of a boxplot, by default they correspond to values below \\(Q_1 - (1.5 \\cdot IQR)\\) or above \\(Q_3 + (1.5 \\cdot IQR)\\). 7.6 Normal distribution Let’s next discuss one particular kind of distribution: normal distributions. Such bell-shaped distributions are defined by two values: (1) the mean \\(\\mu\\) (“mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (“sigma”) which determines the variation of the distribution. In Figure 7.1, we plot three normal distributions where: The solid normal curve has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 2\\). The dotted normal curve has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 5\\). The dashed normal curve has mean \\(\\mu = 15\\) &amp; standard deviation \\(\\sigma = 2\\). FIGURE 7.1: Three normal distributions. Notice how the solid and dotted line normal curves have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma\\) = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma\\) = 2. However, they are centered at different locations. When the mean \\(\\mu\\) = 0 and the standard deviation \\(\\sigma\\) = 1, the normal distribution has a special name. It’s called the standard normal distribution or the \\(z\\)-curve. Furthermore, if a variable follows a normal curve, there are three rules of thumb we can use: 68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean. 95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean. 99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean. Let’s illustrate this on a standard normal curve in Figure 7.2. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example: The middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values. The middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5%= 95% of the area under the curve. In other words, 95% of values. The middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values. FIGURE 7.2: Rules of thumb about areas under normal curves. Learning check Say you have a normal distribution with mean \\(\\mu = 6\\) and standard deviation \\(\\sigma = 3\\). (LCA.1) What proportion of the area under the normal curve is less than 3? Greater than 12? Between 0 and 12? (LCA.2) What is the 2.5th percentile of the area under the normal curve? The 95th percentile? The 100th percentile? 7.7 log10 transformations At its simplest, log10 transformations return base 10 logarithms. For example, since \\(1000 = 10^3\\), running log10(1000) returns 3 in R. To undo a log10 transformation, we raise 10 to this value. For example, to undo the previous log10 transformation and return the original value of 1000, we raise 10 to the power of 3 by running 10^(3) = 1000 in R. Log transformations allow us to focus on changes in orders of magnitude. In other words, they allow us to focus on multiplicative changes instead of additive ones. Let’s illustrate this idea in Table 7.1 with examples of prices of consumer goods in 2019 US dollars. TABLE 7.1: log10 transformed prices, orders of magnitude, and examples Price log10(Price) Order of magnitude Examples $1 0 Singles Cups of coffee $10 1 Tens Books $100 2 Hundreds Mobile phones $1,000 3 Thousands High definition TVs $10,000 4 Tens of thousands Cars $100,000 5 Hundreds of thousands Luxury cars and houses $1,000,000 6 Millions Luxury houses Let’s make some remarks about log10 transformations based on Table 7.1: When purchasing a cup of coffee, we tend to think of prices ranging in single dollars, such as $2 or $3. However, when purchasing a mobile phone, we don’t tend to think of their prices in units of single dollars such as $313 or $727. Instead, we tend to think of their prices in units of hundreds of dollars like $300 or $700. Thus, cups of coffee and mobile phones are of different orders of magnitude in price. Let’s say we want to know the log10 transformed value of $76. This would be hard to compute exactly without a calculator. However, since $76 is between $10 and $100 and since log10(10) = 1 and log10(100) = 2, we know log10(76) will be between 1 and 2. In fact, log10(76) is 1.880814. log10 transformations are monotonic, meaning they preserve orders. So if Price A is lower than Price B, then log10(Price A) will also be lower than log10(Price B). Most importantly, increments of one in log10-scale correspond to relative multiplicative changes in the original scale and not absolute additive changes. For example, increasing a log10(Price) from 3 to 4 corresponds to a multiplicative increase by a factor of 10: $100 to $1000. Here are four examples. (i) Whether someone gets sick in the next month or not is an apparently random process with outcomes sick and not. (ii) We can generate a random process by randomly picking a person and measuring that person’s height. The outcome of this process will be a positive number. (iii) Whether the stock market goes up or down next week is a seemingly random process with possible outcomes up, down, and no_change. Alternatively, we could have used the percent change in the stock market as a numerical outcome. (iv) Whether your roommate cleans her dishes tonight probably seems like a random process with possible outcomes cleans_dishes and leaves_dishes.↩ () The random process is a die roll, and at most one of these outcomes can come up. This means they are disjoint outcomes. (b) \\(P(\\)1 or 4 or 5\\() = P(\\)1\\()+P(\\)4\\()+P(\\)5\\() = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6} = \\frac{1}{2}\\)↩ Since \\(B\\) and \\(D\\) are disjoint events, use the Addition Rule: \\(P(B\\) or \\(D) = P(B) + P(D) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}\\).↩ The 52 cards are split into four : \\(\\clubsuit\\) (club), \\(\\diamondsuit\\) (diamond), \\(\\heartsuit\\) (heart), \\(\\spadesuit\\) (spade). Each suit has its 13 cards labeled: 2, 3, ..., 10, J (jack), Q (queen), K (king), and A (ace). Thus, each card is a unique combination of a suit and a label, e.g. 4\\(\\heartsuit\\) and J\\(\\clubsuit\\). The 12 cards represented by the jacks, queens, and kings are called . The cards that are \\(\\diamondsuit\\) or \\(\\heartsuit\\) are typically colored red while the other two suits are typically colored black.↩ The Venn diagram shows face cards split up into “face card but not \\(\\diamondsuit\\)” and “face card and \\(\\diamondsuit\\)”. Since these correspond to disjoint events, \\(P(\\)face card\\()\\) is found by adding the two corresponding probabilities: \\(\\frac{3}{52} + \\frac{9}{52} = \\frac{12}{52} = \\frac{3}{13}\\).↩ Both the counts and corresponding probabilities (e.g. \\(2659/3921 = 0.678\\)) are shown. Notice that the number of emails represented in the left circle corresponds to \\(2659 + 168 = 2827\\), and the number represented in the right circle is \\(168 + 199 = 367\\). ↩ The probabilities of (a) do not sum to 1. The second probability in (b) is negative. This leaves (c), which sure enough satisfies the requirements of a distribution. One of the three was said to be the actual distribution of US household incomes, so it must be (c).↩ It is also possible to construct a distribution plot when income is not artificially binned into four groups.↩ Brief solutions: (a) \\(A^c=\\{\\)3, 4, 5, 6\\(\\}\\) and \\(B^c=\\{\\)1, 2, 3, 5\\(\\}\\). (b) Noting that each outcome is disjoint, add the individual outcome probabilities to get \\(P(A^c)=2/3\\) and \\(P(B^c)=2/3\\). (c) \\(A\\) and \\(A^c\\) are disjoint, and the same is true of \\(B\\) and \\(B^c\\). Therefore, \\(P(A) + P(A^c) = 1\\) and \\(P(B) + P(B^c) = 1\\).↩ The actual proportion of the U.S. population that is female is about 50%, and so we use 0.5 for the probability of sampling a woman. However, this probability does differ in other countries.↩ Brief answers are provided. (a) This can be written in probability notation as \\(P(\\)a randomly selected person is male and right-handed\\()=0.455\\). (b) 0.207. (c) 0.045. (d) 0.0093.↩ Ellis GJ and Stone LH. 1979. Marijuana Use in College: An Evaluation of a Modeling Explanation. Youth and Society 10:323-334.↩ Each of the four outcome combination are disjoint, all probabilities are indeed non-negative, and the sum of the probabilities is \\(0.28 + 0.19 + 0.21 + 0.32 = 1.00\\).↩ This is an observational study and no causal conclusions may be reached.↩ No. This was an observational study. Two potential confounding variables include and . Can you think of others↩ Fenner F. 1988. Smallpox and Its Eradication (History of International Public Health, No. 6). Geneva: World Health Organization. ISBN 92-4-156110-6.↩ \\(P(\\) = died \\(|\\) = no\\() = \\frac{P(\\text{result = died and inoculated = no})}{P(\\text{inoculated = no})} = \\frac{0.1356}{0.9608} = 0.1411\\).↩ \\(P(\\) = died \\(|\\) = yes\\() = \\frac{P(\\text{result = died and inoculated = yes})}{P(\\text{inoculated = yes})} = \\frac{0.0010}{0.0392} = 0.0255\\). The death rate for individuals who were inoculated is only about 1 in 40 while the death rate is about 1 in 7 for those who were not inoculated.↩ Brief answers: (a) Observational. (b) No, we cannot infer causation from this observational study. (c) Accessibility to the latest and best medical care. There are other valid answers for part (c).↩ The answer is 0.0382.↩ There were only two possible outcomes: lived or died. This means that 100% - 97.45% = 2.55% of the people who were inoculated died.↩ The samples are large relative to the difference in death rates for the “inoculated” and “not inoculated” groups, so it seems there is an association between and . However, as noted in the solution to Guided Practice, this is an observational study and we cannot be sure if there is a causal connection. (Further research has shown that inoculation is effective at reducing death rates.)↩ Brief solutions: (a) \\(1/6\\). (b) \\(1/36\\). (c) \\(\\frac{P(Y = \\text{ 1 and }X=\\text{ 1})}{P(X=\\text{ 1})} = \\frac{1/36}{1/6} = 1/6\\). (d) The probability is the same as in part (c): \\(P(Y=1)=1/6\\). The probability that \\(Y=1\\) was unchanged by knowledge about \\(X\\), which makes sense as \\(X\\) and \\(Y\\) are independent.↩ He has forgotten that the next roulette spin is independent of the previous spins. Casinos do employ this practice; they post the last several outcomes of many betting games to trick unsuspecting gamblers into believing the odds are in their favor. This is called the .↩ If they sell a little more or a little less, this should not be a surprise. For example, if we would flip a coin 100 times, it will not usually come up heads exactly half the time, but it will probably be close.↩ \\(\\mu = \\int xf(x)dx\\) where \\(f(x)\\) represents a function for the density curve.↩ She will make \\(X\\) dollars on the TV but spend \\(Y\\) dollars on the toaster oven: \\(X-Y\\).↩ \\(E(X-Y) = E(X) - E(Y) = 175 - 23 = \\$152\\). She should expect to make about $152.↩ No, since there is probably some variability. For example, the traffic will vary from one day to next, and auction prices will vary depending on the quality of the merchandise and the interest of the attendees.↩ If \\(X\\) and \\(Y\\) are random variables, consider the following combinations: \\(X^{1+Y}\\), \\(X\\times Y\\), \\(X/Y\\). In such cases, plugging in the average value for each random variable and computing the result will not generally lead to an accurate average value for the end result.↩ \\(E(\\$6000\\times X + \\$2000\\times Y) = \\$6000\\times 0.021 + \\$2000\\times 0.004 = \\$134\\).↩ No. While stocks tend to rise over time, they are often volatile in the short term.↩ One concern is whether traffic patterns tend to have a weekly cycle (e.g. Fridays may be worse than other days). If that is the case, and John drives, then the assumption is probably not reasonable. However, if John walks to work, then his commute is probably not affected by any weekly traffic cycle.↩ The equation for Elena can be written as \\[\\begin{aligned} (1)\\times X + (-1)\\times Y\\end{aligned}\\] The variances of \\(X\\) and \\(Y\\) are 625 and 64. We square the coefficients and plug in the variances: \\[\\begin{aligned} (1)^2\\times Var(X) + (-1)^2\\times Var(Y) = 1\\times 625 + 1\\times 64 = 689\\end{aligned}\\] The variance of the linear combination is 689, and the standard deviation is the square root of 689: about $26.25.↩ "],
["8-bayess-theorem.html", "Chapter 8 Bayes’s Theorem 8.1 Conditional probability 8.2 Conjoint probability 8.3 The cookie problem 8.4 Bayes’s Theorem 8.5 The diachronic interpretation 8.6 The M&amp;M problem 8.7 The Monty Hall problem 8.8 Discussion", " Chapter 8 Bayes’s Theorem 8.1 Conditional probability The fundamental idea behind all Bayesian statistics is Bayes’s theorem, which is surprisingly easy to derive, provided that you understand conditional probability. So we’ll start with probability, then conditional probability, then Bayes’s theorem, and on to Bayesian statistics. A probability is a number between \\(0\\) and \\(1\\) (including both) that represents a degree of belief in a fact or prediction. The value 1 represents certainty that a fact is true, or that a prediction will come true. The value \\(0\\) represents certainty that the fact is false. Intermediate values represent degrees of certainty. The value \\(0.5\\), often written as \\(50\\%\\), means that a predicted outcome is as likely to happen as not. For example, the probability that a tossed coin lands face up is very close to \\(50\\%\\). A conditional probability is a probability based on some background information. For example, I want to know the probability that I will have a heart attack in the next year. According to the CDC, “Every year about \\(785,000\\) Americans have a first coronary attack. (http://www.cdc.gov/heartdisease/facts.htm)” The U.S. population is about \\(311\\) million, so the probability that a randomly chosen American will have a heart attack in the next year is roughly \\(0.3\\%\\). But I am not a randomly chosen American. Epidemiologists have identified many factors that affect the risk of heart attacks; depending on those factors, my risk might be higher or lower than average. I am male, 45 years old, and I have borderline high cholesterol. Those factors increase my chances. However, I have low blood pressure and I don’t smoke, and those factors decrease my chances. Plugging everything into the online calculator at http://cvdrisk.nhlbi.nih.gov/calculator.asp, I find that my risk of a heart attack in the next year is about \\(0.2\\%\\), less than the national average. That value is a conditional probability, because it is based on a number of factors that make up my “condition.” The usual notation for conditional probability is \\(p(A|B)\\), which is the probability of \\(A\\) given that \\(B\\) is true. In this example, A represents the prediction that I will have a heart attack in the next year, and B is the set of conditions I listed. 8.2 Conjoint probability Conjoint probability is a fancy way to say the probability that two things are true. I write \\(p(A \\cap B)\\) to mean the probability that \\(A\\) and \\(B\\) are both true. If you learned about probability in the context of coin tosses and dice, you might have learned the following formula: \\[p(A \\cap B) = p(A) p(B) \\text{ WARNING: Only true when A and B are independent}\\] For example, if I toss two coins, and A means the first coin lands face up, and \\(B\\) means the second coin lands face up, then \\(p(A) = p(B) = 0.5\\), and sure enough, \\(p(A \\cap B) = p(A) p(B) = 0.25\\). But this formula only works because in this case \\(A\\) and \\(B\\) are independent; that is, knowing the outcome of the first event does not change the probability of the second. Or, more formally, \\(p(B|A) = p(B)\\). Here is a different example where the events are not independent. Suppose that \\(A\\) means that it rains today and \\(B\\) means that it rains tomorrow. If I know that it rained today, it is more likely that it will rain tomorrow, so \\(p(B|A) &gt; p(B)\\). In general, the probability of a conjunction is \\[p(A \\cap B) = p(A) p(B|A)\\] for any \\(A\\) and \\(B\\). So if the chance of rain on any given day is \\(0.5\\), the chance of rain on two consecutive days is not \\(0.25\\), but probably a bit higher. 8.3 The cookie problem We’ll get to Bayes’s theorem soon, but I want to motivate it with an example called the cookie problem.1 Suppose there are two bowls of cookies. \\(\\text{Bowl 1}\\) contains \\(30\\) vanilla cookies and \\(10\\) chocolate cookies. \\(\\text{Bowl 2}\\) contains \\(20\\) of each. Now suppose you choose one of the bowls at random and, without looking, select a cookie at random. The cookie is vanilla. What is the probability that it came from \\(\\text{Bowl 1}\\)? This is a conditional probability; we want \\(p(\\text{Bowl 1 | vanilla})\\), but it is not obvious how to compute it. If I asked a different question—the probability of a vanilla cookie given \\(\\text{Bowl 1}\\)—it would be easy: \\[p(\\text{vanilla | Bowl 1}) = \\frac{3}{4}\\] Sadly, \\(p(A|B)\\) is not the same as \\(p(B|A)\\), but there is a way to get from one to the other: Bayes’s theorem. 8.4 Bayes’s Theorem At this point we have everything we need to derive Bayes’s theorem. We’ll start with the observation that conjunction is commutative; that is \\[p(A \\cap B) = p(B \\cap A)\\] for any events A and B. Next, we write the probability of a conjunction: \\[p(A \\cap B) = p(A) p(B|A)\\] Since we have not said anything about what A and B mean, they are interchangeable. Interchanging them yields \\[p(B \\cap A) = p(B) p(A|B)\\] That’s all we need. Pulling those pieces together, we get \\[p(B) p(A|B) = p(A) p(B|A)\\] Which means there are two ways to compute the conjunction. If you have \\(p(A)\\), you multiply by the conditional probability \\(p(B|A)\\). Or you can do it the other way around; if you know \\(p(B)\\), you multiply by \\(p(A|B)\\). Either way you should get the same thing. Finally we can divide through by \\(p(B)\\): \\[p(A|B) = \\frac{p(A) p(B|A)}{p(B)}\\] And that’s Bayes’s theorem! It might not look like much, but it turns out to be surprisingly powerful. For example, we can use it to solve the cookie problem. I’ll write \\(\\text{Bowl 1}\\) for the hypothesis that the cookie came from \\(\\text{Bowl 1}\\) and \\(V\\) for the vanilla cookie. Plugging in Bayes’s theorem we get \\[p(B_1|V) = \\frac{p(B_1) p(V|B1)}{p(V)}\\] The term on the left is what we want: the probability of \\(\\text{Bowl 1}\\), given that we chose a vanilla cookie. The terms on the right are: \\(p(B_1)\\): This is the probability that we chose \\(\\text{Bowl 1}\\), unconditioned by what kind of cookie we got. Since the problem says we chose a bowl at random, we can assume \\(p(B_1) = 1/2\\). \\(p(V|B_1)\\): This is the probability of getting a vanilla cookie from \\(\\text{Bowl 1}\\), which is \\(3/4\\). \\(p(V)\\): This is the probability of drawing a vanilla cookie from either bowl. Since we had an equal chance of choosing either bowl and the bowls contain the same number of cookies, we had the same chance of choosing any cookie. Between the two bowls there are \\(50\\) vanilla and \\(30\\) chocolate cookies, so \\(p(V) = 5/8\\). Putting it together, we have \\[p(B_1|V) = \\frac{(1/2) (3/4)} {5/8}\\] which reduces to \\(3/5\\). So the vanilla cookie is evidence in favor of the hypothesis that we chose \\(\\text{Bowl 1}\\), because vanilla cookies are more likely to come from \\(\\text{Bowl 1}\\). This example demonstrates one use of Bayes’s theorem: it provides a strategy to get from \\(p(B|A)\\) to \\(p(A|B)\\). This strategy is useful in cases, like the cookie problem, where it is easier to compute the terms on the right side of Bayes’s theorem than the term on the left 8.5 The diachronic interpretation There is another way to think of Bayes’s theorem: it gives us a way to update the probability of a hypothesis, \\(H\\), in light of some body of data, \\(D\\). This way of thinking about Bayes’s theorem is called the diachronic interpretation. “Diachronic” means that something is happening over time; in this case the probability of the hypotheses changes, over time, as we see new data. Rewriting Bayes’s theorem with \\(H\\) and \\(D\\) yields: \\[p(H|D) = \\frac{p(H) p(D|H)}{p(D)}\\] In this interpretation, each term has a name: \\(p(H)\\) is the probability of the hypothesis before we see the data, called the prior probability, or just prior. \\(p(H|D)\\) is what we want to compute, the probability of the hypothesis after we see the data, called the posterior. \\(p(D|H)\\) is the probability of the data under the hypothesis, called the likelihood. \\(p(D)\\) is the probability of the data under any hypothesis, called the normalizing constant. Sometimes we can compute the prior based on background information. For example, the cookie problem specifies that we choose a bowl at random with equal probability. In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently. The likelihood is usually the easiest part to compute. In the cookie problem, if we know which bowl the cookie came from, we find the probability of a vanilla cookie by counting. The normalizing constant can be tricky. It is supposed to be the probability of seeing the data under any hypothesis at all, but in the most general case it is hard to nail down what that means. Most often we simplify things by specifying a set of hypotheses that are Mutually exclusive: At most one hypothesis in the set can be true, and Collectively exhaustive: There are no other possibilities; at least one of the hypotheses has to be true. I use the word suite for a set of hypotheses that has these properties. In the cookie problem, there are only two hypotheses—the cookie came from \\(\\text{Bowl 1}\\) or \\(\\text{Bowl 2}\\)—and they are mutually exclusive and collectively exhaustive. In that case we can compute \\(p(D)\\) using the law of total probability, which says that if there are two exclusive ways that something might happen, you can add up the probabilities like this: \\[p(D) = p(B_1) p(D|B_1) + p(B_2) p(D|B_2)\\] Plugging in the values from the cookie problem, we have \\[p(D) = (1/2) (3/4) + (1/2) (1/2) = 5/8\\] which is what we computed earlier by mentally combining the two bowls. 8.6 The M&amp;M problem M&amp;M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&amp;M’s, changes the mixture of colors from time to time. In 1995, they introduced blue M&amp;M’s. Before then, the color mix in a bag of plain M&amp;M’s was \\(30\\%\\) Brown, \\(20\\%\\) Yellow, \\(20\\%\\) Red, \\(10\\%\\) Green, \\(10\\%\\) Orange, \\(10\\%\\) Tan. Afterward it was \\(24\\%\\) Blue , \\(20\\%\\) Green, \\(16\\%\\) Orange, \\(14\\%\\) Yellow, \\(13\\%\\) Red, \\(13\\%\\) Brown. Suppose a friend of mine has two bags of M&amp;M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&amp;M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag? This problem is similar to the cookie problem, with the twist that I draw one sample from each bowl/bag. This problem also gives me a chance to demonstrate the table method, which is useful for solving problems like this on paper. In the next chapter we will solve them computationally. The first step is to enumerate the hypotheses. The bag the yellow M&amp;M came from I’ll call Bag 1; I’ll call the other Bag 2. So the hypotheses are: A: Bag 1 is from 1994, which implies that Bag 2 is from 1996. B: Bag 1 is from 1996 and Bag 2 from 1994 Let \\(H\\) be the hypothesis, and let \\(D\\) be the event of the outcome. Note that \\(p(D) = 1\\) since there is a 100 percent chance that we have this outcome since it has already happened Prior \\(p(H)\\) Likelihood \\(P(D|H)\\) \\(p(H)p(D|H)\\) Posterior \\(p(H|D)\\) A 1/2 (20)(20) 200 20/27 B 1/2 (14)(10) 70 7/27 The first column has the priors. Based on the statement of the problem, it is reasonable to choose \\(p(A) = p(B) = 1/2\\). The second column has the likelihoods, which follow from the information in the problem. For example, if A is true, the yellow M&amp;M came from the 1994 bag with probability \\(20\\%\\), and the green came from the 1996 bag with probability \\(20\\%\\). If \\(B\\) is true, the yellow M&amp;M came from the 1996 bag with probability \\(14\\%\\), and the green came from the 1994 bag with probability \\(10\\%\\). Because the selections are independent, we get the conjoint probability by multiplying. The third column is just the product of the previous two. The sum of this column, \\(270\\), is the normalizing constant. To get the last column, which contains the posteriors, we divide the third column by the normalizing constant. That’s it. Simple, right? Well, you might be bothered by one detail. I write \\(p(D|H)\\) in terms of percentages, not probabilities, which means it is off by a factor of \\(10,000\\). But that cancels out when we divide through by the normalizing constant, so it doesn’t affect the result. When the set of hypotheses is mutually exclusive and collectively exhaustive, you can multiply the likelihoods by any factor, if it is convenient, as long as you apply the same factor to the entire column 8.7 The Monty Hall problem The Monty Hall problem might be the most contentious question in the history of probability. The scenario is simple, but the correct answer is so counterintuitive that many people just can’t accept it, and many smart people have embarrassed themselves not just by getting it wrong but by arguing the wrong side, aggressively, in public. Monty Hall was the original host of the game show Let’s Make a Deal. The Monty Hall problem is based on one of the regular games on the show. If you are on the show, here’s what happens: Monty shows you three closed doors and tells you that there is a prize behind each door: one prize is a car, the other two are less valuable prizes like peanut butter and fake finger nails. The prizes are arranged at random. The object of the game is to guess which door has the car. If you guess right, you get to keep the car. You pick a door, which we will call Door \\(A\\). We’ll call the other doors \\(B\\) and \\(C\\). Before opening the door you chose, Monty increases the suspense by opening either Door \\(B\\) or \\(C\\), whichever does not have the car. (If the car is actually behind Door \\(A\\), Monty can safely open \\(B\\) or \\(C\\), so he chooses one at random.) Then Monty offers you the option to stick with your original choice or switch to the one remaining unopened door. Now the hard part is over; the rest is just arithmetic. The sum of the third column is \\(1/2\\). Dividing through yields \\(p(A|D) = 1/3\\) and \\(p(C|D) = 2/3\\). So you are better off switching The question is, should you “stick” or “switch” or does it make no difference? Most people have the strong intuition that it makes no difference. There are two doors left, they reason, so the chance that the car is behind Door \\(A\\) is \\(50\\%\\). But that is wrong. In fact, the chance of winning if you stick with Door \\(A\\) is only \\(1/3\\); if you switch, your chances are \\(2/3\\). By applying Bayes’s theorem, we can break this problem into simple pieces, and maybe convince ourselves that the correct answer is, in fact, correct. To start, we should make a careful statement of the data. In this case \\(D\\) consists of two parts: Monty chooses Door \\(B\\) and there is no car there. Next we define three hypotheses: \\(A\\), \\(B\\), and \\(C\\) represent the hypothesis that the car is behind Door \\(A\\), Door \\(B\\), or Door \\(C\\). Again, let’s apply the table method: Filling in the priors is easy because we are told that the prizes are arranged at random, which suggests that the car is equally likely to be behind any door. Figuring out the likelihoods takes some thought, but with reasonable care we can be confident that we have it right: If the car is actually behind \\(A\\), Monty could safely open Doors \\(B\\) or \\(C\\). So the probability that he chooses \\(B\\) is \\(1/2\\). And since the car is actually behind \\(A\\), the probability that the car is not behind \\(B\\) is \\(1\\). If the car is actually behind \\(B\\), Monty has to open door \\(C\\), so the probability that he opens door \\(B\\) is \\(0\\). Finally, if the car is behind Door \\(C\\), Monty opens \\(B\\) with probability \\(1\\) and finds no car there with probability \\(1\\). Now the hard part is over; the rest is just arithmetic. The sum of the third column is \\(1/2\\). Dividing through yields \\(p(A|D) = 1/3\\) and \\(p(C|D) = 2/3\\). So you are better off switching. Prior \\(p(H)\\) Likelihood \\(P(D|H)\\) \\(p(H)p(D|H)\\) Posterior \\(p(H|D)\\) A 1/3 1/2 1/6 1/3 B 1/3 0 0 0 C 1/3 1 1/3 2/3 Filling in the priors is easy because we are told that the prizes are arranged at random, which suggests that the car is equally likely to be behind any door. Figuring out the likelihoods takes some thought, but with reasonable care we can be confident that we have it right: If the car is actually behind \\(A\\), Monty could safely open Doors \\(B\\) or \\(C\\). So the probability that he chooses \\(B\\) is \\(1/2\\). And since the car is actually behind \\(A\\), the probability that the car is not behind \\(B\\) is \\(1\\). If the car is actually behind \\(B\\), Monty has to open door \\(C\\), so the probability that he opens door \\(B\\) is \\(0\\). Finally, if the car is behind Door \\(C\\), Monty opens \\(B\\) with probability \\(1\\) and finds no car there with probability \\(1\\). Now the hard part is over; the rest is just arithmetic. The sum of the third column is \\(1/2\\). Dividing through yields \\(p(A|D) = 1/3\\) and \\(p(C|D) = 2/3\\). So you are better off switching. There are many variations of the Monty Hall problem. One of the strengths of the Bayesian approach is that it generalizes to handle these variations. For example, suppose that Monty always chooses \\(B\\) if he can, and onlychooses \\(C\\) if he has to (because the car is behind \\(B\\)). In that case the revised table is: Prior \\(p(H)\\) Likelihood \\(P(D|H)\\) \\(p(H)p(D|H)\\) Posterior \\(p(H|D)\\) A 1/3 1 1/3 1/2 B 1/3 0 0 0 C 1/3 1 1/3 1/2 The only change is \\(p(D|A)\\). If the car is behind \\(A\\), Monty can choose to open \\(B\\) or \\(C\\). But in this variation he always chooses \\(B\\), so \\(p(D|A) = 1\\). As a result, the likelihoods are the same for \\(A\\) and \\(C\\), and the posteriors are the same: \\(p(A|D) = p(C|D) = 1/2\\). In this case, the fact that Monty chose \\(B\\) reveals no information about the location of the car, so it doesn’t matter whether the contestant sticks or switches. On the other hand, if he had opened \\(C\\), we would know \\(p(B|D) = 1\\). I included the Monty Hall problem in this chapter because I think it is fun, and because Bayes’s theorem makes the complexity of the problem a little more manageable. But it is not a typical use of Bayes’s theorem, so if you found it confusing, don’t worry! 8.8 Discussion For many problems involving conditional probability, Bayes’s theorem provides a divide-and-conquer strategy. If \\(p(A|B)\\) is hard to compute, or hard to measure experimentally, check whether it might be easier to compute the other terms in Bayes’s theorem, \\(p(B|A)\\), \\(p(A)\\) and \\(p(B)\\). If the Monty Hall problem is your idea of fun, I have collected a number of similar problems in an article called “All your Bayes are belong to us,” which you can read at http://allendowney.blogspot.com/2011/10/all-your-bayes-are-belong-to-us.html. "],
["9-sampling.html", "Chapter 9 Sampling 9.1 Sampling bowl activity 9.2 Virtual sampling 9.3 Sampling framework 9.4 Case study: Polls 9.5 Conclusion", " Chapter 9 Sampling In this chapter, we kick off the third portion of this book on statistical inference by learning about sampling. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we’ll cover in Chapters 10 and ??. We will see that the tools that you learned in the data science portion of this book, in particular data visualization and data wrangling, will also play an important role in the development of your understanding. As mentioned before, the concepts throughout this text all build into a culmination allowing you to “tell your story with data.” Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 5.5 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) 9.1 Sampling bowl activity Let’s start with a hands-on activity. 9.1.1 What proportion of this bowl’s balls are red? Take a look at the bowl in Figure 9.1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls. Let’s now ask ourselves, what proportion of this bowl’s balls are red? FIGURE 9.1: A bowl with red and white balls. One way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process. 9.1.2 Using the shovel once Instead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 9.2. Using the shovel, let’s remove \\(5 \\cdot 10 = 50\\) balls, as seen in Figure 9.3. FIGURE 9.2: Inserting a shovel into the bowl. FIGURE 9.3: Removing 50 balls from the bowl. Observe that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make. However, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe? What if we repeated this activity several times following the process shown in Figure 9.4? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition. 9.1.3 Using the shovel 33 times Each of our 33 groups of friends will do the following: Use the shovel to remove 50 balls each. Count the number of red balls and thus compute the proportion of the 50 balls that are red. Return the balls into the bowl. Mix the contents of the bowl a little to not let a previous group’s results influence the next group’s. FIGURE 9.4: Repeating sampling activity 33 times. Each of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure 9.5. FIGURE 9.5: Constructing a histogram of proportions. Recall from Section 2.4 that histograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in Figure 9.6. FIGURE 9.6: Hand-drawn histogram of first 10 out of 33 proportions. Observe the following in the histogram in Figure 9.6: At the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25. At the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red. However, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution. The shape of this distribution is somewhat bell-shaped. Let’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Chapter 2. We saved our 33 groups of friends’ results in the tactile_prop_red data frame included in the moderndive package. Run the following to display the first 10 of 33 rows: tactile_prop_red # A tibble: 33 x 4 group replicate red_balls prop_red &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Ilyas, Yohan 1 21 0.42 2 Morgan, Terrance 2 17 0.34 3 Martin, Thomas 3 21 0.42 4 Clark, Frank 4 21 0.42 5 Riddhi, Karina 5 18 0.36 6 Andrew, Tyler 6 19 0.38 7 Julia 7 19 0.38 8 Rachel, Lauren 8 11 0.22 9 Daniel, Caroline 9 15 0.3 10 Josh, Maeve 10 17 0.34 # … with 23 more rows Observe for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red. Let’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05 in Figure 9.7. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure 9.6. Note that setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure 9.6. ggplot(tactile_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 9.7: Distribution of 33 proportions based on 33 samples of size 50. 9.1.4 What did we just do? What we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%. Moreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 9.7. This is known as the concept of sampling variation. The purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling: Understanding the effect of sampling variation. Understanding the effect of sample size on sampling variation. In Section 9.2, we’ll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50. Afterwards, we’ll present you with definitions, terminology, and notation related to sampling in Section 9.3. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you’ll be able to master them. To tie the contents of this chapter to the real world, we’ll present an example of one of the most recognizable uses of sampling: polls. In Section 9.4 we’ll look at a particular case study: a 2013 poll on then U.S. President Barack Obama’s popularity among young Americans, conducted by Kennedy School’s Institute of Politics at Harvard University. To close this chapter, we’ll generalize the “sampling from a bowl” exercise to other sampling scenarios and present a theoretical result known as the Central Limit Theorem. 9.2 Virtual sampling In the previous Section 9.1, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel. 9.2.1 Using the virtual shovel once Let’s start by performing the virtual analog of the tactile sampling exercise we performed in Section 9.1. We first need a virtual analog of the bowl seen in Figure 9.1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl. bowl # A tibble: 2,400 x 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # … with 2,390 more rows Observe that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable as discussed in Subsection 1.4.4; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio’s data viewer and scroll through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 9.1. Now that we have a virtual analog of our bowl, we now need a virtual analog to the shovel seen in Figure 9.2 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the moderndive package. This function allows us to take repeated, or replicated, samples of size n. virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) virtual_shovel # A tibble: 50 x 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1970 white 2 1 842 red 3 1 2287 white 4 1 599 white 5 1 108 white 6 1 846 red 7 1 390 red 8 1 344 white 9 1 910 white 10 1 1485 white # … with 40 more rows Observe that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color. However, what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We’ll see shortly that when we “virtually” take 33 samples, replicate will take values between 1 and 33. Let’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Chapter 4. First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function from Section 4.5: virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) # A tibble: 50 x 4 # Groups: replicate [1] replicate ball_ID color is_red &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; 1 1 1970 white FALSE 2 1 842 red TRUE 3 1 2287 white FALSE 4 1 599 white FALSE 5 1 108 white FALSE 6 1 846 red TRUE 7 1 390 red TRUE 8 1 344 white FALSE 9 1 910 white FALSE 10 1 1485 white FALSE # … with 40 more rows Observe that for every row where color == &quot;red&quot;, the Boolean (logical) value TRUE is returned and for every row where color is not equal to &quot;red&quot;, the Boolean FALSE is returned. Second, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 4.3 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum(): virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) %&gt;% summarize(num_red = sum(is_red)) # A tibble: 1 x 2 replicate num_red &lt;int&gt; &lt;int&gt; 1 1 12 Why does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 12 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling. Third and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50: virtual_shovel %&gt;% mutate(is_red = color == &quot;red&quot;) %&gt;% summarize(num_red = sum(is_red)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 x 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 In other words, 24% of this virtual sample’s balls were red. Let’s make this code a little more compact and succinct by combining the first mutate() and the summarize() as follows: virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 x 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 Great! 24% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 24%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 24% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 9.6. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel! 9.2.2 Using the virtual shovel 33 times Recall that in our tactile sampling exercise in Section 9.1, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times. We’ll save these results in a data frame called virtual_samples. While we provide a preview of the first 10 rows of virtual_samples in what follows, we highly suggest you scroll through its contents using RStudio’s spreadsheet viewer by running View(virtual_samples). virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 33) virtual_samples # A tibble: 1,650 x 3 # Groups: replicate [33] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 875 white 2 1 1851 red 3 1 1548 red 4 1 1975 white 5 1 835 white 6 1 16 white 7 1 327 white 8 1 1803 red 9 1 740 red 10 1 179 red # … with 1,640 more rows Observe in the spreadsheet viewer that the first 50 rows of replicate are equal to 1 while the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\cdot\\) 50 = 1650 rows. Let’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Section 4.4 that by assigning the grouping variable “meta-data” before we summarize(), we’ll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows: virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 33 x 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 23 0.46 2 2 19 0.38 3 3 18 0.36 4 4 19 0.38 5 5 15 0.3 6 6 21 0.42 7 7 21 0.42 8 8 16 0.32 9 9 24 0.48 10 10 14 0.28 # … with 23 more rows As with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram in Figure 9.8. Note that we add binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 9.8: Distribution of 33 proportions based on 33 samples of size 50. Observe that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of sampling variation. Let’s now compare our virtual results with our tactile results from the previous section in Figure 9.9. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped. FIGURE 9.9: Comparing 33 virtual and 33 tactile proportions red. 9.2.3 Using the virtual shovel 1000 times Now say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates reps set to 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer. virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) virtual_samples # A tibble: 50,000 x 3 # Groups: replicate [1,000] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1236 red 2 1 1944 red 3 1 1939 white 4 1 780 white 5 1 1956 white 6 1 1003 white 7 1 2113 white 8 1 2213 white 9 1 782 white 10 1 898 white # … with 49,990 more rows Observe that now virtual_samples has 1000 \\(\\cdot\\) 50 = 50,000 rows, instead of the 33 \\(\\cdot\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\cdot\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls. virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 1,000 x 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 18 0.36 2 2 19 0.38 3 3 20 0.4 4 4 15 0.3 5 5 17 0.34 6 6 16 0.32 7 7 23 0.46 8 8 23 0.46 9 9 15 0.3 10 10 18 0.36 # … with 990 more rows Observe that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 9.10. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 1000 proportions red&quot;) FIGURE 9.10: Distribution of 1000 proportions based on 1000 samples of size 50. Once again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by a normal distribution. At this point we recommend you read the “Normal distribution” section (Appendix 7.6) for a brief discussion on the properties of the normal distribution. 9.2.4 Using different shovels Now say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100. FIGURE 9.11: Three shovels to extract three different sample sizes. If your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000: Virtually use the appropriate shovel to generate 1000 samples with size balls. Compute the resulting 1000 replicates of the proportion of the shovel’s balls that are red. Visualize the distribution of these 1000 proportions red using a histogram. Run each of the following code segments individually and then compare the three resulting histograms. # Segment 1: sample size = 25 ------------------------------ # 1.a) Virtually use shovel 1000 times virtual_samples_25 &lt;- bowl %&gt;% rep_sample_n(size = 25, reps = 1000) # 1.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 25) # 1.c) Plot distribution via a histogram ggplot(virtual_prop_red_25, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 25 balls that were red&quot;, title = &quot;25&quot;) # Segment 2: sample size = 50 ------------------------------ # 2.a) Virtually use shovel 1000 times virtual_samples_50 &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # 2.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # 2.c) Plot distribution via a histogram ggplot(virtual_prop_red_50, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;50&quot;) # Segment 3: sample size = 100 ------------------------------ # 3.a) Virtually using shovel with 100 slots 1000 times virtual_samples_100 &lt;- bowl %&gt;% rep_sample_n(size = 100, reps = 1000) # 3.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 100) # 3.c) Plot distribution via a histogram ggplot(virtual_prop_red_100, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 100 balls that were red&quot;, title = &quot;100&quot;) For easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 9.12. FIGURE 9.12: Comparing the distributions of proportion red for different sample sizes. Observe that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 9.12, all three histograms appear to center around roughly 40%. We can be numerically explicit about the amount of variation in our three sets of 1000 values of prop_red using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix 7.5 for a brief discussion on the properties of the standard deviation). For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function. # n = 25 virtual_prop_red_25 %&gt;% summarize(sd = sd(prop_red)) # n = 50 virtual_prop_red_50 %&gt;% summarize(sd = sd(prop_red)) # n = 100 virtual_prop_red_100 %&gt;% summarize(sd = sd(prop_red)) Let’s compare these three measures of distributional variation in Table 9.1. TABLE 9.1: Comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 As we observed in Figure 9.12, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more precise. 9.3 Sampling framework In both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 9.12 and Table 9.1: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. Let’s now introduce some terminology and notation as well as statistical definitions related to sampling. Given the number of new words you’ll need to learn, you will likely have to read this section a few times. Keep in mind, however, that all of the concepts underlying these terminology, notation, and definitions tie directly to the concepts underlying our tactile and virtual sampling activities. It will simply take time and practice to master them. 9.3.1 Terminology and notation Here is a list of terminology and mathematical notation relating to sampling. First, a population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\). In our sampling activities, the (study) population is the collection of \\(N\\) = 2400 identically sized red and white balls contained in the bowl. Second, a population parameter is a numerical summary quantity about the population that is unknown, but you wish you knew. For example, when this quantity is a mean, the population parameter of interest is the population mean. This is mathematically denoted with the Greek letter \\(\\mu\\) pronounced “mu” (we’ll see a sampling activity involving means in the upcoming Section 10.1). In our earlier sampling from the bowl activity, however, since we were interested in the proportion of the bowl’s balls that were red, the population parameter is the population proportion. This is mathematically denoted with the letter \\(p\\). Third, a census is an exhaustive enumeration or counting of all \\(N\\) individuals or observations in the population in order to compute the population parameter’s value exactly. In our sampling activity, this would correspond to counting the number of balls out of \\(N\\) = 2400 that are red and computing the population proportion \\(p\\) that are red exactly. When the number \\(N\\) of individuals or observations in our population is large as was the case with our bowl, a census can be quite expensive in terms of time, energy, and money. Fourth, sampling is the act of collecting a sample from the population when we don’t have the means to perform a census. We mathematically denote the sample’s size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census. In our sampling activities, we used shovels with 25, 50, and 100 slots to extract samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100. Fifth, a point estimate (AKA sample statistic) is a summary statistic computed from a sample that estimates an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the population proportion and that this is mathematically denoted with \\(p\\). Our point estimate is the sample proportion: the proportion of the shovel’s balls that are red. In other words, it is our guess of the proportion of the bowl’s balls balls that are red. We mathematically denote the sample proportion using \\(\\widehat{p}\\). The “hat” on top of the \\(p\\) indicates that it is an estimate of the unknown population proportion \\(p\\). Sixth is the idea of representative sampling. A sample is said to be a representative sample if it roughly looks like the population. In other words, are the sample’s characteristics a good representation of the population’s characteristics? In our sampling activity, are the samples of \\(n\\) balls extracted using our shovels representative of the bowl’s \\(N\\) = 2400 balls? Seventh is the idea of generalizability. We say a sample is generalizable if any results based on the sample can generalize to the population. In other words, does the value of the point estimate generalize to the population? In our sampling activity, can we generalize the sample proportion from our shovels to the entire bowl? Using our mathematical notation, this is akin to asking if \\(\\widehat{p}\\) is a “good guess” of \\(p\\)? Eighth, we say biased sampling occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every observation in a population had an equal chance of being sampled. In our sampling activities, since we mixed all \\(N = 2400\\) balls prior to each group’s sampling and since each of the equally sized balls had an equal chance of being sampled, our samples were unbiased. Ninth and lastly, the idea of random sampling. We say a sampling procedure is random if we sample randomly from the population in an unbiased fashion. In our sampling activities, this would correspond to sufficiently mixing the bowl before each use of the shovel. Phew, that’s a lot of new terminology and notation to learn! Let’s put them all together to describe the paradigm of sampling. In general: If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. Specific to our sampling activity: If we extract a sample of \\(n=50\\) balls at random, in other words, we mix all of the equally sized balls before using the shovel, then the contents of the shovel are an unbiased representation of the contents of the bowl’s 2400 balls, thus any result based on the shovel’s balls can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n=50\\) balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the \\(N=2400\\) balls that are red, thus instead of manually going over all 2400 balls in the bowl, we can infer about the bowl using the shovel. Note that last word we wrote in bold: infer. The act of “inferring” means to deduce or conclude information from evidence and reasoning. In our sampling activities, we wanted to infer about the proportion of the bowl’s balls that are red. Statistical inference is the “theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling.” In other words, statistical inference is the act of inference via sampling. In the upcoming Chapter 10 on confidence intervals, we’ll introduce the infer package, which makes statistical inference “tidy” and transparent. It is why this third portion of the book is called “Statistical inference via infer.” 9.3.2 Statistical definitions Now, for some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 9.2, let’s display Figure 9.12 again as Figure 9.13. FIGURE 9.13: Previously seen three distributions of the sample proportion \\(\\widehat{p}\\). These types of distributions have a special name: sampling distributions; their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. For example, observe the centers of all three sampling distributions: they are all roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of \\(0.2 = 20\\%\\) when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 9.1, which we display again as Table 9.2: TABLE 9.2: Previously seen comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 So as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases. Unfortunately, these names confuse many people who are new to statistical inference. For example, it’s common for people who are new to statistical inference to call the “sampling distribution” the “sample distribution.” Another additional source of confusion is the name “standard deviation” and “standard error.” Remember that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error. To help reinforce these concepts, let’s re-display Figure 9.12 but using our new terminology, notation, and definitions relating to sampling in Figure 9.14. FIGURE 9.14: Three sampling distributions of the sample proportion \\(\\widehat{p}\\). Furthermore, let’s re-display Table 9.1 but using our new terminology, notation, and definitions relating to sampling in Table 9.3. TABLE 9.3: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100 Sample size (n) Standard error of \\(\\widehat{p}\\) n = 25 0.094 n = 50 0.069 n = 100 0.045 Remember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error. 9.3.3 The moral of the story Let’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red. However, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion. In our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate. What was the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the actual bowl that were red? There were 900 red balls, for a proportion red of 900/2400 = 0.375 = 37.5%! How do we know this? Did the authors do an exhaustive count of all the balls? No! They were listed in the contents of the box that the bowl came in! Hence we were able to make the contents of the virtual bowl match the tactile bowl: bowl %&gt;% summarize(sum_red = sum(color == &quot;red&quot;), sum_not_red = sum(color != &quot;red&quot;)) # A tibble: 1 x 2 sum_red sum_not_red &lt;int&gt; &lt;int&gt; 1 900 1500 Let’s re-display our sampling distributions from Figures 9.12 and 9.14, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5% in Figure 9.15. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\). FIGURE 9.15: Three sampling distributions with population proportion \\(p\\) marked by vertical line. We also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 9.15. This is also known as having a precise estimate. So random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 9.16 illustrates the difference. FIGURE 9.16: Comparing accuracy and precision. At this point, you might be asking yourself: “If we already knew the true proportion of the bowl’s balls that are red was 37.5%, then why did we do any sampling?”. You might also be asking: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is merited! The sampling activity involving the bowl is merely an idealized version of how sampling is done in real life. We performed this exercise only to study and understand: The effect of sampling variation. The effect of sample size on sampling variation. This is not how sampling is done in real life. In a real-life scenario, we won’t know what the true value of the population parameter is. Furthermore, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford. In the next section, let’s now study a real-life example of sampling: polls. 9.4 Case study: Polls Let’s now switch gears to a more realistic sampling scenario than our bowl activity: a poll. In practice, pollsters do not take 1000 repeated samples as we did in our previous sampling activities, but rather take only a single sample that’s as large as possible. On December 4, 2013, National Public Radio in the US reported on a poll of President Obama’s approval rating among young Americans aged 18-29 in an article, “Poll: Support For Obama Among Young Americans Eroding.” The poll was conducted by the Kennedy School’s Institute of Politics at Harvard University. A quote from the article: After voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama. According to a new Harvard University Institute of Politics poll, just 41 percent of millennials — adults ages 18-29 — approve of Obama’s job performance, his lowest-ever standing among the group and an 11-point drop from April. Let’s tie elements of the real-life poll in this new article with our “tactile” and “virtual” bowl activity from Sections 9.1 and 9.2 using the terminology, notations, and definitions we learned in Section 9.3. You’ll see that our sampling activity with the bowl is an idealized version of what pollsters are trying to do in real life. First, who is the (Study) Population of \\(N\\) individuals or observations of interest? Bowl: \\(N\\) = 2400 identically sized red and white balls Obama poll: \\(N\\) = ? young Americans aged 18-29 Second, what is the population parameter? Bowl: The population proportion \\(p\\) of all the balls in the bowl that are red. Obama poll: The population proportion \\(p\\) of all young Americans who approve of Obama’s job performance. Third, what would a census look like? Bowl: Manually going over all \\(N\\) = 2400 balls and exactly computing the population proportion \\(p\\) of the balls that are red. Obama poll: Locating all \\(N\\) young Americans and asking them all if they approve of Obama’s job performance. In this case, we don’t even know what the population size \\(N\\) is! Fourth, how do you perform sampling to obtain a sample of size \\(n\\)? Bowl: Using a shovel with \\(n\\) slots. Obama poll: One method is to get a list of phone numbers of all young Americans and pick out \\(n\\) phone numbers. In this poll’s case, the sample size of this poll was \\(n = 2089\\) young Americans. Fifth, what is your point estimate (AKA sample statistic) of the unknown population parameter? Bowl: The sample proportion \\(\\widehat{p}\\) of the balls in the shovel that were red. Obama poll: The sample proportion \\(\\widehat{p}\\) of young Americans in the sample that approve of Obama’s job performance. In this poll’s case, \\(\\widehat{p} = 0.41 = 41\\%\\), the quoted percentage in the second paragraph of the article. Sixth, is the sampling procedure representative? Bowl: Are the contents of the shovel representative of the contents of the bowl? Because we mixed the bowl before sampling, we can feel confident that they are. Obama poll: Is the sample of \\(n = 2089\\) young Americans representative of all young Americans aged 18-29? This depends on whether the sampling was random. Seventh, are the samples generalizable to the greater population? Bowl: Is the sample proportion \\(\\widehat{p}\\) of the shovel’s balls that are red a “good guess” of the population proportion \\(p\\) of the bowl’s balls that are red? Given that the sample was representative, the answer is yes. Obama poll: Is the sample proportion \\(\\widehat{p} = 0.41\\) of the sample of young Americans who supported Obama a “good guess” of the population proportion \\(p\\) of all young Americans who supported Obama at this time in 2013? In other words, can we confidently say that roughly 41% of all young Americans approved of Obama at the time of the poll? Again, this depends on whether the sampling was random. Eighth, is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample? Bowl: Since each ball was equally sized and we mixed the bowl before using the shovel, each ball had an equal chance of being included in a sample and hence the sampling was unbiased. Obama poll: Did all young Americans have an equal chance at being represented in this poll? Again, this depends on whether the sampling was random. Ninth and lastly, was the sampling done at random? Bowl: As long as you mixed the bowl sufficiently before sampling, your samples would be random. Obama poll: Was the sample conducted at random? We can’t answer this question without knowing about the sampling methodology used by Kennedy School’s Institute of Politics at Harvard University. We’ll discuss this more at the end of this section. In other words, the poll by Kennedy School’s Institute of Politics at Harvard University can be thought of as an instance of using the shovel to sample balls from the bowl. Furthermore, if another polling company conducted a similar poll of young Americans at roughly the same time, they would likely get a different estimate than 41%. This is due to sampling variation. Let’s now revisit the sampling paradigm from Subsection 9.3.1: In general: If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. Specific to the bowl: If we extract a sample of \\(n = 50\\) balls at random, in other words, we mix all of the equally sized balls before using the shovel, then the contents of the shovel are an unbiased representation of the contents of the bowl’s 2400 balls, thus any result based on the shovel’s balls can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n = 50\\) balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the \\(N = 2400\\) balls that are red, thus instead of manually going over all 2400 balls in the bowl, we can infer about the bowl using the shovel. Specific to the Obama poll: If we had a way of contacting a randomly chosen sample of 2089 young Americans and polling their approval of President Obama in 2013, then these 2089 young Americans would be an unbiased and representative sample of all young Americans in 2013, thus any results based on this sample of 2089 young Americans can generalize to the entire population of all young Americans in 2013, thus the reported sample approval rating of 41% of these 2089 young Americans is a good guess of the true approval rating among all young Americans in 2013, thus instead of performing an expensive census of all young Americans in 2013, we can infer about all young Americans in 2013 using polling. So as you can see, it was critical for the sample obtained by Kennedy School’s Institute of Politics at Harvard University to be truly random in order to infer about all young Americans’ opinions about Obama. Was their sample truly random? It’s hard to answer such questions without knowing about the sampling methodology they used. For example, if this poll was conducted using only mobile phone numbers, people without mobile phones would be left out and therefore not represented in the sample. What about if Kennedy School’s Institute of Politics at Harvard University conducted this poll on an internet news site? Then people who don’t read this particular internet news site would be left out. Ensuring that our samples were random was easy to do in our sampling bowl exercises; however, in a real-life situation like the Obama poll, this is much harder to do. 9.5 Conclusion 9.5.1 Sampling scenarios In this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\widehat{p}\\) to estimate the population proportion \\(p\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table 9.4. TABLE 9.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) In the rest of this book, we’ll cover all the remaining scenarios as follows: In Chapter 10, we’ll cover examples of statistical inference for Scenario 2: The mean age \\(\\mu\\) of all pennies in circulation in the US. Scenario 3: The difference \\(p_1 - p_2\\) in the proportion of people who yawn when seeing someone else yawn first minus the proportion of people who yawn without seeing someone else yawn first. This is an example of two-sample inference. In Chapter ??, we’ll cover an example of statistical inference for Scenario 4: The difference \\(\\mu_1 - \\mu_2\\) in mean IMDb ratings for action and romance movies. This is another example of two-sample inference. In Chapter ??, we’ll cover an example of statistical inference for regression by revisiting the regression models for teaching score as a function of various instructor demographic variables you saw in Chapters 11 and 12. Scenario 5: The slope \\(\\beta_1\\) of the population regression line. 9.5.2 Central Limit Theorem What you visualized in Figures 9.12 and 9.14 and summarized in Tables 9.1 and 9.3 was a demonstration of a famous theorem, or mathematically proven truth, called the Central Limit Theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow. In other words, their sampling distribution increasingly follows a normal distribution and the variation of these sampling distributions gets smaller, as quantified by their standard errors. Shuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at https://youtu.be/jvoxEYmQHNM explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. Figure 9.17 shows a preview of this video. FIGURE 9.17: Preview of Central Limit Theorem video. 9.5.3 Additional resources 9.5.4 What’s to come? Recall in our Obama poll case study in Section 9.4 that based on this particular sample, the best guess by Kennedy School’s Institute of Politics at Harvard University of the U.S. President Obama’s approval rating among all young Americans was 41%. However, this isn’t the end of the story. If you read the article further, it states: The online survey of 2,089 adults was conducted from Oct. 30 to Nov. 11, just weeks after the federal government shutdown ended and the problems surrounding the implementation of the Affordable Care Act began to take center stage. The poll’s margin of error was plus or minus 2.1 percentage points. Note the term margin of error, which here is “plus or minus 2.1 percentage points.” Most polls won’t produce an estimate that’s perfectly right; there will always be a certain amount of error caused by sampling variation. The margin of error of plus or minus 2.1 percentage points is saying that a typical range of errors for polls of this type is about \\(\\pm\\) 2.1%, in words from about 2.1% too small to about 2.1% too big. We can restate this as the interval of \\([41\\% - 2.1\\%, 41\\% + 2.1\\%] = [37.9\\%, 43.1\\%]\\) (this notation indicates the interval contains all values between 37.9% and 43.1%, including the end points of 37.9% and 43.1%). We’ll see in the next chapter that such intervals are known as confidence intervals. "],
["10-confidence-intervals.html", "Chapter 10 Confidence Intervals 10.1 Pennies activity 10.2 Computer simulation of resampling 10.3 Understanding confidence intervals 10.4 Constructing confidence intervals 10.5 Interpreting confidence intervals 10.6 Case study: Is yawning contagious? 10.7 Conclusion", " Chapter 10 Confidence Intervals In Chapter 9, we studied sampling. We started with a “tactile” exercise where we wanted to know the proportion of balls in the sampling bowl in Figure 9.1 that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead, we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an estimate. Furthermore, we made sure to mix the bowl’s contents before every use of the shovel. Because of the randomness created by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the bowl’s balls that are red. We then mimicked this “tactile” sampling exercise with an equivalent “virtual” sampling exercise performed on the computer. Using our computer’s random number generator, we quickly mimicked the above sampling procedure a large number of times. In Subsection 9.2.4, we quickly repeated this sampling procedure 1000 times, using three different “virtual” shovels with 25, 50, and 100 slots. We visualized these three sets of 1000 estimates in Figure 9.15 and saw that as the sample size increased, the variation in the estimates decreased. In doing so, what we did was construct sampling distributions. The motivation for taking 1000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words, we wanted to study the effect of sampling variation. We quantified the variation of these estimates using their standard deviation, which has a special name: the standard error. In particular, we saw that as the sample size increased from 25 to 50 to 100, the standard error decreased and thus the sampling distributions narrowed. Larger sample sizes led to more precise estimates that varied less around the center. We then tied these sampling exercises to terminology and mathematical notation related to sampling in Subsection 9.3.1. Our study population was the large bowl with \\(N\\) = 2400 balls, while the population parameter, the unknown quantity of interest, was the population proportion \\(p\\) of the bowl’s balls that were red. Since performing a census would be expensive in terms of time and energy, we instead extracted a sample of size \\(n\\) = 50. The point estimate, also known as a sample statistic, used to estimate \\(p\\) was the sample proportion \\(\\widehat{p}\\) of these 50 sampled balls that were red. Furthermore, since the sample was obtained at random, it can be considered as unbiased and representative of the population. Thus any results based on the sample could be generalized to the population. Therefore, the proportion of the shovel’s balls that were red was a “good guess” of the proportion of the bowl’s balls that are red. In other words, we used the sample to infer about the population. However, as described in Section 9.2, both the tactile and virtual sampling exercises are not what one would do in real life; this was merely an activity used to study the effects of sampling variation. In a real-life situation, we would not take 1000 samples of size \\(n\\), but rather take a single representative sample that’s as large as possible. Additionally, we knew that the true proportion of the bowl’s balls that were red was 37.5%. In a real-life situation, we will not know what this value is. Because if we did, then why would we take a sample to estimate it? An example of a realistic sampling situation would be a poll, like the Obama poll you saw in Section 9.4. Pollsters did not know the true proportion of all young Americans who supported President Obama in 2013, and thus they took a single sample of size \\(n\\) = 2089 young Americans to estimate this value. So how does one quantify the effects of sampling variation when you only have a single sample to work with? You cannot directly study the effects of sampling variation when you only have one sample. One common method to study this is bootstrapping resampling, which will be the focus of the earlier sections of this chapter. Furthermore, what if we would like not only a single estimate of the unknown population parameter, but also a range of highly plausible values? Going back to the Obama poll article, it stated that the pollsters’ estimate of the proportion of all young Americans who supported President Obama was 41%. But in addition it stated that the poll’s “margin of error was plus or minus 2.1 percentage points.” This “plausible range” was [41% - 2.1%, 41% + 2.1%] = [38.9%, 43.1%]. This range of plausible values is what’s known as a confidence interval, which will be the focus of the later sections of this chapter. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 5.5 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to tidy format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) 10.1 Pennies activity As we did in Chapter 9, we’ll begin with a hands-on tactile activity. 10.1.1 What is the average year on US pennies in 2019? Try to imagine all the pennies being used in the United States in 2019. That’s a lot of pennies! Now say we’re interested in the average year of minting of all these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. However, this would be near impossible! So instead, let’s collect a sample of 50 pennies from a local bank in downtown Northampton, Massachusetts, USA as seen in Figure 10.1. FIGURE 10.1: Collecting a sample of 50 US pennies from a local bank. An image of these 50 pennies can be seen in Figure 10.2. For each of the 50 pennies starting in the top left, progressing row-by-row, and ending in the bottom right, we assigned an “ID” identification variable and marked the year of minting. FIGURE 10.2: 50 US pennies labelled. The moderndive package contains this data on our 50 sampled pennies in the pennies_sample data frame: pennies_sample # A tibble: 50 x 2 ID year &lt;int&gt; &lt;dbl&gt; 1 1 2002 2 2 1986 3 3 2017 4 4 1988 5 5 2008 6 6 1983 7 7 2008 8 8 1996 9 9 2004 10 10 2000 # … with 40 more rows The pennies_sample data frame has 50 rows corresponding to each penny with two variables. The first variable ID corresponds to the ID labels in Figure 10.2, whereas the second variable year corresponds to the year of minting saved as a numeric variable, also known as a double (dbl). Based on these 50 sampled pennies, what can we say about all US pennies in 2019? Let’s study some properties of our sample by performing an exploratory data analysis. Let’s first visualize the distribution of the year of these 50 pennies using our data visualization tools from Chapter 2. Since year is a numerical variable, we use a histogram in Figure 10.3 to visualize its distribution. ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) FIGURE 10.3: Distribution of year on 50 US pennies. Observe a slightly left-skewed distribution, since most pennies fall between 1980 and 2010 with only a few pennies older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let’s now compute this value exactly using our data wrangling tools from Chapter 4. pennies_sample %&gt;% summarize(mean_year = mean(year)) # A tibble: 1 x 1 mean_year &lt;dbl&gt; 1 1995.44 Thus, if we’re willing to assume that pennies_sample is a representative sample from all US pennies, a “good guess” of the average year of minting of all US pennies would be 1995.44. In other words, around 1995. This should all start sounding similar to what we did previously in Chapter 9! In Chapter 9, our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter was the population proportion of these balls that were red, denoted by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel. We then computed the relevant point estimate: the sample proportion of these 50 balls that were red, denoted mathematically by \\(\\widehat{p}\\). Here our population is \\(N\\) = whatever the number of pennies are being used in the US, a value which we don’t know and probably never will. The population parameter of interest is now the population mean year of all these pennies, a value denoted mathematically by the Greek letter \\(\\mu\\) (pronounced “mu”). In order to estimate \\(\\mu\\), we went to the bank and obtained a sample of 50 pennies and computed the relevant point estimate: the sample mean year of these 50 pennies, denoted mathematically by \\(\\overline{x}\\) (pronounced “x-bar”). An alternative and more intuitive notation for the sample mean is \\(\\widehat{\\mu}\\). However, this is unfortunately not as commonly used, so in this book we’ll stick with convention and always denote the sample mean as \\(\\overline{x}\\). We summarize the correspondence between the sampling bowl exercise in Chapter 9 and our pennies exercise in Table 10.1, which are the first two rows of the previously seen Table 9.4. TABLE 10.1: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) Going back to our 50 sampled pennies in Figure 10.2, the point estimate of interest is the sample mean \\(\\overline{x}\\) of 1995.44. This quantity is an estimate of the population mean year of all US pennies \\(\\mu\\). Recall that we also saw in Chapter 9 that such estimates are prone to sampling variation. For example, in this particular sample in Figure 10.2, we observed three pennies with the year 1999. If we sampled another 50 pennies, would we observe exactly three pennies with the year 1999 again? More than likely not. We might observe none, one, two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies. To study the effects of sampling variation in Chapter 9, we took many samples, something we could easily do with our shovel. In our case with pennies, however, how would we obtain another sample? By going to the bank and getting another roll of 50 pennies. Say we’re feeling lazy, however, and don’t want to go back to the bank. How can we study the effects of sampling variation using our single sample? We will do so using a technique known as bootstrap resampling with replacement, which we now illustrate. 10.1.2 Resampling once Step 1: Let’s print out identically sized slips of paper representing our 50 pennies as seen in Figure 10.4. FIGURE 10.4: Step 1: 50 slips of paper representing 50 US pennies. Step 2: Put the 50 slips of paper into a hat or tuque as seen in Figure 10.5. FIGURE 10.5: Step 2: Putting 50 slips of paper in a hat. Step 3: Mix the hat’s contents and draw one slip of paper at random as seen in Figure 10.6. Record the year. FIGURE 10.6: Step 3: Drawing one slip of paper at random. Step 4: Put the slip of paper back in the hat! In other words, replace it as seen in Figure 10.7. FIGURE 10.7: Step 4: Replacing slip of paper. Step 5: Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. What we just performed was a resampling of the original sample of 50 pennies. We are not sampling 50 pennies from the population of all US pennies as we did in our trip to the bank. Instead, we are mimicking this act by resampling 50 pennies from our original sample of 50 pennies. Now ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 50 original pennies! In other words, replacing the slips of paper induces sampling variation. Being more precise with our terminology, we just performed a resampling with replacement from the original sample of 50 pennies. Had we left the slip of paper out of the hat each time we performed Step 4, this would be resampling without replacement. Let’s study our 50 resampled pennies via an exploratory data analysis. First, let’s load the data into R by manually creating a data frame pennies_resample of our 50 resampled values. We’ll do this using the tibble() command from the dplyr package. Note that the 50 values you resample will almost certainly not be the same as ours given the inherent randomness. pennies_resample &lt;- tibble( year = c(1976, 1962, 1976, 1983, 2017, 2015, 2015, 1962, 2016, 1976, 2006, 1997, 1988, 2015, 2015, 1988, 2016, 1978, 1979, 1997, 1974, 2013, 1978, 2015, 2008, 1982, 1986, 1979, 1981, 2004, 2000, 1995, 1999, 2006, 1979, 2015, 1979, 1998, 1981, 2015, 2000, 1999, 1988, 2017, 1992, 1997, 1990, 1988, 2006, 2000) ) The 50 values of year in pennies_resample represent a resample of size 50 from the original sample of 50 pennies. We display the 50 resampled pennies in Figure 10.8. FIGURE 10.8: 50 resampled US pennies labelled. Let’s compare the distribution of the numerical variable year of our 50 resampled pennies with the distribution of the numerical variable year of our original sample of 50 pennies in Figure 10.9. ggplot(pennies_resample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Resample of 50 pennies&quot;) ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Original sample of 50 pennies&quot;) FIGURE 10.9: Comparing year in the resampled pennies_resample with the original sample pennies_sample. Observe in Figure 10.9 that while the general shapes of both distributions of year are roughly similar, they are not identical. Recall from the previous section that the sample mean of the original sample of 50 pennies from the bank was 1995.44. What about for our resample? Any guesses? Let’s have dplyr help us out as before: pennies_resample %&gt;% summarize(mean_year = mean(year)) # A tibble: 1 x 1 mean_year &lt;dbl&gt; 1 1996 We obtained a different mean year of 1996. This variation is induced by the resampling with replacement we performed earlier. What if we repeated this resampling exercise many times? Would we obtain the same mean year each time? In other words, would our guess at the mean year of all pennies in the US in 2019 be exactly 1996 every time? Just as we did in Chapter 9, let’s perform this resampling activity with the help of some of our friends: 35 friends in total. 10.1.3 Resampling 35 times Each of our 35 friends will repeat the same five steps: Start with 50 identically sized slips of paper representing the 50 pennies. Put the 50 small pieces of paper into a hat or beanie cap. Mix the hat’s contents and draw one slip of paper at random. Record the year in a spreadsheet. Replace the slip of paper back in the hat! Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. Since we had 35 of our friends perform this task, we ended up with \\(35 \\cdot 50 = 1750\\) values. We recorded these values in a shared spreadsheet with 50 rows (plus a header row) and 35 columns. We display a snapshot of the first 10 rows and five columns of this shared spreadsheet in Figure 10.10. FIGURE 10.10: Snapshot of shared spreadsheet of resampled pennies. For your convenience, we’ve taken these 35 \\(\\cdot\\) 50 = 1750 values and saved them in pennies_resamples, a “tidy” data frame included in the moderndive package. We saw what it means for a data frame to be “tidy” in Subsection 5.3.1. pennies_resamples # A tibble: 1,750 x 3 # Groups: name [35] replicate name year &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Arianna 1988 2 1 Arianna 2002 3 1 Arianna 2015 4 1 Arianna 1998 5 1 Arianna 1979 6 1 Arianna 1971 7 1 Arianna 1971 8 1 Arianna 2015 9 1 Arianna 1988 10 1 Arianna 1979 # … with 1,740 more rows What did each of our 35 friends obtain as the mean year? Once again, dplyr to the rescue! After grouping the rows by name, we summarize each group of 50 rows by their mean year: resampled_means &lt;- pennies_resamples %&gt;% group_by(name) %&gt;% summarize(mean_year = mean(year)) resampled_means # A tibble: 35 x 2 name mean_year &lt;chr&gt; &lt;dbl&gt; 1 Arianna 1992.5 2 Artemis 1996.42 3 Bea 1996.32 4 Camryn 1996.9 5 Cassandra 1991.22 6 Cindy 1995.48 7 Claire 1995.52 8 Dahlia 1998.48 9 Dan 1993.86 10 Eindra 1993.56 # … with 25 more rows Observe that resampled_means has 35 rows corresponding to the 35 means based on the 35 resamples. Furthermore, observe the variation in the 35 values in the variable mean_year. Let’s visualize this variation using a histogram in Figure 10.11. Recall that adding the argument boundary = 1990 to the geom_histogram() sets the binning structure so that one of the bin boundaries is at 1990 exactly. ggplot(resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Sampled mean year&quot;) FIGURE 10.11: Distribution of 35 sample means from 35 resamples. Observe in Figure 10.11 that the distribution looks roughly normal and that we rarely observe sample mean years less than 1992 or greater than 2000. Also observe how the distribution is roughly centered at 1995, which is close to the sample mean of 1995.44 of the original sample of 50 pennies from the bank. 10.1.4 What did we just do? What we just demonstrated in this activity is the statistical procedure known as bootstrap resampling with replacement. We used resampling to mimic the sampling variation we studied in Chapter 9 on sampling. However, in this case, we did so using only a single sample from the population. In fact, the histogram of sample means from 35 resamples in Figure 10.11 is called the bootstrap distribution. It is an approximation to the sampling distribution of the sample mean, in the sense that both distributions will have a similar shape and similar spread. In fact in the upcoming Section 10.7, we’ll show you that this is the case. Using this bootstrap distribution, we can study the effect of sampling variation on our estimates. In particular, we’ll study the typical “error” of our estimates, known as the standard error. In Section 10.2 we’ll mimic our tactile resampling activity virtually on the computer, allowing us to quickly perform the resampling many more than 35 times. In Section 10.3 we’ll define the statistical concept of a confidence interval, which builds off the concept of bootstrap distributions. In Section 10.4, we’ll construct confidence intervals using the dplyr package, as well as a new package: the infer package for “tidy” and transparent statistical inference. We’ll introduce the “tidy” statistical inference framework that was the motivation for the infer package pipeline. The infer package will be the driving package throughout the rest of this book. As we did in Chapter 9, we’ll tie all these ideas together with a real-life case study in Section 10.6. This time we’ll look at data from an experiment about yawning from the US television show Mythbusters. 10.2 Computer simulation of resampling Let’s now mimic our tactile resampling activity virtually with a computer. 10.2.1 Virtually resampling once First, let’s perform the virtual analog of resampling once. Recall that the pennies_sample data frame included in the moderndive package contains the years of our original sample of 50 pennies from the bank. Furthermore, recall in Chapter 9 on sampling that we used the rep_sample_n() function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows: virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) Let’s modify this code to perform the resampling with replacement of the 50 slips of paper representing our original sample 50 pennies: virtual_resample &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) Observe how we explicitly set the replace argument to TRUE in order to tell rep_sample_n() that we would like to sample pennies with replacement. Had we not set replace = TRUE, the function would’ve assumed the default value of FALSE and hence done resampling without replacement. Additionally, since we didn’t specify the number of replicates via the reps argument, the function assumes the default of one replicate reps = 1. Lastly, observe also that the size argument is set to match the original sample size of 50 pennies. Let’s look at only the first 10 out of 50 rows of virtual_resample: virtual_resample # A tibble: 50 x 3 # Groups: replicate [1] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 37 1962 2 1 1 2002 3 1 45 1997 4 1 28 2006 5 1 50 2017 6 1 10 2000 7 1 16 2015 8 1 47 1982 9 1 23 1998 10 1 44 2015 # … with 40 more rows The replicate variable only takes on the value of 1 corresponding to us only having reps = 1, the ID variable indicates which of the 50 pennies from pennies_sample was resampled, and year denotes the year of minting. Let’s now compute the mean year in our virtual resample of size 50 using data wrangling functions included in the dplyr package: virtual_resample %&gt;% summarize(resample_mean = mean(year)) # A tibble: 1 x 2 replicate resample_mean &lt;int&gt; &lt;dbl&gt; 1 1 1996 As we saw when we did our tactile resampling exercise, the resulting mean year is different than the mean year of our 50 originally sampled pennies of 1995.44. 10.2.2 Virtually resampling 35 times Let’s now perform the virtual analog of our 35 friends’ resampling. Using these results, we’ll be able to study the variability in the sample means from 35 resamples of size 50. Let’s first add a reps = 35 argument to rep_sample_n() to indicate we would like 35 replicates. Thus, we want to repeat the resampling with the replacement of 50 pennies 35 times. virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 35) virtual_resamples # A tibble: 1,750 x 3 # Groups: replicate [35] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 21 1981 2 1 34 1985 3 1 4 1988 4 1 11 1994 5 1 26 1979 6 1 8 1996 7 1 19 1983 8 1 21 1981 9 1 49 2006 10 1 2 1986 # … with 1,740 more rows The resulting virtual_resamples data frame has 35 \\(\\cdot\\) 50 = 1750 rows corresponding to 35 resamples of 50 pennies. Let’s now compute the resulting 35 sample means using the same dplyr code as we did in the previous section, but this time adding a group_by(replicate): virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 35 x 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1995.58 2 2 1999.74 3 3 1993.7 4 4 1997.1 5 5 1999.42 6 6 1995.12 7 7 1994.94 8 8 1997.78 9 9 1991.26 10 10 1996.88 # … with 25 more rows Observe that virtual_resampled_means has 35 rows, corresponding to the 35 resampled means. Furthermore, observe that the values of mean_year vary. Let’s visualize this variation using a histogram in Figure 10.12. ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Resample mean year&quot;) FIGURE 10.12: Distribution of 35 sample means from 35 resamples. Let’s compare our virtually constructed bootstrap distribution with the one our 35 friends constructed via our tactile resampling exercise in Figure 10.13. Observe how they are somewhat similar, but not identical. FIGURE 10.13: Comparing distributions of means from resamples. Recall that in the “resampling with replacement” scenario we are illustrating here, both of these histograms have a special name: the bootstrap distribution of the sample mean. Furthermore, recall they are an approximation to the sampling distribution of the sample mean, a concept you saw in Chapter 9 on sampling. These distributions allow us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for all US pennies. However, unlike in Chapter 9 where we took multiple samples (something one would never do in practice), bootstrap distributions are constructed by taking multiple resamples from a single sample: in this case, the 50 original pennies from the bank. 10.2.3 Virtually resampling 1000 times Remember that one of the goals of resampling with replacement is to construct the bootstrap distribution, which is an approximation of the sampling distribution. However, the bootstrap distribution in Figure 10.12 is based only on 35 resamples and hence looks a little coarse. Let’s increase the number of resamples to 1000, so that we can hopefully better see the shape and the variability between different resamples. # Repeat resampling 1000 times virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) # Compute 1000 sample means virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) However, in the interest of brevity, going forward let’s combine these two operations into a single chain of pipe (%&gt;%) operators: virtual_resampled_means &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 1,000 x 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1992.6 2 2 1994.78 3 3 1994.74 4 4 1997.88 5 5 1990 6 6 1999.48 7 7 1990.26 8 8 1993.2 9 9 1994.88 10 10 1996.3 # … with 990 more rows In Figure 10.14 let’s visualize the bootstrap distribution of these 1000 means based on 1000 virtual resamples: ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;sample mean&quot;) FIGURE 10.14: Bootstrap resampling distribution based on 1000 resamples. Note here that the bell shape is starting to become much more apparent. We now have a general sense for the range of values that the sample mean may take on. But where is this histogram centered? Let’s compute the mean of the 1000 resample means: virtual_resampled_means %&gt;% summarize(mean_of_means = mean(mean_year)) # A tibble: 1 x 1 mean_of_means &lt;dbl&gt; 1 1995.36 The mean of these 1000 means is 1995.36, which is quite close to the mean of our original sample of 50 pennies of 1995.44. This is the case since each of the 1000 resamples is based on the original sample of 50 pennies. Congratulations! You’ve just constructed your first bootstrap distribution! In the next section, you’ll see how to use this bootstrap distribution to construct confidence intervals. 10.3 Understanding confidence intervals Let’s start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish! Now think back to our pennies exercise where you are trying to estimate the true population mean year \\(\\mu\\) of all US pennies. Think of the value of \\(\\mu\\) as a fish. On the one hand, we could use the appropriate point estimate/sample statistic to estimate \\(\\mu\\), which we saw in Table 10.1 is the sample mean \\(\\overline{x}\\). Based on our sample of 50 pennies from the bank, the sample mean was 1995.44. Think of using this value as “fishing with a spear.” What would “fishing with a net” correspond to? Look at the bootstrap distribution in Figure 10.14 once more. Between which two years would you say that “most” sample means lie? While this question is somewhat subjective, saying that most sample means lie between 1992 and 2000 would not be unreasonable. Think of this interval as the “net.” What we’ve just illustrated is the concept of a confidence interval, which we’ll abbreviate with “CI” throughout this book. As opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a confidence interval gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets. FIGURE 10.15: Analogy of difference between point estimates and confidence intervals. Our proposed interval of 1992 to 2000 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the percentile method and the standard error method. Both methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in Subsection 10.2.3 and visualized in Figure 10.14. Second, they both require you to specify the confidence level. Commonly used confidence levels include 90%, 95%, and 99%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals. In this book, we’ll be mostly using 95% and hence constructing “95% confidence intervals for \\(\\mu\\)” for our pennies activity. 10.3.1 Percentile method One method to construct a confidence interval is to use the middle 95% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 1991.059 and 1999.283, respectively. This is known as the percentile method for constructing confidence intervals. For now, let’s focus only on the concepts behind a percentile method constructed confidence interval; we’ll show you the code that computes these values in the next section. Let’s mark these percentiles on the bootstrap distribution with vertical lines in Figure 10.16. About 95% of the mean_year variable values in virtual_resampled_means fall between 1991.059 and 1999.283, with 2.5% to the left of the leftmost line and 2.5% to the right of the rightmost line. FIGURE 10.16: Percentile method 95% confidence interval. Interval endpoints marked by vertical lines. 10.3.2 Standard error method Recall in Appendix 7.6, we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95% of values fall between \\(\\pm\\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure 10.14 is normally shaped, let’s use this fact about normal distributions to construct a confidence interval in a different way. First, recall the bootstrap distribution has a mean equal to 1995.36. This value almost coincides exactly with the value of the sample mean \\(\\overline{x}\\) of our original 50 pennies of 1995.44. Second, let’s compute the standard deviation of the bootstrap distribution using the values of mean_year in the virtual_resampled_means data frame: virtual_resampled_means %&gt;% summarize(SE = sd(mean_year)) # A tibble: 1 x 1 SE &lt;dbl&gt; 1 2.15466 What is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the standard error. Putting these two facts together, we can say that 2.155 is an approximation of the standard error of \\(\\overline{x}\\). Thus, using our 95% rule of thumb about normal distributions from Appendix 7.6, we can use the following formula to determine the lower and upper endpoints of a 95% confidence interval for \\(\\mu\\): \\[ \\begin{aligned} \\overline{x} \\pm 1.96 \\cdot SE &amp;= (\\overline{x} - 1.96 \\cdot SE, \\overline{x} + 1.96 \\cdot SE)\\\\ &amp;= (1995.44 - 1.96 \\cdot 2.15, 1995.44 + 1.96 \\cdot 2.15)\\\\ &amp;= (1991.15, 1999.73) \\end{aligned} \\] Let’s now add the SE method confidence interval with dashed lines in Figure 10.17. FIGURE 10.17: Comparing two 95% confidence interval methods. We see that both methods produce nearly identical 95% confidence intervals for \\(\\mu\\) with the percentile method yielding \\((1991.06, 1999.28)\\) while the standard error method produces \\((1991.22, 1999.66)\\). However, recall that we can only use the standard error rule when the bootstrap distribution is roughly normally shaped. Now that we’ve introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let’s explore the code that allows us to construct them. 10.4 Constructing confidence intervals Recall that the process of resampling with replacement we performed by hand in Section 10.1 and virtually in Section 10.2 is known as bootstrapping. The term bootstrapping originates in the expression of “pulling oneself up by their bootstraps,” meaning to “succeed only by one’s own efforts or abilities.” From a statistical perspective, bootstrapping alludes to succeeding in being able to study the effects of sampling variation on estimates from the “effort” of a single sample. Or more precisely, it refers to constructing an approximation to the sampling distribution using only one sample. To perform this resampling with replacement virtually in Section 10.2, we used the rep_sample_n() function, making sure that the size of the resamples matched the original sample size of 50. In this section, we’ll build off these ideas to construct confidence intervals using a new package: the infer package for “tidy” and transparent statistical inference. 10.4.1 Original workflow Recall that in Section 10.2, we virtually performed bootstrap resampling with replacement to construct bootstrap distributions. Such distributions are approximations to the sampling distributions we saw in Chapter 9, but are constructed using only a single sample. Let’s revisit the original workflow using the %&gt;% pipe operator. First, we used the rep_sample_n() function to resample size = 50 pennies with replacement from the original sample of 50 pennies in pennies_sample by setting replace = TRUE. Furthermore, we repeated this resampling 1000 times by setting reps = 1000: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) Second, since for each of our 1000 resamples of size 50, we wanted to compute a separate sample mean, we used the dplyr verb group_by() to group observations/rows together by the replicate variable… pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) … followed by using summarize() to compute the sample mean() year for each replicate group: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) For this simple case, we can get by with using the rep_sample_n() function and a couple of dplyr verbs to construct the bootstrap distribution. However, using only dplyr verbs only provides us with a limited set of tools. For more complicated situations, we’ll need a little more firepower. Let’s repeat this using the infer package. 10.4.2 infer package workflow The infer package is an R package for statistical inference. It makes efficient use of the %&gt;% pipe operator we introduced in Section 4.1 to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Furthermore, just as the dplyr package provides functions with verb-like names to perform data wrangling, the infer package provides functions with intuitive verb-like names to perform statistical inference. Let’s go back to our pennies. Previously, we computed the value of the sample mean \\(\\overline{x}\\) using the dplyr function summarize(): pennies_sample %&gt;% summarize(stat = mean(year)) We’ll see that we can also do this using infer functions specify() and calculate(): pennies_sample %&gt;% specify(response = year) %&gt;% calculate(stat = &quot;mean&quot;) You might be asking yourself: “Isn’t the infer code longer? Why would I use that code?”. While not immediately apparent, you’ll see that there are three chief benefits to the infer workflow as opposed to the dplyr workflow. First, the infer verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter ??). We’ll see flowchart diagrams of this framework in the upcoming Figure 10.23 and in Chapter ?? with Figure ??. Second, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent in Subsection ?? when we’ll compare the infer code for both of these inferential methods. Third, the infer workflow is much simpler for conducting inference when you have more than one variable. We’ll see two such situations. We’ll first see situations of two-sample inference where the sample data is collected from two groups, such as in Section 10.6 where we study the contagiousness of yawning and in Section ?? where we compare promotion rates of two groups at banks in the 1970s. Then in Section ??, we’ll see situations of inference for regression using the regression models you fit in Chapter 11. Let’s now illustrate the sequence of verbs necessary to construct a confidence interval for \\(\\mu\\), the population mean year of minting of all US pennies in 2019. 1. specify variables FIGURE 10.18: Diagram of the specify() verb. As shown in Figure 10.18, the specify() function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by specifying the response argument. For example, in our pennies_sample data frame of the 50 pennies sampled from the bank, the variable of interest is year: pennies_sample %&gt;% specify(response = year) Response: year (numeric) # A tibble: 50 x 1 year &lt;dbl&gt; 1 2002 2 1986 3 2017 4 1988 5 2008 6 1983 7 2008 8 1996 9 2004 10 2000 # … with 40 more rows Notice how the data itself doesn’t change, but the Response: year (numeric) meta-data does. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 4.4. We can also specify which variables will be the focus of our statistical inference using a formula = y ~ x. This is the same formula notation you saw in Chapters 11 and 12 on regression models: the response variable y is separated from the explanatory variable x by a ~ (“tilde”). The following use of specify() with the formula argument yields the same result seen previously: pennies_sample %&gt;% specify(formula = year ~ NULL) Since in the case of pennies we only have a response variable and no explanatory variable of interest, we set the x on the right-hand side of the ~ to be NULL. While in the case of the pennies either specification works just fine, we’ll see examples later on where the formula specification is simpler. In particular, this comes up in the upcoming Section 10.6 on comparing two proportions and Section ?? on inference for regression. 2. generate replicates FIGURE 10.19: Diagram of generate() replicates. After we specify() the variables of interest, we pipe the results into the generate() function to generate replicates. Figure 10.19 shows how this is combined with specify() to start the pipeline. In other words, repeat the resampling process a large number of times. Recall in Sections 10.2.2 and 10.2.3 we did this 35 and 1000 times. The generate() function’s first argument is reps, which sets the number of replicates we would like to generate. Since we want to resample the 50 pennies in pennies_sample with replacement 1000 times, we set reps = 1000. The second argument type determines the type of computer simulation we’d like to perform. We set this to type = &quot;bootstrap&quot; indicating that we want to perform bootstrap resampling. You’ll see different options for type in Chapter ??. pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: year (numeric) # A tibble: 50,000 x 2 # Groups: replicate [1,000] replicate year &lt;int&gt; &lt;dbl&gt; 1 1 1981 2 1 1988 3 1 2006 4 1 2016 5 1 2002 6 1 1985 7 1 1979 8 1 2000 9 1 2006 10 1 2016 # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 pennies with replacement 1000 times and 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. The default value of the type argument is &quot;bootstrap&quot; in this scenario, so if the last line was written as generate(reps = 1000), we’d obtain the same results. Comparing with original workflow: Note that the steps of the infer workflow so far produce the same results as the original workflow using the rep_sample_n() function we saw earlier. In other words, the following two code chunks produce similar results: # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) reps = 1000) 3. calculate summary statistics FIGURE 10.20: Diagram of calculate() summary statistics. After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize each of the 1000 resamples of size 50 to a single sample statistic value. As seen in the diagram, the calculate() function does this. In our case, we want to calculate the mean year for each bootstrap resample of size 50. To do so, we set the stat argument to &quot;mean&quot;. You can also set the stat argument to a variety of other common summary statistics, like &quot;median&quot;, &quot;sum&quot;, &quot;sd&quot; (standard deviation), and &quot;prop&quot; (proportion). To see a list of all possible summary statistics you can use, type ?calculate and read the help file. Let’s save the result in a data frame called bootstrap_distribution and explore its contents: bootstrap_distribution &lt;- pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) bootstrap_distribution # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 1995.7 2 2 1994.04 3 3 1993.62 4 4 1994.5 5 5 1994.08 6 6 1993.6 7 7 1995.26 8 8 1996.64 9 9 1994.3 10 10 1995.94 # … with 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate values. It also has the mean year for each bootstrap resample saved in the variable stat. Comparing with original workflow: You may have recognized at this point that the calculate() step in the infer workflow produces the same output as the group_by() %&gt;% summarize() steps in the original workflow. # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) %&gt;% reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) group_by(replicate) %&gt;% summarize(stat = mean(year)) 4. visualize the results FIGURE 10.21: Diagram of visualize() results. The visualize() verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical stat variable’s values. The pipeline of the main infer verbs used for exploring bootstrap distribution results is shown in Figure 10.21. visualize(bootstrap_distribution) FIGURE 10.22: Bootstrap distribution. Comparing with original workflow: In fact, visualize() is a wrapper function for the ggplot() function that uses a geom_histogram() layer. Recall that we illustrated the concept of a wrapper function in Figure 11.5 in Subsection 11.1.2. # infer workflow: # Original workflow: visualize(bootstrap_distribution) ggplot(bootstrap_distribution, aes(x = stat)) + geom_histogram() The visualize() function can take many other arguments which we’ll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values. Let’s recap the steps of the infer workflow for constructing a bootstrap distribution and then visualizing it in Figure 10.23. FIGURE 10.23: infer package workflow for confidence intervals. Recall how we introduced two different methods for constructing 95% confidence intervals for an unknown population parameter in Section 10.3: the percentile method and the standard error method. Let’s now check out the infer package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the infer package! 10.4.3 Percentile method with infer Recall the percentile method for constructing 95% confidence intervals we introduced in Subsection 10.3.1. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution. We can compute the 95% confidence interval by piping bootstrap_distribution into the get_confidence_interval() function from the infer package, with the confidence level set to 0.95 and the confidence interval type to be &quot;percentile&quot;. Let’s save the results in percentile_ci. percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 1991.24 1999.42 Alternatively, we can visualize the interval (1991.24, 1999.42) by piping the bootstrap_distribution data frame into the visualize() function and adding a shade_confidence_interval() layer. We set the endpoints argument to be percentile_ci. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci) FIGURE 10.24: Percentile method 95% confidence interval shaded corresponding to potential values. Observe in Figure 10.24 that 95% of the sample means stored in the stat variable in bootstrap_distribution fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right. You also have the option to change the colors of the shading using the color and fill arguments. You can also use the shorter named function shade_ci() and the results will be the same. This is for folks who don’t want to type out all of confidence_interval and prefer to type out ci instead. Try out the following code! visualize(bootstrap_distribution) + shade_ci(endpoints = percentile_ci, color = &quot;hotpink&quot;, fill = &quot;khaki&quot;) 10.4.4 Standard error method with infer Recall the standard error method for constructing 95% confidence intervals we introduced in Subsection 10.3.2. For any distribution that is normally shaped, roughly 95% of the values lie within two standard deviations of the mean. In the case of the bootstrap distribution, the standard deviation has a special name: the standard error. So in our case, 95% of values of the bootstrap distribution will lie within \\(\\pm 1.96\\) standard errors of \\(\\overline{x}\\). Thus, a 95% confidence interval is \\[\\overline{x} \\pm 1.96 \\cdot SE = (\\overline{x} - 1.96 \\cdot SE, \\, \\overline{x} + 1.96 \\cdot SE).\\] Computation of the 95% confidence interval can once again be done by piping the bootstrap_distribution data frame we created into the get_confidence_interval() function. However, this time we set the first type argument to be &quot;se&quot;. Second, we must specify the point_estimate argument in order to set the center of the confidence interval. We set this to be the sample mean of the original sample of 50 pennies of 1995.44. x_bar # A tibble: 1 x 1 mean_year &lt;dbl&gt; 1 1995.44 standard_error_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = x_bar) standard_error_ci # A tibble: 1 x 2 lower upper &lt;dbl&gt; &lt;dbl&gt; 1 1991.35 1999.53 If we would like to visualize the interval (1991.35, 1999.53), we can once again pipe the bootstrap_distribution data frame into the visualize() function and add a shade_confidence_interval() layer to our plot. We set the endpoints argument to be standard_error_ci. The resulting standard-error method based on a 95% confidence interval for \\(\\mu\\) can be seen in Figure 10.25. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = standard_error_ci) FIGURE 10.25: Standard-error-method 95% confidence interval. As noted in Section 10.3, both methods produce similar confidence intervals: Percentile method: (1991.24, 1999.42) Standard error method: (1991.35, 1999.53) 10.5 Interpreting confidence intervals Now that we’ve shown you how to construct confidence intervals using a sample drawn from a population, let’s now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section 10.3, this is like asking, “Did our net capture the fish?”. So, for example, does our percentile-based confidence interval of (1991.24, 1999.42) “capture” the true mean year \\(\\mu\\) of all US pennies? Alas, we’ll never know, because we don’t know what the true value of \\(\\mu\\) is. After all, we’re sampling to estimate it! In order to interpret a confidence interval’s effectiveness, we need to know what the value of the population parameter is. That way we can say whether or not a confidence interval “captured” this value. Let’s revisit our sampling bowl from Chapter 9. What proportion of the bowl’s 2400 balls are red? Let’s compute this: bowl %&gt;% summarize(p_red = mean(color == &quot;red&quot;)) # A tibble: 1 x 1 p_red &lt;dbl&gt; 1 0.375 In this case, we know what the value of the population parameter is: we know that the population proportion \\(p\\) is 0.375. In other words, we know that 37.5% of the bowl’s balls are red. As we stated in Subsection 9.3.3, the sampling bowl exercise doesn’t really reflect how sampling is done in real life, but rather was an idealized activity. In real life, we won’t know what the true value of the population parameter is, hence the need for estimation. Let’s now construct confidence intervals for \\(p\\) using our 33 groups of friends’ samples from the bowl in Chapter 9. We’ll then see if the confidence intervals “captured” the true value of \\(p\\), which we know to be 37.5%. That is to say, “Did the net capture the fish?”. 10.5.1 Did the net capture the fish? Recall that we had 33 groups of friends each take samples of size 50 from the bowl and then compute the sample proportion of red balls \\(\\widehat{p}\\). This resulted in 33 such estimates of \\(p\\). Let’s focus on Ilyas and Yohan’s sample, which is saved in the bowl_sample_1 data frame in the moderndive package: bowl_sample_1 # A tibble: 50 x 1 color &lt;chr&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # … with 40 more rows They observed 21 red balls out of 50 and thus their sample proportion \\(\\widehat{p}\\) was 21/50 = 0.42 = 42%. Think of this as the “spear” from our fishing analogy. Let’s now follow the infer package workflow from Subsection 10.4.2 to create a percentile-method-based 95% confidence interval for \\(p\\) using Ilyas and Yohan’s sample. Think of this as the “net.” 1. specify variables First, we specify() the response variable of interest color: bowl_sample_1 %&gt;% specify(response = color) Error: A level of the response variable `color` needs to be specified for the `success` argument in `specify()`. Whoops! We need to define which event is of interest! red or white balls? Since we are interested in the proportion red, let’s set success to be &quot;red&quot;: bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) Response: color (factor) # A tibble: 50 x 1 color &lt;fct&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # … with 40 more rows 2. generate replicates Second, we generate() 1000 replicates of bootstrap resampling with replacement from bowl_sample_1 by setting reps = 1000 and type = &quot;bootstrap&quot;. bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: color (factor) # A tibble: 50,000 x 2 # Groups: replicate [1,000] replicate color &lt;int&gt; &lt;fct&gt; 1 1 white 2 1 white 3 1 white 4 1 white 5 1 red 6 1 white 7 1 white 8 1 white 9 1 white 10 1 red # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 balls with replacement 1000 times and thus 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics Third, we summarize each of the 1000 resamples of size 50 with the proportion of successes. In other words, the proportion of the balls that are &quot;red&quot;. We can set the summary statistic to be calculated as the proportion by setting the stat argument to be &quot;prop&quot;. Let’s save the result as sample_1_bootstrap: sample_1_bootstrap &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_1_bootstrap # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.32 2 2 0.42 3 3 0.44 4 4 0.4 5 5 0.44 6 6 0.52 7 7 0.38 8 8 0.44 9 9 0.34 10 10 0.42 # … with 990 more rows Observe there are 1000 rows in this data frame and thus 1000 values of the variable stat. These 1000 values of stat represent our 1000 replicated values of the proportion, each based on a different resample. 4. visualize the results Fourth and lastly, let’s compute the resulting 95% confidence interval. percentile_ci_1 &lt;- sample_1_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_1 # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 0.3 0.56 Let’s visualize the bootstrap distribution along with the percentile_ci_1 percentile-based 95% confidence interval for \\(p\\) in Figure 10.26. We’ll adjust the number of bins to better see the resulting shape. Furthermore, we’ll add a dashed vertical line at Ilyas and Yohan’s observed \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% using geom_vline(). sample_1_bootstrap %&gt;% visualize(bins = 15) + shade_confidence_interval(endpoints = percentile_ci_1) + geom_vline(xintercept = 0.375, linetype = &quot;dashed&quot;) FIGURE 10.26: Bootstrap distribution. Did Ilyas and Yohan’s net capture the fish? Did their 95% confidence interval for \\(p\\) based on their sample contain the true value of \\(p\\) of 0.375? Yes! 0.375 is between the endpoints of their confidence interval (0.3, 0.56). However, will every 95% confidence interval for \\(p\\) capture this value? In other words, if we had a different sample of 50 balls and constructed a different confidence interval, would it necessarily contain \\(p\\) = 0.375 as well? Let’s see! Let’s first take a different sample from the bowl, this time using the computer as we did in Chapter 9: bowl_sample_2 &lt;- bowl %&gt;% rep_sample_n(size = 50) bowl_sample_2 # A tibble: 50 x 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1665 red 2 1 1312 red 3 1 2105 red 4 1 810 white 5 1 189 white 6 1 1429 white 7 1 2294 red 8 1 1233 white 9 1 1951 white 10 1 2061 white # … with 40 more rows Let’s reapply the same infer functions on bowl_sample_2 to generate a different 95% confidence interval for \\(p\\). First, we create the new bootstrap distribution and save the results in sample_2_bootstrap: sample_2_bootstrap &lt;- bowl_sample_2 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_2_bootstrap # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.48 2 2 0.38 3 3 0.32 4 4 0.32 5 5 0.34 6 6 0.26 7 7 0.3 8 8 0.36 9 9 0.44 10 10 0.36 # … with 990 more rows We once again compute a percentile-based 95% confidence interval for \\(p\\): percentile_ci_2 &lt;- sample_2_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_2 # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 0.2 0.48 Does this new net capture the fish? In other words, does the 95% confidence interval for \\(p\\) based on the new sample contain the true value of \\(p\\) of 0.375? Yes again! 0.375 is between the endpoints of our confidence interval (0.2, 0.48). Let’s now repeat this process 100 more times: we take 100 virtual samples from the bowl and construct 100 95% confidence intervals. Let’s visualize the results in Figure 10.27 where: We mark the true value of \\(p = 0.375\\) with a vertical line. We mark each of the 100 95% confidence intervals with horizontal lines. These are the “nets.” The horizontal line is colored grey if the confidence interval “captures” the true value of \\(p\\) marked with the vertical line. The horizontal line is colored black otherwise. FIGURE 10.27: 100 percentile-based 95% confidence intervals for \\(p\\). Of the 100 95% confidence intervals, 95 of them captured the true value \\(p = 0.375\\), whereas 5 of them didn’t. In other words, 95 of our nets caught the fish, whereas 5 of our nets didn’t. This is where the “95% confidence level” we defined in Section 10.3 comes into play: for every 100 95% confidence intervals, we expect that 95 of them will capture \\(p\\) and that five of them won’t. Note that “expect” is a probabilistic statement referring to a long-run average. In other words, for every 100 confidence intervals, we will observe about 95 confidence intervals that capture \\(p\\), but not necessarily exactly 95. In Figure 10.27 for example, 95 of the confidence intervals capture \\(p\\). To further accentuate our point about confidence levels, let’s generate a figure similar to Figure 10.27, but this time constructing 80% standard-error method based confidence intervals instead. Let’s visualize the results in Figure 10.28 with the scale on the x-axis being the same as in Figure 10.27 to make comparison easy. Furthermore, since all standard-error method 95% confidence intervals for \\(p\\) are centered at their respective point estimates \\(\\widehat{p}\\), we mark this value on each line with dots. FIGURE 10.28: 100 SE-based 80% confidence intervals for \\(p\\) with point estimate center marked with dots. Observe how the 80% confidence intervals are narrower than the 95% confidence intervals, reflecting our lower degree of confidence. Think of this as using a smaller “net.” We’ll explore other determinants of confidence interval width in the upcoming Subsection 10.5.3. Furthermore, observe that of the 100 80% confidence intervals, 82 of them captured the population proportion \\(p\\) = 0.375, whereas 18 of them did not. Since we lowered the confidence level from 95% to 80%, we now have a much larger number of confidence intervals that failed to “catch the fish.” 10.5.2 Precise and shorthand interpretation Let’s return our attention to 95% confidence intervals. The precise and mathematically correct interpretation of a 95% confidence interval is a little long-winded: Precise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter. This is what we observed in Figure 10.27. Our confidence interval construction procedure is 95% reliable. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time. A common but incorrect interpretation is: “There is a 95% probability that the confidence interval contains \\(p\\).” Looking at Figure 10.27, each of the confidence intervals either does or doesn’t contain \\(p\\). In other words, the probability is either a 1 or a 0. So if the 95% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? For example, going back to the pennies example, we found that the percentile method 95% confidence interval for \\(\\mu\\) was (1991.24, 1999.42), whereas the standard error method 95% confidence interval was (1991.35, 1999.53). What can be said about these two intervals? Loosely speaking, we can think of these intervals as our “best guess” of a plausible range of values for the mean year \\(\\mu\\) of all US pennies. For the rest of this book, we’ll use the following shorthand summary of the precise interpretation. Short-hand interpretation: We are 95% “confident” that a 95% confidence interval captures the value of the population parameter. We use quotation marks around “confident” to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it’s our best net. So returning to our pennies example and focusing on the percentile method, we are 95% “confident” that the true mean year of pennies in circulation in 2019 is somewhere between 1991.24 and 1999.42. 10.5.3 Width of confidence intervals Now that we know how to interpret confidence intervals, let’s go over some factors that determine their width. Impact of confidence level One factor that determines confidence interval widths is the pre-specified confidence level. For example, in Figures 10.27 and 10.28, we compared the widths of 95% and 80% confidence intervals and observed that the 95% confidence intervals were wider. The quantification of the confidence level should match what many expect of the word “confident.” In order to be more confident in our best guess of a range of values, we need to widen the range of values. To elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul’s temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were absolutely confident about, we would need to widen it. We need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C). Let’s revisit our sampling bowl from Chapter 9. Let’s compare \\(10 \\cdot 3 = 30\\) confidence intervals for \\(p\\) based on three different confidence levels: 80%, 95%, and 99%. Specifically, we’ll first take 30 different random samples of size \\(n\\) = 50 balls from the bowl. Then we’ll construct 10 percentile-based confidence intervals using each of the three different confidence levels. Finally, we’ll compare the widths of these intervals. We visualize the resulting confidence intervals in Figure 10.29 along with a vertical line marking the true value of \\(p\\) = 0.375. FIGURE 10.29: Ten 80, 95, and 99% confidence intervals for \\(p\\) based on \\(n = 50\\). Observe that as the confidence level increases from 80% to 95% to 99%, the confidence intervals tend to get wider as seen in Table 10.2 where we compare their average widths. TABLE 10.2: Average width of 80, 95, and 99% confidence intervals Confidence level Mean width 80% 0.162 95% 0.262 99% 0.338 So in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to be more confident, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level. The moral of the story is: Higher confidence levels tend to produce wider confidence intervals. When looking at Figure 10.29 it is important to keep in mind that we kept the sample size fixed at \\(n\\) = 50. Thus, all \\(10 \\cdot 3 = 30\\) random samples from the bowl had the same sample size. What happens if instead we took samples of different sizes? Recall that we did this in Subsection 9.2.4 using virtual shovels with 25, 50, and 100 slots. Impact of sample size This time, let’s fix the confidence level at 95%, but consider three different sample sizes for \\(n\\): 25, 50, and 100. Specifically, we’ll first take 10 different random samples of size 25, 10 different random samples of size 50, and 10 different random samples of size 100. We’ll then construct 95% percentile-based confidence intervals for each sample. Finally, we’ll compare the widths of these intervals. We visualize the resulting 30 confidence intervals in Figure 10.30. Note also the vertical line marking the true value of \\(p\\) = 0.375. FIGURE 10.30: Ten 95% confidence intervals for \\(p\\) with \\(n = 25, 50,\\) and \\(100\\). Observe that as the confidence intervals are constructed from larger and larger sample sizes, they tend to get narrower. Let’s compare the average widths in Table 10.3. TABLE 10.3: Average width of 95% confidence intervals based on \\(n = 25\\), \\(50\\), and \\(100\\) Sample size Mean width n = 25 0.380 n = 50 0.268 n = 100 0.189 The moral of the story is: Larger sample sizes tend to produce narrower confidence intervals. Recall that this was a key message in Subsection 9.3.3. As we used larger and larger shovels for our samples, the sample proportions red \\(\\widehat{p}\\) tended to vary less. In other words, our estimates got more and more precise. Recall that we visualized these results in Figure 9.15, where we compared the sampling distributions for \\(\\widehat{p}\\) based on samples of size \\(n\\) equal 25, 50, and 100. We also quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the standard error. So as the sample size increases, the standard error decreases. In fact, the standard error is another related factor in determining confidence interval width. We’ll explore this fact in Subsection 10.7.2 when we discuss theory-based methods for constructing confidence intervals using mathematical formulas. Such methods are an alternative to the computer-based methods we’ve been using so far. 10.6 Case study: Is yawning contagious? Let’s apply our knowledge of confidence intervals to answer the question: “Is yawning contagious?”. If you see someone else yawn, are you more likely to yawn? In an episode of the US show Mythbusters, the hosts conducted an experiment to answer this question. The episode is available to view in the United States on the Discovery Network website here and more information about the episode is also available on IMDb. 10.6.1 Mythbusters study data Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the mythbusters_yawn data frame included in the moderndive package: mythbusters_yawn # A tibble: 50 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no 7 7 seed yes 8 8 control no 9 9 control no 10 10 seed no # … with 40 more rows The variables are: subj: The participant ID with values 1 through 50. group: A binary treatment variable indicating whether the participant was exposed to yawning. &quot;seed&quot; indicates the participant was exposed to yawning while &quot;control&quot; indicates the participant was not. yawn: A binary response variable indicating whether the participant ultimately yawned. Recall that you learned about treatment and response variables in Subsection 11.3.1 in our discussion on confounding variables. Let’s use some data wrangling to obtain counts of the four possible outcomes: mythbusters_yawn %&gt;% group_by(group, yawn) %&gt;% summarize(count = n()) # A tibble: 4 x 3 # Groups: group [2] group yawn count &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 control no 12 2 control yes 4 3 seed no 24 4 seed yes 10 Let’s first focus on the &quot;control&quot; group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25% did yawn. Let’s now focus on the &quot;seed&quot; group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4% - 25% = 4.4% more often than those who were not. 10.6.2 Sampling scenario Let’s review the terminology and notation related to sampling we studied in Subsection 9.3.1. In Chapter 9 our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter of interest was the population proportion of these balls that were red, denoted mathematically by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel and computed the relevant point estimate: the sample proportion that were red, denoted mathematically by \\(\\widehat{p}\\). Who is the study population here? All humans? All the people who watch the show Mythbusters? It’s hard to say! This question can only be answered if we know how the show’s hosts recruited participants! In other words, what was the sampling methodology used by the Mythbusters to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we’ll assume that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we’ll be assuming that any results of this experiment will generalize to all \\(N\\) = 327 million Americans (2018 population). Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the difference in population proportions \\(p_{seed} - p_{control}\\), where \\(p_{seed}\\) is the proportion of all Americans who if exposed to yawning will yawn themselves, and \\(p_{control}\\) is the proportion of all Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the Mythbusters’ sample of participants will be the difference in sample proportions \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\). Let’s extend Table 9.4 of scenarios of sampling for inference to include our latest scenario. TABLE 10.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) This is known as a two-sample inference situation since we have two separate samples. Based on their two-samples of size \\(n_{seed}\\) = 34 and \\(n_{control}\\) = 16, the point estimate is \\[ \\widehat{p}_{seed} - \\widehat{p}_{control} = \\frac{24}{34} - \\frac{12}{16} = 0.04411765 \\approx 4.4\\% \\] However, say the Mythbusters repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they obtain the exact same estimated difference of 4.4%? Probably not, again, because of sampling variation. How does this sampling variation affect their estimate of 4.4%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the Mythbusters only have a single two-sample of 50 participants, they would have to construct a 95% confidence interval for \\(p_{seed} - p_{control}\\) using bootstrap resampling with replacement. We make a couple of important notes. First, for the comparison between the &quot;seed&quot; and &quot;control&quot; groups to make sense, however, both groups need to be independent from each other. Otherwise, they could influence each other’s results. This means that a participant being selected for the &quot;seed&quot; or &quot;control&quot; group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn’t necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable. Second, the order of the subtraction in the difference doesn’t matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\) does not make a material difference, you just need to stay consistent and interpret your results accordingly. 10.6.3 Constructing the confidence interval As we did in Subsection 10.4.2, let’s first construct the bootstrap distribution for \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) and then use this to construct 95% confidence intervals for \\(p_{seed} - p_{control}\\). We’ll do this using the infer workflow again. However, since the difference in proportions is a new scenario for inference, we’ll need to use some new arguments in the infer functions along the way. 1. specify variables Let’s take our mythbusters_yawn data frame and specify() which variables are of interest using the y ~ x formula interface where: Our response variable is yawn: whether or not a participant yawned. It has levels &quot;yes&quot; and &quot;no&quot;. The explanatory variable is group: whether or not a participant was exposed to yawning. It has levels &quot;seed&quot; (exposed to yawning) and &quot;control&quot; (not exposed to yawning). mythbusters_yawn %&gt;% specify(formula = yawn ~ group) Error: A level of the response variable `yawn` needs to be specified for the `success` argument in `specify()`. Alas, we got an error message similar to the one from Subsection 10.5.1: infer is telling us that one of the levels of the categorical variable yawn needs to be defined as the success. Recall that we define success to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who &quot;yes&quot; yawned or those who &quot;no&quot; didn’t yawn? This isn’t clear to R or someone just picking up the code and results for the first time, so we need to set the success argument to &quot;yes&quot; as follows to improve the transparency of the code: mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50 x 2 yawn group &lt;fct&gt; &lt;fct&gt; 1 yes seed 2 yes control 3 no seed 4 yes seed 5 no seed 6 no control 7 yes seed 8 no control 9 no control 10 no seed # … with 40 more rows 2. generate replicates Our next step is to perform bootstrap resampling with replacement like we did with the slips of paper in our pennies activity in Section 10.1. We saw how it works with both a single variable in computing bootstrap means in Section 10.4 and in computing bootstrap proportions in Section 10.5, but we haven’t yet worked with bootstrapping involving multiple variables. In the infer package, bootstrapping with multiple variables means that each row is potentially resampled. Let’s investigate this by focusing only on the first six rows of mythbusters_yawn: first_six_rows &lt;- head(mythbusters_yawn) first_six_rows # A tibble: 6 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no When we bootstrap this data, we are potentially pulling the subject’s readings multiple times. Thus, we could see the entries of &quot;seed&quot; for group and &quot;no&quot; for yawn together in a new row in a bootstrap sample. This is further seen by exploring the sample_n() function in dplyr on this smaller 6-row data frame comprised of head(mythbusters_yawn). The sample_n() function can perform this bootstrapping procedure and is similar to the rep_sample_n() function in infer, except that it is not repeated, but rather only performs one sample with or without replacement. first_six_rows %&gt;% sample_n(size = 6, replace = TRUE) # A tibble: 6 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 6 control no 3 1 seed yes 4 5 seed no 5 4 seed yes 6 4 seed yes We can see that in this bootstrap sample generated from the first six rows of mythbusters_yawn, we have some rows repeated. The same is true when we perform the generate() step in infer as done in what follows. Using this fact, we generate 1000 replicates, or, in other words, we bootstrap resample the 50 participants with replacement 1000 times. mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50,000 x 3 # Groups: replicate [1,000] replicate yawn group &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 1 yes seed 2 1 yes control 3 1 no control 4 1 no control 5 1 yes seed 6 1 yes seed 7 1 yes seed 8 1 yes seed 9 1 no seed 10 1 yes seed # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 participants with replacement 1000 times and 50,000 = 1000 \\(\\cdot\\) 50. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size 50 with a single summary statistic, the difference in proportions. We do this by setting the stat argument to &quot;diff in props&quot;: mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;) Error: Statistic is based on a difference; specify the `order` in which to subtract the levels of the explanatory variable. We see another error here. We need to specify the order of the subtraction. Is it \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\). We specify it to be \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) by setting order = c(&quot;seed&quot;, &quot;control&quot;). Note that you could’ve also set order = c(&quot;control&quot;, &quot;seed&quot;). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. Let’s save the output in a data frame bootstrap_distribution_yawning: bootstrap_distribution_yawning &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) bootstrap_distribution_yawning # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.0357143 2 2 0.229167 3 3 0.00952381 4 4 0.0106952 5 5 0.00483092 6 6 0.00793651 7 7 -0.0845588 8 8 -0.00466200 9 9 0.164686 10 10 0.124777 # … with 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate ID’s and the 1000 differences in proportions for each bootstrap resample in stat. 4. visualize the results In Figure 10.31 we visualize() the resulting bootstrap resampling distribution. Let’s also add a vertical line at 0 by adding a geom_vline() layer. visualize(bootstrap_distribution_yawning) + geom_vline(xintercept = 0) FIGURE 10.31: Bootstrap distribution. First, let’s compute the 95% confidence interval for \\(p_{seed} - p_{control}\\) using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 -0.238276 0.302464 Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the point_estimate argument. In our case, we need to set it to be the difference in sample proportions of 4.4% that the Mythbusters observed. We can also use the infer workflow to compute this value by excluding the generate() 1000 bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the generate() line, telling R to ignore it: obs_diff_in_props &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% # generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) obs_diff_in_props # A tibble: 1 x 1 stat &lt;dbl&gt; 1 0.0441176 We thus plug this value in as the point_estimate argument. myth_ci_se &lt;- bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = obs_diff_in_props) myth_ci_se # A tibble: 1 x 2 lower upper &lt;dbl&gt; &lt;dbl&gt; 1 -0.227291 0.315526 Let’s visualize both confidence intervals in Figure 10.32, with the percentile-method interval marked with black lines and the standard-error-method marked with grey lines. Observe that they are both similar to each other. FIGURE 10.32: Two 95% confidence intervals: percentile method (black) and standard error method (grey). 10.6.4 Interpreting the confidence interval Given that both confidence intervals are quite similar, let’s focus our interpretation to only the percentile-method confidence interval of (-0.238, 0.302). Recall from Subsection 10.5.2 that the precise statistical interpretation of a 95% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of \\(p_{seed} - p_{control}\\). In other words, if we gathered 100 samples of \\(n\\) = 50 participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of \\(p_{seed} - p_{control}\\) while about five won’t. Given that this is a little long winded, we use the shorthand interpretation: we’re 95% “confident” that the true difference in proportions \\(p_{seed} - p_{control}\\) is between (-0.238, 0.302). There is one value of particular interest that this 95% confidence interval contains: zero. If \\(p_{seed} - p_{control}\\) were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself. In our case, since the 95% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our 1000 bootstrap resamples with replacement, sometimes \\(\\widehat{p}_{seed}\\) was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened. Say, on the other hand, the 95% confidence interval was entirely above zero. This would suggest that \\(p_{seed} - p_{control} &gt; 0\\), or, in other words \\(p_{seed} &gt; p_{control}\\), and thus we’d have evidence suggesting those exposed to yawning do yawn more often. 10.7 Conclusion 10.7.1 Comparing bootstrap and sampling distributions Let’s talk more about the relationship between sampling distributions and bootstrap distributions. Recall back in Subsection 9.2.3, we took 1000 virtual samples from the bowl using a virtual shovel, computed 1000 values of the sample proportion red \\(\\widehat{p}\\), then visualized their distribution in a histogram. Recall that this distribution is called the sampling distribution of \\(\\widehat{p}\\) . Furthermore, the standard deviation of the sampling distribution has a special name: the standard error. We also mentioned that this sampling activity does not reflect how sampling is done in real life. Rather, it was an idealized version of sampling so that we could study the effects of sampling variation on estimates, like the proportion of the shovel’s balls that are red. In real life, however, one would take a single sample that’s as large as possible, much like in the Obama poll we saw in Section 9.4. But how can we get a sense of the effect of sampling variation on estimates if we only have one sample and thus only one estimate? Don’t we need many samples and hence many estimates? The workaround to having a single sample was to perform bootstrap resampling with replacement from the single sample. We did this in the resampling activity in Section 10.1 where we focused on the mean year of minting of pennies. We used pieces of paper representing the original sample of 50 pennies from the bank and resampled them with replacement from a hat. We had 35 of our friends perform this activity and visualized the resulting 35 sample means \\(\\overline{x}\\) in a histogram in Figure 10.11. This distribution was called the bootstrap distribution of \\(\\overline{x}\\). We stated at the time that the bootstrap distribution is an approximation to the sampling distribution of \\(\\overline{x}\\) in the sense that both distributions will have a similar shape and similar spread. Thus the standard error of the bootstrap distribution can be used as an approximation to the standard error of the sampling distribution. Let’s show you that this is the case by now comparing these two types of distributions. Specifically, we’ll compare the sampling distribution of \\(\\widehat{p}\\) based on 1000 virtual samples from the bowl from Subsection 9.2.3 to the bootstrap distribution of \\(\\widehat{p}\\) based on 1000 virtual resamples with replacement from Ilyas and Yohan’s single sample bowl_sample_1 from Subsection 10.5.1. Sampling distribution Here is the code you saw in Subsection 9.2.3 to construct the sampling distribution of \\(\\widehat{p}\\) shown again in Figure 10.33, with some changes to incorporate the statistical terminology relating to sampling from Subsection 9.3.1. # Take 1000 virtual samples of size 50 from the bowl: virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # Compute the sampling distribution of 1000 values of p-hat sampling_distribution &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # Visualize sampling distribution of p-hat ggplot(sampling_distribution, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Sampling distribution&quot;) FIGURE 10.33: Previously seen sampling distribution of sample proportion red for \\(n = 1000\\). An important thing to keep in mind is the default value for replace is FALSE when using rep_sample_n(). This is because when sampling 50 balls with a shovel, we are extracting 50 balls one-by-one without replacing them. This is in contrast to bootstrap resampling with replacement, where we resample a ball and put it back, and repeat this process 50 times. Let’s quantify the variability in this sampling distribution by calculating the standard deviation of the prop_red variable representing 1000 values of the sample proportion \\(\\widehat{p}\\). Remember that the standard deviation of the sampling distribution is the standard error, frequently denoted as se. sampling_distribution %&gt;% summarize(se = sd(prop_red)) # A tibble: 1 x 1 se &lt;dbl&gt; 1 0.0673987 Bootstrap distribution Here is the code you previously saw in Subsection 10.5.1 to construct the bootstrap distribution of \\(\\widehat{p}\\) based on Ilyas and Yohan’s original sample of 50 balls saved in bowl_sample_1. bootstrap_distribution &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) FIGURE 10.34: Bootstrap distribution of proportion red for \\(n = 1000\\). bootstrap_distribution %&gt;% summarize(se = sd(stat)) # A tibble: 1 x 1 se &lt;dbl&gt; 1 0.0712212 Comparison Now that we have computed both the sampling distribution and the bootstrap distributions, let’s compare them side-by-side in Figure 10.35. We’ll make both histograms have matching scales on the x- and y-axes to make them more comparable. Furthermore, we’ll add: To the sampling distribution on the top: a solid line denoting the proportion of the bowl’s balls that are red \\(p\\) = 0.375. To the bootstrap distribution on the bottom: a dashed line at the sample proportion \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% that Ilyas and Yohan observed. FIGURE 10.35: Comparing the sampling and bootstrap distributions of \\(\\widehat{p}\\). There is a lot going on in Figure 10.35, so let’s break down all the comparisons slowly. First, observe how the sampling distribution on top is centered at \\(p\\) = 0.375. This is because the sampling is done at random and in an unbiased fashion. So the estimates \\(\\widehat{p}\\) are centered at the true value of \\(p\\). However, this is not the case with the following bootstrap distribution. The bootstrap distribution is centered at 0.42, which is the proportion red of Ilyas and Yohan’s 50 sampled balls. This is because we are resampling from the same sample over and over again. Since the bootstrap distribution is centered at the original sample’s proportion, it doesn’t necessarily provide a better estimate of \\(p\\) = 0.375. This leads us to our first lesson about bootstrapping: The bootstrap distribution will likely not have the same center as the sampling distribution. In other words, bootstrapping cannot improve the quality of an estimate. Second, let’s now compare the spread of the two distributions: they are somewhat similar. In the previous code, we computed the standard deviations of both distributions as well. Recall that such standard deviations have a special name: standard errors. Let’s compare them in Table 10.5. TABLE 10.5: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Notice that the bootstrap distribution’s standard error is a rather good approximation to the sampling distribution’s standard error. This leads us to our second lesson about bootstrapping: Even if the bootstrap distribution might not have the same center as the sampling distribution, it will likely have very similar shape and spread. In other words, bootstrapping will give you a good estimate of the standard error. Thus, using the fact that the bootstrap distribution and sampling distributions have similar spreads, we can build confidence intervals using bootstrapping as we’ve done all throughout this chapter! 10.7.2 Theory-based confidence intervals So far in this chapter, we’ve constructed confidence intervals using two methods: the percentile method and the standard error method. Recall also from Subsection 10.3.2 that we can only use the standard-error method if the bootstrap distribution is bell-shaped (i.e., normally distributed). In a similar vein, if the sampling distribution is normally shaped, there is another method for constructing confidence intervals that does not involve using your computer. You can use a theory-based method involving a mathematical formulas! The formula uses the rule of thumb we saw in Appendix 7.6 that 95% of values in a normal distribution are within \\(\\pm 1.96\\) standard deviations of the mean. In the case of sampling and bootstrap distributions, recall that the standard deviation has a special name: the standard error. Theory-based method for computing standard errors There exists in many cases a formula that approximates the standard error! In the case of our bowl where we used the sample proportion red \\(\\widehat{p}\\) to estimate the proportion of the bowl’s balls that are red, the formula that approximates the standard error is: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] For example, recall from bowl_sample_1 that Yohan and Ilyas sampled \\(n = 50\\) balls and observed a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\] The key observation to make here is that there is an \\(n\\) in the denominator. So as the sample size \\(n\\) increases, the standard error decreases. We’ve demonstrated this fact using our virtual shovels in Subsection 9.3.3. If you don’t recall this demonstration, we highly recommend you go back and read that subsection. Let’s compare this theory-based standard error to the standard error of the sampling and bootstrap distributions you computed previously in Subsection 10.7.1 in Table 10.6. Notice how they are all similar! TABLE 10.6: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Formula approximation 0.070 Going back to Yohan and Ilyas’ sample proportion of \\(\\widehat{p}\\) of 21/50 = 0.42, say this were based on a sample of size \\(n\\) = 100 instead of 50. Then the standard error would be: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{100}} = \\sqrt{0.002436} = 0.0494\\] Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the “typical” error of our estimates using \\(n\\) = 100 will go down and hence be more precise. Recall that we illustrated the difference between accuracy and precision of estimates in Figure 9.16. Why is this formula true? Unfortunately, we don’t have the tools at this point to prove this; you’ll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation here if you like.) Theory-based method for constructing confidence intervals Using these theory-based standard errors, let’s present a theory-based method for constructing 95% confidence intervals that does not involve using a computer, but rather mathematical formulas. Note that this theory-based method only holds if the sampling distribution is normally shaped, so that we can use the 95% rule of thumb about normal distributions discussed in Appendix 7.6. Collect a single representative sample of size \\(n\\) that’s as large as possible. Compute the point estimate: the sample proportion \\(\\widehat{p}\\). Think of this as the center of your “net.” Compute the approximation to the standard error \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute a quantity known as the margin of error (more on this later after we list the five steps): \\[\\text{MoE}_{\\widehat{p}} = 1.96 \\cdot \\text{SE}_{\\widehat{p}} = 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute both endpoints of the confidence interval. The lower end-point. Think of this as the left end-point of the net: \\[\\widehat{p} - \\text{MoE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] The upper endpoint. Think of this as the right end-point of the net: \\[\\widehat{p} + \\text{MoE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Alternatively, you can succinctly summarize a 95% confidence interval for \\(p\\) using the \\(\\pm\\) symbol: \\[\\widehat{p} \\pm \\text{MoE}_{\\widehat{p}} = \\widehat{p} \\pm (1.96 \\cdot \\text{SE}_{\\widehat{p}}) = \\widehat{p} \\pm \\left( 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}} \\right)\\] So going back to Yohan and Ilyas’ sample of \\(n = 50\\) balls that had 21 red balls, the 95% confidence interval for \\(p\\) is \\[ \\begin{aligned} 0.41 \\pm 1.96 \\cdot 0.0698 &amp;= 0.41 \\, \\pm \\, 0.137 \\\\ &amp;= (0.41 - 0.137, \\, 0.41 + 0.137) \\\\ &amp;= (0.273, \\, 0.547). \\end{aligned} \\] Yohan and Ilyas are 95% “confident” that the true proportion red of the bowl’s balls is between 28.3% and 55.7%. Given that the true population proportion \\(p\\) was 0.375, in this case they successfully captured the fish. In Step 4, we defined a statistical quantity known as the margin of error. You can think of this quantity as how much the net extends to the left and to the right of the center of our net. The 1.96 multiplier is rooted in the 95% rule of thumb we introduced earlier and the fact that we want the confidence level to be 95%. The value of the margin of error entirely determines the width of the confidence interval. Recall from Subsection 10.5.3 that confidence interval widths are determined by an interplay of the confidence level, the sample size \\(n\\), and the standard error. Let’s revisit the poll of President Obama’s approval rating among young Americans aged 18-29 which we introduced in Section 9.4. Pollsters found that based on a representative sample of \\(n\\) = 2089 young Americans, \\(\\widehat{p}\\) = 0.41 = 41% supported President Obama. If you look towards the end of the article, it also states: “The poll’s margin of error was plus or minus 2.1 percentage points.” This is precisely the \\(\\text{MoE}\\): \\[ \\begin{aligned} \\text{MoE} &amp;= 1.96 \\cdot \\text{SE} = 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}} = 1.96 \\cdot \\sqrt{\\frac{0.41(1-0.41)}{2089}} \\\\ &amp;= 1.96 \\cdot 0.0108 = 0.021 = 2.1\\% \\end{aligned} \\] Their poll results are based on a confidence level of 95% and the resulting 95% confidence interval for the proportion of all young Americans who support Obama is: \\[\\widehat{p} \\pm \\text{MoE} = 0.41 \\pm 0.021 = (0.389, \\, 0.431) = (38.9\\%, \\, 43.1\\%).\\] Confidence intervals based on 33 tactile samples Let’s revisit our 33 friends’ samples from the bowl from Subsection 9.1.3. We’ll use their 33 samples to construct 33 theory-based 95% confidence intervals for \\(p\\). Recall this data was saved in the tactile_prop_red data frame included in the moderndive package: rename() the variable prop_red to p_hat, the statistical name of the sample proportion \\(\\widehat{p}\\). mutate() a new variable n making explicit the sample size of 50. mutate() other new variables computing: The standard error SE for \\(\\widehat{p}\\) using the previous formula. The margin of error MoE by multiplying the SE by 1.96 The left endpoint of the confidence interval lower_ci The right endpoint of the confidence interval upper_ci conf_ints &lt;- tactile_prop_red %&gt;% rename(p_hat = prop_red) %&gt;% mutate( n = 50, SE = sqrt(p_hat * (1 - p_hat) / n), MoE = 1.96 * SE, lower_ci = p_hat - MoE, upper_ci = p_hat + MoE ) # A tibble: 33 x 9 group replicate red_balls p_hat n SE MoE lower_ci upper_ci &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Ilyas, … 1 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 2 Morgan,… 2 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 3 Martin,… 3 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 4 Clark, … 4 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 5 Riddhi,… 5 18 0.36 50 0.0678823 0.133049 0.226951 0.493049 6 Andrew,… 6 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 7 Julia 7 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 8 Rachel,… 8 11 0.22 50 0.0585833 0.114823 0.105177 0.334823 9 Daniel,… 9 15 0.3 50 0.0648074 0.127023 0.172977 0.427023 10 Josh, M… 10 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 # … with 23 more rows In Figure 10.36, let’s plot the 33 confidence intervals for \\(p\\) saved in conf_ints along with a vertical line at \\(p\\) = 0.375 indicating the true proportion of the bowl’s balls that are red. Furthermore, let’s mark the sample proportions \\(\\widehat{p}\\) with dots since they represent the centers of these confidence intervals. FIGURE 10.36: 33 confidence intervals at the 95% level based on 33 tactile samples of size \\(n = 50\\). Observe that 31 of the 33 confidence intervals “captured” the true value of \\(p\\), for a success rate of 31 / 33 = 93.94%. While this is not quite 95%, recall that we expect about 95% of such confidence intervals to capture \\(p\\). The actual observed success rate will vary slightly. Theory-based methods like this have largely been used in the past because we didn’t have the computing power to perform simulation-based methods such as bootstrapping. They are still commonly used, however, and if the sampling distribution is normally distributed, we have access to an alternative method for constructing confidence intervals as well as performing hypothesis tests as we will see in Chapter ??. The kind of computer-based statistical inference we’ve seen so far has a particular name in the field of statistics: simulation-based inference. This is because we are performing statistical inference using computer simulations. In our opinion, two large benefits of simulation-based methods over theory-based methods are that (1) they are easier for people new to statistical inference to understand and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist. 10.7.3 Additional resources If you want more examples of the infer workflow to construct confidence intervals, we suggest you check out the infer package homepage, in particular, a series of example analyses available at https://infer.netlify.com/articles/. 10.7.4 What’s to come? Now that we’ve equipped ourselves with confidence intervals, in Chapter ?? we’ll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we’ll see that the framework for making such inferences is slightly different. "],
["11-regression.html", "Chapter 11 Regression 11.1 One numerical explanatory variable 11.2 One categorical explanatory variable 11.3 Related topics 11.4 Conclusion", " Chapter 11 Regression Now that we are equipped with data visualization skills from Chapter 2, data wrangling skills from Chapter 4, and an understanding of how to import data and the concept of a “tidy” data format from Chapter 5, let’s now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between: an outcome variable \\(y\\), also called a dependent variable or response variable, and an explanatory/predictor variable \\(x\\), also called an independent variable or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) “as a function” of the explanatory/predictor variable \\(x\\). When we say “function” here, we aren’t referring to functions in R like the ggplot() function, but rather as a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes: Modeling for explanation: When you want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(x\\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables. Modeling for prediction: When you want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\). For example, say you are interested in an outcome variable \\(y\\) of whether patients develop lung cancer and information \\(x\\) on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as targeting smokers of a specific age group with advertising for smoking cessation programs. If we are modeling for prediction, however, we wouldn’t care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer. In this book, we’ll focus on modeling for explanation and hence refer to \\(x\\) as explanatory variables. If you are interested in learning about modeling for prediction, we suggest you check out books and courses on the field of machine learning such as An Introduction to Statistical Learning with Applications in R (ISLR) (James et al. 2017). Furthermore, while there exist many techniques for modeling, such as tree-based models and neural networks, in this book we’ll focus on one particular technique: linear regression. Linear regression is one of the most commonly-used and easy-to-understand approaches to modeling. Linear regression involves a numerical outcome variable \\(y\\) and explanatory variables \\(x\\) that are either numerical or categorical. Furthermore, the relationship between \\(y\\) and \\(x\\) is assumed to be linear, or in other words, a line. However, we’ll see that what constitutes a “line” will vary depending on the nature of your explanatory variables \\(x\\) . In Chapter 11 on basic regression, we’ll only consider models with a single explanatory variable \\(x\\). In Section 11.1, the explanatory variable will be numerical. This scenario is known as simple linear regression. In Section 11.2, the explanatory variable will be categorical. In Chapter 12 on multiple regression, we’ll extend the ideas behind basic regression and consider models with two explanatory variables \\(x_1\\) and \\(x_2\\). In Section 12.1, we’ll have two numerical explanatory variables. In Section 12.2, we’ll have one numerical and one categorical explanatory variable. In particular, we’ll consider two such models: interaction and parallel slopes models. In Chapter ?? on inference for regression, we’ll revisit our regression models and analyze the results using the tools for statistical inference you’ll develop in Chapters 9, 10, and ?? on sampling, bootstrapping and confidence intervals, and hypothesis testing and \\(p\\)-values, respectively. Let’s now begin with basic regression, which refers to linear regression models with a single explanatory variable \\(x\\). We’ll also discuss important statistical concepts like the correlation coefficient, that “correlation isn’t necessarily causation,” and what it means for a line to be “best-fitting.” Needed packages Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). In this chapter, we introduce some new packages: The tidyverse “umbrella” (Wickham 2019b) package. Recall from our discussion in Section 5.5 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages The moderndive package of datasets and functions for tidyverse-friendly introductory linear regression. The skimr (Waring et al. 2019) package, which provides a simple-to-use function to quickly compute a wide array of commonly used summary statistics. If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(gapminder) 11.1 One numerical explanatory variable Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer the following research question: what factors explain differences in instructor teaching evaluation scores? To this end, they collected instructor and course information on 463 courses. A full description of the study can be found at openintro.org. In this section, we’ll keep things simple for now and try to explain differences in instructor teaching scores as a function of one numerical variable: the instructor’s “beauty” score (we’ll describe how this score was determined shortly). Could it be that instructors with higher “beauty” scores also have higher teaching evaluations? Could it be instead that instructors with higher “beauty” scores tend to have lower teaching evaluations? Or could it be that there is no relationship between “beauty” score and teaching evaluations? We’ll answer these questions by modeling the relationship between teaching scores and “beauty” scores using simple linear regression where we have: A numerical outcome variable \\(y\\) (the instructor’s teaching score) and A single numerical explanatory variable \\(x\\) (the instructor’s “beauty” score). 11.1.1 Exploratory data analysis The data on the 463 courses at UT Austin can be found in the evals data frame included in the moderndive package. However, to keep things simple, let’s select() only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called evals_ch5: evals_ch5 &lt;- evals %&gt;% select(ID, score, bty_avg, age) A crucial step before doing any kind of analysis or modeling is performing an exploratory data analysis, or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA: Most crucially, looking at the raw data values. Computing summary statistics, such as means, medians, and interquartile ranges. Creating data visualizations. Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() function as introduced in Subsection 1.4.3 on exploring data frames: glimpse(evals_ch5) Observations: 463 Variables: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4… $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 4… Observe that Observations: 463 indicates that there are 463 rows/observations in evals_ch5, where each row corresponds to one observed course at UT Austin. It is important to note that the observational unit is an individual course and not an individual instructor. Recall from Subsection 1.4.3 that the observational unit is the “type of thing” that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 463 unique instructors being represented in evals_ch5. We’ll revisit this idea in Section ??, when we talk about the “independence assumption” for inference for regression. A full description of all the variables included in evals can be found at openintro.org or by reading the associated help file (run ?evals in the console). However, let’s fully describe only the 4 variables we selected in evals_ch5: ID: An identification variable used to distinguish between the 1 through 463 courses in the dataset. score: A numerical variable of the course instructor’s average teaching score, where the average is computed from the evaluation scores from all students in that course. Teaching scores of 1 are lowest and 5 are highest. This is the outcome variable \\(y\\) of interest. bty_avg: A numerical variable of the course instructor’s average “beauty” score, where the average is computed from a separate panel of six students. “Beauty” scores of 1 are lowest and 10 are highest. This is the explanatory variable \\(x\\) of interest. age: A numerical variable of the course instructor’s age. An alternative way to look at the raw data values is by choosing a random sample of the rows in evals_ch5 by piping it into the sample_n() function from the dplyr package. Here we set the size argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 11.1. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. evals_ch5 %&gt;% sample_n(size = 5) TABLE 11.1: A random sample of 5 out of the 463 courses at UT Austin ID score bty_avg age 129 3.7 3.00 62 109 4.7 4.33 46 28 4.8 5.50 62 434 2.8 2.00 62 330 4.0 2.33 64 Now that we’ve looked at the raw values in our evals_ch5 data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable score and our numerical explanatory variable “beauty” score denoted as bty_avg. We’ll do this by using the summarize() function from dplyr along with the mean() and median() summary functions we saw in Section 4.3. evals_ch5 %&gt;% summarize(mean_bty_avg = mean(bty_avg), mean_score = mean(score), median_bty_avg = median(bty_avg), median_score = median(score)) # A tibble: 1 x 4 mean_bty_avg mean_score median_bty_avg median_score &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.41784 4.17473 4.333 4.3 However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in summarize() would be long and tedious. Instead, let’s use the convenient skim() function from the skimr package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our evals_ch5 data frame, select() only the outcome and explanatory variables teaching score and bty_avg, and pipe them into the skim() function: evals_ch5 %&gt;% select(score, bty_avg) %&gt;% skim() TABLE 11.2: Data summary Name Piped data Number of rows 463 Number of columns 2 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00 ▁▁▅▇▇ bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ For the numerical variables teaching score and bty_avg it returns: n_missing: the number of missing values complete_rate: the percentage of non-missing or complete values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it (the minimum value) p25: the 25th percentile: the value at which 25% of observations are smaller than it (the 1st quartile) p50: the 50th percentile: the value at which 50% of observations are smaller than it (the 2nd quartile and more commonly called the median) p75: the 75th percentile: the value at which 75% of observations are smaller than it (the 3rd quartile) p100: the 100th percentile: the value at which 100% of observations are smaller than it (the maximum value) Looking at this output, we can see how the values of both variables distribute. For example, the mean teaching score was 4.17 out of 5, whereas the mean “beauty” score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores was between 3.80 and 4.6 (the first and third quartiles), whereas the middle 50% of “beauty” scores falls within 3.17 to 5.5 out of 10. The skim() function only returns what are known as univariate summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist bivariate summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the correlation coefficient. Generally speaking, coefficients are quantitative expressions of a specific phenomenon. A correlation coefficient is a quantitative expression of the strength of the linear relationship between two numerical variables. Its value ranges between -1 and 1 where: -1 indicates a perfect negative relationship: As one variable increases, the value of the other variable tends to go down, following a straight line. 0 indicates no relationship: The values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion. Figure 11.1 gives examples of 9 different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between \\(x\\) and \\(y\\), but it is not as strong as the negative linear relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -0.9 or -1. FIGURE 11.1: Nine different correlation coefficients. The correlation coefficient can be computed using the cor() summary function within a summarize(): evals_ch5 %&gt;% summarize(correlation = cor(score, bty_avg)) In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and “beauty” average is “weakly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to the extreme values of -1, 0, and 1. To develop your intuition about correlation coefficients, play the “Guess the Correlation” 1980’s style video game mentioned in Subsection 11.4.1. Let’s now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the score and bty_avg variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using geom_point() and display the result in Figure 11.2. Furthermore, let’s highlight the six points in the top right of the visualization in a box. ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Scatterplot of relationship of teaching and beauty scores&quot;) FIGURE 11.2: Instructor evaluation scores at UT Austin. Observe that most “beauty” scores lie between 2 and 8, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between teaching score and “beauty” score is “weakly positive.” This is consistent with our earlier computed correlation coefficient of 0.187. Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from overplotting. Recall from Subsection 2.2.2 that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more. This fact is only apparent when using geom_jitter() in place of geom_point(). We display the resulting plot in Figure 11.3 along with the same small box as in Figure 11.2. ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_jitter() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Scatterplot of relationship of teaching and beauty scores&quot;) FIGURE 11.3: Instructor evaluation scores at UT Austin. It is now apparent that there are 12 points in the area highlighted in the box and not six as originally suggested in Figure 11.2. Recall from Subsection 2.2.2 on overplotting that jittering adds a little random “nudge” to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame evals_ch5. To keep things simple going forward, however, we’ll only present regular scatterplots rather than their jittered counterparts. Let’s build on the unjittered scatterplot in Figure 11.2 by adding a “best-fitting” line: of all possible lines we can draw on this scatterplot, it is the line that “best” fits through the cloud of points. We do this by adding a new geom_smooth(method = &quot;lm&quot;, se = FALSE) layer to the ggplot() code that created the scatterplot in Figure 11.2. The method = &quot;lm&quot; argument sets the line to be a “linear model.” The se = FALSE argument suppresses standard error uncertainty bars. (We’ll define the concept of standard error later in Subsection 9.3.2.) ggplot(evals_ch5, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship between teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 11.4: Regression line. The line in the resulting Figure 11.4 is called a “regression line.” The regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable score and the explanatory variable bty_avg. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.187 suggesting that there is a positive relationship between these two variables: as instructors have higher “beauty” scores, so also do they receive higher teaching evaluations. We’ll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value. Furthermore, a regression line is “best-fitting” in that it minimizes some mathematical criteria. We present these mathematical criteria in Subsection 11.3.2, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable. 11.1.2 Simple linear regression You may recall from secondary/high school algebra that the equation of a line is \\(y = a + b\\cdot x\\). (Note that the \\(\\cdot\\) symbol is equivalent to the \\(\\times\\) “multiply by” mathematical symbol. We’ll use the \\(\\cdot\\) symbol in the rest of this book as it is more succinct.) It is defined by two coefficients \\(a\\) and \\(b\\). The intercept coefficient \\(a\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(b\\) for \\(x\\) is the increase in \\(y\\) for every increase of one in \\(x\\). This is also called the “rise over run.” However, when defining a regression line like the regression line in Figure 11.4, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) . The intercept coefficient is \\(b_0\\), so \\(b_0\\) is the value of \\(\\widehat{y}\\) when \\(x = 0\\). The slope coefficient for \\(x\\) is \\(b_1\\), i.e., the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a “fitted value,” or the value of \\(y\\) on the regression line for a given \\(x\\) value. We’ll discuss this more in the upcoming Subsection 11.1.3. We know that the regression line in Figure 11.4 has a positive slope \\(b_1\\) corresponding to our explanatory \\(x\\) variable bty_avg. Why? Because as instructors tend to have higher bty_avg scores, so also do they tend to have higher teaching evaluation scores. However, what is the numerical value of the slope \\(b_1\\)? What about the intercept \\(b_0\\)? Let’s not compute these two values by hand, but rather let’s use a computer! We can obtain the values of the intercept \\(b_0\\) and the slope for btg_avg \\(b_1\\) by outputting a linear regression table. This is done in two steps: We first “fit” the linear regression model using the lm() function and save it in score_model. We put the name of the outcome variable on the left-hand side of the ~ “tilde” sign, while putting the name of the explanatory variable on the right-hand side. This is known as R’s formula notation. We get the regression table by applying the tidy() function from the broom package to score_model. # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression table: tidy(score_model) TABLE 11.3: Linear regression table term estimate std.error statistic p.value (Intercept) 3.880 0.076 50.96 0 bty_avg 0.067 0.016 4.09 0 Let’s first focus on interpreting the regression table output in Table 11.3, and then we’ll later revisit the code that produced it. In the estimate column of Table 11.3 are the intercept \\(b_0\\) = 3.88 and the slope \\(b_1\\) = 0.067 for bty_avg. Thus the equation of the regression line in Figure 11.4 follows: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{bty}\\_\\text{avg}} \\cdot\\text{bty}\\_\\text{avg}\\\\ &amp;= 3.880 + 0.067\\cdot\\text{bty}\\_\\text{avg} \\end{aligned} \\] The intercept \\(b_0\\) = 3.88 is the average teaching score \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) for those courses where the instructor had a “beauty” score bty_avg of 0. Or in graphical terms, it’s where the line intersects the \\(y\\) axis when \\(x\\) = 0. Note, however, that while the intercept of the regression line has a mathematical interpretation, it has no practical interpretation here, since observing a bty_avg of 0 is impossible; it is the average of six panelists’ “beauty” scores ranging from 1 to 10. Furthermore, looking at the scatterplot with the regression line in Figure 11.4, no instructors had a “beauty” score anywhere near 0. Of greater interest is the slope \\(b_1\\) = \\(b_{\\text{bty\\_avg}}\\) for bty_avg of 0.067, as this summarizes the relationship between the teaching and “beauty” score variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning teachers with higher “beauty” scores also tend to have higher teaching scores. Recall from earlier that the correlation coefficient is 0.187. They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The slope’s interpretation is a little different: For every increase of 1 unit in bty_avg, there is an associated increase of, on average, 0.067 units of score. We only state that there is an associated increase and not necessarily a causal increase. For example, perhaps it’s not that higher “beauty” scores directly cause higher teaching scores per se. Instead, the following could hold true: individuals from wealthier backgrounds tend to have stronger educational backgrounds and hence have higher teaching scores, while at the same time these wealthy individuals also tend to have higher “beauty” scores. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.” We discuss this idea further in Subsection 11.3.1. Furthermore, we say that this associated increase is on average 0.067 units of teaching score, because you might have two instructors whose bty_avg scores differ by 1 unit, but their difference in teaching scores won’t necessarily be exactly 0.067. What the slope of 0.067 is saying is that across all possible courses, the average difference in teaching score between two instructors whose “beauty” scores differ by one is 0.067. Now that we’ve learned how to compute the equation for the regression line in Figure 11.4 using the values in the estimate column of Table 11.3, and how to interpret the resulting intercept and slope, let’s revisit the code that generated this table: # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) First, we “fit” the linear regression model to the data using the lm() function and save this as score_model. When we say “fit”, we mean “find the best fitting line to this data.” lm() stands for “linear model” and is used as follows: lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde ~. In our case, y is set to score. x is the explanatory variable. In our case, x is set to bty_avg. The combination of y ~ x is called a model formula. (Note the order of y and x.) In our case, the model formula is score ~ bty_avg. We saw such model formulas earlier when we computed the correlation coefficient using the get_correlation() function in Subsection 11.1.1. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the evals_ch5 data frame. Second, we take the saved model in score_model and apply the get_regression_table() function from the moderndive package to it to obtain the regression table in Table 11.3. This function is an example of what’s known in computer programming as a wrapper function. They take other pre-existing functions and “wrap” them into a single function that hides its inner workings. This concept is illustrated in Figure 11.5. FIGURE 11.5: The concept of a wrapper function. So all you need to worry about is what the inputs look like and what the outputs look like; you leave all the other details “under the hood of the car.” In our regression modeling example, the get_regression_table() function takes a saved lm() linear regression model as input and returns a data frame of the regression table as output. If you’re interested in learning more about the get_regression_table() function’s inner workings, check out Subsection 11.3.3. Lastly, you might be wondering what the remaining five columns in Table 11.3 are: std_error, statistic, p_value, lower_ci and upper_ci. They are the standard error, test statistic, p-value, lower 95% confidence interval bound, and upper 95% confidence interval bound. They tell us about both the statistical significance and practical significance of our results. This is loosely the “meaningfulness” of our results from a statistical perspective. Let’s put aside these ideas for now and revisit them in Chapter ?? on (statistical) inference for regression. We’ll do this after we’ve had a chance to cover standard errors in Chapter 9, confidence intervals in Chapter 10, and hypothesis testing and \\(p\\)-values in Chapter ??. 11.1.3 Observed/fitted values and residuals We just saw how to get the value of the intercept and the slope of a regression line from the estimate column of a regression table generated by the get_regression_table() function. Now instead say we want information on individual observations. For example, let’s focus on the 21st of the 463 courses in the evals_ch5 data frame in Table 11.4: TABLE 11.4: Data for the 21st course out of 463 ID score bty_avg age 21 4.9 7.33 31 What is the value \\(\\widehat{y}\\) on the regression line corresponding to this instructor’s bty_avg “beauty” score of 7.333? In Figure 11.6 we mark three values corresponding to the instructor for this 21st course and give their statistical names: Circle: The observed value \\(y\\) = 4.9 is this course’s instructor’s actual teaching score. Square: The fitted value \\(\\widehat{y}\\) is the value on the regression line for \\(x\\) = bty_avg = 7.333. This value is computed using the intercept and slope in the previous regression table: \\[\\widehat{y} = b_0 + b_1 \\cdot x = 3.88 + 0.067 \\cdot 7.333 = 4.369\\] Arrow: The length of this arrow is the residual and is computed by subtracting the fitted value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this course’s instructor, it is \\(y - \\widehat{y}\\) = 4.9 - 4.369 = 0.531. FIGURE 11.6: Example of observed value, fitted value, and residual. Now say we want to compute both the fitted value \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) and the residual \\(y - \\widehat{y}\\) for all 463 courses in the study. Recall that each course corresponds to one of the 463 rows in the evals_ch5 data frame and also one of the 463 points in the regression plot in Figure 11.6. We could repeat the previous calculations we performed by hand 463 times, but that would be tedious and time consuming. Instead, let’s do this using a computer with the get_regression_points() function. Just like the get_regression_table() function, the get_regression_points() function is a “wrapper” function. However, this function returns a different output. Let’s apply the get_regression_points() function to score_model, which is where we saved our lm() model in the previous section. In Table 11.5 we present the results of only the 21st through 24th courses for brevity’s sake. regression_points &lt;- get_regression_points(score_model) regression_points TABLE 11.5: Regression points (for only the 21st through 24th courses) ID score bty_avg score_hat residual 21 4.9 7.33 4.37 0.531 22 4.6 7.33 4.37 0.231 23 4.5 7.33 4.37 0.131 24 4.4 5.50 4.25 0.153 Let’s inspect the individual columns and match them with the elements of Figure 11.6: The score column represents the observed outcome variable \\(y\\). This is the y-position of the 463 black points. The bty_avg column represents the values of the explanatory variable \\(x\\). This is the x-position of the 463 black points. The score_hat column represents the fitted values \\(\\widehat{y}\\). This is the corresponding value on the regression line for the 463 \\(x\\) values. The residual column represents the residuals \\(y - \\widehat{y}\\). This is the 463 vertical distances between the 463 black points and the regression line. Just as we did for the instructor of the 21st course in the evals_ch5 dataset (in the first row of the table), let’s repeat the calculations for the instructor of the 24th course (in the fourth row of Table 11.5): score = 4.4 is the observed teaching score \\(y\\) for this course’s instructor. bty_avg = 5.50 is the value of the explanatory variable bty_avg \\(x\\) for this course’s instructor. score_hat = 4.25 = 3.88 + 0.067 \\(\\cdot\\) 5.50 is the fitted value \\(\\widehat{y}\\) on the regression line for this course’s instructor. residual = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model’s fitted value was off by 0.153 teaching score units for this course’s instructor. At this point, you can skip ahead if you like to Subsection 11.3.2 to learn about the processes behind what makes “best-fitting” regression lines. As a primer, a “best-fitting” line refers to the line that minimizes the sum of squared residuals out of all possible lines we can draw through the points. In Section 11.2, we’ll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable. 11.2 One categorical explanatory variable It’s an unfortunate truth that life expectancy is not the same across all countries in the world. International development agencies are interested in studying these differences in life expectancy in the hopes of identifying where governments should allocate resources to address this problem. In this section, we’ll explore differences in life expectancy in two ways: Differences between continents: Are there significant differences in average life expectancy between the five populated continents of the world: Africa, the Americas, Asia, Europe, and Oceania? Differences within continents: How does life expectancy vary within the world’s five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia? To answer such questions, we’ll use the gapminder data frame included in the gapminder package. This dataset has international development statistics such as life expectancy, GDP per capita, and population for 142 countries for 5-year intervals between 1952 and 2007. Recall we visualized some of this data in Figure 2.1 in Subsection 2.1.2 on the grammar of graphics. We’ll use this data for basic regression again, but now using an explanatory variable \\(x\\) that is categorical, as opposed to the numerical explanatory variable model we used in the previous Section 11.1. A numerical outcome variable \\(y\\) (a country’s life expectancy) and A single categorical explanatory variable \\(x\\) (the continent that the country is a part of). When the explanatory variable \\(x\\) is categorical, the concept of a “best-fitting” regression line is a little different than the one we saw previously in Section 11.1 where the explanatory variable \\(x\\) was numerical. We’ll study these differences shortly in Subsection 11.2.2, but first we conduct an exploratory data analysis. 11.2.1 Exploratory data analysis The data on the 142 countries can be found in the gapminder data frame included in the gapminder package. However, to keep things simple, let’s filter() for only those observations/rows corresponding to the year 2007. Additionally, let’s select() only the subset of the variables we’ll consider in this chapter. We’ll save this data in a new data frame called gapminder2007: library(gapminder) gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% select(country, lifeExp, continent, gdpPercap) Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() command as introduced in Subsection 1.4.3 on exploring data frames: glimpse(gapminder2007) Observations: 142 Variables: 4 $ country &lt;fct&gt; Afghanistan, Albania, Algeria, Angola, Argentina, Australia… $ lifeExp &lt;dbl&gt; 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1, 79.4,… $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, As… $ gdpPercap &lt;dbl&gt; 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1391, 33… Observe that Observations: 142 indicates that there are 142 rows/observations in gapminder2007, where each row corresponds to one country. In other words, the observational unit is an individual country. Furthermore, observe that the variable continent is of type &lt;fct&gt;, which stands for factor, which is R’s way of encoding categorical variables. A full description of all the variables included in gapminder can be found by reading the associated help file (run ?gapminder in the console). However, let’s fully describe only the 4 variables we selected in gapminder2007: country: An identification variable of type character/text used to distinguish the 142 countries in the dataset. lifeExp: A numerical variable of that country’s life expectancy at birth. This is the outcome variable \\(y\\) of interest. continent: A categorical variable with five levels. Here “levels” correspond to the possible categories: Africa, Asia, Americas, Europe, and Oceania. This is the explanatory variable \\(x\\) of interest. gdpPercap: A numerical variable of that country’s GDP per capita in US inflation-adjusted dollars. Let’s look at a random sample of five out of the 142 countries in Table 11.6. gapminder2007 %&gt;% sample_n(size = 5) TABLE 11.6: Random sample of 5 out of 142 countries country lifeExp continent gdpPercap Togo 58.4 Africa 883 Sao Tome and Principe 65.5 Africa 1598 Congo, Dem. Rep. 46.5 Africa 278 Lesotho 42.6 Africa 1569 Bulgaria 73.0 Europe 10681 Note that random sampling will likely produce a different subset of 5 rows for you than what’s shown. Now that we’ve looked at the raw values in our gapminder2007 data frame and got a sense of the data, let’s move on to computing summary statistics. Let’s once again apply the skim() function from the skimr package. Recall from our previous EDA that this function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our gapminder2007 data frame, select() only the outcome and explanatory variables lifeExp and continent, and pipe them into the skim() function: gapminder2007 %&gt;% select(lifeExp, continent) %&gt;% skim() TABLE 11.7: Data summary Name Piped data Number of rows 142 Number of columns 2 _______________________ Column type frequency: factor 1 numeric 1 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts continent 0 1 FALSE 5 Afr: 52, Asi: 33, Eur: 30, Ame: 25 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist lifeExp 0 1 67 12.1 39.6 57.2 71.9 76.4 82.6 ▂▃▃▆▇ The skim() output now reports summaries for categorical variables (Variable type:factor) separately from the numerical variables (Variable type:numeric). For the categorical variable continent, it reports: n_missing and complete_rate, which are the number of missing and the completion rate, respectively. n_unique: The number of unique levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania. This refers to how many countries are in the data for each continent. top_counts: In this case, the top four counts: Africa has 52 countries, Asia has 33, Europe has 30, and Americas has 25. Not displayed is Oceania with 2 countries. ordered: This tells us whether the categorical variable is “ordinal”: whether there is an encoded hierarchy (like low, medium, high). In this case, continent is not ordered. Turning our attention to the summary statistics of the numerical variable lifeExp, we observe that the global median life expectancy in 2007 was 71.94. Thus, half of the world’s countries (71 countries) had a life expectancy less than 71.94. The mean life expectancy of 67.01 is lower, however. Why is the mean life expectancy lower than the median? We can answer this question by performing the last of the three common steps in an exploratory data analysis: creating data visualizations. Let’s visualize the distribution of our outcome variable \\(y\\) = lifeExp in Figure 11.7. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) FIGURE 11.7: Histogram of life expectancy in 2007. We see that this data is left-skewed, also known as negatively skewed: there are a few countries with low life expectancy that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers; hence, the median is greater than the mean in this case. Remember, however, that we want to compare life expectancies both between continents and within continents. In other words, our visualizations need to incorporate some notion of the variable continent. We can do this easily with a faceted histogram. Recall from Section 2.5 that facets allow us to split a visualization by the different values of another variable. We display the resulting visualization in Figure 11.8 by adding a facet_wrap(~ continent, nrow = 2) layer. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) + facet_wrap(~ continent, nrow = 2) FIGURE 11.8: Life expectancy in 2007. Observe that unfortunately the distribution of African life expectancies is much lower than the other continents, while in Europe life expectancies tend to be higher and furthermore do not vary as much. On the other hand, both Asia and Africa have the most variation in life expectancies. There is the least variation in Oceania, but keep in mind that there are only two countries in Oceania: Australia and New Zealand. Recall that an alternative method to visualize the distribution of a numerical variable split by a categorical variable is by using a side-by-side boxplot. We map the categorical variable continent to the \\(x\\)-axis and the different life expectancies within each continent on the \\(y\\)-axis in Figure 11.9. ggplot(gapminder2007, aes(x = continent, y = lifeExp)) + geom_boxplot() + labs(x = &quot;Continent&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy by continent&quot;) FIGURE 11.9: Life expectancy in 2007. Some people prefer comparing the distributions of a numerical variable between different levels of a categorical variable using a boxplot instead of a faceted histogram. This is because we can make quick comparisons between the categorical variable’s levels with imaginary horizontal lines. For example, observe in Figure 11.9 that we can quickly convince ourselves that Oceania has the highest median life expectancies by drawing an imaginary horizontal line at \\(y\\) = 80. Furthermore, as we observed in the faceted histogram in Figure 11.8, Africa and Asia have the largest variation in life expectancy as evidenced by their large interquartile ranges (the heights of the boxes). It’s important to remember, however, that the solid lines in the middle of the boxes correspond to the medians (the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years. This tells us that half of all countries in Asia have a life expectancy below 72 years, whereas half have a life expectancy above 72 years. Let’s compute the median and mean life expectancy for each continent with a little more data wrangling and display the results in Table 11.8. lifeExp_by_continent &lt;- gapminder2007 %&gt;% group_by(continent) %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) TABLE 11.8: Life expectancy by continent continent median mean Africa 52.9 54.8 Americas 72.9 73.6 Asia 72.4 70.7 Europe 78.6 77.6 Oceania 80.7 80.7 Observe the order of the second column median life expectancy: Africa is lowest, the Americas and Asia are next with similar medians, then Europe, then Oceania. This ordering corresponds to the ordering of the solid black lines inside the boxes in our side-by-side boxplot in Figure 11.9. Let’s now turn our attention to the values in the third column mean. Using Africa’s mean life expectancy of 54.8 as a baseline for comparison, let’s start making comparisons to the mean life expectancies of the other four continents and put these values in Table 11.9, which we’ll revisit later on in this section. For the Americas, it is 73.6 - 54.8 = 18.8 years higher. For Asia, it is 70.7 - 54.8 = 15.9 years higher. For Europe, it is 77.6 - 54.8 = 22.8 years higher. For Oceania, it is 80.7 - 54.8 = 25.9 years higher. TABLE 11.9: Mean life expectancy by continent and relative differences from mean for Africa continent mean Difference versus Africa Africa 54.8 0.0 Americas 73.6 18.8 Asia 70.7 15.9 Europe 77.6 22.8 Oceania 80.7 25.9 11.2.2 Linear regression In Subsection 11.1.2 we introduced simple linear regression, which involves modeling the relationship between a numerical outcome variable \\(y\\) and a numerical explanatory variable \\(x\\). In our life expectancy example, we now instead have a categorical explanatory variable continent. Our model will not yield a “best-fitting” regression line like in Figure 11.4, but rather offsets relative to a baseline for comparison. As we did in Subsection 11.1.2 when studying the relationship between teaching scores and “beauty” scores, let’s output the regression table for this model. Recall that this is done in two steps: We first “fit” the linear regression model using the lm(y ~ x, data) function and save it in lifeExp_model. We get the regression table by applying the get_regression_table() function from the moderndive package to lifeExp_model. lifeExp_model &lt;- lm(lifeExp ~ continent, data = gapminder2007) get_regression_table(lifeExp_model) TABLE 11.10: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continentAmericas 18.8 1.80 10.45 0 15.2 22.4 continentAsia 15.9 1.65 9.68 0 12.7 19.2 continentEurope 22.8 1.70 13.47 0 19.5 26.2 continentOceania 25.9 5.33 4.86 0 15.4 36.5 Let’s once again focus on the values in the term and estimate columns of Table 11.10. Why are there now 5 rows? Let’s break them down one-by-one: intercept corresponds to the mean life expectancy of countries in Africa of 54.8 years. continentAmericas corresponds to countries in the Americas and the value +18.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 11.9. In other words, the mean life expectancy of countries in the Americas is \\(54.8 + 18.8 = 73.6\\). continentAsia corresponds to countries in Asia and the value +15.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 11.9. In other words, the mean life expectancy of countries in Asia is \\(54.8 + 15.9 = 70.7\\). continentEurope corresponds to countries in Europe and the value +22.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 11.9. In other words, the mean life expectancy of countries in Europe is \\(54.8 + 22.8 = 77.6\\). continentOceania corresponds to countries in Oceania and the value +25.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 11.9. In other words, the mean life expectancy of countries in Oceania is \\(54.8 + 25.9 = 80.7\\). To summarize, the 5 values in the estimate column in Table 11.10 correspond to the “baseline for comparison” continent Africa (the intercept) as well as four “offsets” from this baseline for the remaining 4 continents: the Americas, Asia, Europe, and Oceania. You might be asking at this point why was Africa chosen as the “baseline for comparison” group. This is the case for no other reason than it comes first alphabetically of the five continents; by default R arranges factors/categorical variables in alphanumeric order. You can change this baseline group to be another continent if you manipulate the variable continent’s factor “levels” using the forcats package. See Chapter 15 of R for Data Science (Grolemund and Wickham 2017) for examples. Let’s now write the equation for our fitted values \\(\\widehat{y} = \\widehat{\\text{life exp}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x) \\end{aligned} \\] Whoa! That looks daunting! Don’t fret, however, as once you understand what all the elements mean, things simplify greatly. First, \\(\\mathbb{1}_{A}(x)\\) is what’s known in mathematics as an “indicator function.” It returns only one of two possible values, 0 and 1, where \\[ \\mathbb{1}_{A}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } x \\text{ is in } A \\\\ 0 &amp; \\text{if } \\text{otherwise} \\end{array} \\right. \\] In a statistical modeling context, this is also known as a dummy variable. In our case, let’s consider the first such indicator variable \\(\\mathbb{1}_{\\text{Amer}}(x)\\). This indicator function returns 1 if a country is in the Americas, 0 otherwise: \\[ \\mathbb{1}_{\\text{Amer}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{country } x \\text{ is in the Americas} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, \\(b_0\\) corresponds to the intercept as before; in this case, it’s the mean life expectancy of all countries in Africa. Third, the \\(b_{\\text{Amer}}\\), \\(b_{\\text{Asia}}\\), \\(b_{\\text{Euro}}\\), and \\(b_{\\text{Ocean}}\\) represent the 4 “offsets relative to the baseline for comparison” in the regression table output in Table 11.10: continentAmericas, continentAsia, continentEurope, and continentOceania. Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{life exp}}\\) for a country in Africa. Since the country is in Africa, all four indicator functions \\(\\mathbb{1}_{\\text{Amer}}(x)\\), \\(\\mathbb{1}_{\\text{Asia}}(x)\\), \\(\\mathbb{1}_{\\text{Euro}}(x)\\), and \\(\\mathbb{1}_{\\text{Ocean}}(x)\\) will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 \\end{aligned} \\] In other words, all that’s left is the intercept \\(b_0\\), corresponding to the average life expectancy of African countries of 54.8 years. Next, say we are considering a country in the Americas. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Amer}}(x)\\) for the Americas will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 1 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 18.8 \\\\ &amp; = 73.6 \\end{aligned} \\] which is the mean life expectancy for countries in the Americas of 73.6 years in Table 11.9. Note the “offset from the baseline for comparison” is +18.8 years. Let’s do one more. Say we are considering a country in Asia. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Asia}}(x)\\) for Asia will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 1 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 15.9 \\\\ &amp; = 70.7 \\end{aligned} \\] which is the mean life expectancy for Asian countries of 70.7 years in Table 11.9. The “offset from the baseline for comparison” here is +15.9 years. Let’s generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable \\(x\\) that has \\(k\\) possible categories, the regression table will return an intercept and \\(k - 1\\) “offsets.” In our case, since there are \\(k = 5\\) continents, the regression model returns an intercept corresponding to the baseline for comparison group of Africa and \\(k - 1 = 4\\) offsets corresponding to the Americas, Asia, Europe, and Oceania. Understanding a regression table output when you’re using a categorical explanatory variable is a topic those new to regression often struggle with. The only real remedy for these struggles is practice, practice, practice. However, once you equip yourselves with an understanding of how to create regression models using categorical explanatory variables, you’ll be able to incorporate many new variables into your models, given the large amount of the world’s data that is categorical. If you feel like you’re still struggling at this point, however, we suggest you closely compare Tables 11.9 and 11.10 and note how you can compute all the values from one table using the values in the other. 11.2.3 Observed/fitted values and residuals Recall in Subsection 11.1.3, we defined the following three concepts: Observed values \\(y\\), or the observed value of the outcome variable Fitted values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value Residuals \\(y - \\widehat{y}\\), or the error between the observed value and the fitted value We obtained these values and other values using the get_regression_points() function from the moderndive package. This time, however, let’s add an argument setting ID = &quot;country&quot;: this is telling the function to use the variable country in gapminder2007 as an identification variable in the output. This will help contextualize our analysis by matching values to countries. regression_points &lt;- get_regression_points(lifeExp_model, ID = &quot;country&quot;) regression_points TABLE 11.11: Regression points (First 10 out of 142 countries) country lifeExp continent lifeExp_hat residual Afghanistan 43.8 Asia 70.7 -26.900 Albania 76.4 Europe 77.6 -1.226 Algeria 72.3 Africa 54.8 17.495 Angola 42.7 Africa 54.8 -12.075 Argentina 75.3 Americas 73.6 1.712 Australia 81.2 Oceania 80.7 0.515 Austria 79.8 Europe 77.6 2.180 Bahrain 75.6 Asia 70.7 4.907 Bangladesh 64.1 Asia 70.7 -6.666 Belgium 79.4 Europe 77.6 1.792 Observe in Table 11.11 that lifeExp_hat contains the fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{lifeExp}}\\). If you look closely, there are only 5 possible values for lifeExp_hat. These correspond to the five mean life expectancies for the 5 continents that we displayed in Table 11.9 and computed using the values in the estimate column of the regression table in Table 11.10. The residual column is simply \\(y - \\widehat{y}\\) = lifeExp - lifeExp_hat. These values can be interpreted as the deviation of a country’s life expectancy from its continent’s average life expectancy. For example, look at the first row of Table 11.11 corresponding to Afghanistan. The residual of \\(y - \\widehat{y} = 43.8 - 70.7 = -26.9\\) is telling us that Afghanistan’s life expectancy is a whopping 26.9 years lower than the mean life expectancy of all Asian countries. This can in part be explained by the many years of war that country has suffered. 11.3 Related topics 11.3.1 Correlation is not necessarily causation Throughout this chapter we’ve been cautious when interpreting regression slope coefficients. We always discussed the “associated” effect of an explanatory variable \\(x\\) on an outcome variable \\(y\\). For example, our statement from Subsection 11.1.2 that “for every increase of 1 unit in bty_avg, there is an associated increase of on average 0.067 units of score.” We include the term “associated” to be extra careful not to suggest we are making a causal statement. So while “beauty” score of bty_avg is positively correlated with teaching score, we can’t necessarily make any statements about “beauty” scores’ direct causal effect on teaching score without more information on how this study was conducted. Here is another example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, “Sleeping with shoes on causes headaches!” FIGURE 11.10: Does sleeping with shoes on cause headaches? However, there is a good chance that if someone is sleeping with their shoes on, it’s potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what’s known as a confounding/lurking variable. It “lurks” behind the scenes, confounding the causal relationship (if any) of “sleeping with shoes on” with “waking up with a headache.” We can summarize this in Figure 11.11 with a causal graph where: Y is a response variable; here it is “waking up with a headache.” X is a treatment variable whose causal effect we are interested in; here it is “sleeping with shoes on.” FIGURE 11.11: Causal graph. To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you’ve been doing throughout this chapter. However, Figure 11.11 also includes a third variable with arrows pointing at both X and Y: Z is a confounding variable that affects both X and Y, thereby “confounding” their relationship. Here the confounding variable is alcohol. Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we’ll start covering multiple regression models that allow us to incorporate more than one variable in our regression models. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X. As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation. Check out the Spurious Correlations website for some rather comical examples of variables that are correlated, but are definitely not causally related. 11.3.2 Best-fitting line Regression lines are also known as “best-fitting” lines. But what do we mean by “best”? Let’s unpack the criteria that is used in regression to determine “best.” Recall Figure 11.6, where for an instructor with a beauty score of \\(x = 7.333\\) we mark the observed value \\(y\\) with a circle, the fitted value \\(\\widehat{y}\\) with a square, and the residual \\(y - \\widehat{y}\\) with an arrow. We re-display Figure 11.6 in the top-left plot of Figure 11.12 in addition to three more arbitrarily chosen course instructors: FIGURE 11.12: Example of observed value, fitted value, and residual. The three other plots refer to: A course whose instructor had a “beauty” score \\(x\\) = 2.333 and teaching score \\(y\\) = 2.7. The residual in this case is \\(2.7 - 4.036 = -1.336\\), which we mark with a new blue arrow in the top-right plot. A course whose instructor had a “beauty” score \\(x = 3.667\\) and teaching score \\(y = 4.4\\). The residual in this case is \\(4.4 - 4.125 = 0.2753\\), which we mark with a new blue arrow in the bottom-left plot. A course whose instructor had a “beauty” score \\(x = 6\\) and teaching score \\(y = 3.8\\). The residual in this case is \\(3.8 - 4.28 = -0.4802\\), which we mark with a new blue arrow in the bottom-right plot. Now say we repeated this process of computing residuals for all 463 courses’ instructors, then we squared all the residuals, and then we summed them. We call this quantity the sum of squared residuals; it is a measure of the lack of fit of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model. If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value \\(\\widehat{y}\\) equals the observed value \\(y\\) in all cases, and hence the residual \\(y-\\widehat{y}\\) = 0 in all cases, and the sum of even a large number of 0’s is still 0. Furthermore, of all possible lines we can draw through the cloud of 463 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values \\(\\widehat{y}\\) minimizes the sum of the squared residuals: \\[ \\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2 \\] Let’s use our data wrangling tools from Chapter 4 to compute the sum of squared residuals exactly: # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch5) # Get regression points: regression_points &lt;- get_regression_points(score_model) regression_points # A tibble: 463 x 5 ID score bty_avg score_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 4.7 5 4.214 0.486 2 2 4.100 5 4.214 -0.114 3 3 3.9 5 4.214 -0.314 4 4 4.8 5 4.214 0.586 5 5 4.600 3 4.08 0.52 6 6 4.3 3 4.08 0.22 7 7 2.8 3 4.08 -1.28 8 8 4.100 3.333 4.102 -0.002 9 9 3.4 3.333 4.102 -0.702 10 10 4.5 3.16700 4.091 0.40900 # … with 453 more rows # Compute sum of squared residuals regression_points %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize(sum_of_squared_residuals = sum(squared_residuals)) # A tibble: 1 x 1 sum_of_squared_residuals &lt;dbl&gt; 1 131.879 Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That’s why alternative names for the linear regression line are the best-fitting line and the least-squares line. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally. 11.3.3 get_regression_x() functions Recall in this chapter we introduced two functions from the moderndive package: get_regression_table() that returns a regression table in Subsection 11.1.2 and get_regression_points() that returns point-by-point information from a regression model in Subsection 11.1.3. What is going on behind the scenes with the get_regression_table() and get_regression_points() functions? We mentioned in Subsection 11.1.2 that these were examples of wrapper functions. Such functions take other pre-existing functions and “wrap” them into single functions that hide the user from their inner workings. This way all the user needs to worry about is what the inputs look like and what the outputs look like. In this subsection, we’ll “get under the hood” of these functions and see how the “engine” of these wrapper functions works. Recall our two-step process to generate a regression table from Subsection 11.1.2: # Fit regression model: score_model &lt;- lm(formula = score ~ bty_avg, data = evals_ch5) # Get regression table: get_regression_table(score_model) TABLE 11.12: Regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 The get_regression_table() wrapper function takes two pre-existing functions in other R packages: tidy() from the broom package (Robinson and Hayes 2019) and clean_names() from the janitor package (Firke 2019) and “wraps” them into a single function that takes in a saved lm() linear model model, here score_model, and returns a regression table saved as a “tidy” data frame. Here is how we used the tidy() and clean_names() functions to produce Table 11.13: library(broom) library(janitor) score_model %&gt;% tidy(conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% rename(lower_ci = conf_low, upper_ci = conf_high) TABLE 11.13: Regression table using tidy() from broom package term estimate std_error statistic p_value lower_ci upper_ci (Intercept) 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Yikes! That’s a lot of code! So, in order to simplify your lives, we made the editorial decision to “wrap” all the code into get_regression_table(), freeing you from the need to understand the inner workings of the function. Note that the mutate_if() function is from the dplyr package and applies the round() function to three significant digits precision only to those variables that are numerical. Similarly, the get_regression_points() function is another wrapper function, but this time returning information about the individual points involved in a regression model like the fitted values, observed values, and the residuals. get_regression_points() uses the augment() function in the broom package instead of the tidy() function as with get_regression_table() to produce the data shown in Table 11.14: library(broom) library(janitor) score_model %&gt;% augment() %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% select(-c(&quot;se_fit&quot;, &quot;hat&quot;, &quot;sigma&quot;, &quot;cooksd&quot;, &quot;std_resid&quot;)) TABLE 11.14: Regression points using augment() from broom package score bty_avg fitted resid 4.7 5.00 4.21 0.486 4.1 5.00 4.21 -0.114 3.9 5.00 4.21 -0.314 4.8 5.00 4.21 0.586 4.6 3.00 4.08 0.520 4.3 3.00 4.08 0.220 2.8 3.00 4.08 -1.280 4.1 3.33 4.10 -0.002 3.4 3.33 4.10 -0.702 4.5 3.17 4.09 0.409 In this case, it outputs only the variables of interest to students learning regression: the outcome variable \\(y\\) (score), all explanatory/predictor variables (bty_avg), all resulting fitted values \\(\\hat{y}\\) used by applying the equation of the regression line to bty_avg, and the residual \\(y - \\hat{y}\\). If you’re even more curious about how these and other wrapper functions work, take a look at the source code for these functions on GitHub. 11.4 Conclusion 11.4.1 Additional resources As we suggested in Subsection 11.1.1, interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the 80s-style video game called, “Guess the Correlation”, at http://guessthecorrelation.com/. FIGURE 11.13: Preview of “Guess the Correlation” game. 11.4.2 What’s to come? In this chapter, you’ve studied the term basic regression, where you fit models that only have one explanatory variable. In Chapter 12, we’ll study multiple regression, where our regression models can now have more than one explanatory variable! In particular, we’ll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable \\(y\\). References "],
["12-multiple-regression.html", "Chapter 12 Multiple Regression 12.1 One numerical and one categorical explanatory variable 12.2 Two numerical explanatory variables 12.3 Related topics 12.4 Case study: Seattle house prices 12.5 Case study: Effective data storytelling 12.6 Conclusion", " Chapter 12 Multiple Regression In Chapter 11 we introduced ideas related to modeling for explanation, in particular that the goal of modeling is to make explicit the relationship between some outcome variable \\(y\\) and some explanatory variable \\(x\\). While there are many approaches to modeling, we focused on one particular technique: linear regression, one of the most commonly used and easy-to-understand approaches to modeling. Furthermore to keep things simple, we only considered models with one explanatory \\(x\\) variable that was either numerical in Section 11.1 or categorical in Section 11.2. In this chapter on multiple regression, we’ll start considering models that include more than one explanatory variable \\(x\\). You can imagine when trying to model a particular outcome variable, like teaching evaluation scores as in Section 11.1 or life expectancy as in Section 11.2, that it would be useful to include more than just one explanatory variable’s worth of information. Since our regression models will now consider more than one explanatory variable, the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model. Let’s begin! Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 5.5 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(ISLR) library(fivethirtyeight) 12.1 One numerical and one categorical explanatory variable Let’s revisit the instructor evaluation data from UT Austin we introduced in Section 11.1. We studied the relationship between teaching evaluation scores as given by students and “beauty” scores. The variable teaching score was the numerical outcome variable \\(y\\), and the variable “beauty” score (bty_avg) was the numerical explanatory \\(x\\) variable. In this section, we are going to consider a different model. Our outcome variable will still be teaching score, but we’ll now include two different explanatory variables: age and (binary) gender. Could it be that instructors who are older receive better teaching evaluations from students? Or could it instead be that younger instructors receive better evaluations? Are there differences in evaluations given by students for instructors of different genders? We’ll answer these questions by modeling the relationship between these variables using multiple regression, where we have: A numerical outcome variable \\(y\\), the instructor’s teaching score, and Two explanatory variables: A numerical explanatory variable \\(x_1\\), the instructor’s age. A categorical explanatory variable \\(x_2\\), the instructor’s (binary) gender. It is important to note that at the time of this study due to then commonly held beliefs about gender, this variable was often recorded as a binary variable. While the results of a model that oversimplifies gender this way may be imperfect, we still found the results to be pertinent and relevant today. 12.1.1 Exploratory data analysis Recall that data on the 463 courses at UT Austin can be found in the evals data frame included in the moderndive package. However, to keep things simple, let’s select() only the subset of the variables we’ll consider in this chapter, and save this data in a new data frame called evals_ch6. Note that these are different than the variables chosen in Chapter 11. evals_ch6 &lt;- evals %&gt;% select(ID, score, age, gender) Recall the three common steps in an exploratory data analysis we saw in Subsection 11.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. Let’s first look at the raw data values by either looking at evals_ch6 using RStudio’s spreadsheet viewer or by using the glimpse() function from the dplyr package: glimpse(evals_ch6) Observations: 463 Variables: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40… $ gender &lt;fct&gt; female, female, female, female, male, male, male, male, male, … Let’s also display a random sample of 5 rows of the 463 rows corresponding to different courses in Table 12.1. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. evals_ch6 %&gt;% sample_n(size = 5) TABLE 12.1: A random sample of 5 out of the 463 courses at UT Austin ID score age gender 129 3.7 62 male 109 4.7 46 female 28 4.8 62 male 434 2.8 62 male 330 4.0 64 male Now that we’ve looked at the raw values in our evals_ch6 data frame and got a sense of the data, let’s compute summary statistics. As we did in our exploratory data analyses in Sections 11.1.1 and 11.2.1 from the previous chapter, let’s use the skim() function from the skimr package, being sure to only select() the variables of interest in our model: evals_ch6 %&gt;% select(score, age, gender) %&gt;% skim() TABLE 12.2: Data summary Name Piped data Number of rows 463 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts gender 0 1 FALSE 2 mal: 268, fem: 195 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.3 3.8 4.3 4.6 5 ▁▁▅▇▇ age 0 1 48.37 9.80 29.0 42.0 48.0 57.0 73 ▅▆▇▆▁ Observe that we have no missing data, that there are 268 courses taught by male instructors and 195 courses taught by female instructors, and that the average instructor age is 48.37. Recall that each row represents a particular course and that the same instructor often teaches more than one course. Therefore, the average age of the unique instructors may differ. Furthermore, let’s compute the correlation coefficient between our two numerical variables: score and age. Recall from Subsection 11.1.1 that correlation coefficients only exist between numerical variables. We observe that they are “weakly negatively” correlated. evals_ch6 %&gt;% get_correlation(formula = score ~ age) # A tibble: 1 x 1 cor &lt;dbl&gt; 1 -0.107032 Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Given that the outcome variable score and explanatory variable age are both numerical, we’ll use a scatterplot to display their relationship. How can we incorporate the categorical variable gender, however? By mapping the variable gender to the color aesthetic, thereby creating a colored scatterplot. The following code is similar to the code that created the scatterplot of teaching score over “beauty” score in Figure 11.2, but with color = gender added to the aes()thetic mapping. ggplot(evals_ch6, aes(x = age, y = score, color = gender)) + geom_point() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 12.1: Colored scatterplot of relationship of teaching and beauty scores. In the resulting Figure 12.1, observe that ggplot() assigns a default in red/blue color scheme to the points and to the lines associated with the two levels of gender: female and male. Furthermore, the geom_smooth(method = &quot;lm&quot;, se = FALSE) layer automatically fits a different regression line for each group. We notice some interesting trends. First, there are almost no women faculty over the age of 60 as evidenced by lack of red dots above \\(x\\) = 60. Second, while both regression lines are negatively sloped with age (i.e., older instructors tend to have lower scores), the slope for age for the female instructors is more negative. In other words, female instructors are paying a harsher penalty for advanced age than the male instructors. 12.1.2 Interaction model Let’s now quantify the relationship of our outcome variable \\(y\\) and the two explanatory variables using one type of multiple regression model known as an interaction model. We’ll explain where the term “interaction” comes from at the end of this section. In particular, we’ll write out the equation of the two regression lines in Figure 12.1 using the values from a regression table. Before we do this, however, let’s go over a brief refresher of regression when you have a categorical explanatory variable \\(x\\). Recall in Subsection 11.2.2 we fit a regression model for countries’ life expectancies as a function of which continent the country was in. In other words, we had a numerical outcome variable \\(y\\) = lifeExp and a categorical explanatory variable \\(x\\) = continent which had 5 levels: Africa, Americas, Asia, Europe, and Oceania. Let’s re-display the regression table you saw in Table 11.10: TABLE 12.3: Regression table for life expectancy as a function of continent term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continentAmericas 18.8 1.80 10.45 0 15.2 22.4 continentAsia 15.9 1.65 9.68 0 12.7 19.2 continentEurope 22.8 1.70 13.47 0 19.5 26.2 continentOceania 25.9 5.33 4.86 0 15.4 36.5 Recall our interpretation of the estimate column. Since Africa was the “baseline for comparison” group, the intercept term corresponds to the mean life expectancy for all countries in Africa of 54.8 years. The other four values of estimate correspond to “offsets” relative to the baseline group. So, for example, the “offset” corresponding to the Americas is +18.8 as compared to the baseline for comparison group Africa. In other words, the average life expectancy for countries in the Americas is 18.8 years higher. Thus the mean life expectancy for all countries in the Americas is 54.8 + 18.8 = 73.6. The same interpretation holds for Asia, Europe, and Oceania. Going back to our multiple regression model for teaching score using age and gender in Figure 12.1, we generate the regression table using the same two-step approach from Chapter 11: we first “fit” the model using the lm() “linear model” function and then we apply the get_regression_table() function. This time, however, our model formula won’t be of the form y ~ x, but rather of the form y ~ x1 * x2. In other words, our two explanatory variables x1 and x2 are separated by a * sign: # Fit regression model: score_model_interaction &lt;- lm(score ~ age * gender, data = evals_ch6) # Get regression table: get_regression_table(score_model_interaction) TABLE 12.4: Regression table for interaction model term estimate std_error statistic p_value lower_ci upper_ci intercept 4.883 0.205 23.80 0.000 4.480 5.286 age -0.018 0.004 -3.92 0.000 -0.026 -0.009 gendermale -0.446 0.265 -1.68 0.094 -0.968 0.076 age:gendermale 0.014 0.006 2.45 0.015 0.003 0.024 Looking at the regression table output in Table 12.4, there are four rows of values in the estimate column. While it is not immediately apparent, using these four values we can write out the equations of both lines in Figure 12.1. First, since the word female comes alphabetically before male, female instructors are the “baseline for comparison” group. Thus, intercept is the intercept for only the female instructors. This holds similarly for age. It is the slope for age for only the female instructors. Thus, the red regression line in Figure 12.1 has an intercept of 4.883 and slope for age of -0.018. Remember that for this data, while the intercept has a mathematical interpretation, it has no practical interpretation since instructors can’t have zero age. What about the intercept and slope for age of the male instructors in the blue line in Figure 12.1? This is where our notion of “offsets” comes into play once again. The value for gendermale of -0.446 is not the intercept for the male instructors, but rather the offset in intercept for male instructors relative to female instructors. The intercept for the male instructors is intercept + gendermale = 4.883 + (-0.446) = 4.883 - 0.446 = 4.437. Similarly, age:gendermale = 0.014 is not the slope for age for the male instructors, but rather the offset in slope for the male instructors. Therefore, the slope for age for the male instructors is age + age:gendermale \\(= -0.018 + 0.014 = -0.004\\). Thus, the blue regression line in Figure 12.1 has intercept 4.437 and slope for age of -0.004. Let’s summarize these values in Table 12.5 and focus on the two slopes for age: TABLE 12.5: Comparison of intercepts and slopes for interaction model Gender Intercept Slope for age Female instructors 4.883 -0.018 Male instructors 4.437 -0.004 Since the slope for age for the female instructors was -0.018, it means that on average, a female instructor who is a year older would have a teaching score that is 0.018 units lower. For the male instructors, however, the corresponding associated decrease was on average only 0.004 units. While both slopes for age were negative, the slope for age for the female instructors is more negative. This is consistent with our observation from Figure 12.1, that this model is suggesting that age impacts teaching scores for female instructors more than for male instructors. Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{score}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) + b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}\\\\ &amp;= 4.883 -0.018 \\cdot \\text{age} - 0.446 \\cdot \\mathbb{1}_{\\text{is male}}(x) + 0.014 \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}} \\end{aligned} \\] Whoa! That’s even more daunting than the equation you saw for the life expectancy as a function of continent in Subsection 11.2.2! However, if you recall what an “indicator function” does, the equation simplifies greatly. In the previous equation, we have one indicator function of interest: \\[ \\mathbb{1}_{\\text{is male}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{instructor } x \\text{ is male} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, let’s match coefficients in the previous equation with values in the estimate column in our regression table in Table 12.4: \\(b_0\\) is the intercept = 4.883 for the female instructors \\(b_{\\text{age}}\\) is the slope for age = -0.018 for the female instructors \\(b_{\\text{male}}\\) is the offset in intercept = -0.446 for the male instructors \\(b_{\\text{age,male}}\\) is the offset in slope for age = 0.014 for the male instructors Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{score}}\\) for female instructors. Since for female instructors \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0.446 \\cdot 0 + 0.014 \\cdot \\text{age} \\cdot 0\\\\ &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0 + 0\\\\ &amp;= 4.883 - 0.018 \\cdot \\text{age}\\\\ \\end{aligned} \\] which is the equation of the red regression line in Figure 12.1 corresponding to the female instructors in Table 12.5. Correspondingly, since for male instructors \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.883 - 0.018 \\cdot \\text{age} - 0.446 + 0.014 \\cdot \\text{age}\\\\ &amp;= (4.883 - 0.446) + (- 0.018 + 0.014) * \\text{age}\\\\ &amp;= 4.437 - 0.004 \\cdot \\text{age}\\\\ \\end{aligned} \\] which is the equation of the blue regression line in Figure 12.1 corresponding to the male instructors in Table 12.5. Phew! That was a lot of arithmetic! Don’t fret, however, this is as hard as modeling will get in this book. If you’re still a little unsure about using indicator functions and using categorical explanatory variables in a regression model, we highly suggest you re-read Subsection 11.2.2. This involves only a single categorical explanatory variable and thus is much simpler. Before we end this section, we explain why we refer to this type of model as an “interaction model.” The \\(b_{\\text{age,male}}\\) term in the equation for the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) is what’s known in statistical modeling as an “interaction effect.” The interaction term corresponds to the age:gendermale = 0.014 in the final row of the regression table in Table 12.4. We say there is an interaction effect if the associated effect of one variable depends on the value of another variable. That is to say, the two variables are “interacting” with each other. Here, the associated effect of the variable age depends on the value of the other variable gender. The difference in slopes for age of +0.014 of male instructors relative to female instructors shows this. Another way of thinking about interaction effects on teaching scores is as follows. For a given instructor at UT Austin, there might be an associated effect of their age by itself, there might be an associated effect of their gender by itself, but when age and gender are considered together there might be an additional effect above and beyond the two individual effects. 12.1.3 Parallel slopes model When creating regression models with one numerical and one categorical explanatory variable, we are not just limited to interaction models as we just saw. Another type of model we can use is known as a parallel slopes model. Unlike interaction models where the regression lines can have different intercepts and different slopes, parallel slopes models still allow for different intercepts but force all lines to have the same slope. The resulting regression lines are thus parallel. Let’s visualize the best-fitting parallel slopes model to evals_ch6. Unfortunately, the geom_smooth() function in the ggplot2 package does not have a convenient way to plot parallel slopes models. Evgeni Chasnovski thus created a special purpose function called geom_parallel_slopes() that is included in the moderndive package. You won’t find geom_parallel_slopes() in the ggplot2 package, but rather the moderndive package. Thus, if you want to be able to use it, you will need to load both the ggplot2 and moderndive packages. Using this function, let’s now plot the parallel slopes model for teaching score. Notice how the code is identical to the code that produced the visualization of the interaction model in Figure 12.1, but now the geom_smooth(method = &quot;lm&quot;, se = FALSE) layer is replaced with geom_parallel_slopes(se = FALSE). ggplot(evals_ch6, aes(x = age, y = score, color = gender)) + geom_point() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_parallel_slopes(se = FALSE) FIGURE 12.2: Parallel slopes model of score with age and gender. Observe in Figure 12.2 that we now have parallel lines corresponding to the female and male instructors, respectively: here they have the same negative slope. This is telling us that instructors who are older will tend to receive lower teaching scores than instructors who are younger. Furthermore, since the lines are parallel, the associated penalty for being older is assumed to be the same for both female and male instructors. However, observe also in Figure 12.2 that these two lines have different intercepts as evidenced by the fact that the blue line corresponding to the male instructors is higher than the red line corresponding to the female instructors. This is telling us that irrespective of age, female instructors tended to receive lower teaching scores than male instructors. In order to obtain the precise numerical values of the two intercepts and the single common slope, we once again “fit” the model using the lm() “linear model” function and then apply the get_regression_table() function. However, unlike the interaction model which had a model formula of the form y ~ x1 * x2, our model formula is now of the form y ~ x1 + x2. In other words, our two explanatory variables x1 and x2 are separated by a + sign: # Fit regression model: score_model_parallel_slopes &lt;- lm(score ~ age + gender, data = evals_ch6) # Get regression table: get_regression_table(score_model_parallel_slopes) TABLE 12.6: Regression table for parallel slopes model term estimate std_error statistic p_value lower_ci upper_ci intercept 4.484 0.125 35.79 0.000 4.238 4.730 age -0.009 0.003 -3.28 0.001 -0.014 -0.003 gendermale 0.191 0.052 3.63 0.000 0.087 0.294 Similarly to the regression table for the interaction model from Table 12.4, we have an intercept term corresponding to the intercept for the “baseline for comparison” female instructor group and a gendermale term corresponding to the offset in intercept for the male instructors relative to female instructors. In other words, in Figure 12.2 the red regression line corresponding to the female instructors has an intercept of 4.484 while the blue regression line corresponding to the male instructors has an intercept of 4.484 + 0.191 = 4.675. Once again, since there aren’t any instructors of age 0, the intercepts only have a mathematical interpretation but no practical one. Unlike in Table 12.4, however, we now only have a single slope for age of -0.009. This is because the model dictates that both the female and male instructors have a common slope for age. This is telling us that an instructor who is a year older than another instructor received a teaching score that is on average 0.009 units lower. This penalty for being of advanced age applies equally to both female and male instructors. Let’s summarize these values in Table 12.7, noting the different intercepts but common slopes: TABLE 12.7: Comparison of intercepts and slope for parallel slopes model Gender Intercept Slope for age Female instructors 4.484 -0.009 Male instructors 4.675 -0.009 Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{score}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x)\\\\ &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot \\mathbb{1}_{\\text{is male}}(x) \\end{aligned} \\] Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{score}}\\) for female instructors. Since for female instructors the indicator function \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot 0\\\\ &amp;= 4.484 -0.009 \\cdot \\text{age} \\end{aligned} \\] which is the equation of the red regression line in Figure 12.2 corresponding to the female instructors. Correspondingly, since for male instructors the indicator function \\(\\mathbb{1}_{\\text{is male}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{score}} &amp;= 4.484 -0.009 \\cdot \\text{age} + 0.191 \\cdot 1\\\\ &amp;= (4.484 + 0.191) - 0.009 \\cdot \\text{age}\\\\ &amp;= 4.675 -0.009 \\cdot \\text{age} \\end{aligned} \\] which is the equation of the blue regression line in Figure 12.2 corresponding to the male instructors. Great! We’ve considered both an interaction model and a parallel slopes model for our data. Let’s compare the visualizations for both models side-by-side in Figure 12.3. FIGURE 12.3: Comparison of interaction and parallel slopes models. At this point, you might be asking yourself: “Why would we ever use a parallel slopes model?”. Looking at the left-hand plot in Figure 12.3, the two lines definitely do not appear to be parallel, so why would we force them to be parallel? For this data, we agree! It can easily be argued that the interaction model on the left is more appropriate. However, in the upcoming Subsection 12.3.1 on model selection, we’ll present an example where it can be argued that the case for a parallel slopes model might be stronger. 12.1.4 Observed/fitted values and residuals For brevity’s sake, in this section we’ll only compute the observed values, fitted values, and residuals for the interaction model which we saved in score_model_interaction. Say, you have an instructor who identifies as female and is 36 years old. What fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) would our model yield? Say, you have another instructor who identifies as male and is 59 years old. What would their fitted value \\(\\widehat{y}\\) be? We answer this question visually first for the female instructor by finding the intersection of the red regression line and the vertical line at \\(x\\) = age = 36. We mark this value with a large red dot in Figure 12.4. Similarly, we can identify the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) for the male instructor by finding the intersection of the blue regression line and the vertical line at \\(x\\) = age = 59. We mark this value with a large blue dot in Figure 12.4. FIGURE 12.4: Fitted values for two new professors. What are these two values of \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) precisely? We can use the equations of the two regression lines we computed in Subsection 12.1.2, which in turn were based on values from the regression table in Table 12.4: For all female instructors: \\(\\widehat{y} = \\widehat{\\text{score}} = 4.883 - 0.018 \\cdot \\text{age}\\) For all male instructors: \\(\\widehat{y} = \\widehat{\\text{score}} = 4.437 - 0.004 \\cdot \\text{age}\\) So our fitted values would be: \\(4.883 - 0.018 \\cdot 36 = 4.25\\) and \\(4.437 - 0.004 \\cdot 59 = 4.20\\), respectively. Now what if we want the fitted values not just for these two instructors, but for the instructors of all 463 courses included in the evals_ch6 data frame? Doing this by hand would be long and tedious! This is where the get_regression_points() function from the moderndive package can help: it will quickly automate the above calculations for all 463 courses. We present a preview of just the first 10 rows out of 463 in Table 12.8. regression_points &lt;- get_regression_points(score_model_interaction) regression_points TABLE 12.8: Regression points (First 10 out of 463 courses) ID score age gender score_hat residual 1 4.7 36 female 4.25 0.448 2 4.1 36 female 4.25 -0.152 3 3.9 36 female 4.25 -0.352 4 4.8 36 female 4.25 0.548 5 4.6 59 male 4.20 0.399 6 4.3 59 male 4.20 0.099 7 2.8 59 male 4.20 -1.401 8 4.1 51 male 4.23 -0.133 9 3.4 51 male 4.23 -0.833 10 4.5 40 female 4.18 0.318 It turns out that the female instructor of age 36 taught the first four courses, while the male instructor taught the next 3. The resulting \\(\\widehat{y}\\) = \\(\\widehat{\\text{score}}\\) fitted values are in the score_hat column. Furthermore, the get_regression_points() function also returns the residuals \\(y-\\widehat{y}\\). Notice, for example, the first and fourth courses the female instructor of age 36 taught had positive residuals, indicating that the actual teaching scores they received from students were greater than their fitted score of 4.25. On the other hand, the second and third courses this instructor taught had negative residuals, indicating that the actual teaching scores they received from students were less than 4.25. 12.2 Two numerical explanatory variables Let’s now switch gears and consider multiple regression models where instead of one numerical and one categorical explanatory variable, we now have two numerical explanatory variables. The dataset we’ll use is from An Introduction to Statistical Learning with Applications in R (ISLR), an intermediate-level textbook on statistical and machine learning (James et al. 2017). Its accompanying ISLR R package contains the datasets to which the authors apply various machine learning methods. One frequently used dataset in this book is the Credit dataset, where the outcome variable of interest is the credit card debt of 400 individuals. Other variables like income, credit limit, credit rating, and age are included as well. Note that the Credit data is not based on real individuals’ financial information, but rather is a simulated dataset used for educational purposes. In this section, we’ll fit a regression model where we have A numerical outcome variable \\(y\\), the cardholder’s credit card debt Two explanatory variables: One numerical explanatory variable \\(x_1\\), the cardholder’s credit limit Another numerical explanatory variable \\(x_2\\), the cardholder’s income (in thousands of dollars). 12.2.1 Exploratory data analysis Let’s load the Credit dataset. To keep things simple let’s select() the subset of the variables we’ll consider in this chapter, and save this data in the new data frame credit_ch6. Notice our slightly different use of the select() verb here than we introduced in Subsection 4.10.1. For example, we’ll select the Balance variable from Credit but then save it with a new variable name debt. We do this because here the term “debt” is easier to interpret than “balance.” library(ISLR) credit_ch6 &lt;- Credit %&gt;% as_tibble() %&gt;% select(ID, debt = Balance, credit_limit = Limit, income = Income, credit_rating = Rating, age = Age) You can observe the effect of our use of select() in the first common step of an exploratory data analysis: looking at the raw values either in RStudio’s spreadsheet viewer or by using glimpse(). glimpse(credit_ch6) Observations: 400 Variables: 6 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … $ debt &lt;int&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 140… $ credit_limit &lt;int&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6… $ income &lt;dbl&gt; 14.9, 106.0, 104.6, 148.9, 55.9, 80.2, 21.0, 71.4, 15.1… $ credit_rating &lt;int&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, … $ age &lt;int&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49,… Furthermore, let’s look at a random sample of five out of the 400 credit card holders in Table 12.9. Once again, note that due to the random nature of the sampling, you will likely end up with a different subset of five rows. credit_ch6 %&gt;% sample_n(size = 5) TABLE 12.9: Random sample of 5 credit card holders ID debt credit_limit income credit_rating age 272 436 4866 45.0 347 30 239 52 2910 26.5 236 58 87 815 6340 55.4 448 33 108 0 3189 39.1 263 72 149 0 2420 15.2 192 69 Now that we’ve looked at the raw values in our credit_ch6 data frame and got a sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s use the skim() function from the skimr package, being sure to only select() the columns of interest for our model: credit_ch6 %&gt;% select(debt, credit_limit, income) %&gt;% skim() TABLE 12.10: Data summary Name Piped data Number of rows 400 Number of columns 3 _______________________ Column type frequency: numeric 3 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist debt 0 1 520.0 459.8 0.0 68.8 459.5 863.0 1999 ▇▅▃▂▁ credit_limit 0 1 4735.6 2308.2 855.0 3088.0 4622.5 5872.8 13913 ▆▇▃▁▁ income 0 1 45.2 35.2 10.3 21.0 33.1 57.5 187 ▇▂▁▁▁ Observe the summary statistics for the outcome variable debt: the mean and median credit card debt are $520.01 and $459.50, respectively, and that 25% of card holders had debts of $68.75 or less. Let’s now look at one of the explanatory variables credit_limit: the mean and median credit card limit are $4735.6 and $4622.50, respectively, while 75% of card holders had incomes of $57,470 or less. Since our outcome variable debt and the explanatory variables credit_limit and income are numerical, we can compute the correlation coefficient between the different possible pairs of these variables. First, we can run the get_correlation() command as seen in Subsection 11.1.1 twice, once for each explanatory variable: credit_ch6 %&gt;% get_correlation(debt ~ credit_limit) credit_ch6 %&gt;% get_correlation(debt ~ income) Or we can simultaneously compute them by returning a correlation matrix which we display in Table 12.11. We can see the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination. credit_ch6 %&gt;% select(debt, credit_limit, income) %&gt;% cor() TABLE 12.11: Correlation coefficients between credit card debt, credit limit, and income debt credit_limit income debt 1.000 0.862 0.464 credit_limit 0.862 1.000 0.792 income 0.464 0.792 1.000 For example, the correlation coefficient of: debt with itself is 1 as we would expect based on the definition of the correlation coefficient. debt with credit_limit is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card debts. debt with income is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between debt and credit_limit. As an added bonus, we can read off the correlation coefficient between the two explanatory variables of credit_limit and income as 0.792. We say there is a high degree of collinearity between the credit_limit and income explanatory variables. Collinearity (or multicollinearity) is a phenomenon where one explanatory variable in a multiple regression model is highly correlated with another. So in our case since credit_limit and income are highly correlated, if we knew someone’s credit_limit, we could make pretty good guesses about their income as well. Thus, these two variables provide somewhat redundant information. However, we’ll leave discussion on how to work with collinear explanatory variables to a more intermediate-level book on regression modeling. Let’s visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots in Figure 12.5. ggplot(credit_ch6, aes(x = credit_limit, y = debt)) + geom_point() + labs(x = &quot;Credit limit (in $)&quot;, y = &quot;Credit card debt (in $)&quot;, title = &quot;Debt and credit limit&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(credit_ch6, aes(x = income, y = debt)) + geom_point() + labs(x = &quot;Income (in $1000)&quot;, y = &quot;Credit card debt (in $)&quot;, title = &quot;Debt and income&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 12.5: Relationship between credit card debt and credit limit/income. Observe there is a positive relationship between credit limit and credit card debt: as credit limit increases so also does credit card debt. This is consistent with the strongly positive correlation coefficient of 0.862 we computed earlier. In the case of income, the positive relationship doesn’t appear as strong, given the weakly positive correlation coefficient of 0.464. However, the two plots in Figure 12.5 only focus on the relationship of the outcome variable with each of the two explanatory variables separately. To visualize the joint relationship of all three variables simultaneously, we need a 3-dimensional (3D) scatterplot as seen in Figure 12.6. Each of the 400 observations in the credit_ch6 data frame are marked with a blue point where The numerical outcome variable \\(y\\) debt is on the vertical axis. The two numerical explanatory variables, \\(x_1\\) income and \\(x_2\\) credit_limit, are on the two axes that form the bottom plane. FIGURE 12.6: 3D scatterplot and regression plane. Furthermore, we also include the regression plane. Recall from Subsection 11.3.2 that regression lines are “best-fitting” in that of all possible lines we can draw through a cloud of points, the regression line minimizes the sum of squared residuals. This concept also extends to models with two numerical explanatory variables. The difference is instead of a “best-fitting” line, we now have a “best-fitting” plane that similarly minimizes the sum of squared residuals. Head to this website to open an interactive version of this plot in your browser. 12.2.2 Regression plane Let’s now fit a regression model and get the regression table corresponding to the regression plane in Figure 12.6. To keep things brief in this subsection, we won’t consider an interaction model for the two numerical explanatory variables income and credit_limit like we did in Subsection 12.1.2 using the model formula score ~ age * gender. Rather we’ll only consider a model fit with a formula of the form y ~ x1 + x2. Confusingly, however, since we now have a regression plane instead of multiple lines, the label “parallel slopes” doesn’t apply when you have two numerical explanatory variables. Just as we have done multiple times throughout Chapters 11 and this chapter, the regression table for this model using our two-step process is in Table 12.12. # Fit regression model: debt_model &lt;- lm(debt ~ credit_limit + income, data = credit_ch6) # Get regression table: get_regression_table(debt_model) TABLE 12.12: Multiple regression table term estimate std_error statistic p_value lower_ci upper_ci intercept -385.179 19.465 -19.8 0 -423.446 -346.912 credit_limit 0.264 0.006 45.0 0 0.253 0.276 income -7.663 0.385 -19.9 0 -8.420 -6.906 We first “fit” the linear regression model using the lm(y ~ x1 + x2, data) function and save it in debt_model. We get the regression table by applying the get_regression_table() function from the moderndive package to debt_model. Let’s interpret the three values in the estimate column. First, the intercept value is -$385.179. This intercept represents the credit card debt for an individual who has credit_limit of $0 and income of $0. In our data, however, the intercept has no practical interpretation since no individuals had credit_limit or income values of $0. Rather, the intercept is used to situate the regression plane in 3D space. Second, the credit_limit value is $0.264. Taking into account all the other explanatory variables in our model, for every increase of one dollar in credit_limit, there is an associated increase of on average $0.26 in credit card debt. Just as we did in Subsection 11.1.2, we are cautious not to imply causality as we saw in Subsection 11.3.1 that “correlation is not necessarily causation.” We do this merely stating there was an associated increase. Furthermore, we preface our interpretation with the statement, “taking into account all the other explanatory variables in our model.” Here, by all other explanatory variables we mean income. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. Third, income = -$7.66. Taking into account all other explanatory variables in our model, for every increase of one unit of income ($1000 in actual income), there is an associated decrease of, on average, $7.66 in credit card debt. Putting these results together, the equation of the regression plane that gives us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{debt}}\\) is: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2\\\\ \\widehat{\\text{debt}} &amp;= b_0 + b_{\\text{limit}} \\cdot \\text{limit} + b_{\\text{income}} \\cdot \\text{income}\\\\ &amp;= -385.179 + 0.263 \\cdot\\text{limit} - 7.663 \\cdot\\text{income} \\end{aligned} \\] Recall however in the right-hand plot of Figure 12.5 that when plotting the relationship between debt and income in isolation, there appeared to be a positive relationship. In the last discussed multiple regression, however, when jointly modeling the relationship between debt, credit_limit, and income, there appears to be a negative relationship of debt and income as evidenced by the negative slope for income of -$7.663. What explains these contradictory results? A phenomenon known as Simpson’s Paradox, whereby overall trends that exist in aggregate either disappear or reverse when the data are broken down into groups. In Subsection 12.3.3 we elaborate on this idea by looking at the relationship between credit_limit and credit card debt, but split along different income brackets. 12.2.3 Observed/fitted values and residuals Let’s also compute all fitted values and residuals for our regression model using the get_regression_points() function and present only the first 10 rows of output in Table 12.13. Remember that the coordinates of each of the blue points in our 3D scatterplot in Figure 12.6 can be found in the income, credit_limit, and debt columns. The fitted values on the regression plane are found in the debt_hat column and are computed using our equation for the regression plane in the previous section: \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{debt}} &amp;= -385.179 + 0.263 \\cdot \\text{limit} - 7.663 \\cdot \\text{income} \\end{aligned} \\] get_regression_points(debt_model) TABLE 12.13: Regression points (First 10 credit card holders out of 400) ID debt credit_limit income debt_hat residual 1 333 3606 14.9 454 -120.8 2 903 6645 106.0 559 344.3 3 580 7075 104.6 683 -103.4 4 964 9504 148.9 986 -21.7 5 331 4897 55.9 481 -150.0 6 1151 8047 80.2 1127 23.6 7 203 3388 21.0 349 -146.4 8 872 7114 71.4 948 -76.0 9 279 3300 15.1 371 -92.2 10 1350 6819 71.1 873 477.3 12.3 Related topics 12.3.1 Model selection When should we use an interaction model versus a parallel slopes model? Recall in Sections 12.1.2 and 12.1.3 we fit both interaction and parallel slopes models for the outcome variable \\(y\\) (teaching score) using a numerical explanatory variable \\(x_1\\) (age) and a categorical explanatory variable \\(x_2\\) (gender recorded as a binary variable). We compared these models in Figure 12.3, which we display again now. FIGURE 12.7: Previously seen comparison of interaction and parallel slopes models. A lot of you might have asked yourselves: “Why would I force the lines to have parallel slopes (as seen in the right-hand plot) when they clearly have different slopes (as seen in the left-hand plot)?”. The answer lies in a philosophical principle known as “Occam’s Razor.” It states that, “all other things being equal, simpler solutions are more likely to be correct than complex ones.” When viewed in a modeling framework, Occam’s Razor can be restated as, “all other things being equal, simpler models are to be preferred over complex ones.” In other words, we should only favor the more complex model if the additional complexity is warranted. Let’s revisit the equations for the regression line for both the interaction and parallel slopes model: \\[ \\begin{aligned} \\text{Interaction} &amp;: \\widehat{y} = \\widehat{\\text{score}} = b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) + \\\\ &amp; \\qquad b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}\\\\ \\text{Parallel slopes} &amp;: \\widehat{y} = \\widehat{\\text{score}} = b_0 + b_{\\text{age}} \\cdot \\text{age} + b_{\\text{male}} \\cdot \\mathbb{1}_{\\text{is male}}(x) \\end{aligned} \\] The interaction model is “more complex” in that there is an additional \\(b_{\\text{age,male}} \\cdot \\text{age} \\cdot \\mathbb{1}_{\\text{is male}}\\) interaction term in the equation not present for the parallel slopes model. Or viewed alternatively, the regression table for the interaction model in Table 12.4 has four rows, whereas the regression table for the parallel slopes model in Table 12.6 has three rows. The question becomes: “Is this additional complexity warranted?”. In this case, it can be argued that this additional complexity is warranted, as evidenced by the clear x-shaped pattern of the two regression lines in the left-hand plot of Figure 12.7. However, let’s consider an example where the additional complexity might not be warranted. Let’s consider the MA_schools data included in the moderndive package which contains 2017 data on Massachusetts public high schools provided by the Massachusetts Department of Education. For more details, read the help file for this data by running ?MA_schools in the console. Let’s model the numerical outcome variable \\(y\\), average SAT math score for a given high school, as a function of two explanatory variables: A numerical explanatory variable \\(x_1\\), the percentage of that high school’s student body that are economically disadvantaged and A categorical explanatory variable \\(x_2\\), the school size as measured by enrollment: small (13-341 students), medium (342-541 students), and large (542-4264 students). Let’s create visualizations of both the interaction and parallel slopes model once again and display the output in Figure 12.8. Recall from Subsection 12.1.3 that the geom_parallel_slopes() function is a special purpose function included in the moderndive package, since the geom_smooth() method in the ggplot2 package does not have a convenient way to plot parallel slopes models. # Interaction model ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Percent economically disadvantaged&quot;, y = &quot;Math SAT Score&quot;, color = &quot;School size&quot;, title = &quot;Interaction model&quot;) # Parallel slopes model ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size)) + geom_point(alpha = 0.25) + geom_parallel_slopes(se = FALSE) + labs(x = &quot;Percent economically disadvantaged&quot;, y = &quot;Math SAT Score&quot;, color = &quot;School size&quot;, title = &quot;Parallel slopes model&quot;) FIGURE 12.8: Comparison of interaction and parallel slopes models for Massachusetts schools. Look closely at the left-hand plot of Figure 12.8 corresponding to an interaction model. While the slopes are indeed different, they do not differ by much and are nearly identical. Now compare the left-hand plot with the right-hand plot corresponding to a parallel slopes model. The two models don’t appear all that different. So in this case, it can be argued that the additional complexity of the interaction model is not warranted. Thus following Occam’s Razor, we should prefer the “simpler” parallel slopes model. Let’s explicitly define what “simpler” means in this case. Let’s compare the regression tables for the interaction and parallel slopes models in Tables 12.14 and 12.15. model_2_interaction &lt;- lm(average_sat_math ~ perc_disadvan * size, data = MA_schools) get_regression_table(model_2_interaction) TABLE 12.14: Interaction model regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 594.327 13.288 44.726 0.000 568.186 620.469 perc_disadvan -2.932 0.294 -9.961 0.000 -3.511 -2.353 sizemedium -17.764 15.827 -1.122 0.263 -48.899 13.371 sizelarge -13.293 13.813 -0.962 0.337 -40.466 13.880 perc_disadvan:sizemedium 0.146 0.371 0.393 0.694 -0.585 0.877 perc_disadvan:sizelarge 0.189 0.323 0.586 0.559 -0.446 0.824 model_2_parallel_slopes &lt;- lm(average_sat_math ~ perc_disadvan + size, data = MA_schools) get_regression_table(model_2_parallel_slopes) TABLE 12.15: Parallel slopes regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 588.19 7.607 77.325 0.000 573.23 603.15 perc_disadvan -2.78 0.106 -26.120 0.000 -2.99 -2.57 sizemedium -11.91 7.535 -1.581 0.115 -26.74 2.91 sizelarge -6.36 6.923 -0.919 0.359 -19.98 7.26 Observe how the regression table for the interaction model has 2 more rows (6 versus 4). This reflects the additional “complexity” of the interaction model over the parallel slopes model. Furthermore, note in Table 12.14 how the offsets for the slopes perc_disadvan:sizemedium being 0.146 and perc_disadvan:sizelarge being 0.189 are small relative to the slope for the baseline group of small schools of \\(-2.932\\). In other words, all three slopes are similarly negative: \\(-2.932\\) for small schools, \\(-2.786\\) \\((=-2.932 + 0.146)\\) for medium schools, and \\(-2.743\\) \\((=-2.932 + 0.189)\\) for large schools. These results are suggesting that irrespective of school size, the relationship between average math SAT scores and the percent of the student body that is economically disadvantaged is similar and, alas, quite negative. What you have just performed is a rudimentary model selection: choosing which model fits data best among a set of candidate models. While the model selection approach we just took was visual in nature and hence somewhat qualitative, more statistically rigorous methods for model selection exist in the fields of multiple regression and statistical/machine learning. 12.3.2 Correlation coefficient Recall from Table 12.11 that the correlation coefficient between income in thousands of dollars and credit card debt was 0.464. What if instead we looked at the correlation coefficient between income and credit card debt, but where income was in dollars and not thousands of dollars? This can be done by multiplying income by 1000. credit_ch6 %&gt;% select(debt, income) %&gt;% mutate(income = income * 1000) %&gt;% cor() TABLE 12.16: Correlation between income (in dollars) and credit card debt debt income debt 1.000 0.464 income 0.464 1.000 We see it is the same! We say that the correlation coefficient is invariant to linear transformations. The correlation between \\(x\\) and \\(y\\) will be the same as the correlation between \\(a\\cdot x + b\\) and \\(y\\) for any numerical values \\(a\\) and \\(b\\). 12.3.3 Simpson’s Paradox Recall in Section 12.2, we saw the two seemingly contradictory results when studying the relationship between credit card debt and income. On the one hand, the right hand plot of Figure 12.5 suggested that the relationship between credit card debt and income was positive. We re-display this in Figure 12.9. FIGURE 12.9: Relationship between credit card debt and income. On the other hand, the multiple regression results in Table 12.12 suggested that the relationship between debt and income was negative. We re-display this information in Table 12.17. TABLE 12.17: Multiple regression results term estimate std_error statistic p_value lower_ci upper_ci intercept -385.179 19.465 -19.8 0 -423.446 -346.912 credit_limit 0.264 0.006 45.0 0 0.253 0.276 income -7.663 0.385 -19.9 0 -8.420 -6.906 Observe how the slope for income is \\(-7.663\\) and, most importantly for now, it is negative. This contradicts our observation in Figure 12.9 that the relationship is positive. How can this be? Recall the interpretation of the slope for income in the context of a multiple regression model: taking into account all the other explanatory variables in our model, for every increase of one unit in income (i.e., $1000), there is an associated decrease of on average $7.663 in debt. In other words, while in isolation, the relationship between debt and income may be positive, when taking into account credit_limit as well, this relationship becomes negative. These seemingly paradoxical results are due to a phenomenon aptly named Simpson’s Paradox. Simpson’s Paradox occurs when trends that exist for the data in aggregate either disappear or reverse when the data are broken down into groups. Let’s show how Simpson’s Paradox manifests itself in the credit_ch6 data. Let’s first visualize the distribution of the numerical explanatory variable credit_limit with a histogram in Figure 12.10. FIGURE 12.10: Histogram of credit limits and brackets. The vertical dashed lines are the quartiles that cut up the variable credit_limit into four equally sized groups. Let’s think of these quartiles as converting our numerical variable credit_limit into a categorical variable “credit_limit bracket” with four levels. This means that 25% of credit limits were between $0 and $3088. Let’s assign these 100 people to the “low” credit_limit bracket. 25% of credit limits were between $3088 and $4622. Let’s assign these 100 people to the “medium-low” credit_limit bracket. 25% of credit limits were between $4622 and $5873. Let’s assign these 100 people to the “medium-high” credit_limit bracket. 25% of credit limits were over $5873. Let’s assign these 100 people to the “high” credit_limit bracket. Now in Figure 12.11 let’s re-display two versions of the scatterplot of debt and income from Figure 12.9, but with a slight twist: The left-hand plot shows the regular scatterplot and the single regression line, just as you saw in Figure 12.9. The right-hand plot shows the colored scatterplot, where the color aesthetic is mapped to “credit_limit bracket.” Furthermore, there are now four separate regression lines. In other words, the location of the 400 points are the same in both scatterplots, but the right-hand plot shows an additional variable of information: credit_limit bracket. FIGURE 12.11: Relationship between credit card debt and income by credit limit bracket. The left-hand plot of Figure 12.11 focuses on the relationship between debt and income in aggregate. It is suggesting that overall there exists a positive relationship between debt and income. However, the right-hand plot of Figure 12.11 focuses on the relationship between debt and income broken down by credit_limit bracket. In other words, we focus on four separate relationships between debt and income: one for the “low” credit_limit bracket, one for the “medium-low” credit_limit bracket, and so on. Observe in the right-hand plot that the relationship between debt and income is clearly negative for the “medium-low” and “medium-high” credit_limit brackets, while the relationship is somewhat flat for the “low” credit_limit bracket. The only credit_limit bracket where the relationship remains positive is for the “high” credit_limit bracket. However, this relationship is less positive than in the relationship in aggregate, since the slope is shallower than the slope of the regression line in the left-hand plot. In this example of Simpson’s Paradox, the credit_limit is a confounding variable of the relationship between credit card debt and income as we defined in Subsection 11.3.1. Thus, credit_limit needs to be accounted for in any appropriate model for the relationship between debt and income. 12.4 Case study: Seattle house prices Kaggle.com is a machine learning and predictive modeling competition website that hosts datasets uploaded by companies, governmental organizations, and other individuals. One of their datasets is the “House Sales in King County, USA”. It consists of sale prices of homes sold between May 2014 and May 2015 in King County, Washington, USA, which includes the greater Seattle metropolitan area. This dataset is in the house_prices data frame included in the moderndive package. The dataset consists of 21,613 houses and 21 variables describing these houses (for a full list and description of these variables, see the help file by running ?house_prices in the console). In this case study, we’ll create a multiple regression model where: The outcome variable \\(y\\) is the sale price of houses. Two explanatory variables: A numerical explanatory variable \\(x_1\\): house size sqft_living as measured in square feet of living space. Note that 1 square foot is about 0.09 square meters. A categorical explanatory variable \\(x_2\\): house condition, a categorical variable with five levels where 1 indicates “poor” and 5 indicates “excellent.” 12.4.1 Exploratory data analysis: Part I As we’ve said numerous times throughout this book, a crucial first step when presented with data is to perform an exploratory data analysis (EDA). Exploratory data analysis can give you a sense of your data, help identify issues with your data, bring to light any outliers, and help inform model construction. Recall the three common steps in an exploratory data analysis we introduced in Subsection 11.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. First, let’s look at the raw data using View() to bring up RStudio’s spreadsheet viewer and the glimpse() function from the dplyr package: View(house_prices) glimpse(house_prices) Observations: 21,613 Variables: 21 $ id &lt;chr&gt; &quot;7129300520&quot;, &quot;6414100192&quot;, &quot;5631500400&quot;, &quot;2487200875&quot;,… $ date &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-0… $ price &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500… $ bedrooms &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2… $ bathrooms &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2… $ sqft_living &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 18… $ sqft_lot &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470… $ floors &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, … $ waterfront &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,… $ view &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0… $ condition &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4… $ grade &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, … $ sqft_above &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 18… $ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0,… $ yr_built &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 2… $ yr_renovated &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… $ zipcode &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198,… $ lat &lt;dbl&gt; 47.5, 47.7, 47.7, 47.5, 47.6, 47.7, 47.3, 47.4, 47.5, 4… $ long &lt;dbl&gt; -122, -122, -122, -122, -122, -122, -122, -122, -122, -… $ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 2… $ sqft_lot15 &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113,… Here are some questions you can ask yourself at this stage of an EDA: Which variables are numerical? Which are categorical? For the categorical variables, what are their levels? Besides the variables we’ll be using in our regression model, what other variables do you think would be useful to use in a model for house price? Observe, for example, that while the condition variable has values 1 through 5, these are saved in R as fct standing for “factors.” This is one of R’s ways of saving categorical variables. So you should think of these as the “labels” 1 through 5 and not the numerical values 1 through 5. Let’s now perform the second step in an EDA: computing summary statistics. Recall from Section 4.3 that summary statistics are single numerical values that summarize a large number of values. Examples of summary statistics include the mean, the median, the standard deviation, and various percentiles. We could do this using the summarize() function in the dplyr package along with R’s built-in summary functions, like mean() and median(). However, recall in Section 4.5, we saw the following code that computes a variety of summary statistics of the variable gain, which is the amount of time that a flight makes up mid-air: gain_summary &lt;- flights %&gt;% summarize( min = min(gain, na.rm = TRUE), q1 = quantile(gain, 0.25, na.rm = TRUE), median = quantile(gain, 0.5, na.rm = TRUE), q3 = quantile(gain, 0.75, na.rm = TRUE), max = max(gain, na.rm = TRUE), mean = mean(gain, na.rm = TRUE), sd = sd(gain, na.rm = TRUE), missing = sum(is.na(gain)) ) To repeat this for all three price, sqft_living, and condition variables would be tedious to code up. So instead, let’s use the convenient skim() function from the skimr package we first used in Subsection 12.1.1, being sure to only select() the variables of interest for our model: ## DDK: The current output does not include the p75 and p100 columns. Fixable? house_prices %&gt;% select(price, sqft_living, condition) %&gt;% skim() TABLE 12.18: Data summary Name Piped data Number of rows 21613 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts condition 0 1 FALSE 5 3: 14031, 4: 5679, 5: 1701, 2: 172 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist price 0 1 540088 367127 75000 321950 450000 645000 7700000 ▇▁▁▁▁ sqft_living 0 1 2080 918 290 1427 1910 2550 13540 ▇▂▁▁▁ Observe that the mean price of $540,088 is larger than the median of $450,000. This is because a small number of very expensive houses are inflating the average. In other words, there are “outlier” house prices in our dataset. (This fact will become even more apparent when we create our visualizations next.) However, the median is not as sensitive to such outlier house prices. This is why news about the real estate market generally report median house prices and not mean/average house prices. We say here that the median is more robust to outliers than the mean. Similarly, while both the standard deviation and interquartile-range (IQR) are both measures of spread and variability, the IQR is more robust to outliers. Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Let’s first create univariate visualizations. These are plots focusing on a single variable at a time. Since price and sqft_living are numerical variables, we can visualize their distributions using a geom_histogram() as seen in Section 2.4 on histograms. On the other hand, since condition is categorical, we can visualize its distribution using a geom_bar(). Recall from Section 2.7 on barplots that since condition is not “pre-counted”, we use a geom_bar() and not a geom_col(). # Histogram of house price: ggplot(house_prices, aes(x = price)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;price (USD)&quot;, title = &quot;House price&quot;) # Histogram of sqft_living: ggplot(house_prices, aes(x = sqft_living)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;living space (square feet)&quot;, title = &quot;House size&quot;) # Barplot of condition: ggplot(house_prices, aes(x = condition)) + geom_bar() + labs(x = &quot;condition&quot;, title = &quot;House condition&quot;) In Figure 12.12, we display all three of these visualizations at once. FIGURE 12.12: Exploratory visualizations of Seattle house prices data. First, observe in the bottom plot that most houses are of condition “3”, with a few more of conditions “4” and “5”, and almost none that are “1” or “2”. Next, observe in the histogram for price in the top-left plot that a majority of houses are less than two million dollars. Observe also that the x-axis stretches out to 8 million dollars, even though there does not appear to be any houses close to that price. This is because there are a very small number of houses with prices closer to 8 million. These are the outlier house prices we mentioned earlier. We say that the variable price is right-skewed as exhibited by the long right tail. Further, observe in the histogram of sqft_living in the middle plot as well that most houses appear to have less than 5000 square feet of living space. For comparison, a football field in the US is about 57,600 square feet, whereas a standard soccer/association football field is about 64,000 square feet. Observe also that this variable is also right-skewed, although not as drastically as the price variable. For both the price and sqft_living variables, the right-skew makes distinguishing houses at the lower end of the x-axis hard. This is because the scale of the x-axis is compressed by the small number of quite expensive and immensely-sized houses. So what can we do about this skew? Let’s apply a log10 transformation to these variables. If you are unfamiliar with such transformations, we highly recommend you read Appendix 7.7 on logarithmic (log) transformations. In summary, log transformations allow us to alter the scale of a variable to focus on multiplicative changes instead of additive changes. In other words, they shift the view to be on relative changes instead of absolute changes. Such multiplicative/relative changes are also called changes in orders of magnitude. Let’s create new log10 transformed versions of the right-skewed variable price and sqft_living using the mutate() function from Section 4.5, but we’ll give the latter the name log10_size, which is shorter and easier to understand than the name log10_sqft_living. house_prices &lt;- house_prices %&gt;% mutate( log10_price = log10(price), log10_size = log10(sqft_living) ) Let’s display the before and after effects of this transformation on these variables for only the first 10 rows of house_prices: house_prices %&gt;% select(price, log10_price, sqft_living, log10_size) # A tibble: 21,613 x 4 price log10_price sqft_living log10_size &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 221900 5.34616 1180 3.07188 2 538000 5.73078 2570 3.40993 3 180000 5.25527 770 2.88649 4 604000 5.78104 1960 3.29226 5 510000 5.70757 1680 3.22531 6 1225000 6.08814 5420 3.73400 7 257500 5.41078 1715 3.23426 8 291850 5.46516 1060 3.02531 9 229500 5.36078 1780 3.25042 10 323000 5.50920 1890 3.27646 # … with 21,603 more rows Observe in particular the houses in the sixth and third rows. The house in the sixth row has price $1,225,000, which is just above one million dollars. Since \\(10^6\\) is one million, its log10_price is around 6.09. Contrast this with all other houses with log10_price less than six, since they all have price less than $1,000,000. The house in the third row is the only house with sqft_living less than 1000. Since \\(1000 = 10^3\\), it’s the lone house with log10_size less than 3. Let’s now visualize the before and after effects of this transformation for price in Figure 12.13. # Before log10 transformation: ggplot(house_prices, aes(x = price)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;price (USD)&quot;, title = &quot;House price: Before&quot;) # After log10 transformation: ggplot(house_prices, aes(x = log10_price)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;log10 price (USD)&quot;, title = &quot;House price: After&quot;) FIGURE 12.13: House price before and after log10 transformation. Observe that after the transformation, the distribution is much less skewed, and in this case, more symmetric and more bell-shaped. Now you can more easily distinguish the lower priced houses. Let’s do the same for house size, where the variable sqft_living was log10 transformed to log10_size. # Before log10 transformation: ggplot(house_prices, aes(x = sqft_living)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;living space (square feet)&quot;, title = &quot;House size: Before&quot;) # After log10 transformation: ggplot(house_prices, aes(x = log10_size)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;log10 living space (square feet)&quot;, title = &quot;House size: After&quot;) FIGURE 12.14: House size before and after log10 transformation. Observe in Figure 12.14 that the log10 transformation has a similar effect of unskewing the variable. We emphasize that while in these two cases the resulting distributions are more symmetric and bell-shaped, this is not always necessarily the case. Given the now symmetric nature of log10_price and log10_size, we are going to revise our multiple regression model to use our new variables: The outcome variable \\(y\\) is the sale log10_price of houses. Two explanatory variables: A numerical explanatory variable \\(x_1\\): house size log10_size as measured in log base 10 square feet of living space. A categorical explanatory variable \\(x_2\\): house condition, a categorical variable with five levels where 1 indicates “poor” and 5 indicates “excellent.” 12.4.2 Exploratory data analysis: Part II Let’s now continue our EDA by creating multivariate visualizations. Unlike the univariate histograms and barplot in the earlier Figures 12.12, 12.13, and 12.14, multivariate visualizations show relationships between more than one variable. This is an important step of an EDA to perform since the goal of modeling is to explore relationships between variables. Since our model involves a numerical outcome variable, a numerical explanatory variable, and a categorical explanatory variable, we are in a similar regression modeling situation as in Section 12.1 where we studied the UT Austin teaching scores dataset. Recall in that case the numerical outcome variable was teaching score, the numerical explanatory variable was instructor age, and the categorical explanatory variable was (binary) gender. We thus have two choices of models we can fit: either (1) an interaction model where the regression line for each condition level will have both a different slope and a different intercept or (2) a parallel slopes model where the regression line for each condition level will have the same slope but different intercepts. Recall from Subsection 12.1.3 that the geom_parallel_slopes() function is a special purpose function that Evgeni Chasnovski created and included in the moderndive package, since the geom_smooth() method in the ggplot2 package does not have a convenient way to plot parallel slopes models. We plot both resulting models in Figure 12.15, with the interaction model on the left. # Plot interaction model ggplot(house_prices, aes(x = log10_size, y = log10_price, col = condition)) + geom_point(alpha = 0.05) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(y = &quot;log10 price&quot;, x = &quot;log10 size&quot;, title = &quot;House prices in Seattle&quot;) # Plot parallel slopes model ggplot(house_prices, aes(x = log10_size, y = log10_price, col = condition)) + geom_point(alpha = 0.05) + geom_parallel_slopes(se = FALSE) + labs(y = &quot;log10 price&quot;, x = &quot;log10 size&quot;, title = &quot;House prices in Seattle&quot;) FIGURE 12.15: Interaction and parallel slopes models. In both cases, we see there is a positive relationship between house price and size, meaning as houses are larger, they tend to be more expensive. Furthermore, in both plots it seems that houses of condition 5 tend to be the most expensive for most house sizes as evidenced by the fact that the line for condition 5 is highest, followed by conditions 4 and 3. As for conditions 1 and 2, this pattern isn’t as clear. Recall from the univariate barplot of condition in Figure 12.12, there are only a few houses of condition 1 or 2. Let’s also show a faceted version of just the interaction model in Figure 12.16. It is now much more apparent just how few houses are of condition 1 or 2. ggplot(house_prices, aes(x = log10_size, y = log10_price, col = condition)) + geom_point(alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(y = &quot;log10 price&quot;, x = &quot;log10 size&quot;, title = &quot;House prices in Seattle&quot;) + facet_wrap(~ condition) FIGURE 12.16: Faceted plot of interaction model. Which exploratory visualization of the interaction model is better, the one in the left-hand plot of Figure 12.15 or the faceted version in Figure 12.16? There is no universal right answer. You need to make a choice depending on what you want to convey, and own that choice, with including and discussing both also as an option as needed. 12.4.3 Regression modeling Which of the two models in Figure 12.15 is “better”? The interaction model in the left-hand plot or the parallel slopes model in the right-hand plot? We had a similar discussion in Subsection 12.3.1 on model selection. Recall that we stated that we should only favor more complex models if the additional complexity is warranted. In this case, the more complex model is the interaction model since it considers five intercepts and five slopes total. This is in contrast to the parallel slopes model which considers five intercepts but only one common slope. Is the additional complexity of the interaction model warranted? Looking at the left-hand plot in Figure 12.15, we’re of the opinion that it is, as evidenced by the slight x-like pattern to some of the lines. Therefore, we’ll focus the rest of this analysis only on the interaction model. This visual approach is somewhat subjective, however, so feel free to disagree! What are the five different slopes and five different intercepts for the interaction model? We can obtain these values from the regression table. Recall our two-step process for getting the regression table: # Fit regression model: price_interaction &lt;- lm(log10_price ~ log10_size * condition, data = house_prices) # Get regression table: get_regression_table(price_interaction) TABLE 12.19: Regression table for interaction model term estimate std_error statistic p_value lower_ci upper_ci intercept 3.330 0.451 7.380 0.000 2.446 4.215 log10_size 0.690 0.148 4.652 0.000 0.399 0.980 condition2 0.047 0.498 0.094 0.925 -0.930 1.024 condition3 -0.367 0.452 -0.812 0.417 -1.253 0.519 condition4 -0.398 0.453 -0.879 0.380 -1.286 0.490 condition5 -0.883 0.457 -1.931 0.053 -1.779 0.013 log10_size:condition2 -0.024 0.163 -0.148 0.882 -0.344 0.295 log10_size:condition3 0.133 0.148 0.893 0.372 -0.158 0.424 log10_size:condition4 0.146 0.149 0.979 0.328 -0.146 0.437 log10_size:condition5 0.310 0.150 2.067 0.039 0.016 0.604 Recall we saw in Subsection 12.1.2 how to interpret a regression table when there are both numerical and categorical explanatory variables. Let’s now do the same for all 10 values in the estimate column of Table 12.19. In this case, the “baseline for comparison” group for the categorical variable condition are the condition 1 houses, since “1” comes first alphanumerically. Thus, the intercept and log10_size values are the intercept and slope for log10_size for this baseline group. Next, the condition2 through condition5 terms are the offsets in intercepts relative to the condition 1 intercept. Finally, the log10_size:condition2 through log10_size:condition5 are the offsets in slopes for log10_size relative to the condition 1 slope for log10_size. Let’s simplify this by writing out the equation of each of the five regression lines using these 10 estimate values. We’ll write out each line in the following format: \\[ \\widehat{\\log10(\\text{price})} = \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{size}} \\cdot \\log10(\\text{size}) \\] Condition 1: \\[\\widehat{\\log10(\\text{price})} = 3.33 + 0.69 \\cdot \\log10(\\text{size})\\] Condition 2: \\[ \\begin{aligned} \\widehat{\\log10(\\text{price})} &amp;= (3.33 + 0.047) + (0.69 - 0.024) \\cdot \\log10(\\text{size}) \\\\ &amp;= 3.377 + 0.666 \\cdot \\log10(\\text{size}) \\end{aligned} \\] Condition 3: \\[ \\begin{aligned} \\widehat{\\log10(\\text{price})} &amp;= (3.33 - 0.367) + (0.69 + 0.133) \\cdot \\log10(\\text{size}) \\\\ &amp;= 2.963 + 0.823 \\cdot \\log10(\\text{size}) \\end{aligned} \\] Condition 4: \\[ \\begin{aligned} \\widehat{\\log10(\\text{price})} &amp;= (3.33 - 0.398) + (0.69 + 0.146) \\cdot \\log10(\\text{size}) \\\\ &amp;= 2.932 + 0.836 \\cdot \\log10(\\text{size}) \\end{aligned} \\] Condition 5: \\[ \\begin{aligned} \\widehat{\\log10(\\text{price})} &amp;= (3.33 - 0.883) + (0.69 + 0.31) \\cdot \\log10(\\text{size}) \\\\ &amp;= 2.447 + 1 \\cdot \\log10(\\text{size}) \\end{aligned} \\] These correspond to the regression lines in the left-hand plot of Figure 12.15 and the faceted plot in Figure 12.16. For homes of all five condition types, as the size of the house increases, the price increases. This is what most would expect. However, the rate of increase of price with size is fastest for the homes with conditions 3, 4, and 5 of 0.823, 0.836, and 1, respectively. These are the three largest slopes out of the five. 12.4.4 Making predictions Say you’re a realtor and someone calls you asking you how much their home will sell for. They tell you that it’s in condition = 5 and is sized 1900 square feet. What do you tell them? Let’s use the interaction model we fit to make predictions! We first make this prediction visually in Figure 12.17. The predicted log10_price of this house is marked with a black dot. This is where the following two lines intersect: The regression line for the condition = 5 homes and The vertical dashed black line at log10_size equals 3.28, since our predictor variable is the log10 transformed square feet of living space of \\(\\log10(1900) = 3.28\\). FIGURE 12.17: Interaction model with prediction. Eyeballing it, it seems the predicted log10_price seems to be around 5.75. Let’s now obtain the exact numerical value for the prediction using the equation of the regression line for the condition = 5 houses, being sure to log10() the square footage first. 2.45 + 1 * log10(1900) [1] 5.73 This value is very close to our earlier visually made prediction of 5.75. But wait! Is our prediction for the price of this house $5.75? No! Remember that we are using log10_price as our outcome variable! So, if we want a prediction in dollar units of price, we need to unlog this by taking a power of 10 as described in Appendix 7.7. 10^(2.45 + 1 * log10(1900)) [1] 535493 So our predicted price for this home of condition 5 and of size 1900 square feet is $535,493. 12.5 Case study: Effective data storytelling As we’ve progressed throughout this book, you’ve seen how to work with data in a variety of ways. You’ve learned effective strategies for plotting data by understanding which types of plots work best for which combinations of variable types. You’ve summarized data in spreadsheet form and calculated summary statistics for a variety of different variables. Furthermore, you’ve seen the value of statistical inference as a process to come to conclusions about a population by using sampling. Lastly, you’ve explored how to fit linear regression models and the importance of checking the conditions required so that all confidence intervals and hypothesis tests have valid interpretation. All throughout, you’ve learned many computational techniques and focused on writing R code that’s reproducible. We now present another set of case studies, but this time on the “effective data storytelling” done by data journalists around the world. Great data stories don’t mislead the reader, but rather engulf them in understanding the importance that data plays in our lives through storytelling. 12.5.1 Bechdel test for Hollywood gender representation We recommend you read and analyze Walt Hickey’s FiveThirtyEight.com article, “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women.” In it, Walt completed a multidecade study of how many movies pass the Bechdel test, an informal test of gender representation in a movie that was created by Alison Bechdel. As you read over the article, think carefully about how Walt Hickey is using data, graphics, and analyses to tell the reader a story. In the spirit of reproducibility, FiveThirtyEight have also shared the data and R code that they used for this article. You can also find the data used in many more of their articles on their GitHub page. ModernDive co-authors Chester Ismay and Albert Y. Kim along with Jennifer Chunn went one step further by creating the fivethirtyeight package which provides access to these datasets more easily in R. For a complete list of all 127 datasets included in the fivethirtyeight package, check out the package webpage at https://fivethirtyeight-r.netlify.com/articles/fivethirtyeight.html. Furthermore, example “vignettes” of fully reproducible start-to-finish analyses of some of these data using dplyr, ggplot2, and other packages in the tidyverse are available here. For example, a vignette showing how to reproduce one of the plots at the end of the article on the Bechdel test is available here. 12.5.2 US Births in 1999 The US_births_1994_2003 data frame included in the fivethirtyeight package provides information about the number of daily births in the United States between 1994 and 2003. For more information on this data frame including a link to the original article on FiveThirtyEight.com, check out the help file by running ?US_births_1994_2003 in the console. It’s always a good idea to preview your data, either by using RStudio’s spreadsheet View() function or using glimpse() from the dplyr package: glimpse(US_births_1994_2003) Observations: 3,652 Variables: 6 $ year &lt;int&gt; 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1… $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ date_of_month &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … $ date &lt;date&gt; 1994-01-01, 1994-01-02, 1994-01-03, 1994-01-04, 1994-0… $ day_of_week &lt;ord&gt; Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, Mon, Tu… $ births &lt;int&gt; 8096, 7772, 10142, 11248, 11053, 11406, 11251, 8653, 79… We’ll focus on the number of births for each date, but only for births that occurred in 1999. Recall from Section 4.2 we can do this using the filter() function from the dplyr package: US_births_1999 &lt;- US_births_1994_2003 %&gt;% filter(year == 1999) As discussed in Section 2.3, since date is a notion of time and thus has sequential ordering to it, a linegraph would be a more appropriate visualization to use than a scatterplot. In other words, we should use a geom_line() instead of geom_point(). Recall that such plots are called time series plots. ggplot(US_births_1999, aes(x = date, y = births)) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Number of births&quot;, title = &quot;US Births in 1999&quot;) FIGURE 12.18: Number of births in the US in 1999. We see a big dip occurring just before January 1st, 2000, most likely due to the holiday season. However, what about the large spike of over 14,000 births occurring just before October 1st, 1999? What could be the reason for this anomalously high spike? Let’s sort the rows of US_births_1999 in descending order of the number of births. Recall from Section 4.6 that we can use the arrange() function from the dplyr function to do this, making sure to sort births in descending order: US_births_1999 %&gt;% arrange(desc(births)) # A tibble: 365 x 6 year month date_of_month date day_of_week births &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;int&gt; 1 1999 9 9 1999-09-09 Thurs 14540 2 1999 12 21 1999-12-21 Tues 13508 3 1999 9 8 1999-09-08 Wed 13437 4 1999 9 21 1999-09-21 Tues 13384 5 1999 9 28 1999-09-28 Tues 13358 6 1999 7 7 1999-07-07 Wed 13343 7 1999 7 8 1999-07-08 Thurs 13245 8 1999 8 17 1999-08-17 Tues 13201 9 1999 9 10 1999-09-10 Fri 13181 10 1999 12 28 1999-12-28 Tues 13158 # … with 355 more rows The date with the highest number of births (14,540) is in fact 1999-09-09. If we write down this date in month/day/year format (a standard format in the US), the date with the highest number of births is 9/9/99! All nines! Could it be that parents deliberately induced labor at a higher rate on this date? Maybe? Whatever the cause may be, this fact makes a fun story! Time to think with data and further tell your story with data! How could statistical modeling help you here? What types of statistical inference would be helpful? What else can you find and where can you take this analysis? What assumptions did you make in this analysis? We leave these questions to you as the reader to explore and examine. 12.6 Conclusion 12.6.1 Additional resources 12.6.2 What’s to come? Congratulations! We’ve completed the “Data Modeling with moderndive” portion of this book. We’re ready to proceed to Part III of this book: “Statistical Inference with infer.” Statistical inference is the science of inferring about some unknown quantity using sampling. For example, among the most well-known examples of sampling involves polls. Because asking an entire population about their opinions would be a long and arduous task, pollsters often take a smaller sample that is hopefully representative of the population. Based on the results of this sample, pollsters hope to make claims about the entire population. Once we’ve covered Chapters 9 on sampling, 10 on confidence intervals, and ?? on hypothesis testing, we’ll revisit the regression models we studied in Chapters 11 and 12 in Chapter ?? on inference for regression. So far, we’ve only studied the estimate column of all our regression tables. The next four chapters focus on what the remaining columns mean: the standard error (std_error), the test statistic, the p_value, and the lower and upper bounds of confidence intervals (lower_ci and upper_ci). Furthermore in Chapter ??, we’ll revisit the concept of residuals \\(y - \\widehat{y}\\) and discuss their importance when interpreting the results of a regression model. We’ll perform what is known as a residual analysis of the residual variable of all get_regression_points() outputs. Residual analyses allow you to verify what are known as the conditions for inference for regression. On to Chapter 9 on sampling in Part III as shown in Figure 12.19! FIGURE 12.19: ModernDive flowchart - on to Part III! References "],
["13-classification.html", "Chapter 13 Classification 13.1 Learning Objectives 13.2 Introduction to Logistic Regression 13.3 Case Studies Overview 13.4 Case Study: Soccer Goalkeepers 13.5 Case Study: Reconstructing Alabama 13.6 Least Squares Regression vs. Logistic Regression 13.7 Case Study: Trying to Lose Weight 13.8 Classification and regression trees (CART) 13.9 Random forests", " Chapter 13 Classification \\[ \\newcommand{\\lik}{\\operatorname{Lik}} \\newcommand{\\Lik}{\\operatorname{Lik}} \\] 13.1 Learning Objectives Identify a binomial random variable and assess the validity of the binomial assumptions. Write a generalized linear model for binomial responses in two forms, one as a function of the logit and one as a function of \\(p\\). Explain how fitting a logistic regression differs from fitting an ordinary least squares (OLS) regression model. Interpret estimated coefficients in logistic regression. Differentiate between logistic regression models with binary and binomial responses. Use the residual deviance to compare models, to test for lack-of-fit when appropriate, and to check for unusual observations or needed transformations. 13.2 Introduction to Logistic Regression Logistic regression is characterized by research questions with binary (yes/no or success/failure) or binomial (number of yesses or successes in \\(n\\) trials) responses: Are students with poor grades more likely to binge drink? Is exposure to a particular chemical associated with a cancer diagnosis? Are the number of votes for a congressional candidate associated with the amount of campaign contributions? Binary Responses: Recall from Section ?? that binary responses take on only two values: success (Y=1) or failure (Y=0), Yes (Y=1) or No (Y=0), etc. Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success \\(p\\) based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure. Examples (a) and (b) above would be considered to have binary responses (Does a student binge drink? Was a patient diagnosed with cancer?), assuming that we have a unique set of covariates for each individual student or patient. Binomial Responses: Also recall from Section ?? that binomial responses are number of successes in \\(n\\) identical, independent trials with constant probability \\(p\\) of success. A sequence of independent trials like this with the same probability of success is called a Bernoulli process. As with binary responses, our objective in modeling binomial responses is to quantify how the probability of success, \\(p\\), is associated with relevant covariates. Example (c) above would be considered to have a binonial response, assuming we have vote totals at the congressional district level rather than information on individual voters. 13.2.1 Logistic Regression Assumptions Much like OLS, using logistic regression to make inferences requires model assumptions. Binary Response The response variable is dichotomous (two possible responses) or the sum of dichotomous responses. Independence The observations must be independent of one another. Variance Structure By definition, the variance of a binomial random variable is \\(np(1-p)\\), so that variability is highest when \\(p=.5\\). Linearity The log of the odds ratio, log(\\(\\frac{p}{1-p}\\)), must be a linear function of x. This will be explained further in the context of the first case study. 13.2.2 A Graphical Look at Logistic Regression FIGURE 13.1: Linear vs. Logistic regression models for binary response data. Figure 13.1 illustrates a data set with a binary (0 or 1) response (Y) and a single continuous predictor (X). The blue line is a linear regression fit with OLS to model the probability of a success (Y=1) for a given value of X. With a binary response, the line doesn’t fit the data well, and it produces predicted probabilities below 0 and above 1. On the other hand, the logistic regression fit (red curve) with its typical “S” shape follows the data closely and always produces predicted probabilities between 0 and 1. For these and several other reasons detailed in this chapter, we will focus on the following model for logistic regression with binary or binomial responses: \\[\\begin{equation} log(\\frac{p_i}{1-p_i})=\\beta_0+\\beta_1 x_i \\tag{13.1} \\end{equation}\\] where the observed values \\(Y_i \\sim\\) binomial with \\(p=p_i\\) for a given \\(x_i\\) and \\(n=1\\) for binary responses. 13.3 Case Studies Overview We consider three case studies in this chapter. The first two involve binomial responses (Soccer Goalkeepers and Reconstructing Alabama), while the last case uses a binary response (Trying to Lose Weight). Even though binary responses are much more common, their models have a very similar form to binomial responses, so the first two case studies will illustrate important principles that also apply to the binary case. Here are the statistical concepts you will encounter for each Case Study. The soccer goalkeeper data can be written in the form of a 2 \\(\\times\\) 2 table. This example is used to describe some of the underlying theory for logistic regression. We demonstrate how binomial probability mass functions (pmfs) can be written in one parameter exponential family form, from which we can identify the canonical link as in Chapter ??. Using the canonical link, we write a Generalized Linear Model for binomial counts and determine corresponding MLEs for model coefficients. Interpretation of the estimated parameters involves a fundamental concept, the odds ratio. The Reconstructing Alabama case study is another binomial example which introduces the notion of deviances, which are used to compare and assess models. We will check the assumptions of logistic regression using empirical logit plots and deviance residuals. The last case study addresses why teens try to lose weight. Here the response is a binary variable which allows us to analyze individual level data. The analysis builds on concepts from the previous sections in the context of a random sample from CDC’s Youth Risk Behavior Survey (YRBS). 13.4 Case Study: Soccer Goalkeepers Does the probability of a save in a soccer match depend upon whether the goalkeeper’s team is behind or not? (???) looked at penalty kicks in the men’s World Cup soccer championships from 1982 - 2010, and they assembled data on 204 penalty kicks during shootouts. The data for this study is summarized in Table 13.1. TABLE 13.1: Soccer goalkeepers’ saves when their team is and is not behind. Source: Roskes et al. 2011 Psychological Science Saves Scores Total Behind 2 22 24 Not Behind 39 141 180 Total 41 163 204 13.4.1 Modeling Odds Odds are one way to quantify a goalkeeper’s performance. Here the odds that a goalkeeper makes a save when his team is behind is 2 to 22 or 0.09 to 1. Or equivalently, the odds that a goal is scored on a penalty kick is 22 to 2 or 11 to 1. An odds of 11 to 1 tells you that a shooter whose team is ahead will score 11 times for every 1 shot that the goalkeeper saves. When the goalkeeper’s team is not behind the odds a goal is scored is 141 to 39 or 3.61 to 1. We see that the odds of a goal scored on a penalty kick are better when the goalkeeper’s team is behind than when it is not behind (i.e., better odds of scoring for the shooter when the shooter’s team is ahead). We can compare these odds by calculating the odds ratio (OR), 11/3.61 or 3.05, which tells us that the odds of a successful penalty kick are 3.05 times higher when the shooter’s team is leading. In our example, it is also possible to estimate the probability of a goal, \\(p\\), for either circumstance. When the goalkeeper’s team is behind, the probability of a successful penalty kick is \\(p\\) = 22/24 or 0.833. We can see that the ratio of the probability of a goal scored divided by the probability of no goal is \\((22/24)/(2/24)=22/2\\) or 11, the odds we had calculated above. The same calculation can be made when the goalkeeper’s team is not behind. In general, we now have several ways of finding the odds of success under certain circumstances: \\[\\textrm{Odds} = \\frac{\\# \\textrm{successes}}{\\# \\textrm{failures}}= \\frac{\\# \\textrm{successes}/n}{\\# \\textrm{failures}/n}= \\frac{p}{1-p}.\\] 13.4.2 Logistic Regression Models for Binomial Responses We would like to model the odds of success, however odds are strictly positive. Therefore, similar to modeling log(\\(\\lambda\\)) in Poisson regression, which allowed the response to take on values from \\(-\\infty\\) to \\(\\infty\\), we will model the log(odds), the logit, in logistic regression. Logits will be suitable for modeling with a linear function of the predictors: \\[\\begin{equation} \\log\\left(\\frac{p}{1 - p}\\right)=\\beta_0+\\beta_1X \\tag{13.2} \\end{equation}\\] Models of this form are referred to as binomial regression models, or more generally as logistic regression models. Here we provide intuition for using and interpreting logistic regression models, and then in the short optional section that follows, we present rationale for these models using GLM theory. In our example we could define \\(X=0\\) for not behind and \\(X=1\\) for behind and fit the model: \\[\\begin{equation} \\log\\left(\\frac{p_X}{1-p_X}\\right)=\\beta_0 +\\beta_1X \\tag{13.3} \\end{equation}\\] where \\(p_X\\) is the probability of a successful penalty kick given \\(X\\). So, based on this model, the log odds of a successful penalty kick when the goalkeeper’s team is not behind is: \\[ \\log\\left(\\frac{p_0}{1-p_0}\\right) =\\beta_0 \\nonumber, \\] and the log odds when the team is behind is: \\[ \\log\\left(\\frac{p_1}{1-p_1}\\right)=\\beta_0+\\beta_1. \\nonumber \\] We can see that \\(\\beta_1\\) is the difference between the log odds of a successful penalty kick between games when the goalkeeper’s team is behind and games when the team is not behind. Using rules of logs: \\[\\begin{equation} \\beta_1 = (\\beta_0 + \\beta_1) - \\beta_0 = \\log\\left(\\frac{p_1}{1-p_1}\\right) - \\log\\left(\\frac{p_0}{1-p_0}\\right) = \\log\\left(\\frac{p_1/(1-p_1)}{p_0/{(1-p_0)}}\\right). \\tag{13.4} \\end{equation}\\] Thus \\(e^{\\beta_1}\\) is the ratio of the odds of scoring when the goalkeeper’s team is not behind compared to scoring when the team is behind. In general, exponentiated coefficients in logistic regression are odds ratios (OR). A general interpretation of an OR is the odds of success for group A compared to the odds of success for group B—how many times greater the odds of success are in group A compared to group B. The logit model (Equation (13.3)) can also be re-written in a probability form: \\[\\begin{equation} p_X=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} \\tag{13.5} \\end{equation}\\] which can be re-written for games when the goalkeeper’s team is behind as: \\[\\begin{equation} p_1=\\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}} \\tag{13.6} \\end{equation}\\] and for games when the goalkeeper’s team is not behind as: \\[\\begin{equation} p_0=\\frac{e^{\\beta_0}}{1+e^{\\beta_0}} \\tag{13.7} \\end{equation}\\] We use likelihood methods to estimate \\(\\beta_0\\) and \\(\\beta_1\\). As we had done in Chapter ??, we can write the likelihood for this example in the following form: \\[\\Lik(p_1, p_0) = {28 \\choose 22}p_1^{22}(1-p_1)^{2} {180 \\choose 141}p_0^{141}(1-p_0)^{39}\\] Our interest centers on estimating \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), not \\(p_1\\) or \\(p_0\\). So we replace \\(p_1\\) in the likelihood with an expression for \\(p_1\\) in terms of \\(\\beta_0\\) and \\(\\beta_1\\) as in Equation (13.6). Similarly, \\(p_0\\) in Equation (13.7) involves only \\(\\beta_0\\). After removing constants, the new likelihood looks like: \\[\\begin{equation} \\Lik(\\beta_0,\\beta_1) \\propto \\left( \\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}}\\right)^{22}\\left(1- \\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}}\\right)^{2} \\left(\\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\right)^{141}\\left(1-\\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\right)^{39} \\tag{13.8} \\end{equation}\\] Now what? Fitting the model means finding estimates of \\(\\beta_0\\) and \\(\\beta_1\\), but familiar methods from calculus for maximizing the likelihood don’t work here. Instead, we consider all possible combinations of \\(\\beta_0\\) and \\(\\beta_1\\). That is, we will pick that pair of values for \\(\\beta_0\\) and \\(\\beta_1\\) that yield the largest likelihood for our data. Trial and error to find the best pair is tedious at best, but more efficient numerical methods are available. The MLEs for the coefficients in the soccer goalkeeper study are \\(\\hat{\\beta_0}= 1.2852\\) and \\(\\hat{\\beta_1}=1.1127\\). Exponentiating \\(\\hat{\\beta_1}\\) provides an estimate of the odds ratio (the odds of scoring when the goalkeeper’s team is behind compared to the odds of scoring when the team is not behind) of 3.04, which is consistent with our calculations using the 2 \\(\\times\\) 2 table. We estimate that the odds of scoring when the goalkeeper’s team is behind is over 3 times that of when the team is not behind or, in other words, the odds a shooter is successful in a penalty kick shootout are 3.04 times higher when his team is leading. Time out for study discussion (Optional). Discuss the following quote from the study abstract: “Because penalty takers shot at the two sides of the goal equally often, the goalkeepers’ right-oriented bias was dysfunctional, allowing more goals to be scored.” Construct an argument for why the greater success observed when the goalkeeper’s team was behind might be better explained from the shooter’s perspective. Before we go on, you may be curious as to why there is no error term in our model statements for logistic or Poisson regression. One way to look at it is to consider that all models describe how observed values are generated. With the logistic model we assume that the observations are generated as a binomial random variables. Each observation or realization of \\(Y\\) = number of successes in \\(n\\) independent and identical trials with a probability of success on any one trial of \\(p\\) is produced by \\(Y \\sim \\textrm{Binomial}(n,p)\\). So the randomness in this model is not introduced by an added error term but rather by appealing to a Binomial probability distribution, where variability depends only on \\(n\\) and \\(p\\) through \\(\\textrm{Var}(Y)=np(1-p)\\), and where \\(n\\) is usually considered fixed and \\(p\\) the parameter of interest. 13.4.3 Theoretical rationale for logistic regression models (Optional) Recall from Chapter ?? that generalized linear models (GLMs) are a way in which to model a variety of different types of responses. In this chapter, we apply the general results of GLMs to the specific application of binomial responses. Let \\(Y\\) = the number scored out of \\(n\\) penalty kicks. The parameter, \\(p\\), is the probability of a score on a single penalty kick. Recall that the theory of GLM is based on the unifying notion of the one-parameter exponential family form: \\[\\begin{equation} f(y;\\theta)=e^{[a(y)b(\\theta)+c(\\theta)+d(y)]} \\tag{13.9} \\end{equation}\\] To see that we can apply the general approach of GLMs to binomial responses, we first write an expression for the probability of a binomial response and then use a little algebra to rewrite it until we can demonstrate that it, too, can be written in one-parameter exponential family form with \\(\\theta = p\\). This will provide a way in which to specify the canonical link and the form for the model. Additional theory allows us to deduce the mean, standard deviation, and more from this form. If \\(Y\\) follows a binomial distribution with \\(n\\) trials and probability of success \\(p\\), we can write: \\[\\begin{eqnarray} P(Y=y)&amp;=&amp; \\binom{n}{y}p^y(1-p)^{(n-y)} \\\\ &amp;=&amp;e^{y\\log(p) + (n-y)\\log(1-p) + \\log\\binom{n}{y}} \\tag{13.10} \\end{eqnarray}\\] However, this probability mass function is not quite in one parameter exponential family form. Note that there are two terms in the exponent which consist of a product of functions of \\(y\\) and \\(p\\). So more simplification is in order: \\[\\begin{equation} P(Y=y) = e^{y\\log\\left(\\frac{p}{1-p}\\right) + n\\log(1-p)+ \\log\\binom{n}{y}} \\tag{13.11} \\end{equation}\\] Don’t forget to consider the support; we must make sure that the set of possible values for this response is not dependent upon \\(p\\). For fixed \\(n\\) and any value of \\(p\\), \\(0&lt;p&lt;1\\), all integer values from \\(0\\) to \\(n\\) are possible, so the support is indeed independent of \\(p\\). The one parameter exponential family form for binomial responses shows that the canonical link is \\(\\log\\left(\\frac{p}{1-p}\\right)\\). Thus, GLM theory suggests that constructing a model using the logit, the log odds of a score, as a linear function of covariates is a reasonable approach. 13.5 Case Study: Reconstructing Alabama This case study demonstrates how wide ranging applications of statistics can be. Many would not associate statistics with historical research, but this case study shows that it can be done. US Census data from 1870 helped historian Michael Fitzgerald of St. Olaf College gain insight into important questions about how railroads were supported during the Reconstruction Era. In a paper entitled “Reconstructing Alabama: Reconstruction Era Demographic and Statistical Research,” Ben Bayer performs an analysis of data from 1870 to explain influences on voting on referendums related to railroad subsidies (???). Positive votes are hypothesized to be inversely proportional to the distance a voter is from the proposed railroad, but the racial composition of a community (as measured by the percentage of blacks) is hypothesized to be associated with voting behavior as well. Separate analyses of three counties in Alabama—Hale, Clarke, and Dallas—were performed; we discuss Hale County here. This example differs from the soccer example in that it includes continuous covariates. Was voting on railroad referenda related to distance from the proposed railroad line and the racial composition of a community? 13.5.1 Data Organization The unit of observation for this data is a community in Hale County. We will focus on the following variables from RR_Data_Hale.csv collected for each community: YesVotes = the number of “Yes” votes in favor of the proposed railroad line (our primary response variable) NumVotes = total number of votes cast in the election pctBlack = the percentage of blacks in the community distance = the distance, in miles, the proposed railroad is from the community TABLE 13.2: Sample of the data for the Hale County, Alabama, railroad subsidy vote. community pctBlack distance YesVotes NumVotes Carthage 58.4 17 61 110 Cederville 92.4 7 0 15 Greensboro 59.4 0 1790 1804 Havana 58.4 12 16 68 13.5.2 Exploratory Analyses We first look at a coded scatterplot to see our data. Figure 13.2 portrays the relationship between distance and pctBlack coded by the InFavor status (whether a community supported the referendum with over 50% Yes votes). From this scatterplot, we can see that all of the communities in favor of the railroad referendum are over 55% black, and all of those opposed are 7 miles or farther from the proposed line. The overall percentage of voters in Hale County in favor of the railroad is 87.9%. FIGURE 13.2: Scatterplot of distance from a proposed rail line and percent black in the community coded by whether the community was in favor of the referendum or not. Recall that a model with two covariates has the form: \\[\\log(\\textrm{odds}) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0+\\beta_1X_1+\\beta_2X_2.\\] where \\(p\\) is the proportion of Yes votes in a community. In logistic regression, we expect the logits to be a linear function of \\(X\\), the predictors. To assess the linearity assumption, we construct empirical logit plots, where “empirical” means “based on sample data.” Empirical logits are computed for each community by taking \\(\\log\\left(\\frac{\\textrm{number of successes}}{\\textrm{number of failures}}\\right)\\). In Figure 13.3, we see that the plot of empirical logits versus distance produces a plot that looks linear, as needed for the logistic regression assumption. In contrast, the empirical logits by percent black reveal that Greensboro deviates quite a bit from the otherwise linear pattern; this suggests that Greensboro is an outlier and possibly an influential point. Greensboro has 99.2% voting yes with only 59.4% black. FIGURE 13.3: Empirical logit plots for the Railroad Referendum data. In addition to examining how the response correlates with the predictors, it is a good idea to determine whether the predictors correlate with one another. Here the correlation between distance and percent black is negative and moderately strong with \\(r = -0.49\\). We’ll watch to see if the correlation affects the stability of our odds ratio estimates. 13.5.3 Initial Models The first model includes only one covariate, distance. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 3.309 0.113 29.2 &lt;0.0000000000000002 *** distance -0.288 0.013 -22.1 &lt;0.0000000000000002 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 318.44 on 9 degrees of freedom AIC: 360.7 Our estimated binomial regression model is: \\[\\log\\left(\\frac{\\hat{p}_i}{1-\\hat{p}_i}\\right)=3.309-0.288 (\\textrm{distance}_i)\\] where \\(\\hat{p}_i\\) is the estimated proportion of Yes votes in community \\(i\\). The estimated odds ratio for distance, that is the exponentiated coefficient for distance, in this model is \\(e^{-0.288}=0.750\\). It can be interpreted as follows: for each additional mile from the proposed railroad, the support (odds of a Yes vote) declines by 25.0%. The covariate pctBlack is then added to the first model. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 4.2220 0.2970 14.22 &lt; 0.0000000000000002 *** distance -0.2917 0.0131 -22.27 &lt; 0.0000000000000002 *** pctBlack -0.0132 0.0039 -3.39 0.00069 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 307.22 on 8 degrees of freedom AIC: 351.5 Despite the somewhat strong negative correlation between percent black and distance, the estimated odds ratio for distance remains approximately the same in this new model (OR \\(= e^{-0.29} = 0.747\\)); controlling for percent black does little to change our estimate of the effect of distance. For each additional mile from the proposed railroad, odds of a Yes vote declines by 25.3% after adjusting for the racial composition of a community. We also see that, for a fixed distance from the proposed railroad, the odds of a Yes vote declines by 1.3% (OR \\(= e^{-.0132} = .987\\)) for each additional percent black in the community. 13.5.4 Tests for significance of model coefficients Do we have statistically significant evidence that support for the railroad referendum decreases with higher proportions of black residents in a community, after accounting for the distance a community is from the railroad line? As discussed in Section ?? with Poisson regression, there are two primary approaches to testing signficance of model coefficients: Drop-in-deviance test to compare models and Wald test for a single coefficient. With our larger model given by \\(\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0+\\beta_1(\\textrm{distance}_i)+\\beta_2(\\textrm{pctBlack}_i)\\), the Wald test produces a highly significant p-value (\\(Z=\\frac{-0.0132}{0.0039}= -3.394\\), \\(p=.00069\\)) indicating significant evidience that support for the railroad referendum decreases with higher proportions of black residents in a community, after adjusting for the distance a community is from the railroad line. The drop in deviance test would compare the larger model above to the reduced model \\(\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0+\\beta_1(\\textrm{distance}_i)\\) by comparing residual deviances from the two models. anova(model.HaleD, model.HaleBD, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 9 318 2 8 307 1 11.2 0.00081 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The drop-in-deviance test statistic is \\(318.44 - 307.22 = 11.22\\) on \\(9 - 8 = 1\\) df, producing a p-value of .00081, in close agreement with the Wald test. A third approach to determining significance of \\(\\beta_2\\) would be to generate a 95% confidence interval and then checking if 0 falls within the interval or, equivalently, if 1 falls within a 95% confidence interval for \\(e^{\\beta_2}.\\) The next section describes two approaches to producing a confidence interval for coefficients in logistic regression models. 13.5.5 Confidence intervals for model coefficients Since the Wald statistic follows a normal distribution with \\(n\\) large, we could generate a Wald-type (normal-based) confidence interval for \\(\\beta_2\\) using: \\[\\hat\\beta_2 \\pm 1.96\\cdot\\textrm{SE}(\\hat\\beta_2)\\] and then exponentiating endpoints if we prefer a confidence interval for the odds ratio \\(e^{\\beta_2}\\). In this case, \\[\\begin{eqnarray} 95\\% \\textrm{ CI for } &amp;\\beta_2 &amp; = &amp; \\hat\\beta_2 \\pm 1.96 \\cdot \\textrm{SE}(\\hat\\beta_2) \\\\ &amp; &amp; = &amp; -0.0132 \\pm 1.96 \\cdot 0.0039 \\\\ &amp; &amp; = &amp; -0.0132 \\pm 0.00764 \\\\ &amp; &amp; = &amp; (-0.0208, -0.0056) \\\\ 95\\% \\textrm{ CI for } &amp;e^{\\beta_2} &amp; = &amp; (e^{-0.0208}, e^{-0.0056}) \\\\ &amp; &amp; = &amp; (.979, .994) \\\\ 95\\% \\textrm{ CI for } &amp;e^{10\\beta_2} &amp; = &amp; (e^{-0.208}, e^{-0.056}) \\\\ &amp; &amp; = &amp; (.812, .946) \\end{eqnarray}\\] Thus, we can be 95% confident that a 10% increase in the proportion of black residents is associated with a 5.4% to 18.8% decrease in the odds of a Yes vote for the railroad referendum. This same relationship could be expressed as between a 0.6% and a 2.1% decrease in odds for each 1% increase in the black population, or between a 5.7% (\\(1/e^{-.056}\\)) and a 23.1% (\\(1/e^{-.208}\\)) increase in odds for each 10% decrease in the black population, after adjusting for distance. Of course, with \\(n=11\\), we should be cautious about relying on a Wald-type interval in this example. Another approach available in R is the profile likelihood method, similar to Chapter ??. exp(confint(model.HaleBD)) 2.5 % 97.5 % (Intercept) 38.228 122.612 distance 0.728 0.766 pctBlack 0.979 0.994 In the model with distance and pctBlack, the profile likelihood 95% confidence interval for \\(e^{\\beta_2}\\) is (.979, .994), which is approximately equal to the Wald-based interval despite the small sample size. We can also confirm the statistically significant association between percent black and odds of voting Yes (after controlling for distance), because 1 is not a plausible value of \\(e^{\\beta_2}\\) (where an odds ratio of 1 would imply that the odds of voting Yes do not change with percent black). 13.5.6 Testing for goodness of fit As in Section ??, we can evaluate the goodness of fit for our model by comparing the residual deviance (307.22) to a \\(\\chi^2\\) distribution with \\(n-p\\) (8) degrees of freedom. 1-pchisq(307.2173, 8) [1] 0 The model with pctBlack and distance has statistically significant evidence of lack of fit (\\(p&lt;.001\\)). Similar to the Poisson regression models, this lack of fit could result from (a) missing covariates, (b) outliers, or (c) overdispersion. We will first attempt to address (a) by fitting a model with an interaction between distance and percent black, to determine whether the effect of racial composition differs based on how far a community is from the proposed railroad. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 7.550902 0.638370 11.83 &lt; 0.0000000000000002 *** distance -0.614005 0.057381 -10.70 &lt; 0.0000000000000002 *** pctBlack -0.064731 0.009172 -7.06 0.0000000000017 *** distance:pctBlack 0.005367 0.000898 5.97 0.0000000023207 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 274.23 on 7 degrees of freedom AIC: 320.5 Analysis of Deviance Table Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 8 307 2 7 274 1 33 0.0000000093 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have statistically significant evidence (Wald test: \\(Z = 5.974, p&lt;.001\\); Drop in deviance test: \\(\\chi^2=32.984, p&lt;.001\\)) that the effect of the proportion of the community that is black on the odds of voting Yes depends on the distance of the community from the proposed railroad. To interpret the interaction coefficient in context, we will compare two cases: one where a community is right on the proposed railroad (distance = 0), and the other where the community is 15 miles away (distance = 15). The significant interaction implies that the effect of pctBlack should differ in these two cases. In the first case, the coefficient for pctBlack is -0.0647, while in the second case, the relevant coefficient is \\(-0.0647+15(.00537) = 0.0158\\). Thus, for a community right on the proposed railroad, a 1% increase in percent black is associated with a 6.3% (\\(e^{-.0647}=.937\\)) decrease in the odds of voting Yes, while for a community 15 miles away, a 1% increase in percent black is associated with a (\\(e^{.0158}=1.016\\)) 1.6% increase in the odds of voting Yes. A significant interaction term doesn’t always imply a change in the direction of the association, but it does here. Because our interaction model still exhibits lack of fit (residual deviance of 274.23 on just 7 df), and because we have used the covariates at our disposal, we will assess this model for potential outliers and overdispersion by examining the model’s residuals. 13.5.7 Residuals for Binomial Regression With OLS, residuals were used to assess model assumptions and identify outliers. For binomial regression, two different types of residuals are typically used. One residual, the Pearson residual, has a form similar to that used with OLS. Specifically, the Pearson residual is calculated using: \\[\\begin{equation} \\textrm{Pearson residual}_i = \\frac{\\textrm{actual count}-\\textrm{predicted count}}{\\textrm{SD of count}} = \\frac{Y_i-m_i\\hat{p_i}}{\\sqrt{m_i\\hat{p_i}(1-\\hat{p_i})}} \\tag{13.12} \\end{equation}\\] where \\(m_i\\) is the number of trials for the \\(i^{th}\\) observation and \\(\\hat{p}_i\\) is the estimated probability of success for that same observation. A deviance residual is an alternative residual for binomial regression based on the discrepancy between the observed values and those estimated using the likelihood. A deviance residual can be calculated for each observation using: \\[\\begin{equation} \\textrm{d}_i = \\textrm{sign}(Y_i-m_i\\hat{p_i})\\sqrt{2[Y_i \\log\\left(\\frac{Y_i}{m_i \\hat{p_i}}\\right)+ (m_i - Y_i) \\log\\left(\\frac{m_i - Y_i}{m_i - m_i \\hat{p_i}}\\right)]} \\tag{13.13} \\end{equation}\\] When the number of trials is large for all of the observations and the models are appropriate, both sets of residuals should follow a standard normal distribution. The sum of the individual deviance residuals is referred to as the deviance or residual deviance. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred. In the case of binomial regression, when the denominators, \\(m_i\\), are large and a model fits, the residual deviance follows a \\(\\chi^2\\) distribution with \\(n-p\\) degrees of freedom (the residual degrees of freedom). Thus for a good fitting model the residual deviance should be approximately equal to its corresponding degrees of freedom. When binomial data meets these conditions, the deviance can be used for a goodness-of-fit test. The p-value for lack-of-fit is the proportion of values from a \\(\\chi_{n-p}^2\\) that are greater than the observed residual deviance. We begin a residual analysis of our interaction model by plotting the residuals against the fitted values in Figure 13.4. This kind of plot for binomial regression would produce two linear trends with similar negative slopes if there were equal sample sizes \\(m_i\\) for each observation. FIGURE 13.4: Fitted values by residuals for the interaction model for the Railroad Referendum data. From this residual plot, Greensboro does not stand out as an outlier. If it did, we could remove Greensboro and refit our interaction model, checking to see if model coefficients changed in a noticeable way. Instead, we will continue to include Greensboro in our modeling efforts. Because the large residual deviance cannot be explained by outliers, and given we have included all of the covariates at hand as well as an interaction term, the observed binomial counts are likely overdispersed. This means that they exhibit more variation than the model would suggest, and we must consider ways to handle this overdispersion. 13.5.8 Overdispersion Similarly to Poisson regression we can adjust for overdispersion in binomial regression. With overdispersion there is extra-binomial variation, so the actual variance will be greater than the variance of a binomial variable, \\(np(1-p)\\). One way to adjust for overdispersion is to estimate a multiplier (dispersion parameter), \\(\\hat{\\phi}\\), for the variance that will inflate it and reflect the reduction in the amount of information we would otherwise have with independent observations. We used a similar approach to adjust for overdispersion in a Poisson regression model in Section ??, and we will use the same estimate here: \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\). When overdispersion is adjusted for in this way, we can no longer use maximum likelihood to fit our regression model; instead we use a quasi-likelihood approach. Quasi-likelihood is similar to likelihood-based inference, but because the model uses the dispersion parameter, it is not a binomial model with a true likelihood. R offers quasi-likelihood as an option when model fitting. The quasi-likelihood approach will yield the same coefficient point estimates as maximum likelihood; however, the variances will be larger in the presence of overdispersion (assuming \\(\\phi&gt;1\\)). We will see other ways in which to deal with overdispersion and clusters in the remaining chapters in the book, but the following describes how overdispersion is accounted for using \\(\\hat{\\phi}\\): Summary: Accounting for Overdispersion Use the dispersion parameter \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\) to inflate standard errors of model coefficients Wald test statistics: multiply the standard errors by \\(\\sqrt{\\hat{\\phi}}\\) so that \\(\\textrm{SE}_\\textrm{Q}(\\hat\\beta)=\\sqrt{\\hat\\phi}\\cdot\\textrm{SE}(\\hat\\beta)\\) and conduct tests using the \\(t\\)-distribution CIs use the adjusted standard errors and multiplier based on \\(t\\), so they are thereby wider: \\(\\hat\\beta \\pm t_{n-p} \\cdot \\textrm{SE}_\\textrm{Q}(\\hat\\beta)\\) Drop-in-deviance test statistic comparing Model 1 (larger model with \\(p\\) parameters) to Model 2 (smaller model with \\(q&lt;p\\) parameters) = \\[\\begin{equation} F = \\frac{1}{\\hat\\phi} \\cdot \\frac{D_2 - D_1}{p-q} \\tag{13.14} \\end{equation}\\] where \\(D_1\\) and \\(D_2\\) are the residual deviances for models 1 and 2 respectively and \\(p-q\\) is the difference in the number of parameters for the two models. Note that both \\(D_2-D_1\\) and \\(p-q\\) are positive. This test statistic is compared to an F-distribution with \\(p-q\\) and \\(n-p\\) degrees of freedom. Output for a model which adjusts our interaction model for overdispersion appears below, where \\(\\hat{\\phi}=51.6\\) is used to adjust the standard errors for the coefficients and the drop-in-deviance tests during model building. Standard errors will be inflated by a factor of \\(\\sqrt{51.6}=7.2\\). As a results, there are no significant terms in the adjusted interaction model below. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack, family = quasibinomial, data = rrHale.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.55090 4.58546 1.65 0.14 distance -0.61401 0.41217 -1.49 0.18 pctBlack -0.06473 0.06589 -0.98 0.36 distance:pctBlack 0.00537 0.00645 0.83 0.43 (Dispersion parameter for quasibinomial family taken to be 51.6) Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 274.23 on 7 degrees of freedom We therefore remove the interaction term and refit the model, adjusting for the extra-binomial variation that still exists. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, family = quasibinomial, data = rrHale.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.2220 1.9903 2.12 0.067 . distance -0.2917 0.0878 -3.32 0.010 * pctBlack -0.0132 0.0261 -0.51 0.626 --- (Dispersion parameter for quasibinomial family taken to be 44.9) Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 307.22 on 8 degrees of freedom By removing the interaction term and using the overdispersion parameter, we see that distance is significantly associated with support, but percent black is no longer significant after adjusting for distance. Because quasi-likelihood methods do not change estimated coefficients, we still estimate a 25% decline \\((1-e^{-0.292})\\) in support for each additional mile from the proposed railroad (odds ratio of .75). exp(confint(model.HaleBDq)) 2.5 % 97.5 % (Intercept) 1.361 5006.722 distance 0.609 0.871 pctBlack 0.937 1.044 While we previously found a 95% confidence interval for the odds ratio of (.728, .766), our confidence interval is now much wider: (.609, .871). Appropriately accounting for overdispersion has changed both the significance of certain terms and the precision of our coefficient estimates. 13.5.9 Summary We began by fitting a logistic regression model with distance alone. Then we added covariate pctBlack, and the Wald-type test and the drop-in-deviance provided strong support for the addition of pctBlack to the model. The model with distance and pctBlack had a large residual deviance suggesting an ill-fitted model. When we looked at the residuals, we saw that Greensboro is an extreme observation. Models without Greensboro were fitted and compared to our initial models. Seeing no appreciable improvement or differences with Greensboro removed, we left it in the model. There remained a large residual deviance so we attempted to account for it by using an estimated dispersion parameter similar to Section ?? with Poisson regression. The final model included distance and percent black, although percent black was no longer significant after adjusting for overdispersion. 13.6 Least Squares Regression vs. Logistic Regression \\[ \\underline{\\textrm{Response}} \\\\ \\mathbf{OLS:}\\textrm{ normal} \\\\ \\mathbf{Binomial\\ Regression:}\\textrm{ number of successes in n trials} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Variance}} \\\\ \\mathbf{OLS:}\\textrm{ Equal for each level of}\\ X \\\\ \\mathbf{Binomial\\ Regression:}\\ np(1-p)\\textrm{ for each level of}\\ X \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Model Fitting}} \\\\ \\mathbf{OLS:}\\ \\mu=\\beta_0+\\beta_1x \\textrm{ using Least Squares}\\\\ \\mathbf{Binomial\\ Regression:}\\ \\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1x \\textrm{ using Maximum Likelihood}\\\\ \\textrm{ } \\\\ \\underline{\\textrm{EDA}} \\\\ \\mathbf{OLS:}\\textrm{ plot $X$ vs. $Y$; add line} \\\\ \\mathbf{Binomial\\ Regression:}\\textrm{ find $\\log(\\textrm{odds})$ for several subgroups; plot vs. $X$} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Comparing Models}} \\\\ \\mathbf{OLS:}\\textrm{ extra sum of squares F-tests; AIC/BIC} \\\\ \\mathbf{Binomial\\ Regression:}\\textrm{ Drop in Deviance tests; AIC/BIC} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Interpreting Coefficients}} \\\\ \\mathbf{OLS:}\\ \\beta_1=\\textrm{ change in }\\mu_y\\textrm{ for unit change in $X$} \\\\ \\mathbf{Binomial\\ Regression:}\\ e^{\\beta_1}=\\textrm{ percent change in odds for unit change in $X$} \\] 13.7 Case Study: Trying to Lose Weight The final case study uses individual-specific information so that our response, rather than the number of successes out of some number of trials, is simply a binary variable taking on values of 0 or 1 (for failure/success, no/yes, etc.). This type of problem—binary logistic regression—is exceedingly common in practice. Here we examine characteristics of young people who are trying to lose weight. The prevalence of obesity among US youth suggests that wanting to lose weight is sensible and desirable for some young people such as those with a high body mass index (BMI). On the flip side, there are young people who do not need to lose weight but make ill-advised attempts to do so nonetheless. A multitude of studies on weight loss focus specifically on youth and propose a variety of motivations for the young wanting to lose weight; athletics and the media are two commonly cited sources of motivation for losing weight for young people. Sports have been implicated as a reason for young people wanting to shed pounds, but not all studies are consistent with this idea. For example, a study by (???) reported that, despite preconceptions to the contrary, there was a higher rate of self-reported eating disorders among controls (non-elite athletes) as opposed to elite athletes. Interestingly, the kind of sport was not found to be a factor, as participants in leanness sports (for example, distance running, swimming, gymnastics, dance, and diving) did not differ in the proportion with eating disorders when compared to those in non-leanness sports. So, in our analysis, we will not make a distinction between different sports. Other studies suggest that mass media is the culprit. They argue that students’ exposure to unrealistically thin celebrities may provide unhealthy motivation for some, particularly young women, to try to slim down. An examination and analysis of a large number of related studies (referred to as a meta-analysis) (???) found a strong relationship between exposure to mass media and the amount of time that adolescents spend talking about what they see in the media, deciphering what it means, and figuring out how they can be more like the celebrities. We are interested in the following questions: Are the odds that young females report trying to lose weight greater that the odds that males do? Is increasing BMI is associated with an interest in losing weight, regardless of sex? Does sports participation increase the desire to lose weight? Is media exposure is associated with more interest in losing weight? We have sample of 500 teens from data collected in 2009 through the U.S. Youth Risk Behavior Surveillance System (YRBSS) (???). The YRBSS is an annual national school-based survey conducted by the Centers for Disease Control and Prevention (CDC) and state, territorial, and local education and health agencies and tribal governments. More information on this survey can be found here. 13.7.1 Data Organization Here are the three questions from the YRBSS we use for our investigation: Q66. Which of the following are you trying to do about your weight? A. Lose weight B. Gain weight C. Stay the same weight D. I am not trying to do anything about my weight Q81. On an average school day, how many hours do you watch TV? A. I do not watch TV on an average school day B. Less than 1 hour per day C. 1 hour per day D. 2 hours per day E. 3 hours per day F. 4 hours per day G. 5 or more hours per day Q84. During the past 12 months, on how many sports teams did you play? (Include any teams run by your school or community groups.) A. 0 teams B. 1 team C. 2 teams D. 3 or more teams Answers to Q66 are used to define our response variable: Y = 1 corresponds to “(A) trying to lose weight”, while Y = 0 corresponds to the other non-missing values. Q84 provides information on students’ sports participation and is treated as numerical, 0 through 3, with 3 representing 3 or more. As a proxy for media exposure we use answers to Q81 as numerical values 0, 0.5, 1, 2, 3, 4, and 5, with 5 representing 5 or more. Media exposure and sports participation are also considered as categorical factors, that is, as variables with distinct levels which can be denoted by indicator variables as opposed to their numerical values. BMI is included in this study as the percentile for a given BMI for members of the same sex. This facilitates comparisons when modeling with males and females. We will use the terms BMI and BMI percentile interchangeably with the understanding that we are always referring to the percentile. With our sample, we use only the cases that include all of the data for these four questions. This is referred to as a complete case analysis. That brings our sample of 500 to 426. There are limitations of complete case analyses that we address in the Discussion. 13.7.2 Exploratory Data Analysis Nearly half (44.5%) of our sample of 426 youth report that they are trying to lose weight, 49.9% percent of the sample are females, and 56.6% play on one or more sports teams. 9.3 percent report that they do not watch any TV on school days, whereas another 9.3% watched 5 or more hours each day. Interestingly, the median BMI percentile for our 426 youth is 70. The most dramatic difference in the proportions of those who are trying to lose weight is by sex; 58% of the females want to lose weight in contrast to only 31% of the males (see Figure 13.5). This provides strong support for the inclusion of a sex term in every model considered. FIGURE 13.5: Weight loss plans vs. Sex TABLE 13.3: Mean BMI percentile by sex and desire to lose weight. Sex Weight loss status mean BMI percentile SD n Female No weight loss 48.2 26.2 90 Lose weight 76.3 19.5 124 Male No weight loss 54.9 28.6 148 Lose weight 84.4 17.7 67 Table 13.3 displays the mean BMI of those wanting and not wanting to lose weight for males and females. The mean BMI is greater for those trying to lose weight compared to those not trying to lose weight, regardless of sex. The size of the difference is remarkably similar for the two sexes. If we consider including a BMI term in our model(s), the logit should be linearly related to BMI. We can investigate this assumption by constructing an empirical logit plot. In order to calculate empirical logits, we first divide our data by sex. Within each sex, we generate 10 groups of equal sizes, the first holding the bottom 10% in BMI percentile for that sex, the second holding the next lowest 10%, etc.. Within each group, we calculate the proporton, \\(\\hat{p}\\) that reported wanting to lose weight, and then the empirical log odds, \\(log(\\frac{\\hat{p}}{1-\\hat{p}})\\), that a young person in that group wants to lose weight. FIGURE 13.6: Empirical logits of trying to lose weight by BMI and Sex. Figure 13.6 presents the empirical logits for the BMI intervals by sex. Both males and females exhibit an increasing linear trend on the logit scale indicating that increasing BMI is associated a greater desire to lose weight and that modeling log odds as a linear function of BMI is reasonable. The slope for the females appears to be similar to the slope for males, so we do not need to consider an interaction term between BMI and sex in the model. FIGURE 13.7: Weight loss plans vs. sex and sports participation Out of those who play sports, 43% want to lose weight whereas 46% want to lose weight among those who do not play sports. Figure 13.7 compares the proportion of respondents who want to lose weight by their sex and sport participation. The data suggest that sports participation is associated with the same or even a slightly lower desire to lose weight, contrary to what had originally been hypothesized. While the overall levels of those wanting to lose weight differ considerably between the sexes, the differences between those in and out of sports within sex appear to be very small. A term for sports participation or number of teams will be considered, but there is not compelling evidence that an interaction term will be needed. It was posited that increased exposure to media, here measured as hours of TV daily, is associated with increased desire to lose weight, particularly for females. Overall, the percentage who want to lose weight ranges from 35% of those watching 5 hours of TV per day to 52% among those watching 3 hours daily. There is minimal variation in the proportion wanting to lose weight with both sexes combined. However, we are more interested in differences between the sexes (see Figure 13.8). We create empirical logits using the proportion of students trying to lose weight for each level of hours spent watching daily and look at the trends in the logits separately for males and females. From Figure 13.9, there does not appear to be a linear relationship for males or females. FIGURE 13.8: Weight loss plans vs. daily hours of TV and sex. FIGURE 13.9: Empirical logits for the odds of trying to lose weight by TV watching and sex. 13.7.3 Initial Models Our strategy for modeling is to use our questions of interest and what we have learned in the exploratory data analysis. For each model we interpret the coefficient of interest, look at the corresponding Wald test and, as a final step, compare the deviances for the different models we considered. We first use a model where sex is our only predictor. glm(formula = lose.wt.01 ~ female, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.597 0.148 -4.04 0.0000532 *** female 0.944 0.198 4.77 0.0000018 *** --- Null deviance: 607.92 on 438 degrees of freedom Residual deviance: 584.45 on 437 degrees of freedom AIC: 588.5 Our estimated binomial regression model is: \\[\\log\\left(\\frac{{p}}{1-{p}}\\right)=-0.79+1.11 (\\textrm{female})\\] where \\({p}\\) is the estimated proportion of youth wanting to lose weight. We can interpret the coefficient on female by exponentiating \\(e^{1.1130} = 3.04\\) (95% CI = \\((2.05, 4.54)\\)) indicating that the odds of a female trying to lose weight is over three times the odds of a male trying to lose weight (\\(Z=5.506\\), \\(p=3.67e-08\\)). We retain sex in the model and consider adding the BMI percentile: glm(formula = lose.wt.01 ~ female + bmipct, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.40051 0.40554 -8.39 &lt; 0.0000000000000002 *** female 1.35923 0.23198 5.86 0.00000000464976492 *** bmipct 0.03900 0.00482 8.09 0.00000000000000061 *** --- Null deviance: 607.92 on 438 degrees of freedom Residual deviance: 498.64 on 436 degrees of freedom AIC: 504.6 We see that there is statistically significant evidence (\\(Z=9.067, p&lt;.001\\)) that BMI is positively associated with the odds of trying to lose weight, after controlling for sex. Clearly BMI percentile belongs in the model with sex. Our estimated binomial regression model is: \\[\\log\\left(\\frac{{p}}{1-{p}}\\right)= -4.46+1.54\\textrm{female}+0.051\\textrm{bmipct}\\] To interpret the coefficient on bmipct, we will consider a 10 unit increase in bmipct. Because \\(e^{10*0.0509}=1.664\\), then there is an estimated 66.4% increase in the odds of wanting to lose weight for each additional 10 percentile points of BMI for members of the same sex. Just as we had done in other multiple regression models we need to interpret our coefficient given that the other variables remain constant. An interaction term for BMI by sex was tested (not shown) and it was not significant (\\(Z=-0.405\\), \\(p=0.6856\\)), so the effect of BMI does not differ by sex. We next add sport to our model. Sports participation was considered for inclusion in the model in three ways: an indicator of sports participation (0 = no teams, 1 = one or more teams), treating the number of teams (0, 1, 2, or 3) as numeric, and treating the number of teams as a factor. The models below treat sports participation using an indicator variable, but all three models produced similar results. glm(formula = lose.wt.01 ~ female + bmipct + sport, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.40264 0.43028 -7.91 0.00000000000000262 *** female 1.35958 0.23318 5.83 0.00000000552187757 *** bmipct 0.03900 0.00482 8.08 0.00000000000000063 *** sportSports 0.00325 0.21933 0.01 0.99 Null deviance: 607.92 on 438 degrees of freedom Residual deviance: 498.64 on 435 degrees of freedom glm(formula = lose.wt.01 ~ female + bmipct + sport + female:sport + bmipct:sport, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.04792 0.56322 -5.41 0.000000062 *** female 1.13005 0.33724 3.35 0.00081 *** bmipct 0.03582 0.00663 5.40 0.000000067 *** sportSports -0.69916 0.81496 -0.86 0.39094 female:sportSports 0.44009 0.46892 0.94 0.34798 bmipct:sportSports 0.00669 0.00967 0.69 0.48896 Null deviance: 607.92 on 438 degrees of freedom Residual deviance: 497.59 on 433 degrees of freedom Sports teams were not significant in any of these models, nor were interaction terms (sex by sports) and (bmipct by sports). As a result, sports participation was no longer considered for inclusion in the model. We last look at adding media to our model. glm(formula = lose.wt.01 ~ bmipct + female + media, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.35140 0.41414 -8.09 0.00000000000000059 *** bmipct 0.03942 0.00489 8.06 0.00000000000000077 *** female 1.37105 0.23322 5.88 0.00000000413223676 *** media -0.03990 0.06999 -0.57 0.57 --- Null deviance: 607.92 on 438 degrees of freedom Residual deviance: 498.31 on 435 degrees of freedom AIC: 506.3 Media is not a statistically significant term (\\(p=0.456\\), \\(Z=-0.745\\)). However, because our interest centers on how media may affect attempts to lose weight and how its effect might be different for females and males, we fit a model with a media term and a sex by media interaction term (not shown). Neither term was statistically significant, so we have no support in our data that media exposure as measured by hours spent watching TV is associated with the odds a teen is trying to lose weight after accounting for sex and BMI. 13.7.4 Drop-in-deviance Tests Analysis of Deviance Table Model 1: lose.wt.01 ~ female Model 2: lose.wt.01 ~ female + bmipct Model 3: lose.wt.01 ~ female + bmipct + sport Model 4: lose.wt.01 ~ bmipct + female + media Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 437 584 2 436 499 1 85.8 &lt;0.0000000000000002 *** 3 435 499 1 0.0 0.99 4 435 498 0 0.3 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 df AIC model1 2 588 model2 3 505 model3 4 507 model4 4 506 Comparing models using differences in deviances requires that the models be nested, meaning each smaller model is a simplified version of the larger model. In our case, Models 1, 2, and 4 are nested, as are Models 1, 2, and 3, but Models 3 and 4 cannot be compared using a drop-in-deviance test. There is a large drop-in-deviance adding BMI to the model with sex (Model 1 to Model 2, 123.13), which is clearly statistically significant when compared to a \\(\\chi^2\\) distribution with 1 df. The drop-in-deviance for adding an indicator variable for sports to the model with sex and BMI is only 434.88 - 434.75 = 0.13. There is a difference of a single parameter, so the drop-in-deviance would be compared to a \\(\\chi^2\\) distribution with 1 df. The resulting \\(p\\)-value is very large (.7188) suggesting that adding an indicator for sports is not helpful once we’ve already accounted for BMI and sex. For comparing Models 3 and 4, one approach is to look at the AIC. In this case, the AIC is (barely) smaller for the model with media, providing evidence that the latter model is slightly preferable. 13.7.5 Model Discussion and Summary We found that the odds of wanting to lose weight are considerably greater for females compared to males. In addition, respondents with greater BMI percentiles express a greater desire to lose weight for members of the same sex. Regardless of sex or BMI percentile, sports participation and TV watching are not associated with different odds for wanting to lose weight. A limitation of this analysis is that we used complete cases in place of a method of imputing responses or modeling missingness. This reduced our sample from 500 to 429, and it may have introduced bias. For example, if respondents who watch a lot of TV were unwilling to reveal as much, and if they differed with respect to their desire to lose weight from those respondents who reported watching little TV, our inferences regarding the relationship between lots of TV and desire to lose weight may be biased. Other limitations may result from definitions. Trying to lose weight is self-reported and may not correlate with any action undertaken to do so. The number of sports teams may not accurately reflect sports related pressures to lose weight. For example, elite athletes may focus on a single sport and be subject to greater pressures whereas athletes who casually participate in three sports may not feel any pressure to lose weight. Hours spent watching TV is not likely to encompass the totality of media exposure, particularly because exposure to celebrities occurs often online. Furthermore, this analysis does not explore in any detail maladaptions—inappropriate motivations for wanting to lose weight. For example, we did not focus our study on subsets of respondents with low BMI who are attempting to lose weight. It would be instructive to use data science methodologies to explore the entire data set of 16,000 instead of sampling 500. However, the types of exploration and models used here could translate to the larger sample size. Finally a limitation may be introduced as a result of the acknowledged variation in the administration of the YRBSS. States and local authorities are allowed to administer the survey as they see fit, which at times results in significant variation in sample selection and response. 13.8 Classification and regression trees (CART) 13.8.1 The curse of dimensionality We described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. For example, with the digits example \\(p=784\\), we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space. A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility. For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1: Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\): Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing. library(tidyverse) p &lt;- 1:100 qplot(p, .1^(1/p), ylim = c(0,1)) By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset. Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests. 13.8.2 CART motivation To motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids: library(tidyverse) library(dslabs) Attaching package: &#39;dslabs&#39; The following object is masked from &#39;package:gapminder&#39;: gapminder data(&quot;olive&quot;) names(olive) [1] &quot;region&quot; &quot;area&quot; &quot;palmitic&quot; &quot;palmitoleic&quot; &quot;stearic&quot; [6] &quot;oleic&quot; &quot;linoleic&quot; &quot;linolenic&quot; &quot;arachidic&quot; &quot;eicosenoic&quot; For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors. table(olive$region) Northern Italy Sardinia Southern Italy 151 98 323 We remove the area column because we won’t use it as a predictor. olive &lt;- select(olive, -area) Let’s very quickly try to predict the region using kNN: library(caret) fit &lt;- train(region ~ ., method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(1, 15, 2)), data = olive) ggplot(fit) We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia. olive %&gt;% gather(fatty_acid, percentage, -region) %&gt;% ggplot(aes(region, percentage, fill = region)) + geom_boxplot() + facet_wrap(~fatty_acid, scales = &quot;free&quot;, ncol = 4) + theme(axis.text.x = element_blank(), legend.position=&quot;bottom&quot;) This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic. olive %&gt;% ggplot(aes(eicosenoic, linoleic, color = region)) + geom_point() + geom_vline(xintercept = 0.065, lty = 2) + geom_segment(x = -0.2, y = 10.54, xend = 0.065, yend = 10.54, color = &quot;black&quot;, lty = 2) In Section ?? we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than \\(10.535\\), predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this: Decision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following: (Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-18462.) A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes. Regression and decision trees operate by predicting an outcome variable \\(Y\\) by partitioning the predictors. 13.8.3 Regression trees When the outcome is continuous, we call the method a regression tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mbox{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day. data(&quot;polls_2008&quot;) qplot(day, margin, data = polls_2008) The general idea here is to build a decision tree and, at the end of each node, obtain a predictor \\(\\hat{y}\\). A mathematical way to describe this is to say that we are partitioning the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), estimate \\(f(x)\\) with the average of the training observations \\(y_i\\) for which the associated predictor \\(x_i\\) is also in \\(R_j\\). But how do we decide on the partition \\(R_1, R_2, \\ldots, R_J\\) and how do we choose \\(J\\)? Here is where the algorithm gets a bit complicated. Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later. Once we select a partition \\(\\mathbf{x}\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, which we will call \\(R_1(j,s)\\) and \\(R_2(j,s)\\), that split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\): \\[ R_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\} \\] In our current example we only have one predictor, so we will always choose \\(j=1\\), but in general this will not be the case. Now, after we define the new partitions \\(R_1\\) and \\(R_2\\), and we decide to stop the partitioning, we compute predictors by taking the average of all the observations \\(y\\) for which the associated \\(\\mathbf{x}\\) is in \\(R_1\\) and \\(R_2\\). We refer to these two as \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) respectively. But how do we pick \\(j\\) and \\(s\\)? Basically we find the pair that minimizes the residual sum of square (RSS): \\[ \\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2 \\] This is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the rpart function in the rpart package. library(rpart) fit &lt;- rpart(margin ~ ., data = polls_2008) Here, there is only one predictor. Thus we do not have to decide which predictor \\(j\\) to split by, we simply have to decide what value \\(s\\) we use to split. We can visually see where the splits were made: plot(fit, margin = 0.1) text(fit, cex = 0.75) The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate \\(\\hat{f}(x)\\) looks like this: polls_2008 %&gt;% mutate(y_hat = predict(fit)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made. First we need to define the term complexity parameter (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the complexity parameter (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes. However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the rpart function is minsplit and the default is 20. The rpart implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is minbucket and defaults to round(minsplit/3). As expected, if we set cp = 0 and minsplit = 2, then our prediction is as flexible as possible and our predictor is our original data: fit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0, minsplit = 2)) polls_2008 %&gt;% mutate(y_hat = predict(fit)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) Intuitively we know that this is not a good approach as it will generally result in over-training. These cp, minsplit, and minbucket, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility. So how do we pick these parameters? We can use cross validation, described in Chapter 14.6, just like with any tuning parameter. Here is an example of using cross validation to chose cp. library(caret) train_rpart &lt;- train(margin ~ ., method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)), data = polls_2008) ggplot(train_rpart) To see the resulting tree, we access the finalModel and plot it: plot(train_rpart$finalModel, margin = 0.1) text(train_rpart$finalModel, cex = 0.75) And because we only have one predictor, we can actually plot \\(\\hat{f}(x)\\): polls_2008 %&gt;% mutate(y_hat = predict(train_rpart)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_step(aes(day, y_hat), col=&quot;red&quot;) Note that if we already have a tree and want to apply a higher cp value, we can use the prune function. We call this pruning a tree because we are snipping off partitions that do not meet a cp criterion. We previously created a tree that used a cp = 0 and saved it to fit. We can prune it like this: pruned_fit &lt;- prune(fit, cp = 0.01) 13.8.4 Classification (decision) trees Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome. The first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories). The second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the Gini Index and Entropy. In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The Gini Index is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define \\(\\hat{p}_{j,k}\\) as the proportion of observations in partition \\(j\\) that are of class \\(k\\). The Gini Index is defined as \\[ \\mbox{Gini}(j) = \\sum_{k=1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k}) \\] If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above. Entropy is a very similar quantity, defined as \\[ \\mbox{entropy}(j) = -\\sum_{k=1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0 \\] Let us look at how a classification tree performs on the digits example we examined before: We can use this code to run the algorithm and plot the resulting tree: train_rpart &lt;- train(y ~ ., method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)), data = mnist_27$train) plot(train_rpart) The accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods: y_hat &lt;- predict(train_rpart, mnist_27$test) confusionMatrix(y_hat, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.82 The plot of the estimated conditional probability shows us the limitations of classification trees: Note that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity. Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings. 13.9 Random forests Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this. The first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows. 1. Build \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\). We later explain how we ensure they are different. 2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\). 3. For continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j=1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_T\\)). So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let \\(N\\) be the number of observations in the training set. To create \\(T_j, \\, j=1,\\ldots,B\\) from the training set we do the following: 1. Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness. 2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to the 2008 polls data. We will use the randomForest function in the randomForest package: library(randomForest) fit &lt;- randomForest(margin~., data = polls_2008) Note that if we apply the function plot to the resulting object, stored in fit, we see how the error rate of our algorithm changes as we add trees. rafalib::mypar() plot(fit) We can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes. The resulting estimate for this random forest can be seen like this: polls_2008 %&gt;% mutate(y_hat = predict(fit, newdata = polls_2008)) %&gt;% ggplot() + geom_point(aes(day, margin)) + geom_line(aes(day, y_hat), col=&quot;red&quot;) Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of \\(b\\) and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point. Here is the random forest fit for our digits example based on two predictors: library(randomForest) train_rf &lt;- randomForest(y ~ ., data=mnist_27$train) confusionMatrix(predict(train_rf, mnist_27$test), mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.79 Here is what the conditional probabilities look like: Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the caret package to optimize over the minimum node size. Because, this is not one of the parameters that the caret package optimizes by default we will write our own code: nodesize &lt;- seq(1, 51, 10) acc &lt;- sapply(nodesize, function(ns){ train(y ~ ., method = &quot;rf&quot;, data = mnist_27$train, tuneGrid = data.frame(mtry = 2), nodesize = ns)$results$Accuracy }) qplot(nodesize, acc) We can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data. train_rf_2 &lt;- randomForest(y ~ ., data=mnist_27$train, nodesize = nodesize[which.max(acc)]) confusionMatrix(predict(train_rf_2, mnist_27$test), mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.815 The selected model improves accuracy and provides a smoother estimate. Note that we can avoid writing our own code by using other random forest implementations as described in the caret manual63. Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance we count how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book64. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section. https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2↩ http://topepo.github.io/caret/available-models.html↩ https://web.stanford.edu/~hastie/Papers/ESLII.pdf↩ "],
["14-machine.html", "Chapter 14 Machine Learning 14.1 Notation 14.2 An example 14.3 Evaluation metrics 14.4 Conditional probabilities and expectations 14.5 Case study: is it a 2 or a 7? 14.6 Cross validation 14.7 Motivation with k-nearest neighbors 14.8 Mathematical description of cross validation 14.9 K-fold cross validation 14.10 Bootstrap", " Chapter 14 Machine Learning Perhaps the most popular data science methodologies come from the field of machine learning. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars. Although today Artificial Intelligence and machine learning are often used interchangeably, we make the following distinction: while the first artificial intelligence algorithms, such as those used by chess playing machines, implemented decision making based on programmable rules derived from theory or first principles, in machine learning decisions are based on algorithms built with data. 14.1 Notation In machine learning, data comes in the form of: the outcome we want to predict and the features that we will use to predict the outcome We want to build an algorithm that takes feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to train an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome. Here we will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms. Prediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, \\(Y\\) can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later. The general setup is as follows. We have a series of features and an unknown outcome we want to predict: outcome feature 1 feature 2 feature 3 feature 4 feature 5 ? \\(X_1\\) \\(X_2\\) \\(X_3\\) \\(X_4\\) \\(X_5\\) To build a model that provides a prediction for any set of observed values \\(X_1=x_1, X_2=x_2, \\dots X_5=x_5\\), we collect data for which we know the outcome: outcome feature 1 feature 2 feature 3 feature 4 feature 5 \\(y_{1}\\) \\(x_{1,1}\\) \\(x_{1,2}\\) \\(x_{1,3}\\) \\(x_{1,4}\\) \\(x_{1,5}\\) \\(y_{2}\\) \\(x_{2,1}\\) \\(x_{2,2}\\) \\(x_{2,3}\\) \\(x_{2,4}\\) \\(x_{2,5}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(y_n\\) \\(x_{n,1}\\) \\(x_{n,2}\\) \\(x_{n,3}\\) \\(x_{n,4}\\) \\(x_{n,5}\\) When the output is continuous we refer to the machine learning task as prediction, and the main output of the model is a function \\(f\\) that automatically produces a prediction, denoted with \\(\\hat{y}\\), for any set of predictors: \\(\\hat{y} = f(x_1, x_2, \\dots, x_p)\\). We use the term actual outcome to denote what we ended up observing. So we want the prediction \\(\\hat{y}\\) to match the actual outcome \\(y\\) as well as possible. Because our outcome is continuous, our predictions \\(\\hat{y}\\) will not be either exactly right or wrong, but instead we will determine an error defined as the difference between the prediction and the actual outcome \\(y - \\hat{y}\\). When the outcome is categorical, we refer to the machine learning task as classification, and the main output of the model will be a decision rule which prescribes which of the \\(K\\) classes we should predict. In this scenario, most models provide functions of the predictors for each class \\(k\\), \\(f_k(x_1, x_2, \\dots, x_p)\\), that are used to make this decision. When the data is binary a typical decision rules looks like this: if \\(f_1(x_1, x_2, \\dots, x_p) &gt; C\\), predict category 1, if not the other category, with \\(C\\) a predetermined cutoff. Because the outcomes are categorical, our predictions will be either right or wrong. Notice that these terms vary among courses, text books, and other publications. Often prediction is used for both categorical and continuous outcomes, and the term regression can be used for the continuous case. Here we avoid using regression to avoid confusion with our previous use of the term linear regression. In most cases it will be clear if our outcomes are categorical or continuous, so we will avoid using these terms when possible. 14.2 An example Let’s consider the zip code reader example. The first step in handling mail received in the post office is sorting letters by zip code: Originally, humans had to sort these by hand. To do this, they had to read the zip codes on each letter. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. In this part of the book, we will learn how to build algorithms that can read a digit. The first step in building an algorithm is to understand what are the outcomes and features. Below are three images of written digits. These have already been read by a human and assigned an outcome \\(Y\\). These are considered known and serve as the training set. The images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black), which we consider continuous for now. The following plot shows the individual features for each image: For each digitized image \\(i\\), we have a categorical outcome \\(Y_i\\) which can be one of 10 values (\\(0,1,2,3,4,5,6,7,8,9\\)), and features \\(X_{i,1}, \\dots, X_{i,784}\\). We use bold face \\(\\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,784})\\) to distinguish the vector of predictors from the individual predictors. When referring to an arbitrary set of features rather than a specific image in our dataset, we drop the index \\(i\\) and use \\(Y\\) and \\(\\mathbf{X} = (X_{1}, \\dots, X_{784})\\). We use upper case variables because, in general, we think of the predictors as random variables. We use lower case, for example \\(\\mathbf{X} = \\mathbf{x}\\), to denote observed values. When we code we stick to lower case. The machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features. Here, we will learn several approaches to building these algorithms. Although at this point it might seem impossible to achieve this, we will start with simple examples and build up our knowledge until we can attack more complex ones. In fact, we start with an artificially simple example with just one predictor and then move on to a slightly more realistic example with two predictors. Once we understand these, we will attack real-world machine learning challenges involving many predictors. 14.3 Evaluation metrics Before we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better”. For our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain machine learning step by step, this example will let us set down the first building block. Soon enough, we will be attacking more interesting challenges. We use the caret package, which has several useful functions for building and assessing machine learning methods and we introduce in more detail in Section ??. library(tidyverse) library(caret) For a first example, we use the height data in dslabs: library(dslabs) data(heights) We start by defining the outcome and predictors. y &lt;- heights$sex x &lt;- heights$height In this case, we have only one predictor, height, and y is clearly a categorical outcome since observed values are either Male or Female. We know that we will not be able to predict \\(Y\\) very accurately based on \\(X\\) because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of better. 14.3.1 Training and test sets Ultimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the training set. We refer to the group for which we pretend we don’t know the outcome as the test set. A standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generates indexes for randomly splitting the data into training and test sets: set.seed(2007) test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE) The argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not. We can use the result of the createDataPartition function call to define the training and test sets like this: test_set &lt;- heights[test_index, ] train_set &lt;- heights[-test_index, ] We will now develop an algorithm using only the training set. Once we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set. This metric is usually referred to as overall accuracy. 14.3.2 Overall accuracy To demonstrate the use of overall accuracy, we will build two competing algorithms and compare them. Let’s start by developing the simplest possible machine algorithm: guessing the outcome. y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE) Note that we are completely ignoring the predictor and simply guessing the sex. In machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function: y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE) %&gt;% factor(levels = levels(test_set$sex)) The overall accuracy is simply defined as the overall proportion that is predicted correctly: mean(y_hat == test_set$sex) [1] 0.51 Not surprisingly, our accuracy is about 50%. We are guessing! Can we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females: heights %&gt;% group_by(sex) %&gt;% summarize(mean(height), sd(height)) # A tibble: 2 x 3 sex `mean(height)` `sd(height)` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Female 64.9394 3.76066 2 Male 69.3148 3.61102 But how do we make use of this insight? Let’s try another simple approach: predict Male if height is within two standard deviations from the average male: y_hat &lt;- ifelse(x &gt; 62, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) The accuracy goes up from 0.50 to about 0.80: mean(y == y_hat) [1] 0.793 But can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in dangerously over-optimistic assessments. Here we examine the accuracy of 10 different cutoffs and pick the one yielding the best result: cutoff &lt;- seq(61, 70) accuracy &lt;- map_dbl(cutoff, function(x){ y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) mean(y_hat == train_set$sex) }) We can make a plot showing the accuracy obtained on the training set for males and females: We see that the maximum value is: max(accuracy) [1] 0.85 which is much higher than 0.5. The cutoff resulting in this accuracy is: best_cutoff &lt;- cutoff[which.max(accuracy)] best_cutoff [1] 64 We can now test this cutoff on our test set to make sure our accuracy is not overly optimistic: y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) y_hat &lt;- factor(y_hat) mean(y_hat == test_set$sex) [1] 0.804 We see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result. 14.3.3 The confusion matrix The prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict Female? Generally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value. We can do this in R using the function table: table(predicted = y_hat, actual = test_set$sex) actual predicted Female Male Female 48 32 Male 71 374 If we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get: test_set %&gt;% mutate(y_hat = y_hat) %&gt;% group_by(sex) %&gt;% summarize(accuracy = mean(y_hat == sex)) # A tibble: 2 x 2 sex accuracy &lt;fct&gt; &lt;dbl&gt; 1 Female 0.403361 2 Male 0.921182 There is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the prevalence of males in this dataset is high. These heights were collected from three data sciences courses, two of which had more males enrolled: prev &lt;- mean(y == &quot;Male&quot;) prev [1] 0.773 So when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This can actually be a big problem in machine learning. If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm. There are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study sensitivity and specificity separately. 14.3.4 Sensitivity and specificity To define sensitivity and specificity, we need a binary outcome. When the outcomes are categorical, we can define these terms for a specific category. In the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit. Once we specify a category of interest, then we can talk about positive outcomes, \\(Y=1\\), and negative outcomes, \\(Y=0\\). In general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\). Because an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine specificity, which is generally defined as the ability of an algorithm to not predict a positive \\(\\hat{Y}=0\\) when the actual outcome is not a positive \\(Y=0\\). We can summarize in the following way: High sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\) High specificity: \\(Y=0 \\implies \\hat{Y} = 0\\) Although the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive: High specificity: \\(\\hat{Y}=1 \\implies Y=1\\). To provide precise definitions, we name the four entries of the confusion matrix: Actually Positive Actually Negative Predicted positive True positives (TP) False positives (FP) Predicted negative False negatives (FN) True negatives (TN) Sensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)). This quantity is referred to as the true positive rate (TPR) or recall. Specificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)). This quantity is referred to as positive predictive value (PPV) and also as precision. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing. The multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities. Measure of Name 1 Name 2 Definition Probability representation sensitivity TPR Recall \\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\) \\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\) specificity TNR 1-FPR \\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\) \\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\) specificity PPV Precision \\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\) \\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\) Here TPR is True Positive Rate, FPR is False Positive Rate, and PPV is Positive Predictive Value. The caret function confusionMatrix computes all these metrics for us once we define what category “positive” is. The function expects factors as input, and the first level is considered the positive outcome or \\(Y=1\\). In our example, Female is the first level because it comes before Male alphabetically. If you type this into R you will see several metrics including accuracy, sensitivity, specificity, and PPV. cm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex) You can acceess these directly, for example, like this: cm$overall[&quot;Accuracy&quot;] Accuracy 0.804 cm$byClass[c(&quot;Sensitivity&quot;,&quot;Specificity&quot;, &quot;Prevalence&quot;)] Sensitivity Specificity Prevalence 0.403 0.921 0.227 We can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same. 14.3.5 Balanced accuracy and \\(F_1\\) score Although we usually recommend studying both specificity and sensitivity, very often it is useful to have a one-number summary, for example for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall: \\[ \\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} + \\frac{1}{\\mbox{precision}}\\right) } \\] Because it is easier to write, you often see this harmonic average rewritten as: \\[ 2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}} {\\mbox{precision} + \\mbox{recall}} \\] when defining \\(F_1\\). Remember that, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently. To do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average: \\[ \\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} + \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} } \\] The F_meas function in the caret package computes this summary with beta defaulting to 1. Let’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy: cutoff &lt;- seq(61, 70) F_1 &lt;- map_dbl(cutoff, function(x){ y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) F_meas(data = y_hat, reference = factor(train_set$sex)) }) As before, we can plot these \\(F_1\\) measures versus the cutoffs: We see that it is maximized at \\(F_1\\) value of: max(F_1) [1] 0.647 This maximum is achieved when we use the following cutoff: best_cutoff &lt;- cutoff[which.max(F_1)] best_cutoff [1] 66 A cutoff of 65 makes more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix: y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% factor(levels = levels(test_set$sex)) sensitivity(data = y_hat, reference = test_set$sex) [1] 0.63 specificity(data = y_hat, reference = test_set$sex) [1] 0.833 We now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm. It takes height as a predictor and predicts female if you are 65 inches or shorter. 14.3.6 Prevalence matters in practice A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease. The doctor shares data with you and you then develop an algorithm with very high sensitivity. You explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly. You also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: \\(\\mbox{Pr}(\\hat{Y}=1)\\). The doctor is neither concerned nor impressed and explains that what is important is the precision of the test: \\(\\mbox{Pr}(Y=1 | \\hat{Y}=1)\\). Using Bayes theorem, we can connect the two measures: \\[ \\mbox{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)}\\] The doctor knows that the prevalence of the disease is 5 in 1,000, which implies that \\(\\mbox{Pr}(Y=1) \\, / \\,\\mbox{Pr}(\\hat{Y}=1) = 1/100\\) and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm. 14.3.7 ROC and precision-recall curves When comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\). The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Note that guessing Male with higher probability would give us higher accuracy due to the bias in the sample: p &lt;- 0.9 n &lt;- length(test_index) y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), n, replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = levels(test_set$sex)) mean(y_hat == test_set$sex) [1] 0.739 But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this. Remember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both. A widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering where this name comes from, you can consult the ROC Wikipedia page65. The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male: probs &lt;- seq(0, 1, length.out = 10) guessing &lt;- map_df(probs, function(p){ y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), n, replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = c(&quot;Female&quot;, &quot;Male&quot;)) list(method = &quot;Guessing&quot;, FPR = 1 - specificity(y_hat, test_set$sex), TPR = sensitivity(y_hat, test_set$sex)) }) We can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity: We can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method. Note that ROC curves for guessing always fall on the identiy line. Also note that when making ROC curves, it is often nice to add the cutoff associated with each point. The packages pROC and plotROC are useful for generating these plots. ROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall: From this plot we immediately see that the precision of guessing is not high. This is because the prevalence is low. We also see that if we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes. 14.3.8 The loss function Up to now we have described evaluation metrics that apply exclusively to categorical data. Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes. In this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data. The most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply: \\[ (\\hat{y} - y)^2 \\] Because we often have a test set with many observations, say \\(N\\), we use the mean squared error (MSE): \\[ \\mbox{MSE} = \\frac{1}{N} \\mbox{RSS} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\] In practice, we often report the root mean squared error (RMSE), which is \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes. But doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms. If the outcomes are binary, both RMSE and MSE are equivalent to accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible. Because our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this: \\[ \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\} \\] This is a theoretical concept because in practice we only have one dataset to work with. But in theory, we think of having a very large number of random samples (call it \\(B\\)), apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as: \\[ \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2 \\] with \\(y_{i}^b\\) denoting the \\(i\\)th observation in the \\(b\\)th random sample and \\(\\hat{y}_i^b\\) the resulting prediction obtained from applying the exact same algorithm to the \\(b\\)th random sample. Again, in practice we only observe one random sample, so the expected MSE is only theoretical. However, in Chapter 14.6 we describe an approach to estimating the MSE that tries to mimic this theoretical quantity. Note that there are loss functions other than the squared loss. For example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors \\((\\hat{Y}_i - Y_i)^2\\). However, in this book we focus on minimizing square loss since it is the most widely used. 14.4 Conditional probabilities and expectations In machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and your bank at times thinks your card was stolen when it was not. The most common reason for not being able to build perfect algorithms is that it is impossible. To see this, note that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes. Because our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height \\(x\\), you will have both males and females that are \\(x\\) inches tall. However, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in Section ??. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data. 14.4.1 Conditional probabilities We use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_p\\) for covariates \\(X_1, \\dots, X_p\\). This does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability. In particular, we denote the conditional probabilities for each class \\(k\\): \\[ \\mbox{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K \\] To avoid writing out all the predictors, we will use the bold letters like this: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)\\). We will also use the following notation for the conditional probability of being class \\(k\\): \\[ p_k(\\mathbf{x}) = \\mbox{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K \\] Note: We will be using the \\(p(x)\\) notation to represent conditional probabilities as functions of the predictors. Do not confuse it with the \\(p\\) that represents the number of predictors. These probabilities guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(x), p_2(x), \\dots p_K(x)\\). In mathematical notation, we write it like this: \\(\\hat{Y} = \\max_k p_k(\\mathbf{x})\\). In machine learning, we refer to this as Bayes’ Rule. But keep in mind that this is a theoretical rule since in practice we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\). In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor: \\[\\hat{Y} = \\max_k \\hat{p}_k(\\mathbf{x})\\] So what we will predict depends on two things: 1) how close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and 2) how close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\). We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities. The first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others our success is restricted by the randomness of the process, with movie recommendations for example. Before we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed above, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired. 14.4.2 Conditional expectations For binary data, you can think of the probability \\(\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\). Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations. Because the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones: \\[ \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}). \\] As a result, we often only use the expectation to denote both the conditional probability and conditional expectation. Just like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors. 14.4.3 Conditional expectation minimizes squared loss function Why do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions \\(\\hat{Y}\\), \\[ \\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2 \\mid \\mathbf{X}=\\mathbf{x} \\} \\] Due to this property, a succinct description of the main task of machine learning is that we use data to estimate: \\[ f(\\mathbf{x}) \\equiv \\mbox{E}( Y \\mid \\mathbf{X}=\\mathbf{x} ) \\] for any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)\\). Of course this is easier said than done, since this function can take any shape and \\(p\\) can be very large. Consider a case in which we only have one predictor \\(x\\). The expectation \\(\\mbox{E}\\{ Y \\mid X=x \\}\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)! The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation. 14.5 Case study: is it a 2 or a 7? In the two simple examples above, we only had one predictor. We actually do not consider these machine learning challenges, which are characterized by cases with many predictors. Let’s go back to the digits example in which we had 784 predictors. For illustrative purposes, we will start by simplifying this problem to one with two predictors and two classes. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the predictors. We are not quite ready to build algorithms with 784 predictors, so we will extract two simple predictors from the 784: the proportion of dark pixels that are in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)). We then select a random sample of 1,000 digits, 500 in the training set and 500 in the test set. We provide this dataset in the dslabs package: library(tidyverse) library(dslabs) data(&quot;mnist_27&quot;) We can explore the data by plotting the two predictors and using colors to denote the labels: mnist_27$train %&gt;% ggplot(aes(x_1, x_2, color = y)) + geom_point() We can immediately see some patterns. For example, if \\(X_1\\) (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of \\(X_1\\), the 2s appear to be in the mid range values of \\(X_2\\). These are the images of the digits with the largest and smallest values for \\(X_1\\): And here are the original images corresponding to the largest and smallest value of \\(X_2\\): We can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging. We haven’t really learned any algorithms yet, so let’s try building an algorithm using regression. The model is simply: \\[ p(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] We fit it like this: fit &lt;- mnist_27$train %&gt;% mutate(y = ifelse(y==7, 1, 0)) %&gt;% lm(y ~ x_1 + x_2, data = .) We can now build a decision rule based on the estimate of \\(\\hat{p}(x_1, x_2)\\): library(caret) p_hat &lt;- predict(fit, newdata = mnist_27$test) y_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2)) confusionMatrix(y_hat, mnist_27$test$y)$overall[[&quot;Accuracy&quot;]] [1] 0.75 We get an accuracy well above 50%. Not bad for our first try. But can we do better? Because we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(x_1, x_2)\\). Keep in mind that this is something we don’t have access to in practice, but we include it in this example because it permits the comparison of \\(\\hat{p}(x_1, x_2)\\) to the true \\(p(x_1, x_2)\\). This comparison teaches us the limitations of different algorithms. Let’s do that here. We have stored the true \\(p(x_1,x_2)\\) in the mnist_27 object and can plot the image using the ggplot2 function geom_raster(). We choose better colors and use the stat_contour function to draw a curve that separates pairs \\((x_1,x_2)\\) for which \\(p(x_1,x_2) &gt; 0.5\\) and pairs for which \\(p(x_1,x_2) &lt; 0.5\\): mnist_27$true_p %&gt;% ggplot(aes(x_1, x_2, z = p, fill = p)) + geom_raster() + scale_fill_gradientn(colors=c(&quot;#F8766D&quot;, &quot;white&quot;, &quot;#00BFC4&quot;)) + stat_contour(breaks=c(0.5), color=&quot;black&quot;) Above you see a plot of the true \\(p(x,y)\\). To start understanding the limitations of logistic regression here, first note that with logistic regression \\(\\hat{p}(x,y)\\) has to be a plane, and as a result the boundary defined by the decision rule is given by: \\(\\hat{p}(x,y) = 0.5\\), which implies the boundary can’t be anything other than a straight line: \\[ \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies x_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2 -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1 \\] Note that for this boundary, \\(x_2\\) is a linear function of \\(x_1\\). This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(x_1,x_2)\\). Below is a visual representation of \\(\\hat{p}(x_1, x_2)\\). We used the squish function from the scales package to constrain estimates to be between 0 and 1. We can see where the mistakes were made by also showing the data and the boundary. They mainly come from low values \\(x_1\\) that have either high or low value of \\(x_2\\). Regression can’t catch this. We need something more flexible: a method that permits estimates with shapes other than a plane. We are going to learn a few new algorithms based on different ideas and concepts. But what they all have in common is that they permit more flexible approaches. We will start by describing nearest neighbor and kernel approaches. To introduce the concepts behinds these approaches, we will again start with a simple one-dimensional example and describe the concept of smoothing. 14.6 Cross validation In this chapter we introduce cross validation, one of the most important ideas in machine learning. Here we focus on the conceptual and mathematical aspects. We will describe how to implement cross validation in practice with the caret package later, in Section ?? in the next chapter. To motivate the concept, we will use the two predictor digits data presented in Section 14.5 and introduce, for the first time, an actual machine learning algorithm: k-nearest neighbors (kNN). 14.7 Motivation with k-nearest neighbors Let’s start by loading the data and showing a plot of the predictors with outcome represented with color. library(tidyverse) library(dslabs) data(&quot;mnist_27&quot;) mnist_27$test%&gt;% ggplot(aes(x_1, x_2, color = y)) + geom_point() We will use these data to estimate the conditional probability function \\[ p(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2). \\] as defined in Section ??. With k-nearest neighbors (kNN) we estimate \\(p(x_1, x_2)\\) in a similar way to bin smoothing. However, as we will see, kNN is easier to adapt to multiple dimensions. First we define the distance between all observations based on the features. Then, for any point \\((x_1,x_2)\\) for which we want an estimate of \\(p(x_1, x_2)\\), we look for the \\(k\\) nearest points to \\((x_1,x_2)\\) and then take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the neighborhood. Due to the connection we described earlier between conditional expectations and conditional probabilities, this gives us a \\(\\hat{p}(x_1,x_2)\\), just like the bin smoother gave us an estimate of a trend. As with bin smoothers, we can control the flexibility of our estimate, in this case through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and more wiggly estimates. To implement the algorithm, we can use the knn3 function from the caret package. Looking at the help file for this package, we see that we can call it in one of two ways. We will use the first in which we specify a formula and a data frame. The data frame contains all the data to be used. The formula has the form outcome ~ predictor_1 + predictor_2 + predictor_3 and so on. Therefore, we would type y ~ x_1 + x_2. If we are going to use all the predictors, we can use the . like this y ~ .. The final call looks like this: library(caret) knn_fit &lt;- knn3(y ~ ., data = mnist_27$train) For this function, we also need to pick a parameter: the number of neighbors to include. Let’s start with the default \\(k=5\\). knn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5) In this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance. The predict function for knn produces a probability for each class. We keep the probability of being a 7 as the estimate \\(\\hat{p}(x_1, x_2)\\) y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;) confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.815 In Section 14.5 we used linear regression to generate an estimate. fit_lm &lt;- mnist_27$train %&gt;% mutate(y = ifelse(y == 7, 1, 0)) %&gt;% lm(y ~ x_1 + x_2, data = .) p_hat_lm &lt;- predict(fit_lm, mnist_27$test) y_hat_lm &lt;- factor(ifelse(p_hat_lm &gt; 0.5, 7, 2)) confusionMatrix(y_hat_lm, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.75 And we see that kNN, with the default parameter, already beats regression. To see why this is the case, we will plot \\(\\hat{p}(x_1, x_2)\\) and compare it to the true conditional probability \\(p(x_1, x_2)\\): We see that kNN better adapts to the non-linear shape of \\(p(x_1, x_2)\\). However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. This is due to what we call over-training. We describe over-training in detail below. Over-training is the reason that we have higher accuracy in the train set compared to the test set: y_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = &quot;class&quot;) confusionMatrix(y_hat_knn, mnist_27$train$y)$overall[&quot;Accuracy&quot;] Accuracy 0.882 y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;) confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.815 14.7.1 Over-training Over-training is at its worst when we set \\(k=1\\). With \\(k=1\\), the estimate for each \\((x_1, x_2)\\) in the training set is obtained with just the \\(y\\) corresponding to that point. In this case, if the \\((x_1, x_2)\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself. Remember that if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly. Here we fit a kNN model with \\(k=1\\): knn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1) y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = &quot;class&quot;) confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[&quot;Accuracy&quot;]] [1] 0.998 However, the test set accuracy is actually worse than logistic regression: y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = &quot;class&quot;) confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.74 We can see the over-fitting problem in this figure. The black curves denote the decision rule boundaries. The estimate \\(\\hat{p}(x_1, x_2)\\) follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points \\((x_1, x_2)\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions. 14.7.2 Over-smoothing Although not as badly as with the previous examples, we saw that with \\(k=5\\) we also over-trained. Hence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k=401\\). knn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401) y_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = &quot;class&quot;) confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[&quot;Accuracy&quot;] Accuracy 0.79 This turns out to be similar to regression: This size of \\(k\\) is so large that it does not permit enough flexibility. We call this over-smoothing. 14.7.3 Picking the \\(k\\) in kNN So how do we pick \\(k\\)? In principle we want to pick the \\(k\\) that maximizes accuracy, or minimizes the expected MSE as defined in 14.3.8. The goal of cross validation is to estimate these quantities for any given algorithm and set of tuning parameters such as \\(k\\). To understand why we need a special method to do this let’s repeat what we did above but for different values of \\(k\\): ks &lt;- seq(3, 251, 2) We do this using map_df function to repeat the above for each one. library(purrr) accuracy &lt;- map_df(ks, function(k){ fit &lt;- knn3(y ~ ., data = mnist_27$train, k = k) y_hat &lt;- predict(fit, mnist_27$train, type = &quot;class&quot;) cm_train &lt;- confusionMatrix(y_hat, mnist_27$train$y) train_error &lt;- cm_train$overall[&quot;Accuracy&quot;] y_hat &lt;- predict(fit, mnist_27$test, type = &quot;class&quot;) cm_test &lt;- confusionMatrix(y_hat, mnist_27$test$y) test_error &lt;- cm_test$overall[&quot;Accuracy&quot;] tibble(train = train_error, test = test_error) }) Note that we estimate accuracy by using both the training set and the test set. We can now plot the accuracy estimates for each value of \\(k\\): First, note that the estimate obtained on the training set is generally lower than the estimate obtained with the test set, with the difference larger for smaller values of \\(k\\). This is due to over-training. Also note that the accuracy versus \\(k\\) plot is quite jagged. We do not expect this because small changes in \\(k\\) should not affect the algorithm’s performance too much. The jaggedness is explained by the fact that the accuracy is computed on a sample and therefore is a random variable. This demonstrates why we prefer to minimize the expected loss rather than the loss we observe with one dataset. If we were to use these estimates to pick the \\(k\\) that maximizes accuracy, we would use the estimates built on the test data: ks[which.max(accuracy$test)] [1] 41 max(accuracy$test) [1] 0.86 Another reason we need a better estimate of accuracy is that if we use the test set to pick this \\(k\\), we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here we broke a golden rule of machine learning: we selected the \\(k\\) using the test set. Cross validation also provides an estimate that takes this into account. 14.8 Mathematical description of cross validation In Section 14.3.8, we described that a common goal of machine learning is to find an algorithm that produces predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the MSE: \\[ \\mbox{MSE} = \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\} \\] When all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this: \\[ \\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\] These two are often referred to as the true error and apparent error, respectively. There are two important characteristics of the apparent error we should always keep in mind: Because our data is random, the apparent error is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck. If we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We will see an extreme example of this with k-nearest neighbors. Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to \\(B\\) new random samples of the data, none of them used to train the algorithm. As shown in a previous chapter, we think of the true error as: \\[ \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2 \\] with \\(B\\) a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because we only have available one set of outcomes: \\(y_1, \\dots, y_n\\). Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have. To do this, we have to generate a series of different random samples. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error. 14.9 K-fold cross validation The first one we describe is K-fold cross validation. Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow). But we don’t get to see these independent datasets. So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes. We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing. Let’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, nothing! Now this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors \\(k\\) in k-nearest neighbors. Here, we will refer to the set of parameters as \\(\\lambda\\). We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful. For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate. First, before we start the cross validation procedure, it is important to fix all the algorithm parameters. Although we will train the algorithm on the set of training sets, the parameters \\(\\lambda\\) will be the same across all training sets. We will use \\(\\hat{y}_i(\\lambda)\\) to denote the predictors obtained when we use parameters \\(\\lambda\\). So, if we are going to imitate this definition: \\[ \\mbox{MSE}(\\lambda) = \\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2 \\] we want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation, we do it \\(K\\) times. In the cartoons, we are showing an example that uses \\(K=5\\). We will eventually end up with \\(K\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M=N/K\\) observations at random (we round if \\(M\\) is not a round number) and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b=1\\). We call this the validation set: Now we can fit the model in the training set, then compute the apparent error on the independent set: \\[ \\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i=1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2 \\] Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take \\(K\\) samples, not just one. In K-cross validation, we randomly split the observations into \\(K\\) non-overlapping sets: Now we repeat the calculation above for each of these sets \\(b=1,\\dots,K\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_K(\\lambda)\\). Then, for our final estimate, we compute the average: \\[ \\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b=1}^K \\hat{\\mbox{MSE}}_b(\\lambda) \\] and obtain an estimate of our loss. A final step would be to select the \\(\\lambda\\) that minimizes the MSE. We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on: We can do cross validation again: and obtain a final estimate of our expected loss. However, note that this means that our entire compute time gets multiplied by \\(K\\). You will soon learn that performing this task takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set. Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters. Now how do we pick the cross validation \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original dataset. However, larger values of \\(K\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of \\(K=5\\) and \\(K=10\\) are popular. One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick \\(K\\) sets of some size at random. One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the bootstrap. In fact, this is the default approach in the caret package. We describe how to implement cross validation with the caret package in the next chapter. In the next section, we include an explanation of how the bootstrap works in general. 14.10 Bootstrap Suppose the income distribution of your population is as follows: set.seed(1995) n &lt;- 10^6 income &lt;- 10^(rnorm(n, log10(45000), log10(3))) qplot(log10(income), bins = 30, color = I(&quot;black&quot;)) The population median is: m &lt;- median(income) m [1] 44939 Suppose we don’t have access to the entire population, but want to estimate the median \\(m\\). We take a sample of 100 and estimate the population median \\(m\\) with the sample median \\(M\\): N &lt;- 100 X &lt;- sample(income, N) median(X) [1] 38461 Can we construct a confidence interval? What is the distribution of \\(M\\) ? Because we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of \\(M\\). library(gridExtra) B &lt;- 10^4 M &lt;- replicate(B, { X &lt;- sample(income, N) median(X) }) p1 &lt;- qplot(M, bins = 30, color = I(&quot;black&quot;)) p2 &lt;- qplot(sample = scale(M), xlab = &quot;theoretical&quot;, ylab = &quot;sample&quot;) + geom_abline() grid.arrange(p1, p2, ncol = 2) If we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. In the past, we have used the Central Limit Theorem, but the CLT we studied applies to averages and here we are interested in the median. We can see that the 95% confidence interval based on CLT median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1) [1] 21018 55905 is quite different from the confidence interval we would generate if we know the actual distribution of \\(M\\): quantile(M, c(0.025, 0.975)) 2.5% 97.5% 34438 59050 The bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample (with replacement) datasets, of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on these bootstrap samples. Theory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution: B &lt;- 10^4 M_star &lt;- replicate(B, { X_star &lt;- sample(X, N, replace = TRUE) median(X_star) }) Note a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution: quantile(M_star, c(0.025, 0.975)) 2.5% 97.5% 30253 56909 For more on the Bootstrap, including corrections one can apply to improve these confidence intervals, please consult the book An introduction to the bootstrap by Efron, B., &amp; Tibshirani, R. J. Note that we can use ideas similar to those used in the bootstrap in cross validation: instead of dividing the data into equal partitions, we simply bootstrap many times. https://en.wikipedia.org/wiki/Receiver_operating_characteristic↩ "],
["A-rubin-causal-model.html", "A Rubin Causal Model A.1 Causal effects A.2 Potential outcomes A.3 No causation without manipulation A.4 Average treatment effect A.5 Stable unit treatment value assumption (SUTVA) A.6 The fundamental problem of causal inference A.7 The assignment mechanism A.8 Permutation tests A.9 Confounding and selection bias A.10 Internal and external validity A.11 Survey research and external validity A.12 Conclusion A.13 References", " A Rubin Causal Model The Rubin causal model (RCM) is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes, named after Donald Rubin.66 The Rubin causal model (RCM) is based on the idea of potential outcomes. For example, let’s say you took aspirin and now no longer have a headache. To measure the causal effect of taking the aspirin, we need to compare the outcome for you in one alternative future (where you took the aspirin) to another (where you did not). Since it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing. This dilemma is the “fundamental problem of causal inference.” A.1 Causal effects Here’s how Rubin defines a “causal effect”: Intuitively, the causal effect of one treatment, E, over another, C, for a particular unit and an interval of time from \\(t_1\\) to \\(t_2\\) is the difference between what would have happened at time \\(t_2\\) if the unit had been exposed to E initiated at \\(t_1\\) and what would have happened at \\(t_2\\) if the unit had been exposed to C initiated at \\(t_1\\): ‘If an hour ago I had taken two aspirins instead of just a glass of water, my headache would now be gone,’ or ‘because an hour ago I took two aspirins instead of just a glass of water, my headache is now gone.’ Our definition of the causal effect of the E versus C treatment will reflect this intuitive meaning.67 Thus, according to the RCM, the causal effect of your taking or not taking aspirin one hour ago is the difference between how your head would have felt in case 1 (taking the aspirin) and case 2 (not taking the aspirin). If your headache would remain without aspirin but disappear if you took aspirin, then the causal effect of taking aspirin is headache relief. In most circumstances, we are interested in comparing two futures, one generally termed “treatment” and the other “control.” These labels are somewhat arbitrary. The difference between the potential outcome under treatment and the potential outcome under control is called a “causal effect” or a “treatment effect.” The scenario that didn’t actually happen, and thus that we didn’t observe, is called a “counterfactual.” A.2 Potential outcomes Suppose that Joe is participating in an FDA test for a new hypertension drug. If we were omniscient, we would know the outcomes for Joe under both treatment (the new drug) and control (either no treatment or the current standard treatment). The causal effect, or treatment effect, is the difference between these two potential outcomes. Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(135\\) \\(-5\\) \\(Y_{t}(u)\\) is Joe’s systolic blood pressure if he takes the new pill. In general, this notation expresses the potential outcome which results from a treatment, \\(t\\), on a unit, \\(u\\). Similarly, \\(Y_{c}(u)\\) is the effect of a different treatment, \\(c\\) or control, on a unit, \\(u\\). In this case, \\(Y_{c}(u)\\) is Joe’s blood pressure if he doesn’t take the pill. \\(Y_{t}(u)-Y_{c}(u)\\) is the causal effect of taking the new drug. Thus, Joe would have a blood pressure of \\(130\\) if he took the pill and \\(135\\) if he did not; the causal effect of taking the pill is \\(-5\\). Joe should take the pill! From this table we only know the causal effect on Joe. Everyone else in the study might have an increase in blood pressure if they take the pill. However, regardless of what the causal effect is for the other subjects, the causal effect for Joe is lower blood pressure, relative to what his blood pressure would have been if he had not taken the pill. A.3 No causation without manipulation In order for a potential outcome to make sense, it must be possible, at least a priori. For example, if there is no way for Joe, under any circumstance, to obtain the new drug, then \\(Y_{t}(u)\\) is impossible for him. It can never happen. And if \\(Y_{t}(u)\\) can never be observed, even in theory, then the causal effect of treatment on Joe’s blood pressure is not defined. The causal effect of new drug is well defined because it is the simple difference of two potential outcomes, both of which might happen. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing or a different thing might happen. This definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to happen, ever. For example, what is the causal effect of Joe’s height on his weight? Naively, this seems similar to our other examples. We just need to compare two potential outcomes: what would Joe’s weight be under the treatment (where treatment is defined as being 3 inches taller) and what would Joe’s weight be under the control (where control is defined as his current height). A moment’s reflection highlights the problem: we can’t increase Joe’s height. There is no way to observe, even conceptually, what Joe’s weight would be if he were taller because there is no way to make him taller. We can’t manipulate Joe’s height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: No causation without manipulation. A.4 Average treatment effect Consider a larger sample of patients: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(135\\) \\(-5\\) Mary \\(130\\) \\(145\\) \\(-15\\) Sally \\(130\\) \\(145\\) \\(-15\\) Bob \\(140\\) \\(150\\) \\(-10\\) James \\(145\\) \\(140\\) \\(+5\\) MEAN \\(135\\) \\(143\\) \\(-8\\) The causal effect is different for every subject, but the drug works for Joe, Mary, Sally and Bob because the causal effect is negative. Their blood pressure is lower with the drug than it would have been if each did not take the drug. For James, however, the drug causes an increase in blood pressure. One may calculate the average treatment effect (often abbreviated ATE) by taking the mean of all the causal effects. Here, the mean is \\(-8\\). Note that we can calculate the average treatment effect in this case because we are omniscient and know all the potential outcomes under both treatment and control. We will soon see what assumptions we need to make in order to calculate the average treatment effect in the real world where only one potential outcome can ever be observed. In particular, while we cannot estimate individual causal effects without very strong assumptions, randomized experiments can allow for good estimates of the ATE, as we will see later in this chapter. A.5 Stable unit treatment value assumption (SUTVA) One important assumption for causal inference is that “the [potential outcome] observation on one unit should be unaffected by the particular assignment of treatments to the other units.”68 This is called the Stable Unit Treatment Value Assumption (SUTVA). In the context of our example, Joe’s blood pressure should not depend on whether Mary receives the drug. But what if it does? Suppose that Joe and Mary live in the same house and Mary always cooks. The drug causes Mary to crave salty foods, so if she takes the drug she will cook with more salt than she would have otherwise. A high salt diet increases Joe’s blood pressure. Therefore, his outcome will depend on both which treatment he received and which treatment Mary receives. SUTVA violation makes causal inference more difficult. We can account for dependent observations by considering more treatments. We create 4 treatments by taking into account whether or not Mary receives treatment: Subject Joe = c,Mary = t Joe = t,Mary = t Joe = c,Mary = c Joe = t,Mary = c Joe \\(140\\) \\(130\\) \\(125\\) \\(120\\) Recall that a causal effect is defined as the difference between two potential outcomes. In this case, there are multiple causal effects because there are more than two potential outcomes: One is the causal effect of the drug on Joe when Mary receives treatment (\\(130-140\\)). Another is the causal effect on Joe when Mary does not receive treatment (\\(120-125\\)). A third is the causal effect of Mary’s treatment on Joe when Joe is not treated (\\(140-125\\)). Note that Mary taking the drug has a larger causal effect on Joe than Joe himself taking the drug, and what’s more, the effect is in the opposite dierction! By considering more potential outcomes in this way, we can cause SUTVA to hold. However, if any units other than Joe are dependent on Mary, then we must consider further potential outcomes. The greater the number of dependent units, the more potential outcomes we must consider and the more complex the calculations become (consider an experiment with 20 different people, each of whose treatment status can affect outcomes for every one else). In order (easily) to estimate the causal effect of a single treatment relative to a control, SUTVA should hold. A.6 The fundamental problem of causal inference The results we have seen up to this point could never be measured in practice. It is impossible, by definition, to observe the effects of more than one treatment on a single subject. Joe cannot both take the pill and not take the pill at the same time! Therefore, the data we actually observe would look something like this: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(?\\) \\(?\\) Mary \\(120\\) \\(?\\) \\(?\\) Sally \\(?\\) \\(125\\) \\(?\\) Bob \\(?\\) \\(130\\) \\(?\\) James \\(115\\) \\(?\\) \\(?\\) MEAN \\(121.67\\) \\(127.5\\) \\(-5.83\\) Question marks are responses that could not be observed. The Fundamental Problem of Causal Inference69 is that directly observing unit-level causal effects is impossible. However, this does not make causal inference impossible. Certain techniques and assumptions allow the fundamental problem to be overcome. In particular, we may want to know whether we can get a good estimate of the average treatment effect. If we knew that the drug lowered blood pressure on average, that would help us make decisions even if we can’t observe the causal effect of the drug on individual people (because we don’t observe counterfactuals). The most natural place to start is with the difference in the sample mean outcomes between treatment and control, \\(-5.83\\). Note that unlike earlier, this is an estimate of the ATE, not the ATE itself. Why? Because we have missing data! For each unit, we only observe one of the potential outcomes. A.7 The assignment mechanism Is the difference in sample means between treated units and control units a good estimate of the average treatment effect? That depends entirely on the method by which units are assigned treatment, which is called the assignment mechanism. One such assignment mechanism is randomization. For each subject we could flip a coin to determine if she receives treatment. If we wanted five subjects to receive treatment, we could assign treatment to the first five names we pick out of a hat. Randomized assignment is the best assignment mechanism for inferring average treatment effects because if the sample is large enough, the difference in sample means between treated and control units will be very close to the true average treatment effect in that sample. However, when the sample size is small, our estimated average treatment effect may deviate considerably from the truth. For example, the data in the last section were from just one possible random assignment, where Joe, Mary and James receive the treatment. Here’s another possible random assignment, where Joe, Mary and Sally receive the treatment: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(?\\) \\(?\\) Mary \\(120\\) \\(?\\) \\(?\\) Sally \\(100\\) \\(?\\) \\(?\\) Bob \\(?\\) \\(130\\) \\(?\\) James \\(?\\) \\(120\\) \\(?\\) MEAN \\(115\\) \\(123\\) \\(-8.33\\) Note that with one assignment, the estimate of the ATE is \\(-5.83\\), while with another it is \\(-8.33\\). Assume that these data are the truth: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(115\\) \\(+15\\) Mary \\(120\\) \\(125\\) \\(-5\\) Sally \\(100\\) \\(125\\) \\(-25\\) Bob \\(110\\) \\(130\\) \\(-20\\) James \\(115\\) \\(120\\) \\(-5\\) MEAN \\(115\\) \\(123\\) \\(-8\\) Thus, the true average treatment effect is \\(-8\\). However, our estimates of the average treatment effect vary because our sample is small and the responses have a large variance. If the sample were larger and the variance were less, the average treatment effect would be closer to the true average treatment effect regardless of the specific units randomly assigned to treatment. A.8 Permutation tests Let’s say we observed the following results, which we know came from a random treatment assignment: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Mary \\(120\\) \\(?\\) \\(?\\) Sally \\(100\\) \\(?\\) \\(?\\) Bob \\(?\\) \\(130\\) \\(?\\) James \\(?\\) \\(120\\) \\(?\\) MEAN \\(110\\) \\(125\\) \\(-15\\) The drug looks very useful! However, how likely is it that we would observe these results if the drug did nothing at all? That is, how confident should we be that the drug is actually helpful? To answer this question, we can use a permutation test.70 The intuition behind a permutation test is simple. We observed four outcomes, two of which were assigned to treatment and two that were assigned to control. To conduct a permutation test, we calculate our quantity of interest (here, the difference in means between treated and control) for every possible arrangement of the labels “treatment” and “control” across these four numbers. This is easiest to understand visually: Permutation \\(120\\) \\(100\\) \\(130\\) \\(120\\) ATE #1 T T C C \\(-15\\) #2 T C T C \\(+15\\) #3 T C C T \\(+5\\) #4 C T T C \\(-5\\) #5 C T C T \\(-15\\) #6 C C T T \\(+15\\) Here, permutation #1 is what we actually observed, and permutations #2-6 are all the possible rearrangement of the two “treatment” labels and the two “control” labels. What do we see? 4/6 (67%) of the permutations produce calculated ATEs are at least as large as the effect we actually observed (\\(-15\\)), and two of them are the opposite sign! We therefore should not have much confidence from these data alone that the drug really lowers blood pressure, even though the effect size appeared large. The moral of the story? Don’t conduct an experiment on only four people! Let’s say that we had a larger experiment. We can no longer calculate the results of the permutation test by hand, since the number of possible permutations will quickly become very large. However, you can use R to calculate far more permutations than you can do by hand. Furthermore, if the number of permutations becomes too much even for your computer, you can take a random sample of all the possible permutations instead; if the random sample is large enough, this will give you a result very close to what you would get if you considered all the permutations. A.9 Confounding and selection bias With a large enough sample and randomized assignment, we can get a good estimate of the ATE. Other assignment mechanisms, however, can make it difficult or impossible to engage in causal inference. Why is that? Consider the use of the perfect doctor as an assignment mechanism. The perfect doctor knows how each subject will respond to the drug or the control and assigns each subject to the treatment that will most benefit her. The perfect doctor knows this information about a sample of patients: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(130\\) \\(115\\) \\(+15\\) Mary \\(120\\) \\(125\\) \\(-5\\) Sally \\(100\\) \\(150\\) \\(-50\\) Bob \\(115\\) \\(125\\) \\(-10\\) James \\(120\\) \\(130\\) \\(-10\\) Susie \\(135\\) \\(105\\) \\(+30\\) MEAN \\(120\\) \\(125\\) \\(-5\\) Based on this knowledge she would make the following treatment assignments: Subject \\(Y_t(u)\\) \\(Y_c(u)\\) \\(Y_t(u) - Y_c(u)\\) Joe \\(?\\) \\(115\\) \\(?\\) Mary \\(120\\) \\(?\\) \\(?\\) Sally \\(100\\) \\(?\\) \\(?\\) Bob \\(115\\) \\(?\\) \\(?\\) James \\(120\\) \\(?\\) \\(?\\) Susie \\(?\\) \\(105\\) \\(?\\) MEAN \\(113.75\\) \\(110\\) \\(+3.75\\) The perfect doctor distorts the averages of both \\(Y_t(u)\\) and \\(Y_c(u)\\) by filtering out poor responses to both the treatment and control. The difference in means thus is distorted in a direction that depends on the details. For instance, a subject like Susie who is harmed by taking the drug would be assigned to the control group by the perfect doctor and thus the negative effect of the drug would be masked. Therefore, the difference in means is no longer a good estimate of the ATE. In fact, in this case it has the wrong sign! This is not merely a consequence of our small sample: even if the perfect doctor saw a million patients, we could not get a good estimate of the ATE. This is an extreme example of a problem called selection bias. The perfect doctor is not choosing which patients are receiving the drug at random. Rather, the perfect doctor is making treatment decisions based directly on the patient’s potential outcomes. Whenever the assignment mechanism is correlated with the potential outcomes, we say that there is confounding. Confounding is a problem, since it means that our simple estimate of the ATE is biased. Thus, if you are interested in causal inference, randomized trials are the best approach. In many circumstances, however, randomized trials are not possible due to ethical or practical concerns. In such scenarios there is by necessity a non-random assignment mechanism. For instance, let’s say you were interested in the effect of college attendence on earnings. People are not randomly assigned to attend college. Rather, people may choose to attend college based on their financial situation, parents’ education, and so on. This can introduce confounding if the assignment mechanism affects future earnings. For example, if people choose to go to college at higher rates when they are on career paths where a college degree is particularly beneficial – or, in RCM language, whose potential outcomes are on average higher under treatment – that would introduce confounding. Many statistical methods have been developed for causal inference when there is a non-random assignment mechanism, such as propensity score matching. These methods attempt to correct for the assignment mechanism by finding control units similar to treatment units. What you should not do is naively compare the sample means under treatment and control and assume that is a good estimate of the ATE. Without randomization, this could be very misleading! A.10 Internal and external validity If we have randomized assignment and a large sample, we can be confident that we have a good estimate of the average treatment effect in that sample. We say that the experiment has high internal validity: the inferences we are making are likely to reflect the truth about that sample. However, we may be interested in a population beyond our particular sample. For example, consider the hypertension drug example discussed above. Let’s say that we ran a randomized trial on 10,000 patients, with 5,000 receiving the new drug and 5,000 in the contol group. We estimate an average treatment effect of \\(-5\\) units of systolic blood pressure. Should we recommend this new drug? The answer to that question depends in part on the external validity of the study. Are the 10,000 people in the study similar to the people for whom we will be making recommendations? Let’s say that the 10,000 study participants were chosen because they all lived near the location that the study took place. That’s one example of selection bias, where the sample is not a random sample of the population in which we are interested. Why is that a problem? Those people may differ systematically from other people, including in ways that affect their response to blood pressure medicine. For example, what if the people in this area are younger on average than most people who take blood pressure medicine? This concern can be expressed in terms of the assignment mechanism. People who live far away from the location of the study have a 0% chance of receiving the treatment. Thus, the study can’t directly speak to whether the treatment is effective for them; the only way we can make such claims is by making additional assumptions, such as that the non-participants will respond the same on average to the drug as the participants. The circumstances of the experiment may also affect the external validity of the study. Perhaps the study participants were all eating a diet that was controlled by the experimenters. Then, while we have variation in one aspect of the treatment (whether the participants received the blood pressure medicine), we don’t have variation in another (diet). It may be that the blood pressure medicine works when on the controlled diet but doesn’t work otherwise. When dealing with human subjects, there is a particular concern regarding external validity: the Hawthorne effect. When human subjects know that they are part of an experiment, they may change their behavior. This can lead to a version of the above concern. Suppose again that the blood pressure drug works when combined with a controlled diet, say a low-salt diet, but it doesn’t work for those who have a high-salt diet. If the subjects in the experiment are paying greater attention to their blood pressure than they otherwise would and thus are eating lower sodium diets than other people, one could make the mistaken inference that it was the blood pressure drug alone that led to a decrease in blood pressure, and not the combination of the drug and the experimental subjects’ changing behavior. A.11 Survey research and external validity In political science and other social sciences, survey research poses some particular problems of external validity. Let’s say that you want to measure presidential approval among all U.S. voters. How can you make sure that the sample you collect is representative of your sampling frame? (In the language of survey sampling, the “sampling frame” is the population you want to study: here, voters in the United States.) The most natural approach is to engage in a simple random sample: each U.S. voter has the same probability of being in the survey. This would ensure that one’s sample was on average representative of the population, although there will still be some sampling error because the sample is finite. About 139 million people voted in the 2016 general election,71 so if one wanted a sample of 1,390 voters, one would like a sampling method that gives each voter an equivalent 0.001% chance of being surveyed. But of course, there’s no method of assuring that each voter has an equivalent chance of being in your survey! Let’s say that you decided to conduct your survey via random digit dialing, where you call possible phone numbers at random and give your survey to those who answer. There are many possible sources of bias in this measure: Some voters may not have telephone numbers Some voters may be more likely to answer the phone and take your survey than others Some people may lie or misremember about whether they voted in the 2016 general election More subtly, respondents’ presidential approval may be affected by other questions you ask in the survey. (Perhaps this is what you are studying, for instance if you were testing the effect of a particular message on presidential approval, but if not, you need to be careful in interpreting your results.) And these are just the most obvious! If you take a course in survey research, you’ll learn more about the ways in which pollsters attempt to deal with these problems. With the growing popularity of survey experiments in social research, it is always worth considering how the sampling was conducted. A.12 Conclusion The causal effect of a treatment on a single unit at a point in time is the difference between the outcome variable with the treatment and without the treatment. The Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. You either take the aspirin now or you don’t. As a consequence, assumptions must be made in order to estimate the missing counterfactuals. A.13 References Cox, D. R. (1958). Planning of Experiments. Wiley. Holland, Paul W. (1986). “Statistics and Causal Inference.” J. Amer. Statist. Assoc. 81 (396): 945–960. doi:10.1080/01621459.1986.10478354. Neyman, Jerzy. (1923). Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes. Master’s Thesis. Excerpts reprinted in English, Statistical Science, Vol. 5, pp. 463–472. (D. M. Dabrowska, and T. P. Speed, Translators.) Rubin, Donald (1974). “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” J. Educ. Psychol. 66 (5): 688–701 [p. 689]. doi:10.1037/h0037350. Rubin, Donald (2005). “Causal Inference Using Potential Outcomes.” J. Amer. Statist. Assoc. 100 (469): 322–331. doi:10.1198/016214504000001880. This chapter remixes some material from Rubin causal model (Wikipedia). The name “Rubin causal model” was first coined by Paul W. Holland (1986). The potential outcomes framework was first proposed by Jerzy Neyman in his 1923 Master’s thesis (Neyman 1923), though he discussed it only in the context of completely randomized experiments (Rubin 2005)..↩ Rubin 1974.↩ Cox 1958, §2.4.↩ Holland 1986.↩ See this post for a longer discussion.↩ See here.↩ "],
["B-maps.html", "B Maps B.1 Tidycensus", " B Maps In order to make maps, we first need some data. B.1 Tidycensus "],
["C-animation.html", "C Animation", " C Animation "],
["D-shiny.html", "D Shiny D.1 Helpful Resources D.2 Set Up and Getting Started D.3 Building Your Basic App D.4 Organization D.5 Customizations", " D Shiny D.1 Helpful Resources There are a number of resources on Shiny to get you started, so definitely take a look at the following materials in addition to using this guide: Shiny Video Tutorials is a comprehensive video tutorial that goes through all of the basics of building a Shiny app and adding customization. It is 2 and a half hours long, but individual chapters can be found at the same link for walk throughs on smaller topics. The first 40 or so minutes of the tutorial are particularly helpful in getting a basic app running and available on shinyapps.io. Shiny Written Tutorials is also quite comprehensive, and walks through templates for the example Shiny document. It breaks down the set up of UI, control widgets, reactive output (like dropdown variables), using R scripts and data, using reactive expressions, and sharing the app. Mastering Shiny by Hadley Wickham is a really nice guide to using Shiny and best practices for higher-quality apps, including reducing code duplication. D.2 Set Up and Getting Started Sign up for a Shiny account at https://www.shinyapps.io/. Install the Shiny package with: install.packages(“shiny”) Create a Shiny app from RStudio with: File &gt; New File &gt; Shiny Web App… In the popup, choose Single File (app.R) - this doesn’t particularly matter, but is the way that the Preceptor taught in class, and is the way that his example is set up. If you would prefer to have two separate files for ui and server, this is also valid and might allow for cleaner code. You should now have a functioning example Shiny App - click Run App or use the keyboard shortcut Cmd/Ctrl + Shift + Enter to see this app in action! You can view it in a new window, or in an external web browser (see the dropdown arrow next to Run App for the options). Notice that the example includes a slider that allows the viewer to change the number of the bins that the histogram has. D.3 Building Your Basic App Now that you have your functioning Shiny App up and running, let’s take a closer look at our files and directories. Notice that the app.R file has been created within a directory. It is important to remember that only the files within this directory will be accessible to the app when it is online. Taking a look at the app.R file, we can see that there are four necessary elements in this file that create the working app: First, it calls library(shiny) Then, it defines a user interface with ui &lt;- fluidPage(…) It also defines server logic, which takes in an input from the UI, and produces an output based on that input, as defined by server &lt;- function(input, output) {…} Finally, it calls shinyApp(ui, server) to run the app. D.3.1 Setting Up the Basic UI According to the Preceptor, the Gov 1005 Final Projects should be formatted such that they have three tabs, as well as an embedded video. A basic set up for that might be something like this: ui &lt;- navbarPage( &quot;Final Project Title&quot;, tabPanel(&quot;Model&quot;, fluidPage( titlePanel(&quot;Model Title&quot;), sidebarLayout( sidebarPanel( selectInput( &quot;plot_type&quot;, &quot;Plot Type&quot;, c(&quot;Option A&quot; = &quot;a&quot;, &quot;Option B&quot; = &quot;b&quot;) )), mainPanel(plotOutput(&quot;line_plot&quot;))) )), tabPanel(&quot;Discussion&quot;, titlePanel(&quot;Discussion Title&quot;), p(&quot;Tour of the modeling choices you made and an explanation of why you made them&quot;)), tabPanel(&quot;About&quot;, titlePanel(&quot;About&quot;), h3(&quot;Project Background and Motivations&quot;), p(&quot;Hello, this is where I talk about my project.&quot;), h3(&quot;About Me&quot;), p(&quot;My name is ______ and I study ______. You can reach me at ______@college.harvard.edu.&quot;))) You can run the example app in this repo to see this code in action. Breaking this code down, we have created a navigation bar page with three tabs. The first tab will display the model, and has two potential input options, Option A or Option B. This can be customized based on your model, and the number of inputs desired. Note that this is wrapped inside of a fluidPage(), which simply creates a basic page layout that can support rows and columns if so desired. Breaking down this tab further, we have sidebarLayout() defining the layout of this page with a sidebar panel and a main panel. Within the sidebar panel, we have inserted a selectInput, which will allow us to have reactive pages. Note that I have used HTML formatting for the Discussion and About pages. D.3.2 Setting up the Server The server function is where we perform our backend logic for our app. We can put any R code there to create plots or tables as needed. The inputs come from UI definitions, like selectInput() or sliderInput(). In our UI example, we used a select input named “plot_type”, which had two options: A or B. The outputs are defined within the server function, and then rendered as defined in the UI. In our UI example, we want to render a plotOutput named “line_plot”. So our server function would ideally use the plot_type input to create a line_plot. An example of a basic server function might be something like this: server &lt;- function(input, output) { output$line_plot &lt;- renderPlot({ # generate type based on input$plot_type from ui ifelse( input$plot_type == &quot;a&quot;, # if input$plot_type is &quot;a&quot;, plot histogram of &quot;waiting&quot; column # from the faithful dataframe x &lt;- faithful[, 2], # if input$plot_type is &quot;b&quot;, plot histogram of &quot;eruptions&quot; column # from the faithful dataframe x &lt;- faithful[, 1] ) # draw the histogram with the specified number of bins hist(x, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) } D.4 Organization Now that we have the basic app running, let’s talk about organization. One common problem that shows up a lot is that people tend to throw all of their code into their app.R file, causing it to grow to be hundreds or even thousands of lines of code long. This happens easily because not only can backend logic be written into the server function, but additionally entire HTML or markdown pages can be formatted in the UI section. Here are a few ways to stop the app.R file from becoming incredibly messy. First, you can put all large blocks of text into separate files. For example, we could take the HTML formatting from the “About” tab section and put it into a separate about.html file, or similarly format the same section using markdown. Then you would simply use the shiny function includeHTML() or includeMarkdown() to insert your file contents. Second, put as much prep work as possible into a separate (or multiple separate) R file. This is helpful for readability, as it will generate more small files, but it is also helpful in that the app will not have to run all of the heavy prep calculations while it’s trying to load. For this, a tip is to perform heavy operations in a document titled prep-shiny.R, for example, and to save any helpful outputs as graphic images or gifs, or as rds files that can then be loaded into the app.R file. Third, keep your repository clean. Do not save unneccessary raw data files or the like to your repository, as this may just create clutter. Make sure to update your .gitignore file with anything that you would not like to have on your Github repository or on the Shiny App. This includes the .Rproj file, raw data files, and cache files. D.5 Customizations Finally, we can take a look at a simple way to customize the look of your Shiny App. The package shinythemes can easily be added to give your app a theme. The shinythemes website displays the various themes quite nicely, and you can use the theme selector to click through different themes. The Shiny gallery is helpful for browsing through different layouts and types of visualization for inspiration. "],
["references-1.html", "References", " References "]
]
