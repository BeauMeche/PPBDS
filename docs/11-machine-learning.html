<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Machine Learning | Preceptor’s Primer for Bayesian Data Science</title>
  <meta name="description" content="Chapter 11 Machine Learning | Preceptor’s Primer for Bayesian Data Science" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Machine Learning | Preceptor’s Primer for Bayesian Data Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="davidkane9/PPBDS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Machine Learning | Preceptor’s Primer for Bayesian Data Science" />
  
  
  

<meta name="author" content="David Kane" />


<meta name="date" content="2020-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="10-classification.html"/>
<link rel="next" href="A-productivity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="forward.html"><a href="forward.html"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="warning.html"><a href="warning.html"><i class="fa fa-check"></i>Warning</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="1" data-path="1-viz.html"><a href="1-viz.html"><i class="fa fa-check"></i><b>1</b> Visualization</a><ul>
<li class="chapter" data-level="1.1" data-path="1-viz.html"><a href="1-viz.html#r-rstudio"><i class="fa fa-check"></i><b>1.1</b> What are R and RStudio?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-viz.html"><a href="1-viz.html#installing"><i class="fa fa-check"></i><b>1.1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-viz.html"><a href="1-viz.html#using-r-via-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Using R via RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-viz.html"><a href="1-viz.html#code"><i class="fa fa-check"></i><b>1.2</b> How do I code in R?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-viz.html"><a href="1-viz.html#programming-concepts"><i class="fa fa-check"></i><b>1.2.1</b> Basic programming concepts and terminology</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-viz.html"><a href="1-viz.html#messages"><i class="fa fa-check"></i><b>1.2.2</b> Errors, warnings, and messages</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-viz.html"><a href="1-viz.html#tips-code"><i class="fa fa-check"></i><b>1.2.3</b> Tips on learning to code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-viz.html"><a href="1-viz.html#packages"><i class="fa fa-check"></i><b>1.3</b> What are R packages?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-viz.html"><a href="1-viz.html#package-installation"><i class="fa fa-check"></i><b>1.3.1</b> Package installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-viz.html"><a href="1-viz.html#package-loading"><i class="fa fa-check"></i><b>1.3.2</b> Package loading</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-viz.html"><a href="1-viz.html#package-use"><i class="fa fa-check"></i><b>1.3.3</b> Package use</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-viz.html"><a href="1-viz.html#nycflights13"><i class="fa fa-check"></i><b>1.4</b> Explore your first datasets</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-viz.html"><a href="1-viz.html#nycflights13-package"><i class="fa fa-check"></i><b>1.4.1</b> <code>nycflights13</code> package</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-viz.html"><a href="1-viz.html#flights-data-frame"><i class="fa fa-check"></i><b>1.4.2</b> <code>flights</code> data frame</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-viz.html"><a href="1-viz.html#exploredataframes"><i class="fa fa-check"></i><b>1.4.3</b> Exploring data frames</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-viz.html"><a href="1-viz.html#identification-vs-measurement-variables"><i class="fa fa-check"></i><b>1.4.4</b> Identification and measurement variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-viz.html"><a href="1-viz.html#help-files"><i class="fa fa-check"></i><b>1.4.5</b> Help files</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-viz.html"><a href="1-viz.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-viz.html"><a href="1-viz.html#additional-resources"><i class="fa fa-check"></i><b>1.5.1</b> Additional resources</a></li>
<li class="chapter" data-level="" data-path="1-viz.html"><a href="1-viz.html#needed-packages"><i class="fa fa-check"></i>Needed packages</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-viz.html"><a href="1-viz.html#grammarofgraphics"><i class="fa fa-check"></i><b>1.6</b> The grammar of graphics</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-viz.html"><a href="1-viz.html#components-of-the-grammar"><i class="fa fa-check"></i><b>1.6.1</b> Components of the grammar</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-viz.html"><a href="1-viz.html#gapminder"><i class="fa fa-check"></i><b>1.6.2</b> Gapminder data</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-viz.html"><a href="1-viz.html#other-components"><i class="fa fa-check"></i><b>1.6.3</b> Other components</a></li>
<li class="chapter" data-level="1.6.4" data-path="1-viz.html"><a href="1-viz.html#ggplot2-package"><i class="fa fa-check"></i><b>1.6.4</b> ggplot2 package</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="1-viz.html"><a href="1-viz.html#scatterplots"><i class="fa fa-check"></i><b>1.7</b> Scatterplots</a><ul>
<li class="chapter" data-level="1.7.1" data-path="1-viz.html"><a href="1-viz.html#geompoint"><i class="fa fa-check"></i><b>1.7.1</b> Scatterplots via <code>geom_point</code></a></li>
<li class="chapter" data-level="1.7.2" data-path="1-viz.html"><a href="1-viz.html#overplotting"><i class="fa fa-check"></i><b>1.7.2</b> Overplotting</a></li>
<li class="chapter" data-level="1.7.3" data-path="1-viz.html"><a href="1-viz.html#summary"><i class="fa fa-check"></i><b>1.7.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="1-viz.html"><a href="1-viz.html#linegraphs"><i class="fa fa-check"></i><b>1.8</b> Linegraphs</a><ul>
<li class="chapter" data-level="1.8.1" data-path="1-viz.html"><a href="1-viz.html#geomline"><i class="fa fa-check"></i><b>1.8.1</b> Linegraphs via <code>geom_line</code></a></li>
<li class="chapter" data-level="1.8.2" data-path="1-viz.html"><a href="1-viz.html#summary-1"><i class="fa fa-check"></i><b>1.8.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="1-viz.html"><a href="1-viz.html#histograms"><i class="fa fa-check"></i><b>1.9</b> Histograms</a><ul>
<li class="chapter" data-level="1.9.1" data-path="1-viz.html"><a href="1-viz.html#geomhistogram"><i class="fa fa-check"></i><b>1.9.1</b> Histograms via <code>geom_histogram</code></a></li>
<li class="chapter" data-level="1.9.2" data-path="1-viz.html"><a href="1-viz.html#adjustbins"><i class="fa fa-check"></i><b>1.9.2</b> Adjusting the bins</a></li>
<li class="chapter" data-level="1.9.3" data-path="1-viz.html"><a href="1-viz.html#summary-2"><i class="fa fa-check"></i><b>1.9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="1-viz.html"><a href="1-viz.html#facets"><i class="fa fa-check"></i><b>1.10</b> Facets</a></li>
<li class="chapter" data-level="1.11" data-path="1-viz.html"><a href="1-viz.html#boxplots"><i class="fa fa-check"></i><b>1.11</b> Boxplots</a><ul>
<li class="chapter" data-level="1.11.1" data-path="1-viz.html"><a href="1-viz.html#geomboxplot"><i class="fa fa-check"></i><b>1.11.1</b> Boxplots via <code>geom_boxplot</code></a></li>
<li class="chapter" data-level="1.11.2" data-path="1-viz.html"><a href="1-viz.html#summary-3"><i class="fa fa-check"></i><b>1.11.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="1-viz.html"><a href="1-viz.html#geombar"><i class="fa fa-check"></i><b>1.12</b> Barplots</a><ul>
<li class="chapter" data-level="1.12.1" data-path="1-viz.html"><a href="1-viz.html#barplots-via-geom_bar-or-geom_col"><i class="fa fa-check"></i><b>1.12.1</b> Barplots via <code>geom_bar</code> or <code>geom_col</code></a></li>
<li class="chapter" data-level="1.12.2" data-path="1-viz.html"><a href="1-viz.html#must-avoid-pie-charts"><i class="fa fa-check"></i><b>1.12.2</b> Must avoid pie charts!</a></li>
<li class="chapter" data-level="1.12.3" data-path="1-viz.html"><a href="1-viz.html#two-categ-barplot"><i class="fa fa-check"></i><b>1.12.3</b> Two categorical variables</a></li>
<li class="chapter" data-level="1.12.4" data-path="1-viz.html"><a href="1-viz.html#summary-4"><i class="fa fa-check"></i><b>1.12.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="1-viz.html"><a href="1-viz.html#conclusion-1"><i class="fa fa-check"></i><b>1.13</b> Conclusion</a><ul>
<li class="chapter" data-level="1.13.1" data-path="1-viz.html"><a href="1-viz.html#summary-table"><i class="fa fa-check"></i><b>1.13.1</b> Summary table</a></li>
<li class="chapter" data-level="1.13.2" data-path="1-viz.html"><a href="1-viz.html#function-argument-specification"><i class="fa fa-check"></i><b>1.13.2</b> Function argument specification</a></li>
<li class="chapter" data-level="1.13.3" data-path="1-viz.html"><a href="1-viz.html#additional-resources-1"><i class="fa fa-check"></i><b>1.13.3</b> Additional resources</a></li>
<li class="chapter" data-level="1.13.4" data-path="1-viz.html"><a href="1-viz.html#whats-to-come-3"><i class="fa fa-check"></i><b>1.13.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-wrangling.html"><a href="2-wrangling.html"><i class="fa fa-check"></i><b>2</b> Tidyverse</a><ul>
<li class="chapter" data-level="2.1" data-path="2-wrangling.html"><a href="2-wrangling.html#piping"><i class="fa fa-check"></i><b>2.1</b> The pipe operator: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="2.2" data-path="2-wrangling.html"><a href="2-wrangling.html#filter"><i class="fa fa-check"></i><b>2.2</b> <code>filter</code> rows</a></li>
<li class="chapter" data-level="2.3" data-path="2-wrangling.html"><a href="2-wrangling.html#summarize"><i class="fa fa-check"></i><b>2.3</b> <code>summarize</code> variables</a></li>
<li class="chapter" data-level="2.4" data-path="2-wrangling.html"><a href="2-wrangling.html#groupby"><i class="fa fa-check"></i><b>2.4</b> <code>group_by</code> rows</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-wrangling.html"><a href="2-wrangling.html#grouping-by-more-than-one-variable"><i class="fa fa-check"></i><b>2.4.1</b> Grouping by more than one variable</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-wrangling.html"><a href="2-wrangling.html#mutate"><i class="fa fa-check"></i><b>2.5</b> <code>mutate</code> existing variables</a></li>
<li class="chapter" data-level="2.6" data-path="2-wrangling.html"><a href="2-wrangling.html#arrange"><i class="fa fa-check"></i><b>2.6</b> <code>arrange</code> and sort rows</a></li>
<li class="chapter" data-level="2.7" data-path="2-wrangling.html"><a href="2-wrangling.html#factors"><i class="fa fa-check"></i><b>2.7</b> Factors</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-wrangling.html"><a href="2-wrangling.html#the-forcats-package"><i class="fa fa-check"></i><b>2.7.1</b> The <strong>forcats</strong> package</a></li>
<li class="chapter" data-level="2.7.2" data-path="2-wrangling.html"><a href="2-wrangling.html#dropping-unused-levels"><i class="fa fa-check"></i><b>2.7.2</b> Dropping unused levels</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-wrangling.html"><a href="2-wrangling.html#reorder-factors"><i class="fa fa-check"></i><b>2.7.3</b> Change order of the levels, principled</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-wrangling.html"><a href="2-wrangling.html#change-order-of-the-levels-because-i-said-so"><i class="fa fa-check"></i><b>2.7.4</b> Change order of the levels, “because I said so”</a></li>
<li class="chapter" data-level="2.7.5" data-path="2-wrangling.html"><a href="2-wrangling.html#recode-the-levels"><i class="fa fa-check"></i><b>2.7.5</b> Recode the levels</a></li>
<li class="chapter" data-level="2.7.6" data-path="2-wrangling.html"><a href="2-wrangling.html#grow-a-factor"><i class="fa fa-check"></i><b>2.7.6</b> Grow a factor</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="2-wrangling.html"><a href="2-wrangling.html#character-vectors"><i class="fa fa-check"></i><b>2.8</b> Character Vectors</a><ul>
<li class="chapter" data-level="2.8.1" data-path="2-wrangling.html"><a href="2-wrangling.html#manipulating-character-vectors"><i class="fa fa-check"></i><b>2.8.1</b> Manipulating character vectors</a></li>
<li class="chapter" data-level="2.8.2" data-path="2-wrangling.html"><a href="2-wrangling.html#regular-expressions-resources"><i class="fa fa-check"></i><b>2.8.2</b> Regular expressions resources</a></li>
<li class="chapter" data-level="2.8.3" data-path="2-wrangling.html"><a href="2-wrangling.html#character-encoding-resources"><i class="fa fa-check"></i><b>2.8.3</b> Character encoding resources</a></li>
<li class="chapter" data-level="2.8.4" data-path="2-wrangling.html"><a href="2-wrangling.html#character-vectors-that-live-in-a-data-frame"><i class="fa fa-check"></i><b>2.8.4</b> Character vectors that live in a data frame</a></li>
<li class="chapter" data-level="2.8.5" data-path="2-wrangling.html"><a href="2-wrangling.html#regex-free-string-manipulation-with-stringr-and-tidyr"><i class="fa fa-check"></i><b>2.8.5</b> Regex-free string manipulation with stringr and tidyr</a></li>
<li class="chapter" data-level="2.8.6" data-path="2-wrangling.html"><a href="2-wrangling.html#detect-or-filter-on-a-target-string"><i class="fa fa-check"></i><b>2.8.6</b> Detect or filter on a target string</a></li>
<li class="chapter" data-level="2.8.7" data-path="2-wrangling.html"><a href="2-wrangling.html#string-splitting-by-delimiter"><i class="fa fa-check"></i><b>2.8.7</b> String splitting by delimiter</a></li>
<li class="chapter" data-level="2.8.8" data-path="2-wrangling.html"><a href="2-wrangling.html#substring-extraction-and-replacement-by-position"><i class="fa fa-check"></i><b>2.8.8</b> Substring extraction (and replacement) by position</a></li>
<li class="chapter" data-level="2.8.9" data-path="2-wrangling.html"><a href="2-wrangling.html#collapse-a-vector"><i class="fa fa-check"></i><b>2.8.9</b> Collapse a vector</a></li>
<li class="chapter" data-level="2.8.10" data-path="2-wrangling.html"><a href="2-wrangling.html#catenate-vectors"><i class="fa fa-check"></i><b>2.8.10</b> Create a character vector by catenating multiple vectors</a></li>
<li class="chapter" data-level="2.8.11" data-path="2-wrangling.html"><a href="2-wrangling.html#substring-replacement"><i class="fa fa-check"></i><b>2.8.11</b> Substring replacement</a></li>
<li class="chapter" data-level="2.8.12" data-path="2-wrangling.html"><a href="2-wrangling.html#regular-expressions-with-stringr"><i class="fa fa-check"></i><b>2.8.12</b> Regular expressions with stringr</a></li>
<li class="chapter" data-level="2.8.13" data-path="2-wrangling.html"><a href="2-wrangling.html#characters-with-special-meaning"><i class="fa fa-check"></i><b>2.8.13</b> Characters with special meaning</a></li>
<li class="chapter" data-level="2.8.14" data-path="2-wrangling.html"><a href="2-wrangling.html#character-classes"><i class="fa fa-check"></i><b>2.8.14</b> Character classes</a></li>
<li class="chapter" data-level="2.8.15" data-path="2-wrangling.html"><a href="2-wrangling.html#quantifiers"><i class="fa fa-check"></i><b>2.8.15</b> Quantifiers</a></li>
<li class="chapter" data-level="2.8.16" data-path="2-wrangling.html"><a href="2-wrangling.html#escaping"><i class="fa fa-check"></i><b>2.8.16</b> Escaping</a></li>
<li class="chapter" data-level="2.8.17" data-path="2-wrangling.html"><a href="2-wrangling.html#groups-and-backreferences"><i class="fa fa-check"></i><b>2.8.17</b> Groups and backreferences</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="2-wrangling.html"><a href="2-wrangling.html#combining-data"><i class="fa fa-check"></i><b>2.9</b> Combining Data</a><ul>
<li class="chapter" data-level="2.9.1" data-path="2-wrangling.html"><a href="2-wrangling.html#bind"><i class="fa fa-check"></i><b>2.9.1</b> Bind</a></li>
<li class="chapter" data-level="2.9.2" data-path="2-wrangling.html"><a href="2-wrangling.html#joins-in-dplyr"><i class="fa fa-check"></i><b>2.9.2</b> Joins in dplyr</a></li>
<li class="chapter" data-level="2.9.3" data-path="2-wrangling.html"><a href="2-wrangling.html#joining"><i class="fa fa-check"></i><b>2.9.3</b> Joining</a></li>
<li class="chapter" data-level="2.9.4" data-path="2-wrangling.html"><a href="2-wrangling.html#matching-key-variable-names"><i class="fa fa-check"></i><b>2.9.4</b> Matching “key” variable names</a></li>
<li class="chapter" data-level="2.9.5" data-path="2-wrangling.html"><a href="2-wrangling.html#diff-key"><i class="fa fa-check"></i><b>2.9.5</b> Different “key” variable names</a></li>
<li class="chapter" data-level="2.9.6" data-path="2-wrangling.html"><a href="2-wrangling.html#multiple-key-variables"><i class="fa fa-check"></i><b>2.9.6</b> Multiple “key” variables</a></li>
<li class="chapter" data-level="2.9.7" data-path="2-wrangling.html"><a href="2-wrangling.html#normal-forms"><i class="fa fa-check"></i><b>2.9.7</b> Normal forms</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="2-wrangling.html"><a href="2-wrangling.html#other-verbs"><i class="fa fa-check"></i><b>2.10</b> Other Verbs</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2-wrangling.html"><a href="2-wrangling.html#select"><i class="fa fa-check"></i><b>2.10.1</b> <code>select</code> variables</a></li>
<li class="chapter" data-level="2.10.2" data-path="2-wrangling.html"><a href="2-wrangling.html#rename"><i class="fa fa-check"></i><b>2.10.2</b> <code>rename</code> variables</a></li>
<li class="chapter" data-level="2.10.3" data-path="2-wrangling.html"><a href="2-wrangling.html#top_n-values-of-a-variable"><i class="fa fa-check"></i><b>2.10.3</b> <code>top_n</code> values of a variable</a></li>
<li class="chapter" data-level="2.10.4" data-path="2-wrangling.html"><a href="2-wrangling.html#slice-and-pull-and"><i class="fa fa-check"></i><b>2.10.4</b> <code>slice</code> and <code>pull</code> and <code>[]</code></a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2-wrangling.html"><a href="2-wrangling.html#conclusion-2"><i class="fa fa-check"></i><b>2.11</b> Conclusion</a><ul>
<li class="chapter" data-level="2.11.1" data-path="2-wrangling.html"><a href="2-wrangling.html#summary-table-1"><i class="fa fa-check"></i><b>2.11.1</b> Summary table</a></li>
<li class="chapter" data-level="2.11.2" data-path="2-wrangling.html"><a href="2-wrangling.html#additional-resources-2"><i class="fa fa-check"></i><b>2.11.2</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="2-wrangling.html"><a href="2-wrangling.html#tidy"><i class="fa fa-check"></i><b>2.12</b> Tidy</a></li>
<li class="chapter" data-level="2.13" data-path="2-wrangling.html"><a href="2-wrangling.html#csv"><i class="fa fa-check"></i><b>2.13</b> Importing data</a></li>
<li class="chapter" data-level="2.14" data-path="2-wrangling.html"><a href="2-wrangling.html#web-scraping"><i class="fa fa-check"></i><b>2.14</b> Web scraping</a><ul>
<li class="chapter" data-level="2.14.1" data-path="2-wrangling.html"><a href="2-wrangling.html#html"><i class="fa fa-check"></i><b>2.14.1</b> HTML</a></li>
<li class="chapter" data-level="2.14.2" data-path="2-wrangling.html"><a href="2-wrangling.html#the-rvest-package"><i class="fa fa-check"></i><b>2.14.2</b> The rvest package</a></li>
<li class="chapter" data-level="2.14.3" data-path="2-wrangling.html"><a href="2-wrangling.html#css-selectors"><i class="fa fa-check"></i><b>2.14.3</b> CSS selectors</a></li>
<li class="chapter" data-level="2.14.4" data-path="2-wrangling.html"><a href="2-wrangling.html#json"><i class="fa fa-check"></i><b>2.14.4</b> JSON</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="2-wrangling.html"><a href="2-wrangling.html#tidy-data-ex"><i class="fa fa-check"></i><b>2.15</b> “Tidy” data</a><ul>
<li class="chapter" data-level="2.15.1" data-path="2-wrangling.html"><a href="2-wrangling.html#tidy-definition"><i class="fa fa-check"></i><b>2.15.1</b> Definition of “tidy” data</a></li>
<li class="chapter" data-level="2.15.2" data-path="2-wrangling.html"><a href="2-wrangling.html#converting-to-tidy-data"><i class="fa fa-check"></i><b>2.15.2</b> Converting to “tidy” data</a></li>
<li class="chapter" data-level="2.15.3" data-path="2-wrangling.html"><a href="2-wrangling.html#nycflights13-package-1"><i class="fa fa-check"></i><b>2.15.3</b> <code>nycflights13</code> package</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="2-wrangling.html"><a href="2-wrangling.html#case-study-tidy"><i class="fa fa-check"></i><b>2.16</b> Case study: Democracy in Guatemala</a></li>
<li class="chapter" data-level="2.17" data-path="2-wrangling.html"><a href="2-wrangling.html#tidyverse-package"><i class="fa fa-check"></i><b>2.17</b> <code>tidyverse</code> package</a></li>
<li class="chapter" data-level="2.18" data-path="2-wrangling.html"><a href="2-wrangling.html#conclusion-3"><i class="fa fa-check"></i><b>2.18</b> Conclusion</a><ul>
<li class="chapter" data-level="2.18.1" data-path="2-wrangling.html"><a href="2-wrangling.html#additional-resources-3"><i class="fa fa-check"></i><b>2.18.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html"><i class="fa fa-check"></i><b>3</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="3.1" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#what-is-a-causal-effect"><i class="fa fa-check"></i><b>3.1</b> What is a causal effect?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#everything-is-a-missing-data-problem"><i class="fa fa-check"></i><b>3.1.1</b> Everything is a missing data problem</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#potential-outcomes-introduction-to-the-rubin-table"><i class="fa fa-check"></i><b>3.1.2</b> Potential outcomes: introduction to the Rubin Table</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#estimands"><i class="fa fa-check"></i><b>3.1.3</b> Estimands</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#the-infinite-rubin-table-many-kinds-of-missing-data"><i class="fa fa-check"></i><b>3.1.4</b> The infinite Rubin Table: many kinds of missing data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#how-do-we-fill-in-the-missing-values"><i class="fa fa-check"></i><b>3.2</b> How do we fill in the missing values?</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#average-treatment-effect"><i class="fa fa-check"></i><b>3.2.1</b> Average treatment effect</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#the-assignment-mechanism"><i class="fa fa-check"></i><b>3.2.2</b> The assignment mechanism</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#uncertainty-and-permutation-tests"><i class="fa fa-check"></i><b>3.2.3</b> Uncertainty and permutation tests</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#everything-is-a-missing-data-problem-revisited-internal-and-external-validity"><i class="fa fa-check"></i><b>3.2.4</b> Everything is a missing data problem revisited: Internal and external validity</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#more-complex-models-for-causal-inference"><i class="fa fa-check"></i><b>3.2.5</b> More complex models for causal inference</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#causal-inference-and-prediction"><i class="fa fa-check"></i><b>3.2.6</b> Causal inference and prediction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#other-issues-with-causal-inference"><i class="fa fa-check"></i><b>3.3</b> Other issues with causal inference</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#no-causation-without-manipulation"><i class="fa fa-check"></i><b>3.3.1</b> No causation without manipulation</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#stable-unit-treatment-value-assumption-sutva"><i class="fa fa-check"></i><b>3.3.2</b> Stable unit treatment value assumption (SUTVA)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#conclusion-4"><i class="fa fa-check"></i><b>3.4</b> Conclusion</a></li>
<li class="chapter" data-level="3.5" data-path="3-rubin-causal-model.html"><a href="3-rubin-causal-model.html#references"><i class="fa fa-check"></i><b>3.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-functions.html"><a href="4-functions.html"><i class="fa fa-check"></i><b>4</b> Functions</a><ul>
<li class="chapter" data-level="4.1" data-path="4-functions.html"><a href="4-functions.html#part-1"><i class="fa fa-check"></i><b>4.1</b> Part 1</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-functions.html"><a href="4-functions.html#get-something-that-works"><i class="fa fa-check"></i><b>4.1.1</b> Get something that works</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-functions.html"><a href="4-functions.html#skateboard-perfectly-formed-rear-view-mirror"><i class="fa fa-check"></i><b>4.1.2</b> Skateboard &gt;&gt; perfectly formed rear-view mirror</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-functions.html"><a href="4-functions.html#turn-the-working-interactive-code-into-a-function"><i class="fa fa-check"></i><b>4.1.3</b> Turn the working interactive code into a function</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-functions.html"><a href="4-functions.html#test-your-function"><i class="fa fa-check"></i><b>4.1.4</b> Test your function</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-functions.html"><a href="4-functions.html#check-the-validity-of-arguments"><i class="fa fa-check"></i><b>4.1.5</b> Check the validity of arguments</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-functions.html"><a href="4-functions.html#wrap-up-and-whats-next"><i class="fa fa-check"></i><b>4.1.6</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-functions.html"><a href="4-functions.html#part-2"><i class="fa fa-check"></i><b>4.2</b> Part 2</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-functions.html"><a href="4-functions.html#load-the-gapminder-data"><i class="fa fa-check"></i><b>4.2.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-functions.html"><a href="4-functions.html#restore-our-max-minus-min-function"><i class="fa fa-check"></i><b>4.2.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-functions.html"><a href="4-functions.html#generalize-our-function-to-other-quantiles"><i class="fa fa-check"></i><b>4.2.3</b> Generalize our function to other quantiles</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-functions.html"><a href="4-functions.html#get-something-that-works-again"><i class="fa fa-check"></i><b>4.2.4</b> Get something that works, again</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-functions.html"><a href="4-functions.html#turn-the-working-interactive-code-into-a-function-again"><i class="fa fa-check"></i><b>4.2.5</b> Turn the working interactive code into a function, again</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-functions.html"><a href="4-functions.html#argument-names-freedom-and-conventions"><i class="fa fa-check"></i><b>4.2.6</b> Argument names: freedom and conventions</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-functions.html"><a href="4-functions.html#what-a-function-returns"><i class="fa fa-check"></i><b>4.2.7</b> What a function returns</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-functions.html"><a href="4-functions.html#default-values-freedom-to-not-specify-the-arguments"><i class="fa fa-check"></i><b>4.2.8</b> Default values: freedom to NOT specify the arguments</a></li>
<li class="chapter" data-level="4.2.9" data-path="4-functions.html"><a href="4-functions.html#wrap-up-and-whats-next-1"><i class="fa fa-check"></i><b>4.2.9</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-functions.html"><a href="4-functions.html#part-3"><i class="fa fa-check"></i><b>4.3</b> Part 3</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-functions.html"><a href="4-functions.html#load-the-gapminder-data-1"><i class="fa fa-check"></i><b>4.3.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-functions.html"><a href="4-functions.html#restore-our-max-minus-min-function-1"><i class="fa fa-check"></i><b>4.3.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-functions.html"><a href="4-functions.html#be-proactive-about-nas"><i class="fa fa-check"></i><b>4.3.3</b> Be proactive about <code>NA</code>s</a></li>
<li class="chapter" data-level="4.3.4" data-path="4-functions.html"><a href="4-functions.html#the-useful-but-mysterious-...-argument"><i class="fa fa-check"></i><b>4.3.4</b> The useful but mysterious <code>...</code> argument</a></li>
<li class="chapter" data-level="4.3.5" data-path="4-functions.html"><a href="4-functions.html#use-testthat-for-formal-unit-tests"><i class="fa fa-check"></i><b>4.3.5</b> Use testthat for formal unit tests</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-functions.html"><a href="4-functions.html#list-columns-and-map_-functions"><i class="fa fa-check"></i><b>4.4</b> List columns and <code>map_*</code> functions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-functions.html"><a href="4-functions.html#what-are-list-columns"><i class="fa fa-check"></i><b>4.4.1</b> What are list columns?</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-functions.html"><a href="4-functions.html#creating-list-columns-with-mutate"><i class="fa fa-check"></i><b>4.4.2</b> Creating list columns with <code>mutate()</code></a></li>
<li class="chapter" data-level="4.4.3" data-path="4-functions.html"><a href="4-functions.html#map_-functions"><i class="fa fa-check"></i><b>4.4.3</b> <code>map_*</code> functions</a></li>
<li class="chapter" data-level="4.4.4" data-path="4-functions.html"><a href="4-functions.html#using-map_-functions-to-create-list-columns"><i class="fa fa-check"></i><b>4.4.4</b> Using <code>map_*</code> functions to create list columns</a></li>
<li class="chapter" data-level="4.4.5" data-path="4-functions.html"><a href="4-functions.html#practice-with-map_-functions-and-list-columns"><i class="fa fa-check"></i><b>4.4.5</b> Practice with <code>map_*</code> functions and list columns</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-probability.html"><a href="5-probability.html"><i class="fa fa-check"></i><b>5</b> Probability</a><ul>
<li class="chapter" data-level="5.1" data-path="5-probability.html"><a href="5-probability.html#basicsOfProbability"><i class="fa fa-check"></i><b>5.1</b> Defining probability</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-probability.html"><a href="5-probability.html#intro-questions"><i class="fa fa-check"></i><b>5.1.1</b> Intro Questions</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-probability.html"><a href="5-probability.html#probability-1"><i class="fa fa-check"></i><b>5.1.2</b> Probability</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-probability.html"><a href="5-probability.html#disjoint-or-mutually-exclusive-outcomes"><i class="fa fa-check"></i><b>5.1.3</b> Disjoint or mutually exclusive outcomes</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-probability.html"><a href="5-probability.html#probabilities-when-events-are-not-disjoint"><i class="fa fa-check"></i><b>5.1.4</b> Probabilities when events are not disjoint</a></li>
<li class="chapter" data-level="5.1.5" data-path="5-probability.html"><a href="5-probability.html#probability-distributions"><i class="fa fa-check"></i><b>5.1.5</b> Probability distributions</a></li>
<li class="chapter" data-level="5.1.6" data-path="5-probability.html"><a href="5-probability.html#complement-of-an-event"><i class="fa fa-check"></i><b>5.1.6</b> Complement of an event</a></li>
<li class="chapter" data-level="5.1.7" data-path="5-probability.html"><a href="5-probability.html#probabilityIndependence"><i class="fa fa-check"></i><b>5.1.7</b> Independence</a></li>
<li class="chapter" data-level="5.1.8" data-path="5-probability.html"><a href="5-probability.html#conditionalProbabilitySection"><i class="fa fa-check"></i><b>5.1.8</b> Conditional probability</a></li>
<li class="chapter" data-level="5.1.9" data-path="5-probability.html"><a href="5-probability.html#marginalAndJointProbabilities"><i class="fa fa-check"></i><b>5.1.9</b> Marginal and joint probabilities</a></li>
<li class="chapter" data-level="5.1.10" data-path="5-probability.html"><a href="5-probability.html#defining-conditional-probability"><i class="fa fa-check"></i><b>5.1.10</b> Defining conditional probability</a></li>
<li class="chapter" data-level="5.1.11" data-path="5-probability.html"><a href="5-probability.html#smallpox-in-boston-1721"><i class="fa fa-check"></i><b>5.1.11</b> Smallpox in Boston, 1721</a></li>
<li class="chapter" data-level="5.1.12" data-path="5-probability.html"><a href="5-probability.html#general-multiplication-rule"><i class="fa fa-check"></i><b>5.1.12</b> General multiplication rule</a></li>
<li class="chapter" data-level="5.1.13" data-path="5-probability.html"><a href="5-probability.html#tree-diagrams"><i class="fa fa-check"></i><b>5.1.13</b> Tree diagrams</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-probability.html"><a href="5-probability.html#randomVariablesSection"><i class="fa fa-check"></i><b>5.2</b> Random variables</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-probability.html"><a href="5-probability.html#expectation"><i class="fa fa-check"></i><b>5.2.1</b> Expectation</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-probability.html"><a href="5-probability.html#variability-in-random-variables"><i class="fa fa-check"></i><b>5.2.2</b> Variability in random variables</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-probability.html"><a href="5-probability.html#linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>5.2.3</b> Linear combinations of random variables</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-probability.html"><a href="5-probability.html#variability-in-linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>5.2.4</b> Variability in linear combinations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-probability.html"><a href="5-probability.html#appendixA"><i class="fa fa-check"></i><b>5.3</b> Statistical Background</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-probability.html"><a href="5-probability.html#appendix-stat-terms"><i class="fa fa-check"></i><b>5.3.1</b> Basic statistical terms</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-probability.html"><a href="5-probability.html#appendix-normal-curve"><i class="fa fa-check"></i><b>5.3.2</b> Normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-probability.html"><a href="5-probability.html#bayess-theorem"><i class="fa fa-check"></i><b>5.4</b> Bayes’s Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="5-probability.html"><a href="5-probability.html#conditional-probability"><i class="fa fa-check"></i><b>5.5</b> Conditional probability</a></li>
<li class="chapter" data-level="5.6" data-path="5-probability.html"><a href="5-probability.html#conjoint-probability"><i class="fa fa-check"></i><b>5.6</b> Conjoint probability</a></li>
<li class="chapter" data-level="5.7" data-path="5-probability.html"><a href="5-probability.html#the-cookie-problem"><i class="fa fa-check"></i><b>5.7</b> The cookie problem</a></li>
<li class="chapter" data-level="5.8" data-path="5-probability.html"><a href="5-probability.html#bayess-theorem-1"><i class="fa fa-check"></i><b>5.8</b> Bayes’s Theorem</a></li>
<li class="chapter" data-level="5.9" data-path="5-probability.html"><a href="5-probability.html#the-diachronic-interpretation"><i class="fa fa-check"></i><b>5.9</b> The diachronic interpretation</a></li>
<li class="chapter" data-level="5.10" data-path="5-probability.html"><a href="5-probability.html#the-mm-problem"><i class="fa fa-check"></i><b>5.10</b> The M&amp;M problem</a></li>
<li class="chapter" data-level="5.11" data-path="5-probability.html"><a href="5-probability.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>5.11</b> The Monty Hall problem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-sampling.html"><a href="6-sampling.html"><i class="fa fa-check"></i><b>6</b> Sampling</a><ul>
<li class="chapter" data-level="" data-path="6-sampling.html"><a href="6-sampling.html#needed-packages-1"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="6.1" data-path="6-sampling.html"><a href="6-sampling.html#sampling-activity"><i class="fa fa-check"></i><b>6.1</b> Sampling urn activity</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-sampling.html"><a href="6-sampling.html#what-proportion-of-this-urns-balls-are-red"><i class="fa fa-check"></i><b>6.1.1</b> What proportion of this urn’s balls are red?</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-sampling.html"><a href="6-sampling.html#using-the-shovel-once"><i class="fa fa-check"></i><b>6.1.2</b> Using the shovel once</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-sampling.html"><a href="6-sampling.html#student-shovels"><i class="fa fa-check"></i><b>6.1.3</b> Using the shovel 33 times</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-sampling.html"><a href="6-sampling.html#what-did-we-just-do"><i class="fa fa-check"></i><b>6.1.4</b> What did we just do?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-sampling.html"><a href="6-sampling.html#sampling-simulation"><i class="fa fa-check"></i><b>6.2</b> Virtual sampling</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-sampling.html"><a href="6-sampling.html#using-the-virtual-shovel-once"><i class="fa fa-check"></i><b>6.2.1</b> Using the virtual shovel once</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-sampling.html"><a href="6-sampling.html#using-the-virtual-shovel-33-times"><i class="fa fa-check"></i><b>6.2.2</b> Using the virtual shovel 33 times</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-sampling.html"><a href="6-sampling.html#shovel-1000-times"><i class="fa fa-check"></i><b>6.2.3</b> Using the virtual shovel 1,000 times</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-sampling.html"><a href="6-sampling.html#different-shovels"><i class="fa fa-check"></i><b>6.2.4</b> Using different shovels</a></li>
<li class="chapter" data-level="6.2.5" data-path="6-sampling.html"><a href="6-sampling.html#using-many-shovels-at-once"><i class="fa fa-check"></i><b>6.2.5</b> Using many shovels at once</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-sampling.html"><a href="6-sampling.html#sampling-framework"><i class="fa fa-check"></i><b>6.3</b> Sampling framework</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-sampling.html"><a href="6-sampling.html#terminology-and-notation"><i class="fa fa-check"></i><b>6.3.1</b> Terminology and notation</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-sampling.html"><a href="6-sampling.html#sampling-definitions"><i class="fa fa-check"></i><b>6.3.2</b> Statistical definitions</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-sampling.html"><a href="6-sampling.html#moral-of-the-story"><i class="fa fa-check"></i><b>6.3.3</b> The moral of the story</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-sampling.html"><a href="6-sampling.html#sampling-case-study"><i class="fa fa-check"></i><b>6.4</b> Case study: Polls</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-sampling.html"><a href="6-sampling.html#sampling-conclusion-central-limit-theorem"><i class="fa fa-check"></i><b>6.4.1</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-sampling.html"><a href="6-sampling.html#conclusion-5"><i class="fa fa-check"></i><b>6.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#needed-packages-2"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="7.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#resampling-tactile"><i class="fa fa-check"></i><b>7.1</b> Pennies activity</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#what-is-the-average-year-on-us-pennies-in-2019"><i class="fa fa-check"></i><b>7.1.1</b> What is the average year on US pennies in 2019?</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#resampling-once"><i class="fa fa-check"></i><b>7.1.2</b> Resampling once</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#student-resamples"><i class="fa fa-check"></i><b>7.1.3</b> Resampling 35 times</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#what-did-we-just-do-1"><i class="fa fa-check"></i><b>7.1.4</b> What did we just do?</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#resampling-simulation"><i class="fa fa-check"></i><b>7.1.5</b> Virtually resampling once</a></li>
<li class="chapter" data-level="7.1.6" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#bootstrap-35-replicates"><i class="fa fa-check"></i><b>7.1.6</b> Virtually resampling 35 times</a></li>
<li class="chapter" data-level="7.1.7" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#bootstrap-1000-replicates"><i class="fa fa-check"></i><b>7.1.7</b> Virtually resampling 1,000 times</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#ci-build-up"><i class="fa fa-check"></i><b>7.2</b> Measuring uncertainty with confidence intervals</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#percentile-method"><i class="fa fa-check"></i><b>7.2.1</b> Percentile method</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#se-method"><i class="fa fa-check"></i><b>7.2.2</b> Standard error method</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#one-prop-ci"><i class="fa fa-check"></i><b>7.2.3</b> Interpreting confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#ci-width"><i class="fa fa-check"></i><b>7.3</b> Width of confidence intervals</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#impact-of-confidence-level"><i class="fa fa-check"></i><b>7.3.1</b> Impact of confidence level</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#fitting-multiple-models-using-map"><i class="fa fa-check"></i><b>7.3.2</b> Fitting multiple models using <code>map()</code></a></li>
<li class="chapter" data-level="7.3.3" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#impact-of-sample-size"><i class="fa fa-check"></i><b>7.3.3</b> Impact of sample size</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut"><i class="fa fa-check"></i><b>7.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="7.5" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#case-study-two-prop-ci"><i class="fa fa-check"></i><b>7.5</b> Case study: Is yawning contagious?</a><ul>
<li class="chapter" data-level="7.5.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#mythbusters-study-data"><i class="fa fa-check"></i><b>7.5.1</b> <em>Mythbusters</em> study data</a></li>
<li class="chapter" data-level="7.5.2" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#sampling-scenario"><i class="fa fa-check"></i><b>7.5.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="7.5.3" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#ci-build"><i class="fa fa-check"></i><b>7.5.3</b> Constructing the confidence interval</a></li>
<li class="chapter" data-level="7.5.4" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut-1"><i class="fa fa-check"></i><b>7.5.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#ci-conclusion"><i class="fa fa-check"></i><b>7.6</b> Conclusion</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7-confidence-intervals.html"><a href="7-confidence-intervals.html#bootstrap-vs-sampling"><i class="fa fa-check"></i><b>7.6.1</b> Comparing bootstrap and sampling distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-regression.html"><a href="8-regression.html"><i class="fa fa-check"></i><b>8</b> Regression</a><ul>
<li class="chapter" data-level="" data-path="8-regression.html"><a href="8-regression.html#needed-packages-3"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="8.1" data-path="8-regression.html"><a href="8-regression.html#model1"><i class="fa fa-check"></i><b>8.1</b> Teaching evaluations: one numerical explanatory variable</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-regression.html"><a href="8-regression.html#model1EDA"><i class="fa fa-check"></i><b>8.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-regression.html"><a href="8-regression.html#model1table"><i class="fa fa-check"></i><b>8.1.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-regression.html"><a href="8-regression.html#interpreting-regression-coefficients"><i class="fa fa-check"></i><b>8.1.3</b> Interpreting regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-regression.html"><a href="8-regression.html#uncertainty-in-simple-linear-regressions"><i class="fa fa-check"></i><b>8.2</b> Uncertainty in simple linear regressions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-regression.html"><a href="8-regression.html#using-lm-and-tidy-as-a-shortcut-2"><i class="fa fa-check"></i><b>8.2.1</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-regression.html"><a href="8-regression.html#model1points"><i class="fa fa-check"></i><b>8.2.2</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-regression.html"><a href="8-regression.html#model2"><i class="fa fa-check"></i><b>8.3</b> Life expectancy: one categorical explanatory variable</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8-regression.html"><a href="8-regression.html#model2EDA"><i class="fa fa-check"></i><b>8.3.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-regression.html"><a href="8-regression.html#model2table"><i class="fa fa-check"></i><b>8.3.2</b> Linear regression</a></li>
<li class="chapter" data-level="8.3.3" data-path="8-regression.html"><a href="8-regression.html#model2points"><i class="fa fa-check"></i><b>8.3.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-regression.html"><a href="8-regression.html#case-study-2018-gubernatorial-forecasts"><i class="fa fa-check"></i><b>8.4</b> Case study: 2018 gubernatorial forecasts</a><ul>
<li class="chapter" data-level="8.4.1" data-path="8-regression.html"><a href="8-regression.html#fitting-multiple-models-using-map-1"><i class="fa fa-check"></i><b>8.4.1</b> Fitting multiple models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-regression.html"><a href="8-regression.html#leastsquares"><i class="fa fa-check"></i><b>8.5</b> Appendix: Best-fitting line</a></li>
<li class="chapter" data-level="8.6" data-path="8-regression.html"><a href="8-regression.html#conclusion-6"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html"><i class="fa fa-check"></i><b>9</b> Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#needed-packages-4"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="9.1" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model4"><i class="fa fa-check"></i><b>9.1</b> Teaching evaluations revisited: one numerical and one categorical explanatory variable</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model4EDA"><i class="fa fa-check"></i><b>9.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model4interactiontable"><i class="fa fa-check"></i><b>9.1.2</b> Interaction model</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#interpreting-regression-coefficients-with-interactions"><i class="fa fa-check"></i><b>9.1.3</b> Interpreting regression coefficients with interactions</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model4table"><i class="fa fa-check"></i><b>9.1.4</b> Parallel slopes model</a></li>
<li class="chapter" data-level="9.1.5" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model4points"><i class="fa fa-check"></i><b>9.1.5</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model3"><i class="fa fa-check"></i><b>9.2</b> Credit card debt: two numerical explanatory variables</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model3EDA"><i class="fa fa-check"></i><b>9.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model3table"><i class="fa fa-check"></i><b>9.2.2</b> Regression plane</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model3points"><i class="fa fa-check"></i><b>9.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#seattle-house-prices"><i class="fa fa-check"></i><b>9.3</b> Case study: Seattle house prices</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#house-prices-EDA-I"><i class="fa fa-check"></i><b>9.3.1</b> Exploratory data analysis: Part I</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#house-prices-EDA-II"><i class="fa fa-check"></i><b>9.3.2</b> Exploratory data analysis: Part II</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#house-prices-regression"><i class="fa fa-check"></i><b>9.3.3</b> Regression modeling</a></li>
<li class="chapter" data-level="9.3.4" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#house-prices-making-predictions"><i class="fa fa-check"></i><b>9.3.4</b> Making predictions</a></li>
<li class="chapter" data-level="9.3.5" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#fitting-many-models-using-map"><i class="fa fa-check"></i><b>9.3.5</b> Fitting many models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#other-topics"><i class="fa fa-check"></i><b>9.4</b> Other topics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#model-selection"><i class="fa fa-check"></i><b>9.4.1</b> Model selection</a></li>
<li class="chapter" data-level="9.4.2" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#correlationcoefficient2"><i class="fa fa-check"></i><b>9.4.2</b> Correlation coefficient</a></li>
<li class="chapter" data-level="9.4.3" data-path="9-multiple-regression.html"><a href="9-multiple-regression.html#simpsonsparadox"><i class="fa fa-check"></i><b>9.4.3</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-classification.html"><a href="10-classification.html"><i class="fa fa-check"></i><b>10</b> Classification</a><ul>
<li class="chapter" data-level="" data-path="10-classification.html"><a href="10-classification.html#needed-packages-5"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="10.1" data-path="10-classification.html"><a href="10-classification.html#logistic-regression"><i class="fa fa-check"></i><b>10.1</b> Logistic regression</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-classification.html"><a href="10-classification.html#what-is-logistic-regression"><i class="fa fa-check"></i><b>10.1.1</b> What is logistic regression?</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-classification.html"><a href="10-classification.html#house-elections-exploratory-data-analysis"><i class="fa fa-check"></i><b>10.1.2</b> House elections: exploratory data analysis</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-classification.html"><a href="10-classification.html#one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>10.1.3</b> One categorical explanatory variable</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-classification.html"><a href="10-classification.html#observedfitted-values-and-residuals"><i class="fa fa-check"></i><b>10.1.4</b> Observed/fitted values and residuals</a></li>
<li class="chapter" data-level="10.1.5" data-path="10-classification.html"><a href="10-classification.html#one-numerical-explanatory-variable"><i class="fa fa-check"></i><b>10.1.5</b> One numerical explanatory variable</a></li>
<li class="chapter" data-level="10.1.6" data-path="10-classification.html"><a href="10-classification.html#one-numerical-and-one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>10.1.6</b> One numerical and one categorical explanatory variable</a></li>
<li class="chapter" data-level="10.1.7" data-path="10-classification.html"><a href="10-classification.html#fitting-many-models-using-map-1"><i class="fa fa-check"></i><b>10.1.7</b> Fitting many models using <code>map()</code></a></li>
<li class="chapter" data-level="10.1.8" data-path="10-classification.html"><a href="10-classification.html#professional-models"><i class="fa fa-check"></i><b>10.1.8</b> Professional models</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-classification.html"><a href="10-classification.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>10.2</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-classification.html"><a href="10-classification.html#what-is-cart"><i class="fa fa-check"></i><b>10.2.1</b> What is CART?</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-classification.html"><a href="10-classification.html#one-categorical-explanatory-variable-1"><i class="fa fa-check"></i><b>10.2.2</b> One categorical explanatory variable</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-classification.html"><a href="10-classification.html#one-numerical-explanatory-variable-1"><i class="fa fa-check"></i><b>10.2.3</b> One numerical explanatory variable</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-classification.html"><a href="10-classification.html#multiple-explanatory-variables"><i class="fa fa-check"></i><b>10.2.4</b> Multiple explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-classification.html"><a href="10-classification.html#random-forests"><i class="fa fa-check"></i><b>10.3</b> Random forests</a><ul>
<li class="chapter" data-level="10.3.1" data-path="10-classification.html"><a href="10-classification.html#what-are-random-forests"><i class="fa fa-check"></i><b>10.3.1</b> What are random forests?</a></li>
<li class="chapter" data-level="10.3.2" data-path="10-classification.html"><a href="10-classification.html#fitting-random-forests"><i class="fa fa-check"></i><b>10.3.2</b> Fitting random forests</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-classification.html"><a href="10-classification.html#comparing-the-three-approaches"><i class="fa fa-check"></i><b>10.4</b> Comparing the three approaches</a><ul>
<li class="chapter" data-level="10.4.1" data-path="10-classification.html"><a href="10-classification.html#is-accuracy-the-right-measure"><i class="fa fa-check"></i><b>10.4.1</b> Is accuracy the right measure?</a></li>
<li class="chapter" data-level="10.4.2" data-path="10-classification.html"><a href="10-classification.html#modeling-for-prediction-vs.-explanation"><i class="fa fa-check"></i><b>10.4.2</b> Modeling for prediction vs. explanation</a></li>
<li class="chapter" data-level="10.4.3" data-path="10-classification.html"><a href="10-classification.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.3</b> Out-of-sample predictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machine-learning.html"><a href="11-machine-learning.html"><i class="fa fa-check"></i><b>11</b> Machine Learning</a><ul>
<li class="chapter" data-level="11.1" data-path="11-machine-learning.html"><a href="11-machine-learning.html#the-process-of-machine-learning"><i class="fa fa-check"></i><b>11.1</b> The process of machine learning</a></li>
<li class="chapter" data-level="11.2" data-path="11-machine-learning.html"><a href="11-machine-learning.html#what-does-it-mean-for-a-model-to-be-good"><i class="fa fa-check"></i><b>11.2</b> What does it mean for a model to be “good?”</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-machine-learning.html"><a href="11-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>11.2.1</b> Training and test sets</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-machine-learning.html"><a href="11-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>11.2.2</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-machine-learning.html"><a href="11-machine-learning.html#data-cooperative-congressional-election-study-cces"><i class="fa fa-check"></i><b>11.3</b> Data: Cooperative Congressional Election Study (CCES)</a></li>
<li class="chapter" data-level="11.4" data-path="11-machine-learning.html"><a href="11-machine-learning.html#the-modeling-process-using-tidymodels"><i class="fa fa-check"></i><b>11.4</b> The modeling process using <strong>tidymodels</strong></a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-machine-learning.html"><a href="11-machine-learning.html#parsnip-build-the-model"><i class="fa fa-check"></i><b>11.4.1</b> <strong>parsnip</strong>: build the model</a></li>
<li class="chapter" data-level="11.4.2" data-path="11-machine-learning.html"><a href="11-machine-learning.html#recipes-not-happening-here-folks"><i class="fa fa-check"></i><b>11.4.2</b> <strong>recipes</strong>: not happening here, folks</a></li>
<li class="chapter" data-level="11.4.3" data-path="11-machine-learning.html"><a href="11-machine-learning.html#rsample-initial-split"><i class="fa fa-check"></i><b>11.4.3</b> <strong>rsample</strong>: initial split</a></li>
<li class="chapter" data-level="11.4.4" data-path="11-machine-learning.html"><a href="11-machine-learning.html#fitting-the-model-once"><i class="fa fa-check"></i><b>11.4.4</b> Fitting the model once</a></li>
<li class="chapter" data-level="11.4.5" data-path="11-machine-learning.html"><a href="11-machine-learning.html#fitting-many-models-using-map-2"><i class="fa fa-check"></i><b>11.4.5</b> Fitting many models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11-machine-learning.html"><a href="11-machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>11.5</b> Cross validation</a><ul>
<li class="chapter" data-level="11.5.1" data-path="11-machine-learning.html"><a href="11-machine-learning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.5.1</b> K-fold cross validation</a></li>
<li class="chapter" data-level="11.5.2" data-path="11-machine-learning.html"><a href="11-machine-learning.html#implementing-cross-validation-using-rsample"><i class="fa fa-check"></i><b>11.5.2</b> Implementing cross-validation using <strong>rsample</strong></a></li>
<li class="chapter" data-level="11.5.3" data-path="11-machine-learning.html"><a href="11-machine-learning.html#bootstrap"><i class="fa fa-check"></i><b>11.5.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="11-machine-learning.html"><a href="11-machine-learning.html#machine-learning-and-classification"><i class="fa fa-check"></i><b>11.6</b> Machine learning and classification</a></li>
<li class="chapter" data-level="11.7" data-path="11-machine-learning.html"><a href="11-machine-learning.html#conclusion-7"><i class="fa fa-check"></i><b>11.7</b> Conclusion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-productivity.html"><a href="A-productivity.html"><i class="fa fa-check"></i><b>A</b> Productivity</a><ul>
<li class="chapter" data-level="A.1" data-path="A-productivity.html"><a href="A-productivity.html#set-up"><i class="fa fa-check"></i><b>A.1</b> Set Up</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-productivity.html"><a href="A-productivity.html#terminal-on-mac"><i class="fa fa-check"></i><b>A.1.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="A.1.2" data-path="A-productivity.html"><a href="A-productivity.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>A.1.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="A.1.3" data-path="A-productivity.html"><a href="A-productivity.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>A.1.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="A.1.4" data-path="A-productivity.html"><a href="A-productivity.html#terminal-on-windows"><i class="fa fa-check"></i><b>A.1.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-productivity.html"><a href="A-productivity.html#unix"><i class="fa fa-check"></i><b>A.2</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="A.2.1" data-path="A-productivity.html"><a href="A-productivity.html#naming-convention"><i class="fa fa-check"></i><b>A.2.1</b> Naming convention</a></li>
<li class="chapter" data-level="A.2.2" data-path="A-productivity.html"><a href="A-productivity.html#the-terminal"><i class="fa fa-check"></i><b>A.2.2</b> The terminal</a></li>
<li class="chapter" data-level="A.2.3" data-path="A-productivity.html"><a href="A-productivity.html#filesystem"><i class="fa fa-check"></i><b>A.2.3</b> The filesystem</a></li>
<li class="chapter" data-level="A.2.4" data-path="A-productivity.html"><a href="A-productivity.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>A.2.4</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="A.2.5" data-path="A-productivity.html"><a href="A-productivity.html#the-home-directory"><i class="fa fa-check"></i><b>A.2.5</b> The home directory</a></li>
<li class="chapter" data-level="A.2.6" data-path="A-productivity.html"><a href="A-productivity.html#working-directory"><i class="fa fa-check"></i><b>A.2.6</b> Working directory</a></li>
<li class="chapter" data-level="A.2.7" data-path="A-productivity.html"><a href="A-productivity.html#paths"><i class="fa fa-check"></i><b>A.2.7</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="A-productivity.html"><a href="A-productivity.html#unix-commands"><i class="fa fa-check"></i><b>A.3</b> Unix commands</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-productivity.html"><a href="A-productivity.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>A.3.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="A.3.2" data-path="A-productivity.html"><a href="A-productivity.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>A.3.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="A.3.3" data-path="A-productivity.html"><a href="A-productivity.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>A.3.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
<li class="chapter" data-level="A.3.4" data-path="A-productivity.html"><a href="A-productivity.html#some-examples"><i class="fa fa-check"></i><b>A.3.4</b> Some examples</a></li>
<li class="chapter" data-level="A.3.5" data-path="A-productivity.html"><a href="A-productivity.html#more-unix-commands"><i class="fa fa-check"></i><b>A.3.5</b> More Unix commands</a></li>
<li class="chapter" data-level="A.3.6" data-path="A-productivity.html"><a href="A-productivity.html#advanced-unix"><i class="fa fa-check"></i><b>A.3.6</b> Advanced Unix</a></li>
<li class="chapter" data-level="A.3.7" data-path="A-productivity.html"><a href="A-productivity.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>A.3.7</b> File manipulation in R</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-productivity.html"><a href="A-productivity.html#git"><i class="fa fa-check"></i><b>A.4</b> Git and GitHub</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-productivity.html"><a href="A-productivity.html#github-accounts"><i class="fa fa-check"></i><b>A.4.1</b> GitHub accounts</a></li>
<li class="chapter" data-level="A.4.2" data-path="A-productivity.html"><a href="A-productivity.html#github-repos"><i class="fa fa-check"></i><b>A.4.2</b> GitHub repositories</a></li>
<li class="chapter" data-level="A.4.3" data-path="A-productivity.html"><a href="A-productivity.html#git-overview"><i class="fa fa-check"></i><b>A.4.3</b> Overview of Git</a></li>
<li class="chapter" data-level="A.4.4" data-path="A-productivity.html"><a href="A-productivity.html#rstudio-git"><i class="fa fa-check"></i><b>A.4.4</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="A-productivity.html"><a href="A-productivity.html#r"><i class="fa fa-check"></i><b>A.5</b> R</a><ul>
<li class="chapter" data-level="A.5.1" data-path="A-productivity.html"><a href="A-productivity.html#rstudio-projects"><i class="fa fa-check"></i><b>A.5.1</b> RStudio projects</a></li>
<li class="chapter" data-level="A.5.2" data-path="A-productivity.html"><a href="A-productivity.html#r-markdown"><i class="fa fa-check"></i><b>A.5.2</b> R markdown</a></li>
<li class="chapter" data-level="A.5.3" data-path="A-productivity.html"><a href="A-productivity.html#help-for-r"><i class="fa fa-check"></i><b>A.5.3</b> Help for R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-shiny.html"><a href="B-shiny.html"><i class="fa fa-check"></i><b>B</b> Shiny</a><ul>
<li class="chapter" data-level="B.1" data-path="B-shiny.html"><a href="B-shiny.html#helpful-resources"><i class="fa fa-check"></i><b>B.1</b> Helpful Resources</a></li>
<li class="chapter" data-level="B.2" data-path="B-shiny.html"><a href="B-shiny.html#set-up-and-getting-started"><i class="fa fa-check"></i><b>B.2</b> Set Up and Getting Started</a></li>
<li class="chapter" data-level="B.3" data-path="B-shiny.html"><a href="B-shiny.html#building-your-basic-app"><i class="fa fa-check"></i><b>B.3</b> Building Your Basic App</a><ul>
<li class="chapter" data-level="B.3.1" data-path="B-shiny.html"><a href="B-shiny.html#setting-up-the-basic-ui"><i class="fa fa-check"></i><b>B.3.1</b> Setting Up the Basic UI</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-shiny.html"><a href="B-shiny.html#setting-up-the-server"><i class="fa fa-check"></i><b>B.3.2</b> Setting up the Server</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-shiny.html"><a href="B-shiny.html#organization"><i class="fa fa-check"></i><b>B.4</b> Organization</a></li>
<li class="chapter" data-level="B.5" data-path="B-shiny.html"><a href="B-shiny.html#customizations"><i class="fa fa-check"></i><b>B.5</b> Customizations</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-maps.html"><a href="C-maps.html"><i class="fa fa-check"></i><b>C</b> Maps</a><ul>
<li class="chapter" data-level="C.1" data-path="C-maps.html"><a href="C-maps.html#tidycensus"><i class="fa fa-check"></i><b>C.1</b> Tidycensus</a></li>
<li class="chapter" data-level="C.2" data-path="C-maps.html"><a href="C-maps.html#conceptual-introduction-to-mapping"><i class="fa fa-check"></i><b>C.2</b> Conceptual introduction to mapping</a><ul>
<li class="chapter" data-level="C.2.1" data-path="C-maps.html"><a href="C-maps.html#vector-versus-spatial-data"><i class="fa fa-check"></i><b>C.2.1</b> Vector versus spatial data</a></li>
<li class="chapter" data-level="C.2.2" data-path="C-maps.html"><a href="C-maps.html#sf-vs-sp"><i class="fa fa-check"></i><b>C.2.2</b> <strong>sf</strong> vs <strong>sp</strong></a></li>
<li class="chapter" data-level="C.2.3" data-path="C-maps.html"><a href="C-maps.html#shapefiles"><i class="fa fa-check"></i><b>C.2.3</b> Shapefiles</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="C-maps.html"><a href="C-maps.html#mapping-with-tidycensus-and-geom_sf"><i class="fa fa-check"></i><b>C.3</b> Mapping with <strong>tidycensus</strong> and <code>geom_sf()</code></a><ul>
<li class="chapter" data-level="C.3.1" data-path="C-maps.html"><a href="C-maps.html#making-maps-pretty"><i class="fa fa-check"></i><b>C.3.1</b> Making maps pretty</a></li>
<li class="chapter" data-level="C.3.2" data-path="C-maps.html"><a href="C-maps.html#adding-back-alaska-and-hawaii"><i class="fa fa-check"></i><b>C.3.2</b> Adding back Alaska and Hawaii</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="C-maps.html"><a href="C-maps.html#faceting-maps"><i class="fa fa-check"></i><b>C.4</b> Faceting maps</a><ul>
<li class="chapter" data-level="C.4.1" data-path="C-maps.html"><a href="C-maps.html#transforming-and-mapping-the-data"><i class="fa fa-check"></i><b>C.4.1</b> Transforming and mapping the data</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="C-maps.html"><a href="C-maps.html#want-to-explore-further"><i class="fa fa-check"></i><b>C.5</b> Want to explore further?</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-animation.html"><a href="D-animation.html"><i class="fa fa-check"></i><b>D</b> Animation</a><ul>
<li class="chapter" data-level="D.1" data-path="D-animation.html"><a href="D-animation.html#gganimate-how-to-create-plots-with-beautiful-animation-in-r"><i class="fa fa-check"></i><b>D.1</b> gganimate: How to Create Plots with Beautiful Animation in R</a><ul>
<li class="chapter" data-level="D.1.1" data-path="D-animation.html"><a href="D-animation.html#prerequisites"><i class="fa fa-check"></i><b>D.1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="D.1.2" data-path="D-animation.html"><a href="D-animation.html#demo-dataset"><i class="fa fa-check"></i><b>D.1.2</b> Demo dataset</a></li>
<li class="chapter" data-level="D.1.3" data-path="D-animation.html"><a href="D-animation.html#static-plot"><i class="fa fa-check"></i><b>D.1.3</b> Static plot</a></li>
<li class="chapter" data-level="D.1.4" data-path="D-animation.html"><a href="D-animation.html#transition-through-distinct-states-in-time"><i class="fa fa-check"></i><b>D.1.4</b> Transition through distinct states in time</a></li>
<li class="chapter" data-level="D.1.5" data-path="D-animation.html"><a href="D-animation.html#reveal-data-along-a-given-dimension"><i class="fa fa-check"></i><b>D.1.5</b> Reveal data along a given dimension</a></li>
<li class="chapter" data-level="D.1.6" data-path="D-animation.html"><a href="D-animation.html#transition-between-several-distinct-stages-of-the-data"><i class="fa fa-check"></i><b>D.1.6</b> Transition between several distinct stages of the data</a></li>
<li class="chapter" data-level="D.1.7" data-path="D-animation.html"><a href="D-animation.html#read-more"><i class="fa fa-check"></i><b>D.1.7</b> Read more</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="D-animation.html"><a href="D-animation.html#how-to-save-your-animation"><i class="fa fa-check"></i><b>D.2</b> How to save your animation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Preceptor’s Primer for Bayesian Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Machine Learning</h1>
<!-- AR: I used material from this blog post: https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101
and from this github:
https://github.com/rstudio-conf-2020/applied-ml
Should be added to the Acknowledgements section -->
<!-- Model Review: Not for this week, but maybe for next week. Add a section which reviews how we can re-estime all these models using tidymodels. (I know that you do some of that in chapter 13, but now we are more formal and thorough.) This also provides a nice occasion to review what we have learned, to see the larger framework. We don't start with a model. We start with the data. The type of Y variable we have guides our model choice. Maybe we should add another model which can be used on continuos data. Maybe random forests? Would be nice to compare two models for continuous data and three for categorical data. Not this week! -->
<!-- Other tutorials: -->
<!-- https://github.com/rstudio-conf-2020/applied-ml -->
<!-- https://dnield.com/posts/tidymodels-intro/ -->
<!-- https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101 -->
<!-- A much better book is: https://bradleyboehmke.github.io/HOML/. But that won't be open for another year. And, it is too advanced. And it does not quite use all the latest machine learning approaches in R. But the style is nice, and perhaps worth emulating. A related issue is tying this work back to the last three weeks. After all, in each of the last three weeks we fitted hundreds of models. Isn't that what machine learning is? Sort of! Big difference is that, previously, the separate models have shared no data with each other. With machine learning, they will! How can we teach this in a way which is closest to the approaches they have learned? Hmmm. -->
<p>We have learned four models: linear regression, logistic regression, CART, and random forest. But there are hundreds more! We need a consistent way to try lots of models and to compare them. Hence, we introduced in the last chapter the <strong>tidymodels</strong> collection of packages.</p>
<p>Also, how do we know which models are best? Recall that we came up with several different models for each of our example data sets. Which one should we use? The framework of <strong>machine learning</strong> helps.</p>
<p>Perhaps the most popular data science methodologies come from the field of machine learning. Machine learning is the study and application of algorithms that learn from and make predictions on data. From search results to self-driving cars, it has manifested itself in all areas of our lives and is one of the most exciting and fast-growing fields of research in the world of data science. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars.</p>
<p>There are a wide variety of machine learning algorithms, including the models we have learned so far. We won’t introduce new models this chapter. Rather, we’ll use linear regression as an example for how to apply machine learning techniques to improve your predictive modeling.</p>
<div id="the-process-of-machine-learning" class="section level2">
<h2><span class="header-section-number">11.1</span> The process of machine learning</h2>
<p>In machine learning, modeling is a <em>process</em>, not a single step. Common steps during model building are:</p>
<ul>
<li>Estimating model parameters (i.e. training models)</li>
<li>Determining the values of tuning parameters that cannot be directly calculated from the data</li>
<li>Model selection (within a model type) and model comparison (between types)</li>
<li>Calculating the performance of the final model that will generalize to new data</li>
</ul>
<p>Many books and courses portray predictive modeling as a short sprint. A better analogy would be a marathon or campaign (depending on how hard the problem is).</p>
<p>We often think of the model as the only real data analysis step in this process. However, there are other procedures that are often applied before or after the model fit that are data-driven and have an impact. If we only think of the model as being important, we might end up accidentally <em>overfitting</em> to the data in-hand. This is very similar to the problems of “the garden of forking paths” and “<a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">p-hacking</a>.”</p>
<p>Let’s conceptualize a process or <em>workflow</em> that involves all of the steps where the data are analyzed in a significant way. This includes the model but might also include other <em>estimation</em> steps. Admittedly, there is some grey area here. This includes data preparation steps (e.g., imputation, encoding, transformations) and selection of which terms go into the model.</p>
<p>This concept of a “modeling workflow” will become important when we talk about measuring performance of the modeling process. Ultimately, when we evaluate models, we are evaluating the whole process. All the steps involved in the process can affect the performance of the final model.</p>
</div>
<div id="what-does-it-mean-for-a-model-to-be-good" class="section level2">
<h2><span class="header-section-number">11.2</span> What does it mean for a model to be “good?”</h2>
<p>Before we start describing machine learning approaches to optimize the way we build models, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better.”</p>
<div id="training-and-test-sets" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Training and test sets</h3>
<p>Ultimately, a machine learning algorithm should be evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only <em>after</em> we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the <em>training</em> set. We refer to the group for which we pretend we don’t know the outcome as the <em>test</em> set. A standard way of generating the training and test sets is by randomly splitting the data.</p>
<ul>
<li>Training Set: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.</li>
<li>Test Set: these data can be used to get an independent assessment of model efficacy. They should not be used during model training.</li>
</ul>
<p>We then develop an algorithm using <strong>only</strong> the training set. Once we are done developing the algorithm, we will <em>freeze</em> it and evaluate it using the test set. But remember, <strong>it is important that we optimize the model using only the training set</strong>: the test set is only for evaluation. Evaluating an algorithm on the training set can lead to <em>overfitting</em>, which often results in dangerously over-optimistic assessments.</p>
</div>
<div id="loss-function" class="section level3">
<h3><span class="header-section-number">11.2.2</span> The loss function</h3>
<p>The general approach to defining “best” in machine learning is to define a <em>loss function</em>. This concept can be applied to both categorical and continuous data.</p>
<p>The most commonly used loss function is the <em>squared loss function</em>.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> If <span class="math inline">\(\hat{y}\)</span> is our predictor and <span class="math inline">\(y\)</span> is the observed outcome, the squared loss function is simply:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Because we often have a test set with many observations, say <span class="math inline">\(N\)</span>, we use the mean squared error (MSE):</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>In practice, we often report the root mean squared error (RMSE), which is <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, because it is in the same units as the outcomes.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a></p>
<p>If the outcomes are binary, both RMSE and MSE are equivalent to accuracy, since <span class="math inline">\((\hat{y} - y)^2\)</span> is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.</p>
</div>
</div>
<div id="data-cooperative-congressional-election-study-cces" class="section level2">
<h2><span class="header-section-number">11.3</span> Data: Cooperative Congressional Election Study (CCES)</h2>
<p>Now that we have a way to evaluate models, we’re almost ready to start going through the modeling process using <strong>tidymodels</strong>. However, we’ll first need some data!</p>
<p>For this chapter, we’ll be using data from the Cooperative Congressional Election Study, a large survey of Americans that asks many questions relevant to American politics and elections. You can learn more about the study <a href="https://cces.gov.harvard.edu/">here</a>.</p>
<p>Let’s start by downloading the file called “cumulative_2006_2018.rds” at the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/II2DB6">CCES Dataverse</a>. After you have downloaded it, load it into your environment with the name “cces” using <code>cces &lt;- read_rds("cumulative_2006_2018.rds")</code>. Be sure to load the <strong>tidyverse</strong> and <strong>tidymodels</strong> packages before running the code in this chapter.</p>
<p>Take a look at the <code>cces</code> object:</p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb890-1"><a href="11-machine-learning.html#cb890-1"></a>cces</span></code></pre></div>
<pre><code># A tibble: 452,755 x 73
    year case_id weight weight_cumulati… state st    cd     dist dist_up  cong
   &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;
 1  2006  439219  1.85             1.67  Nort… NC    NC-10    10      10   109
 2  2006  439224  0.968            0.872 Ohio  OH    OH-3      3       3   109
 3  2006  439228  1.59             1.44  New … NJ    NJ-1      1       1   109
 4  2006  439237  1.40             1.26  Illi… IL    IL-9      9       9   109
 5  2006  439238  0.903            0.813 New … NY    NY-22    22      22   109
 6  2006  439242  0.839            0.756 Texas TX    TX-11    11      11   109
 7  2006  439251  0.777            0.700 Minn… MN    MN-3      3       3   109
 8  2006  439254  0.839            0.756 Neva… NV    NV-2      2       2   109
 9  2006  439255  0.331            0.299 Texas TX    TX-24    24      24   109
10  2006  439263  1.10             0.993 Mary… MD    MD-2      2       2   109
# … with 452,745 more rows, and 63 more variables: cong_up &lt;int&gt;,
#   zipcode &lt;chr&gt;, county_fips &lt;chr&gt;, tookpost &lt;int+lbl&gt;, weight_post &lt;dbl&gt;,
#   rvweight &lt;dbl&gt;, rvweight_post &lt;dbl&gt;, starttime &lt;dttm&gt;, pid3 &lt;int+lbl&gt;,
#   pid3_leaner &lt;int+lbl&gt;, pid7 &lt;int+lbl&gt;, ideo5 &lt;fct&gt;, gender &lt;int+lbl&gt;,
#   birthyr &lt;int&gt;, age &lt;int&gt;, race &lt;int+lbl&gt;, hispanic &lt;int+lbl&gt;,
#   educ &lt;int+lbl&gt;, faminc &lt;fct&gt;, marstat &lt;int+lbl&gt;, economy_retro &lt;int+lbl&gt;,
#   newsint &lt;int+lbl&gt;, approval_pres &lt;int+lbl&gt;, approval_rep &lt;fct&gt;,
#   approval_sen1 &lt;fct&gt;, approval_sen2 &lt;fct&gt;, approval_gov &lt;int+lbl&gt;,
#   intent_pres_08 &lt;fct&gt;, intent_pres_12 &lt;fct&gt;, intent_pres_16 &lt;fct&gt;,
#   voted_pres_08 &lt;fct&gt;, voted_pres_12 &lt;fct&gt;, voted_pres_16 &lt;fct&gt;,
#   vv_regstatus &lt;fct&gt;, vv_party_gen &lt;fct&gt;, vv_party_prm &lt;fct&gt;,
#   vv_turnout_gvm &lt;fct&gt;, vv_turnout_pvm &lt;fct&gt;, intent_rep &lt;fct&gt;,
#   intent_rep_party &lt;fct&gt;, voted_rep &lt;fct&gt;, voted_rep_party &lt;fct&gt;,
#   intent_gov &lt;fct&gt;, intent_gov_party &lt;fct&gt;, voted_gov &lt;fct&gt;,
#   voted_gov_party &lt;fct&gt;, intent_sen &lt;fct&gt;, intent_sen_party &lt;fct&gt;,
#   voted_sen &lt;fct&gt;, voted_sen_party &lt;fct&gt;, intent_rep_chosen &lt;chr&gt;,
#   intent_sen_chosen &lt;chr&gt;, intent_gov_chosen &lt;chr&gt;, voted_rep_chosen &lt;chr&gt;,
#   voted_sen_chosen &lt;chr&gt;, voted_gov_chosen &lt;chr&gt;, rep_current &lt;chr&gt;,
#   rep_icpsr &lt;dbl&gt;, sen1_current &lt;chr&gt;, sen1_icpsr &lt;dbl&gt;, sen2_current &lt;chr&gt;,
#   sen2_icpsr &lt;dbl&gt;, gov_current &lt;chr&gt;</code></pre>
<p>We have a lot of observations (more than 450,000), each of which is a single survey respondent.</p>
<p>The outcome we’ll focus on for this chapter is <em>presidential approval.</em></p>
<p>Let’s take a look at the presidential approval variable in the CCES. We see that it is an “int+lbl” object, which means that it an integer variable with labels, which we can see using the <code>print_labels()</code> function from the <strong>haven</strong> package:</p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb892-1"><a href="11-machine-learning.html#cb892-1"></a><span class="kw">library</span>(haven)</span>
<span id="cb892-2"><a href="11-machine-learning.html#cb892-2"></a></span>
<span id="cb892-3"><a href="11-machine-learning.html#cb892-3"></a><span class="kw">print_labels</span>(cces<span class="op">$</span>approval_pres)</span></code></pre></div>
<pre><code>
Labels:
 value                            label
     4              Strongly Disapprove
     1                 Strongly Approve
     3 Disapprove / Somewhat Disapprove
     2       Approve / Somewhat Approve
     5           Never Heard / Not Sure
    NA                             &lt;NA&gt;
     6   Neither Approve Nor Disapprove</code></pre>
<p>We can see that there are 6 levels to this variable. We’ll <code>mutate()</code> the variable so it goes from 1 to 5, which 1 being “strongly disapprove” and 5 being “strongly approve,” and 3 will be a middle category that combines responses from “never heard / not sure” and “neither approve nor disapprove,” so that higher values indicate greater approval.</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb894-1"><a href="11-machine-learning.html#cb894-1"></a>cces &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb894-2"><a href="11-machine-learning.html#cb894-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pres_approval =</span> <span class="kw">case_when</span>(approval_pres <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb894-3"><a href="11-machine-learning.html#cb894-3"></a>                                   approval_pres <span class="op">==</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">2</span>,</span>
<span id="cb894-4"><a href="11-machine-learning.html#cb894-4"></a>                                   approval_pres <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">6</span>) <span class="op">~</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb894-5"><a href="11-machine-learning.html#cb894-5"></a>                                   approval_pres <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">4</span>,</span>
<span id="cb894-6"><a href="11-machine-learning.html#cb894-6"></a>                                   approval_pres <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">5</span>,</span>
<span id="cb894-7"><a href="11-machine-learning.html#cb894-7"></a>                                   <span class="ot">TRUE</span> <span class="op">~</span><span class="st"> </span><span class="ot">NA_real_</span>))</span></code></pre></div>
<p>We’ll also <code>mutate()</code> some other variables for interpretability this chapter (just copy this code–if you want a better sense of what these lines are doing, take a look at the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/II2DB6">CCES documentation</a>):</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb895-1"><a href="11-machine-learning.html#cb895-1"></a>cces &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb895-2"><a href="11-machine-learning.html#cb895-2"></a><span class="st">  </span><span class="kw">mutate</span>(</span>
<span id="cb895-3"><a href="11-machine-learning.html#cb895-3"></a>    </span>
<span id="cb895-4"><a href="11-machine-learning.html#cb895-4"></a>    <span class="co"># Create variable for if the president is a Republican</span></span>
<span id="cb895-5"><a href="11-machine-learning.html#cb895-5"></a>    </span>
<span id="cb895-6"><a href="11-machine-learning.html#cb895-6"></a>    <span class="dt">pres_gop =</span> <span class="kw">ifelse</span>(year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2009</span><span class="op">:</span><span class="dv">2016</span>), <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb895-7"><a href="11-machine-learning.html#cb895-7"></a>    </span>
<span id="cb895-8"><a href="11-machine-learning.html#cb895-8"></a>    <span class="co"># Combine &quot;not sure&quot; and &quot;independent&quot; on party identification scale</span></span>
<span id="cb895-9"><a href="11-machine-learning.html#cb895-9"></a>    </span>
<span id="cb895-10"><a href="11-machine-learning.html#cb895-10"></a>    <span class="dt">pid7 =</span> <span class="kw">ifelse</span>(pid7 <span class="op">==</span><span class="st"> </span><span class="dv">8</span>, <span class="dv">4</span>, pid7),</span>
<span id="cb895-11"><a href="11-machine-learning.html#cb895-11"></a>    </span>
<span id="cb895-12"><a href="11-machine-learning.html#cb895-12"></a>    <span class="co"># Combine &quot;not sure&quot; and &quot;moderate&quot; on ideology scale</span></span>
<span id="cb895-13"><a href="11-machine-learning.html#cb895-13"></a>    </span>
<span id="cb895-14"><a href="11-machine-learning.html#cb895-14"></a>    <span class="dt">ideo5 =</span> <span class="kw">ifelse</span>(ideo5 <span class="op">==</span><span class="st"> &quot;Not Sure&quot;</span>, <span class="dv">3</span>, <span class="kw">as.numeric</span>(ideo5)),</span>
<span id="cb895-15"><a href="11-machine-learning.html#cb895-15"></a>    </span>
<span id="cb895-16"><a href="11-machine-learning.html#cb895-16"></a>    <span class="co"># Create &quot;female&quot; variable</span></span>
<span id="cb895-17"><a href="11-machine-learning.html#cb895-17"></a>    </span>
<span id="cb895-18"><a href="11-machine-learning.html#cb895-18"></a>    <span class="dt">female =</span> <span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(gender <span class="op">==</span><span class="st"> </span><span class="dv">2</span>),</span>
<span id="cb895-19"><a href="11-machine-learning.html#cb895-19"></a>  </span>
<span id="cb895-20"><a href="11-machine-learning.html#cb895-20"></a>    <span class="co"># Turn race into a character variable</span></span>
<span id="cb895-21"><a href="11-machine-learning.html#cb895-21"></a>    </span>
<span id="cb895-22"><a href="11-machine-learning.html#cb895-22"></a>    <span class="dt">race =</span> <span class="kw">as_factor</span>(race),</span>
<span id="cb895-23"><a href="11-machine-learning.html#cb895-23"></a>  </span>
<span id="cb895-24"><a href="11-machine-learning.html#cb895-24"></a>    <span class="co"># Simplify race categories (because some categories appear rarely)</span></span>
<span id="cb895-25"><a href="11-machine-learning.html#cb895-25"></a>    </span>
<span id="cb895-26"><a href="11-machine-learning.html#cb895-26"></a>    <span class="dt">race =</span> <span class="kw">fct_lump_n</span>(race, <span class="dv">4</span>))</span></code></pre></div>
<p>Finally, we’ll <code>select()</code> the variables we’ll be using and <code>glimpse()</code> our tibble:</p>
<div class="sourceCode" id="cb896"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb896-1"><a href="11-machine-learning.html#cb896-1"></a>cces &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb896-2"><a href="11-machine-learning.html#cb896-2"></a><span class="st">  </span><span class="kw">select</span>(pres_approval, pres_gop, pid7, ideo5, race, female, educ, age) <span class="op">%&gt;%</span></span>
<span id="cb896-3"><a href="11-machine-learning.html#cb896-3"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">complete.cases</span>(.))</span>
<span id="cb896-4"><a href="11-machine-learning.html#cb896-4"></a></span>
<span id="cb896-5"><a href="11-machine-learning.html#cb896-5"></a><span class="kw">glimpse</span>(cces)</span></code></pre></div>
<pre><code>Rows: 447,211
Columns: 8
$ pres_approval &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 4, 1, 4, 5, 1, 1, 1, 3, 1…
$ pres_gop      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
$ pid7          &lt;dbl&gt; 1, 3, 1, 1, 1, 3, 7, 1, 1, 1, 7, 6, 5, 5, 6, 3, 4, 4, 3…
$ ideo5         &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 4, 2, 2, 3, 4, 3, 5, 5, 3, 2, 3, 4, 1…
$ race          &lt;fct&gt; White, White, White, Black, White, White, White, White,…
$ female        &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1…
$ educ          &lt;int+lbl&gt; 2, 6, 2, 5, 3, 3, 4, 3, 3, 3, 3, 2, 4, 2, 2, 3, 3, …
$ age           &lt;int&gt; 32, 49, 54, 34, 20, 27, 47, 20, 77, 19, 53, 55, 38, 72,…</code></pre>
<p>Now we have one outcome (<code>pres_approval</code>, which ranges from 1 to 5) and eight potential predictors:</p>
<ul>
<li><code>pres_gop</code>: Binary variable for whether the president was a Republican when the survey was taken</li>
<li><code>pid7</code>: Respondent’s party identification on a 1-7 scale from strong Democrat to strong Republican</li>
<li><code>ideo5</code>: Respondent’s ideology on a 1-5 scale from very liberal to very conservative</li>
<li><code>race</code>: Respondent’s race, from eight choices (white, black, Hispanic, Asian, Native American, Middle Eastern, mixed, or other)</li>
<li><code>female</code>: Whether the respondent is female (0 or 1)</li>
<li><code>educ</code>: Respondent’s education on a 1-6 scale (no high school, high school, some college, two-year college, four-year college, and post-graduate education)</li>
<li><code>age</code>: Respondent’s age</li>
</ul>
<p>In past chapters, we’ve shown example models with one or two predictors, perhaps with an interaction. When you have more variables in your dataset, how can you decide which predictors to include? The techniques of machine learning can help answer this question.</p>
<p>For this chapter, we’ll consider x possible models. We have some intuitions about what should be in a useful model for presidential approval. First, since the effect of all the predictors likely vary based on whether the president is a Democrat or Republican, all predictors should be interacted with <code>pres_gop</code>. Also, party identification is likely a large predictor of presidential approval, and thus should be included. But what about the other variables? Let’s consider the following combinations:</p>
<ol style="list-style-type: decimal">
<li><code>pid7</code> alone</li>
<li><code>pid7</code> and <code>ideo5</code></li>
<li><code>pid7</code> plus demographic variables (<code>race</code>, <code>female</code>, <code>educ</code>, and <code>age</code>)</li>
<li>Same as above, but with all two-way interactions between the demographic variables</li>
<li>The kitchen sink: demographic variables with the interactions in #4, plus <code>ideo5</code></li>
</ol>
<p>Of course, these are a small subset of the possible models we could consider, either with the variables we have selected or with the larger set of all the variables in the CCES. But we’ll use these as examples for the machine learning techniques in this chapter; if you’d like, you can use the methods we learn here to test additional models.</p>
<p>Let’s save these as <code>formula</code> objects in R, so we can easily access them later. We’ll start with the simplest model we’ll consider, as <code>basic_form</code>:</p>
<div class="sourceCode" id="cb898"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb898-1"><a href="11-machine-learning.html#cb898-1"></a>basic_form &lt;-<span class="st"> </span><span class="kw">formula</span>(pres_approval <span class="op">~</span><span class="st"> </span>pid7 <span class="op">*</span><span class="st"> </span>pres_gop)</span></code></pre></div>
<p>Next, we can use <code>update()</code> to create the more complicated formulas. <code>update()</code> takes as its first argument a formula and as its second argument the additions you want to make. To keep all the predictors from the first formula and add more, you will start with <code>~ . +</code> and then add more predictors, like so:</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="11-machine-learning.html#cb899-1"></a>ideo_form &lt;-<span class="st"> </span><span class="kw">update</span>(basic_form,</span>
<span id="cb899-2"><a href="11-machine-learning.html#cb899-2"></a>                    <span class="op">~</span><span class="st"> </span>ideo5 <span class="op">*</span><span class="st"> </span>pres_gop)</span>
<span id="cb899-3"><a href="11-machine-learning.html#cb899-3"></a></span>
<span id="cb899-4"><a href="11-machine-learning.html#cb899-4"></a>demo_form &lt;-<span class="st"> </span><span class="kw">update</span>(basic_form,</span>
<span id="cb899-5"><a href="11-machine-learning.html#cb899-5"></a>                    <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> </span>race <span class="op">*</span><span class="st"> </span>pres_gop <span class="op">+</span><span class="st"> </span></span>
<span id="cb899-6"><a href="11-machine-learning.html#cb899-6"></a><span class="st">                      </span>female <span class="op">*</span><span class="st"> </span>pres_gop <span class="op">+</span><span class="st"> </span></span>
<span id="cb899-7"><a href="11-machine-learning.html#cb899-7"></a><span class="st">                      </span>educ <span class="op">*</span><span class="st"> </span>pres_gop <span class="op">+</span><span class="st"> </span></span>
<span id="cb899-8"><a href="11-machine-learning.html#cb899-8"></a><span class="st">                      </span>age <span class="op">*</span><span class="st"> </span>pres_gop)</span>
<span id="cb899-9"><a href="11-machine-learning.html#cb899-9"></a></span>
<span id="cb899-10"><a href="11-machine-learning.html#cb899-10"></a>demo_interact_form &lt;-<span class="st"> </span><span class="kw">update</span>(basic_form,</span>
<span id="cb899-11"><a href="11-machine-learning.html#cb899-11"></a>                             <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> </span>race <span class="op">*</span><span class="st"> </span>educ <span class="op">*</span><span class="st"> </span>pres_gop <span class="op">+</span><span class="st"> </span></span>
<span id="cb899-12"><a href="11-machine-learning.html#cb899-12"></a><span class="st">                               </span>female <span class="op">*</span><span class="st"> </span>age <span class="op">*</span><span class="st"> </span>pres_gop)</span></code></pre></div>
<p>Since the last model is the same as <code>demo_interact_form</code> but with <code>ideo5 * pres_gop</code> added, we’ll use <code>update(demo_interact_form)</code> rather than <code>update(basic_form)</code> here:</p>
<div class="sourceCode" id="cb900"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb900-1"><a href="11-machine-learning.html#cb900-1"></a>full_form &lt;-<span class="st"> </span><span class="kw">update</span>(demo_interact_form,</span>
<span id="cb900-2"><a href="11-machine-learning.html#cb900-2"></a>                    <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> </span>ideo5 <span class="op">*</span><span class="st"> </span>pres_gop)</span></code></pre></div>
<p>Now we have five <code>formula</code> objects we can use to fit models.</p>
<p>So we can access them easily, we’ll save them in a tibble and give them easy-to-remember names:</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb901-1"><a href="11-machine-learning.html#cb901-1"></a>cces_formulas &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">formula =</span> <span class="kw">c</span>(basic_form,</span>
<span id="cb901-2"><a href="11-machine-learning.html#cb901-2"></a>                                    ideo_form,</span>
<span id="cb901-3"><a href="11-machine-learning.html#cb901-3"></a>                                    demo_form,</span>
<span id="cb901-4"><a href="11-machine-learning.html#cb901-4"></a>                                    demo_interact_form,</span>
<span id="cb901-5"><a href="11-machine-learning.html#cb901-5"></a>                                    full_form),</span>
<span id="cb901-6"><a href="11-machine-learning.html#cb901-6"></a>                       <span class="dt">group =</span> <span class="kw">c</span>(<span class="st">&quot;Basic model&quot;</span>,</span>
<span id="cb901-7"><a href="11-machine-learning.html#cb901-7"></a>                                 <span class="st">&quot;Ideology model&quot;</span>,</span>
<span id="cb901-8"><a href="11-machine-learning.html#cb901-8"></a>                                 <span class="st">&quot;Demographic model&quot;</span>,</span>
<span id="cb901-9"><a href="11-machine-learning.html#cb901-9"></a>                                 <span class="st">&quot;Demographic interaction model&quot;</span>,</span>
<span id="cb901-10"><a href="11-machine-learning.html#cb901-10"></a>                                 <span class="st">&quot;Full model&quot;</span>))</span></code></pre></div>
</div>
<div id="the-modeling-process-using-tidymodels" class="section level2">
<h2><span class="header-section-number">11.4</span> The modeling process using <strong>tidymodels</strong></h2>
<p>If you are using <strong>tidymodels</strong>, many machine learning tasks are simplified, since you can use the same kind of code as the building blocks for any predictive modeling pipeline.</p>
<div id="parsnip-build-the-model" class="section level3">
<h3><span class="header-section-number">11.4.1</span> <strong>parsnip</strong>: build the model</h3>
<p>This step is really three, using only the <a href="https://tidymodels.github.io/parsnip/"><strong>parsnip</strong> package</a>. In this setp, we can choose the <em>model</em>, the <em>engine</em> to run the model in R, and, for some models, the <em>mode</em>. Here, our model will be linear regression, the engine <code>lm</code>, and the mode “regression” (the only possible mode for a linear regression).</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb902-1"><a href="11-machine-learning.html#cb902-1"></a>lm_spec &lt;-<span class="st"> </span></span>
<span id="cb902-2"><a href="11-machine-learning.html#cb902-2"></a><span class="st">  </span></span>
<span id="cb902-3"><a href="11-machine-learning.html#cb902-3"></a><span class="st">  </span><span class="co"># Pick model</span></span>
<span id="cb902-4"><a href="11-machine-learning.html#cb902-4"></a><span class="st">  </span></span>
<span id="cb902-5"><a href="11-machine-learning.html#cb902-5"></a><span class="st">  </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb902-6"><a href="11-machine-learning.html#cb902-6"></a><span class="st">  </span></span>
<span id="cb902-7"><a href="11-machine-learning.html#cb902-7"></a><span class="st">  </span><span class="co"># Set engine</span></span>
<span id="cb902-8"><a href="11-machine-learning.html#cb902-8"></a><span class="st">  </span></span>
<span id="cb902-9"><a href="11-machine-learning.html#cb902-9"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;lm&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb902-10"><a href="11-machine-learning.html#cb902-10"></a><span class="st">  </span></span>
<span id="cb902-11"><a href="11-machine-learning.html#cb902-11"></a><span class="st">  </span><span class="co"># Set mode</span></span>
<span id="cb902-12"><a href="11-machine-learning.html#cb902-12"></a><span class="st">  </span></span>
<span id="cb902-13"><a href="11-machine-learning.html#cb902-13"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>) </span>
<span id="cb902-14"><a href="11-machine-learning.html#cb902-14"></a></span>
<span id="cb902-15"><a href="11-machine-learning.html#cb902-15"></a>lm_spec</span></code></pre></div>
<pre><code>Linear Regression Model Specification (regression)

Computational engine: lm </code></pre>
<p>To keep things simple, we’ll only be evaluating linear regressions in this chapter, although there are many other modeling choices one could make for predicting presidential approval, some of which may be superior.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> Note that you could evaluate the performances of those other models using the same building blocks of code that we show you here.</p>
<p>Things that are missing: data (we haven’t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a>.</p>
</div>
<div id="recipes-not-happening-here-folks" class="section level3">
<h3><span class="header-section-number">11.4.2</span> <strong>recipes</strong>: not happening here, folks</h3>
<p>This is where one would normally insert some code for <em>feature engineering</em> using the <strong>recipes</strong> package. Feature engineering involves transforming your data to create different predictors, such as by taking log transformations, turning numerical variables into factors or vise versa, and so on. We engaged in some rudimentary feature engineering when we <code>mutate</code>d the CCES at the beginning of this chapter. But for the purposes of this chapter, we will treat our data as-is.</p>
</div>
<div id="rsample-initial-split" class="section level3">
<h3><span class="header-section-number">11.4.3</span> <strong>rsample</strong>: initial split</h3>
<p>We’ll use the <a href="https://tidymodels.github.io/rsample/"><strong>rsample</strong> package</a> to split the CCES up into two datasets: training and testing. The <code>initial_split()</code> function takes a dataset and splits it into a training and test set. By default, 75% of the data is kept in the training set and the rest are allocated to the test set. This can be changed with the <code>prop</code> argument – we’ll set it at 0.8. Because the split is done at random, we need to use <code>set.seed()</code> to ensure our results are replicable.</p>
<div class="sourceCode" id="cb904"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb904-1"><a href="11-machine-learning.html#cb904-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb904-2"><a href="11-machine-learning.html#cb904-2"></a></span>
<span id="cb904-3"><a href="11-machine-learning.html#cb904-3"></a>cces_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(cces, <span class="dt">prop =</span> <span class="fl">0.8</span>)</span>
<span id="cb904-4"><a href="11-machine-learning.html#cb904-4"></a>cces_train &lt;-<span class="st"> </span><span class="kw">training</span>(cces_split)</span>
<span id="cb904-5"><a href="11-machine-learning.html#cb904-5"></a>cces_test &lt;-<span class="st"> </span><span class="kw">testing</span>(cces_split)</span></code></pre></div>
</div>
<div id="fitting-the-model-once" class="section level3">
<h3><span class="header-section-number">11.4.4</span> Fitting the model once</h3>
<p>Fitting a single model once is… not <em>exactly</em> the hardest part.</p>
<p>First, we can get the fitted model using the <code>fit()</code> function:</p>
<div class="sourceCode" id="cb905"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb905-1"><a href="11-machine-learning.html#cb905-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb905-2"><a href="11-machine-learning.html#cb905-2"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> cces_train)</span></code></pre></div>
<pre><code>parsnip model object

Fit time:  87ms 

Call:
stats::lm(formula = formula, data = data)

Coefficients:
  (Intercept)           pid7       pres_gop  pid7:pres_gop  
         4.90          -0.57          -4.50           1.11  </code></pre>
<p>Note that we can use <code>tidy()</code>, just like we did in previous chapters, to take a look at the results:</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb907-1"><a href="11-machine-learning.html#cb907-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb907-2"><a href="11-machine-learning.html#cb907-2"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> cces_train) <span class="op">%&gt;%</span></span>
<span id="cb907-3"><a href="11-machine-learning.html#cb907-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb907-4"><a href="11-machine-learning.html#cb907-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code># A tibble: 4 x 4
  term          estimate conf.low conf.high
  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)      4.90     4.89      4.91 
2 pid7            -0.570   -0.572    -0.568
3 pres_gop        -4.50    -4.51     -4.48 
4 pid7:pres_gop    1.11     1.11      1.11 </code></pre>
<p>As we’d expect, Republicans approve of Republican presidents more.</p>
<p>Now that we have fit a model on the <em>training</em> set, is it time to make predictions on the <em>test</em> set? In general, we would <strong>not</strong> want to predict the test set at this point, although we will do so to illustrate how the code works. In a real scenario, we would use <em>resampling</em> methods (e.g., cross-validation, bootstrapping) to evaluate how well the model is doing. <strong>tidymodels</strong> has a great infrastructure to do this with <strong>rsample</strong>, and we will talk about this soon to demonstrate how we should really evaluate models.</p>
<p>To make predictions, we’ll use the <code>predict()</code> function. We will use the argument <code>new_data = cces_test</code> to make predictions on the test set.</p>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb909-1"><a href="11-machine-learning.html#cb909-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb909-2"><a href="11-machine-learning.html#cb909-2"></a><span class="st">  </span></span>
<span id="cb909-3"><a href="11-machine-learning.html#cb909-3"></a><span class="st">  </span><span class="co"># Train: get fitted model</span></span>
<span id="cb909-4"><a href="11-machine-learning.html#cb909-4"></a><span class="st">  </span></span>
<span id="cb909-5"><a href="11-machine-learning.html#cb909-5"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> cces_train) <span class="op">%&gt;%</span></span>
<span id="cb909-6"><a href="11-machine-learning.html#cb909-6"></a><span class="st">  </span></span>
<span id="cb909-7"><a href="11-machine-learning.html#cb909-7"></a><span class="st">  </span><span class="co"># Test: get predictions</span></span>
<span id="cb909-8"><a href="11-machine-learning.html#cb909-8"></a><span class="st">  </span></span>
<span id="cb909-9"><a href="11-machine-learning.html#cb909-9"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> cces_test)</span></code></pre></div>
<pre><code># A tibble: 89,442 x 1
   .pred
   &lt;dbl&gt;
 1 0.944
 2 3.10 
 3 2.56 
 4 4.18 
 5 4.18 
 6 2.56 
 7 3.64 
 8 2.56 
 9 4.18 
10 4.18 
# … with 89,432 more rows</code></pre>
<p>Now we have a tibble of predictions. How can we evaluate it? The <strong>yardstick</strong> package is a tidy interface for computing measures of performance, with individual functions for specific metrics (e.g., <code>accuracy()</code>, <code>rmse()</code>). The <code>rmse()</code> function in the <strong>yardstick</strong> package will compute the RMSE for us, as long as we have the actual values. So we’ll <code>bind_cols()</code> to the test data and use <code>rmse()</code> to evaluate our model. <code>rmse()</code> requires that we give it the <code>truth</code> (here, <code>pres_approval</code>) and the our <code>estimate</code> (here, <code>.pred</code>):</p>
<div class="sourceCode" id="cb911"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb911-1"><a href="11-machine-learning.html#cb911-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb911-2"><a href="11-machine-learning.html#cb911-2"></a><span class="st">  </span></span>
<span id="cb911-3"><a href="11-machine-learning.html#cb911-3"></a><span class="st">  </span><span class="co"># Train: get fitted model</span></span>
<span id="cb911-4"><a href="11-machine-learning.html#cb911-4"></a><span class="st">  </span></span>
<span id="cb911-5"><a href="11-machine-learning.html#cb911-5"></a><span class="st">  </span><span class="kw">fit</span>(basic_form, <span class="dt">data =</span> cces_train) <span class="op">%&gt;%</span></span>
<span id="cb911-6"><a href="11-machine-learning.html#cb911-6"></a><span class="st">  </span></span>
<span id="cb911-7"><a href="11-machine-learning.html#cb911-7"></a><span class="st">  </span><span class="co"># Test: get predictions</span></span>
<span id="cb911-8"><a href="11-machine-learning.html#cb911-8"></a><span class="st">  </span></span>
<span id="cb911-9"><a href="11-machine-learning.html#cb911-9"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> cces_test) <span class="op">%&gt;%</span></span>
<span id="cb911-10"><a href="11-machine-learning.html#cb911-10"></a><span class="st">  </span></span>
<span id="cb911-11"><a href="11-machine-learning.html#cb911-11"></a><span class="st">  </span><span class="co"># Compare: get metrics</span></span>
<span id="cb911-12"><a href="11-machine-learning.html#cb911-12"></a><span class="st">  </span></span>
<span id="cb911-13"><a href="11-machine-learning.html#cb911-13"></a><span class="st">  </span><span class="kw">bind_cols</span>(cces_test) <span class="op">%&gt;%</span></span>
<span id="cb911-14"><a href="11-machine-learning.html#cb911-14"></a><span class="st">  </span><span class="kw">rmse</span>(<span class="dt">truth =</span> pres_approval, <span class="dt">estimate =</span> .pred)</span></code></pre></div>
<pre><code># A tibble: 1 x 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard        1.14</code></pre>
</div>
<div id="fitting-many-models-using-map-2" class="section level3">
<h3><span class="header-section-number">11.4.5</span> Fitting many models using <code>map()</code></h3>
<p>If you squint, you might see that we could make this process into a function like the one below below:</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb913-1"><a href="11-machine-learning.html#cb913-1"></a>fit_lm_split &lt;-<span class="st"> </span><span class="cf">function</span>(formula, train, test) {</span>
<span id="cb913-2"><a href="11-machine-learning.html#cb913-2"></a>  lm_spec <span class="op">%&gt;%</span></span>
<span id="cb913-3"><a href="11-machine-learning.html#cb913-3"></a><span class="st">    </span><span class="kw">fit</span>(formula, <span class="dt">data =</span> train) <span class="op">%&gt;%</span></span>
<span id="cb913-4"><a href="11-machine-learning.html#cb913-4"></a><span class="st">    </span><span class="kw">predict</span>(<span class="dt">new_data =</span> test) <span class="op">%&gt;%</span></span>
<span id="cb913-5"><a href="11-machine-learning.html#cb913-5"></a><span class="st">    </span><span class="kw">bind_cols</span>(test)</span>
<span id="cb913-6"><a href="11-machine-learning.html#cb913-6"></a>}</span></code></pre></div>
<p>This function takes a <code>formula</code> object and fits it a linear regression on the training data. It returns the test data with a new column (<code>.pred</code>) that contains predictions from the model that we fit on the training data.</p>
<p>It’s not a great leap to can then create a tibble that has all the predictions for every specification in <code>cces_formualas</code>, using our old friend <code>map()</code>:</p>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb914-1"><a href="11-machine-learning.html#cb914-1"></a>cces_test_preds &lt;-<span class="st"> </span>cces_formulas <span class="op">%&gt;%</span></span>
<span id="cb914-2"><a href="11-machine-learning.html#cb914-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">preds =</span> <span class="kw">map</span>(formula, <span class="op">~</span><span class="st"> </span><span class="kw">fit_lm_split</span>(., cces_train, cces_test))) <span class="op">%&gt;%</span></span>
<span id="cb914-3"><a href="11-machine-learning.html#cb914-3"></a><span class="st">  </span><span class="kw">unnest</span>(preds)</span></code></pre></div>
<p>Finally, we can use the <code>rmse()</code> function to compare our five specifications.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb915-1"><a href="11-machine-learning.html#cb915-1"></a>cces_test_preds <span class="op">%&gt;%</span></span>
<span id="cb915-2"><a href="11-machine-learning.html#cb915-2"></a><span class="st">  </span><span class="kw">group_by</span>(group) <span class="op">%&gt;%</span></span>
<span id="cb915-3"><a href="11-machine-learning.html#cb915-3"></a><span class="st">  </span><span class="kw">rmse</span>(<span class="dt">truth =</span> pres_approval, <span class="dt">estimate =</span> .pred)</span></code></pre></div>
<pre><code># A tibble: 5 x 4
  group                         .metric .estimator .estimate
  &lt;chr&gt;                         &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 Basic model                   rmse    standard        1.14
2 Demographic interaction model rmse    standard        1.11
3 Demographic model             rmse    standard        1.11
4 Full model                    rmse    standard        1.08
5 Ideology model                rmse    standard        1.32</code></pre>
<p>Here we see that adding ideology to the model above and beyond partisanship makes the predictions worse. The two demographic models perform similarly. Interestingly, adding back in ideology to the demographic model with interactions performs the best on the test set.</p>
<p>But, unfortunately, we shouldn’t be predicting with the test set over and over again like this. It isn’t good practice to predict with the test set more than one time. What is a good predictive modeler to do? We should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end — after we’ve compared different models, selected features, and tuned hyperparameters. How do you do this? You do <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html">cross-validation</a> with the training set, and you leave the testing set for <a href="https://tidymodels.github.io/tune/reference/last_fit.html"><em>the very last fit you do</em></a>.</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">11.5</span> Cross validation</h2>
<p>In this section we introduce cross validation, one of the most important ideas in machine learning.</p>
<p>In Section <a href="11-machine-learning.html#loss-function">11.2.2</a>, we described that a common goal of machine learning is to find an algorithm that produces predictors <span class="math inline">\(\hat{Y}\)</span> for an outcome <span class="math inline">\(Y\)</span> that minimizes the MSE:</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span>
There are two important characteristics of the MSE we should always keep in mind:</p>
<ol style="list-style-type: decimal">
<li><p>We can think our estimate of the MSE is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.</p></li>
<li><p>If we train an algorithm on the same dataset that we use to compute the MSE, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error.</p></li>
</ol>
<p>Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the <em>true error</em>, a theoretical quantity, as the average of many <em>apparent errors</em> obtained by applying the algorithm to new random samples of the data, none of them used to train the algorithm.</p>
<p>However, we only have available one set of outcomes: the ones we actually observed. Cross validation is based on the idea of generating a series of different random samples on which to apply our algorithm. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.</p>
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">11.5.1</span> K-fold cross validation</h3>
<p>The first approach we describe is <em>K-fold cross validation</em>.</p>
<p>Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).</p>
<p><img src="images/cv-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>But we don’t get to see these independent datasets.</p>
<p><img src="images/cv-2.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a <em>training set</em> (blue) and a <em>test set</em> (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.</p>
<p>We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. The <code>initial_split()</code> function reserves 25% of the data for testing by default.</p>
<p><img src="images/cv-3.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>Let’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting predictors, nothing!</p>
<p>Now this presents a new problem because for most machine learning algorithms we need to select parameters. We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful.</p>
<p>For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.</p>
<p>Let’s start by describing how to construct the first “fold”: we simply pick <span class="math inline">\(M=N/K\)</span> observations at random (we round if <span class="math inline">\(M\)</span> is not a round number) and think of these as a random sample. We call this the <em>validation set</em>:</p>
<p><img src="images/cv-4.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>Now we can fit the model in the training set, then compute the MSE on the validation set. Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take <span class="math inline">\(K\)</span> samples, not just one. In K-cross validation, we randomly split the observations into <span class="math inline">\(K\)</span> non-overlapping sets:</p>
<p><img src="images/cv-5.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>Then, for our final estimate, we compute the average MSE across our <span class="math inline">\(K\)</span> samples.</p>
<p>We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:</p>
<p><img src="images/cv-6.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.</p>
<p><img src="images/cv-8.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" style="display: block; margin: auto;" /></p>
<p>Now how do we pick the cross validation <span class="math inline">\(K\)</span>? Large values of <span class="math inline">\(K\)</span> are preferable because the training data better imitates the original dataset. However, larger values of <span class="math inline">\(K\)</span> will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of <span class="math inline">\(K=5\)</span> and <span class="math inline">\(K=10\)</span> are popular.</p>
</div>
<div id="implementing-cross-validation-using-rsample" class="section level3">
<h3><span class="header-section-number">11.5.2</span> Implementing cross-validation using <strong>rsample</strong></h3>
<p>Now let’s add cross-validation to our <strong>tidymodels</strong> workflow! To do this, we’ll use a function called <code>vfold_cv()</code> in the <strong>rsample</strong> package. The argument <code>v</code> sets the number of folds, which is 10 by default. We’ll do 5-fold cross-validation in this example.</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb917-1"><a href="11-machine-learning.html#cb917-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb917-2"><a href="11-machine-learning.html#cb917-2"></a></span>
<span id="cb917-3"><a href="11-machine-learning.html#cb917-3"></a>cces_folds &lt;-<span class="st"> </span>cces_train <span class="op">%&gt;%</span></span>
<span id="cb917-4"><a href="11-machine-learning.html#cb917-4"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>How can we work with the <code>cces_folds</code> object? <strong>tidymodels</strong> makes it easy by using the <code>fit_resamples()</code> function in the <strong>tune</strong> package. (<strong>Note</strong>: make sure you have at least version 0.1.0 of the <strong>tune</strong> package installed, as the following code uses new syntax.) The <code>fit_resamples()</code> function takes as its first argument a model specification (such as <code>lm_spec</code>), then a formula as its second argument, called <code>preprocessor</code>.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> Finally, the <code>resamples</code> argument is where you input the cross-validation dataset.</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="11-machine-learning.html#cb918-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb918-2"><a href="11-machine-learning.html#cb918-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb918-3"><a href="11-machine-learning.html#cb918-3"></a>                <span class="dt">resamples =</span> cces_folds)</span></code></pre></div>
<pre><code>#  5-fold cross-validation 
# A tibble: 5 x 4
  splits                 id    .metrics         .notes          
  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
1 &lt;split [286.2K/71.6K]&gt; Fold1 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
2 &lt;split [286.2K/71.6K]&gt; Fold2 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
3 &lt;split [286.2K/71.6K]&gt; Fold3 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
4 &lt;split [286.2K/71.6K]&gt; Fold4 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
5 &lt;split [286.2K/71.6K]&gt; Fold5 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;</code></pre>
<p>To inspect the average metrics across all the folds, we can use the <code>collect_metrics()</code> function:</p>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb920-1"><a href="11-machine-learning.html#cb920-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb920-2"><a href="11-machine-learning.html#cb920-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb920-3"><a href="11-machine-learning.html#cb920-3"></a>                <span class="dt">resamples =</span> cces_folds) <span class="op">%&gt;%</span></span>
<span id="cb920-4"><a href="11-machine-learning.html#cb920-4"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code># A tibble: 2 x 5
  .metric .estimator  mean     n  std_err
  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;
1 rmse    standard   1.13      5 0.000948
2 rsq     standard   0.537     5 0.000700</code></pre>
<p>It’s not that hard to extend this to all of our formulas using <code>map()</code>.</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="11-machine-learning.html#cb922-1"></a>folds_metrics &lt;-<span class="st"> </span>cces_formulas <span class="op">%&gt;%</span></span>
<span id="cb922-2"><a href="11-machine-learning.html#cb922-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">metrics =</span> <span class="kw">map</span>(formula, <span class="op">~</span><span class="st"> </span><span class="kw">fit_resamples</span>(lm_spec,</span>
<span id="cb922-3"><a href="11-machine-learning.html#cb922-3"></a>                                                <span class="dt">preprocessor =</span> .,</span>
<span id="cb922-4"><a href="11-machine-learning.html#cb922-4"></a>                                                <span class="dt">resamples =</span> cces_folds) <span class="op">%&gt;%</span></span>
<span id="cb922-5"><a href="11-machine-learning.html#cb922-5"></a><span class="st">                         </span><span class="kw">collect_metrics</span>()))</span></code></pre></div>
<p>(Note that <code>fit_resamples()</code> currently gives the warning message “prediction from a rank-deficient fit may be misleading” whenever a factor or character variable is used as a predictor; this does not necessarily mean that the fit was actually rank-deficient.)</p>
<!-- AR: this annoying behavior appears to be because they want you to use
recipes and then step_dummy() the factor variables -->
<p>Let’s present the results stored in our <code>folds_metrics</code> object:</p>
<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb923-1"><a href="11-machine-learning.html#cb923-1"></a>folds_metrics <span class="op">%&gt;%</span></span>
<span id="cb923-2"><a href="11-machine-learning.html#cb923-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean_rmse =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(mean)),</span>
<span id="cb923-3"><a href="11-machine-learning.html#cb923-3"></a>         <span class="dt">se_rmse =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(std_err))) <span class="op">%&gt;%</span></span>
<span id="cb923-4"><a href="11-machine-learning.html#cb923-4"></a><span class="st">  </span><span class="kw">select</span>(group, mean_rmse, se_rmse)</span></code></pre></div>
<pre><code># A tibble: 5 x 3
  group                         mean_rmse  se_rmse
  &lt;chr&gt;                             &lt;dbl&gt;    &lt;dbl&gt;
1 Basic model                        1.13 0.000948
2 Ideology model                     1.32 0.00105 
3 Demographic model                  1.11 0.00112 
4 Demographic interaction model      1.11 0.00104 
5 Full model                         1.08 0.00109 </code></pre>
</div>
<div id="bootstrap" class="section level3">
<h3><span class="header-section-number">11.5.3</span> Bootstrap</h3>
<p>One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick <span class="math inline">\(K\)</span> sets of some size at random.</p>
<p>One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice) – our old friend the <em>bootstrap</em>.</p>
<p>In <strong>rsample</strong>, we can do that using the <code>boostraps()</code> function. The <code>times</code> argument states how many bootstrap samples you want to take:</p>
<div class="sourceCode" id="cb925"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb925-1"><a href="11-machine-learning.html#cb925-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb925-2"><a href="11-machine-learning.html#cb925-2"></a></span>
<span id="cb925-3"><a href="11-machine-learning.html#cb925-3"></a>cces_boots &lt;-<span class="st"> </span>cces_train <span class="op">%&gt;%</span></span>
<span id="cb925-4"><a href="11-machine-learning.html#cb925-4"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Now we can use <code>fit_resample()</code> just like we did with <code>cces_folds</code>. Here’s how we do it with one formula:</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb926-1"><a href="11-machine-learning.html#cb926-1"></a>lm_spec <span class="op">%&gt;%</span></span>
<span id="cb926-2"><a href="11-machine-learning.html#cb926-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">preprocessor =</span> basic_form,</span>
<span id="cb926-3"><a href="11-machine-learning.html#cb926-3"></a>                <span class="dt">resamples =</span> cces_boots) <span class="op">%&gt;%</span></span>
<span id="cb926-4"><a href="11-machine-learning.html#cb926-4"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code># A tibble: 2 x 5
  .metric .estimator  mean     n  std_err
  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;
1 rmse    standard   1.13     25 0.000384
2 rsq     standard   0.537    25 0.000315</code></pre>
<p>And we can also adapt our code to run this for every formula in <code>cces_formulas</code>:</p>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb928-1"><a href="11-machine-learning.html#cb928-1"></a>boots_metrics &lt;-<span class="st"> </span>cces_formulas <span class="op">%&gt;%</span></span>
<span id="cb928-2"><a href="11-machine-learning.html#cb928-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">metrics =</span> <span class="kw">map</span>(formula, <span class="op">~</span><span class="st"> </span><span class="kw">fit_resamples</span>(lm_spec,</span>
<span id="cb928-3"><a href="11-machine-learning.html#cb928-3"></a>                                                <span class="dt">preprocessor =</span> .,</span>
<span id="cb928-4"><a href="11-machine-learning.html#cb928-4"></a>                                                <span class="dt">resamples =</span> cces_boots) <span class="op">%&gt;%</span></span>
<span id="cb928-5"><a href="11-machine-learning.html#cb928-5"></a><span class="st">                         </span><span class="kw">collect_metrics</span>()))</span>
<span id="cb928-6"><a href="11-machine-learning.html#cb928-6"></a></span>
<span id="cb928-7"><a href="11-machine-learning.html#cb928-7"></a>boots_metrics <span class="op">%&gt;%</span></span>
<span id="cb928-8"><a href="11-machine-learning.html#cb928-8"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean_rmse =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(mean)),</span>
<span id="cb928-9"><a href="11-machine-learning.html#cb928-9"></a>         <span class="dt">se_rmse =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;rmse&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(std_err))) <span class="op">%&gt;%</span></span>
<span id="cb928-10"><a href="11-machine-learning.html#cb928-10"></a><span class="st">  </span><span class="kw">select</span>(group, mean_rmse, se_rmse)</span></code></pre></div>
<!-- AR: this takes a while, so I saved the result, but be sure to uncomment
this if you need to run this code again -->
<pre><code># A tibble: 5 x 3
  group                         mean_rmse  se_rmse
  &lt;chr&gt;                             &lt;dbl&gt;    &lt;dbl&gt;
1 Basic model                        1.13 0.000384
2 Ideology model                     1.32 0.000467
3 Demographic model                  1.11 0.000375
4 Demographic interaction model      1.11 0.000371
5 Full model                         1.08 0.000355</code></pre>
<p>Cross-validation and the bootstrap lead to similar estimates of the RMSE, but note that the standard error of the RMSE goes down using the bootstrap method, since more samples were taken. However, the trade-off is that the more samples you take, the more computing time you’ll use.</p>
</div>
</div>
<div id="machine-learning-and-classification" class="section level2">
<h2><span class="header-section-number">11.6</span> Machine learning and classification</h2>
<p><strong>tidymodels</strong> makes it easy for us to adapt the code we used above for a continuous dependent variable for a binary dependent variable. Let’s grab the variable for a respondent’s House vote. We’ll make a new variable, <code>dem_house_vote</code>, that takes on two values: 1 if the respondent voted for a Democrat, 0 if not.</p>
<p>We’ll also <code>mutate()</code> and <code>select()</code> some other variables as predictors. Most of these are the same as we used when predicting presidential approval, but we’ll add variables for approval of the respondent’s current representative and whether the current representative is a Democrat.</p>
<p>Next, let’s some up with some candidate formulas to test. Here are five:</p>
<ol style="list-style-type: decimal">
<li>The “approval model”: Predicting <code>dem_house_vote</code> with <code>approval_rep5</code> (approval of the current representative), <code>dem_representative</code> (whether the current representative is a Democrat), and the interaction between the two</li>
<li>The “party and ideology model”: Predicting <code>dem_house_vote</code> with party identification and ideology</li>
<li>The “demographic model”: Predicting <code>dem_house_vote</code> with the demographic variables</li>
<li>The “approval, party, and ideology model”: A combination of 1 and 2.</li>
<li>The “full model”: All predictors.</li>
</ol>
<p>We’ll save these formulas in a tibble called <code>house_formulas</code>:</p>
<div class="sourceCode" id="cb930"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb930-1"><a href="11-machine-learning.html#cb930-1"></a>house_formulas &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">formula =</span> <span class="kw">c</span>(dem_house_vote <span class="op">~</span><span class="st"> </span>approval_rep5 <span class="op">*</span><span class="st"> </span>dem_representative,</span>
<span id="cb930-2"><a href="11-machine-learning.html#cb930-2"></a>                                     dem_house_vote <span class="op">~</span><span class="st"> </span>pid7 <span class="op">+</span><span class="st"> </span>ideo5,</span>
<span id="cb930-3"><a href="11-machine-learning.html#cb930-3"></a>                                     dem_house_vote <span class="op">~</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>female <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>educ,</span>
<span id="cb930-4"><a href="11-machine-learning.html#cb930-4"></a>                                     dem_house_vote <span class="op">~</span><span class="st"> </span>approval_rep5 <span class="op">*</span><span class="st"> </span>dem_representative <span class="op">+</span><span class="st"> </span></span>
<span id="cb930-5"><a href="11-machine-learning.html#cb930-5"></a><span class="st">                                       </span>pid7 <span class="op">+</span><span class="st"> </span>ideo5,</span>
<span id="cb930-6"><a href="11-machine-learning.html#cb930-6"></a>                                     dem_house_vote <span class="op">~</span><span class="st"> </span>approval_rep5 <span class="op">*</span><span class="st"> </span>dem_representative <span class="op">+</span></span>
<span id="cb930-7"><a href="11-machine-learning.html#cb930-7"></a><span class="st">                                       </span>pid7 <span class="op">+</span><span class="st"> </span>ideo5 <span class="op">+</span></span>
<span id="cb930-8"><a href="11-machine-learning.html#cb930-8"></a><span class="st">                                       </span>race <span class="op">+</span><span class="st"> </span>female <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>educ),</span>
<span id="cb930-9"><a href="11-machine-learning.html#cb930-9"></a>                         <span class="dt">group =</span> <span class="kw">c</span>(<span class="st">&quot;Approval model&quot;</span>,</span>
<span id="cb930-10"><a href="11-machine-learning.html#cb930-10"></a>                                   <span class="st">&quot;Party and ideology model&quot;</span>,</span>
<span id="cb930-11"><a href="11-machine-learning.html#cb930-11"></a>                                   <span class="st">&quot;Demographic model&quot;</span>,</span>
<span id="cb930-12"><a href="11-machine-learning.html#cb930-12"></a>                                   <span class="st">&quot;Approval, party, and ideology model&quot;</span>,</span>
<span id="cb930-13"><a href="11-machine-learning.html#cb930-13"></a>                                   <span class="st">&quot;Full model&quot;</span>))</span></code></pre></div>
<p>We’ll then divide our data into a training and test set:</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="11-machine-learning.html#cb931-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb931-2"><a href="11-machine-learning.html#cb931-2"></a></span>
<span id="cb931-3"><a href="11-machine-learning.html#cb931-3"></a>cces_house_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(cces_house, <span class="dt">prop =</span> <span class="fl">0.8</span>)</span>
<span id="cb931-4"><a href="11-machine-learning.html#cb931-4"></a>cces_house_train &lt;-<span class="st"> </span><span class="kw">training</span>(cces_house_split)</span>
<span id="cb931-5"><a href="11-machine-learning.html#cb931-5"></a>cces_house_test &lt;-<span class="st"> </span><span class="kw">testing</span>(cces_house_split)</span></code></pre></div>
<p>Using our training data, we can employ 5-fold cross validation:</p>
<div class="sourceCode" id="cb932"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb932-1"><a href="11-machine-learning.html#cb932-1"></a>cces_house_folds &lt;-<span class="st"> </span>cces_house_train <span class="op">%&gt;%</span></span>
<span id="cb932-2"><a href="11-machine-learning.html#cb932-2"></a><span class="st">  </span><span class="kw">vfold_cv</span>(<span class="dt">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>We’ll run a logistic regression here, but note that we can use CART and random forest as well if we’d like to test those:</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb933-1"><a href="11-machine-learning.html#cb933-1"></a>logistic_spec &lt;-<span class="st"> </span></span>
<span id="cb933-2"><a href="11-machine-learning.html#cb933-2"></a><span class="st">  </span></span>
<span id="cb933-3"><a href="11-machine-learning.html#cb933-3"></a><span class="st">  </span><span class="co"># Pick model</span></span>
<span id="cb933-4"><a href="11-machine-learning.html#cb933-4"></a><span class="st">  </span></span>
<span id="cb933-5"><a href="11-machine-learning.html#cb933-5"></a><span class="st">  </span><span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb933-6"><a href="11-machine-learning.html#cb933-6"></a><span class="st">  </span></span>
<span id="cb933-7"><a href="11-machine-learning.html#cb933-7"></a><span class="st">  </span><span class="co"># Set engine</span></span>
<span id="cb933-8"><a href="11-machine-learning.html#cb933-8"></a><span class="st">  </span></span>
<span id="cb933-9"><a href="11-machine-learning.html#cb933-9"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;glm&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb933-10"><a href="11-machine-learning.html#cb933-10"></a><span class="st">  </span></span>
<span id="cb933-11"><a href="11-machine-learning.html#cb933-11"></a><span class="st">  </span><span class="co"># Set mode</span></span>
<span id="cb933-12"><a href="11-machine-learning.html#cb933-12"></a><span class="st">  </span></span>
<span id="cb933-13"><a href="11-machine-learning.html#cb933-13"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>) </span></code></pre></div>
<p>We’ll use <code>map_()</code> to <code>fit_resamples()</code> for all five models.</p>
<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb934-1"><a href="11-machine-learning.html#cb934-1"></a>folds_metrics &lt;-<span class="st"> </span>house_formulas <span class="op">%&gt;%</span></span>
<span id="cb934-2"><a href="11-machine-learning.html#cb934-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">metrics =</span> <span class="kw">map</span>(formula, <span class="op">~</span><span class="st"> </span><span class="kw">fit_resamples</span>(logistic_spec,</span>
<span id="cb934-3"><a href="11-machine-learning.html#cb934-3"></a>                                                <span class="dt">preprocessor =</span> .,</span>
<span id="cb934-4"><a href="11-machine-learning.html#cb934-4"></a>                                                <span class="dt">resamples =</span> cces_house_folds) <span class="op">%&gt;%</span></span>
<span id="cb934-5"><a href="11-machine-learning.html#cb934-5"></a><span class="st">                         </span><span class="kw">collect_metrics</span>()))</span></code></pre></div>
<p>We’ll look at accuracy, which is equivalent to the RMSE.</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb935-1"><a href="11-machine-learning.html#cb935-1"></a>folds_metrics <span class="op">%&gt;%</span></span>
<span id="cb935-2"><a href="11-machine-learning.html#cb935-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean_acc =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(mean)),</span>
<span id="cb935-3"><a href="11-machine-learning.html#cb935-3"></a>         <span class="dt">se_acc =</span> <span class="kw">map_dbl</span>(metrics, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., .metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(std_err))) <span class="op">%&gt;%</span></span>
<span id="cb935-4"><a href="11-machine-learning.html#cb935-4"></a><span class="st">  </span><span class="kw">select</span>(group, mean_acc, se_acc)</span></code></pre></div>
<pre><code># A tibble: 5 x 3
  group                               mean_acc   se_acc
  &lt;chr&gt;                                  &lt;dbl&gt;    &lt;dbl&gt;
1 Approval model                         0.817 0.00123 
2 Party and ideology model               0.883 0.00110 
3 Demographic model                      0.614 0.000707
4 Approval, party, and ideology model    0.906 0.000949
5 Full model                             0.907 0.000915</code></pre>
<p>The best performing model is the full model, although the model with approval, party, and ideology does almost as well, suggesting that demographic variables are not especially predictive once you take into account these other factors. Interestingly, party and ideology predict vote choice better than approval.</p>
</div>
<div id="conclusion-7" class="section level2">
<h2><span class="header-section-number">11.7</span> Conclusion</h2>
<p>In this chapter, as in the Primer as a whole, we have been making models to better understand the world. But we need to act, as well as to understand.</p>
<p>We’ve given you tools to construct models and to evaluate features of these models, such as the causal effects of variables of interest and predicted values of the outcome. We’ve also shown you how to construct measures of uncertainty around these measures.</p>
<p>Once you have this information, what should you do? Remember that all models are meant to simplify some real-world phenomenon. Ultimately, you have to use the informations from the models to make a decision. For example, let’s say that you construct a model to evaluate the benefits of a hypertension drug. Should you prescribe the drug? You can take the estimates of the likely drug from the model to help with that decision. The model won’t make the decision for you, however – you’ll also want to incorporate other information, such as the costs of the drug.</p>
<p>From the other perspective, you should always make models while keeping in mind what unknown features of the real world you need to estimate. Maybe you are trying to estimate a single value, such as a mean (how many adults are in the United States right now?) Maybe you want to predict the future value of some variable (how many adults will be in the United States ten years from now?). Or maybe you want to estimate the relationship between two variables (how do changes in immigration policy affect the U.S. adult population?).</p>
<p>We’ve given you tools to help answer all these questions. Now it’s up to you to use those tools to help make decisions in real life!</p>

</div>
</div>



<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>Note that there are loss functions other than the squared loss. For example, the <em>Mean Absolute Error</em> uses absolute values, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> instead of squaring the errors <span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. However, in this chapter we focus on minimizing square loss since it is the most widely used.<a href="11-machine-learning.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>Doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.<a href="11-machine-learning.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>For instance, linear regression allows for predicted values which are below 1 and above 5, which are theoretically forbidden. Furthermore, linear regression assumes that the distance between each response category is the same, since the distance between 1 and 2 is the same as 2 and 3, and so on, but there may be real world “breakpoints,” for instance if it is more important to go from neutral to somewhat approval than from somewhat approval to strong approval. However, for the purposes of this chapter, we will proceed with linear regression.<a href="11-machine-learning.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>Or a recipe, if you delve more deeply into <strong>tidymodels</strong>.<a href="11-machine-learning.html#fnref22" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="10-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="A-productivity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
