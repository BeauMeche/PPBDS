<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science</title>
  <meta name="description" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="davidkane9/PPBDS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  
  
  

<meta name="author" content="David Kane" />


<meta name="date" content="2020-03-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12-multiple-regression.html"/>
<link rel="next" href="14-machine.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="forward.html"><a href="forward.html"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="warning.html"><a href="warning.html"><i class="fa fa-check"></i>Warning</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="1" data-path="1-getting-started.html"><a href="1-getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#r-rstudio"><i class="fa fa-check"></i><b>1.1</b> What are R and RStudio?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#installing"><i class="fa fa-check"></i><b>1.1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#using-r-via-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Using R via RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#code"><i class="fa fa-check"></i><b>1.2</b> How do I code in R?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-getting-started.html"><a href="1-getting-started.html#programming-concepts"><i class="fa fa-check"></i><b>1.2.1</b> Basic programming concepts and terminology</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-getting-started.html"><a href="1-getting-started.html#messages"><i class="fa fa-check"></i><b>1.2.2</b> Errors, warnings, and messages</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-getting-started.html"><a href="1-getting-started.html#tips-code"><i class="fa fa-check"></i><b>1.2.3</b> Tips on learning to code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-getting-started.html"><a href="1-getting-started.html#packages"><i class="fa fa-check"></i><b>1.3</b> What are R packages?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-getting-started.html"><a href="1-getting-started.html#package-installation"><i class="fa fa-check"></i><b>1.3.1</b> Package installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-getting-started.html"><a href="1-getting-started.html#package-loading"><i class="fa fa-check"></i><b>1.3.2</b> Package loading</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-getting-started.html"><a href="1-getting-started.html#package-use"><i class="fa fa-check"></i><b>1.3.3</b> Package use</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13"><i class="fa fa-check"></i><b>1.4</b> Explore your first datasets</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13-package"><i class="fa fa-check"></i><b>1.4.1</b> <code>nycflights13</code> package</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-getting-started.html"><a href="1-getting-started.html#flights-data-frame"><i class="fa fa-check"></i><b>1.4.2</b> <code>flights</code> data frame</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-getting-started.html"><a href="1-getting-started.html#exploredataframes"><i class="fa fa-check"></i><b>1.4.3</b> Exploring data frames</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-getting-started.html"><a href="1-getting-started.html#identification-vs-measurement-variables"><i class="fa fa-check"></i><b>1.4.4</b> Identification and measurement variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-getting-started.html"><a href="1-getting-started.html#help-files"><i class="fa fa-check"></i><b>1.4.5</b> Help files</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-getting-started.html"><a href="1-getting-started.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-getting-started.html"><a href="1-getting-started.html#additional-resources"><i class="fa fa-check"></i><b>1.5.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-viz.html"><a href="2-viz.html"><i class="fa fa-check"></i><b>2</b> Visualization</a><ul>
<li class="chapter" data-level="" data-path="2-viz.html"><a href="2-viz.html#needed-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="2.1" data-path="2-viz.html"><a href="2-viz.html#grammarofgraphics"><i class="fa fa-check"></i><b>2.1</b> The grammar of graphics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-viz.html"><a href="2-viz.html#components-of-the-grammar"><i class="fa fa-check"></i><b>2.1.1</b> Components of the grammar</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-viz.html"><a href="2-viz.html#gapminder"><i class="fa fa-check"></i><b>2.1.2</b> Gapminder data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-viz.html"><a href="2-viz.html#other-components"><i class="fa fa-check"></i><b>2.1.3</b> Other components</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-viz.html"><a href="2-viz.html#ggplot2-package"><i class="fa fa-check"></i><b>2.1.4</b> ggplot2 package</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-viz.html"><a href="2-viz.html#scatterplots"><i class="fa fa-check"></i><b>2.2</b> Scatterplots</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-viz.html"><a href="2-viz.html#geompoint"><i class="fa fa-check"></i><b>2.2.1</b> Scatterplots via <code>geom_point</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="2-viz.html"><a href="2-viz.html#overplotting"><i class="fa fa-check"></i><b>2.2.2</b> Overplotting</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-viz.html"><a href="2-viz.html#summary"><i class="fa fa-check"></i><b>2.2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-viz.html"><a href="2-viz.html#linegraphs"><i class="fa fa-check"></i><b>2.3</b> Linegraphs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-viz.html"><a href="2-viz.html#geomline"><i class="fa fa-check"></i><b>2.3.1</b> Linegraphs via <code>geom_line</code></a></li>
<li class="chapter" data-level="2.3.2" data-path="2-viz.html"><a href="2-viz.html#summary-1"><i class="fa fa-check"></i><b>2.3.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-viz.html"><a href="2-viz.html#histograms"><i class="fa fa-check"></i><b>2.4</b> Histograms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-viz.html"><a href="2-viz.html#geomhistogram"><i class="fa fa-check"></i><b>2.4.1</b> Histograms via <code>geom_histogram</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="2-viz.html"><a href="2-viz.html#adjustbins"><i class="fa fa-check"></i><b>2.4.2</b> Adjusting the bins</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-viz.html"><a href="2-viz.html#summary-2"><i class="fa fa-check"></i><b>2.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-viz.html"><a href="2-viz.html#facets"><i class="fa fa-check"></i><b>2.5</b> Facets</a></li>
<li class="chapter" data-level="2.6" data-path="2-viz.html"><a href="2-viz.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-viz.html"><a href="2-viz.html#geomboxplot"><i class="fa fa-check"></i><b>2.6.1</b> Boxplots via <code>geom_boxplot</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="2-viz.html"><a href="2-viz.html#summary-3"><i class="fa fa-check"></i><b>2.6.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-viz.html"><a href="2-viz.html#geombar"><i class="fa fa-check"></i><b>2.7</b> Barplots</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-viz.html"><a href="2-viz.html#barplots-via-geom_bar-or-geom_col"><i class="fa fa-check"></i><b>2.7.1</b> Barplots via <code>geom_bar</code> or <code>geom_col</code></a></li>
<li class="chapter" data-level="2.7.2" data-path="2-viz.html"><a href="2-viz.html#must-avoid-pie-charts"><i class="fa fa-check"></i><b>2.7.2</b> Must avoid pie charts!</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-viz.html"><a href="2-viz.html#two-categ-barplot"><i class="fa fa-check"></i><b>2.7.3</b> Two categorical variables</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-viz.html"><a href="2-viz.html#summary-4"><i class="fa fa-check"></i><b>2.7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="2-viz.html"><a href="2-viz.html#conclusion-1"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a><ul>
<li class="chapter" data-level="2.8.1" data-path="2-viz.html"><a href="2-viz.html#summary-table"><i class="fa fa-check"></i><b>2.8.1</b> Summary table</a></li>
<li class="chapter" data-level="2.8.2" data-path="2-viz.html"><a href="2-viz.html#function-argument-specification"><i class="fa fa-check"></i><b>2.8.2</b> Function argument specification</a></li>
<li class="chapter" data-level="2.8.3" data-path="2-viz.html"><a href="2-viz.html#additional-resources-1"><i class="fa fa-check"></i><b>2.8.3</b> Additional resources</a></li>
<li class="chapter" data-level="2.8.4" data-path="2-viz.html"><a href="2-viz.html#whats-to-come-3"><i class="fa fa-check"></i><b>2.8.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-productivity.html"><a href="3-productivity.html"><i class="fa fa-check"></i><b>3</b> Productivity</a><ul>
<li class="chapter" data-level="3.1" data-path="3-productivity.html"><a href="3-productivity.html#set-up"><i class="fa fa-check"></i><b>3.1</b> Set Up</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-mac"><i class="fa fa-check"></i><b>3.1.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>3.1.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>3.1.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-windows"><i class="fa fa-check"></i><b>3.1.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-productivity.html"><a href="3-productivity.html#unix"><i class="fa fa-check"></i><b>3.2</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-productivity.html"><a href="3-productivity.html#naming-convention"><i class="fa fa-check"></i><b>3.2.1</b> Naming convention</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-productivity.html"><a href="3-productivity.html#the-terminal"><i class="fa fa-check"></i><b>3.2.2</b> The terminal</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-productivity.html"><a href="3-productivity.html#filesystem"><i class="fa fa-check"></i><b>3.2.3</b> The filesystem</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-productivity.html"><a href="3-productivity.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>3.2.4</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-productivity.html"><a href="3-productivity.html#the-home-directory"><i class="fa fa-check"></i><b>3.2.5</b> The home directory</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-productivity.html"><a href="3-productivity.html#working-directory"><i class="fa fa-check"></i><b>3.2.6</b> Working directory</a></li>
<li class="chapter" data-level="3.2.7" data-path="3-productivity.html"><a href="3-productivity.html#paths"><i class="fa fa-check"></i><b>3.2.7</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-productivity.html"><a href="3-productivity.html#unix-commands"><i class="fa fa-check"></i><b>3.3</b> Unix commands</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-productivity.html"><a href="3-productivity.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>3.3.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-productivity.html"><a href="3-productivity.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>3.3.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-productivity.html"><a href="3-productivity.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>3.3.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-productivity.html"><a href="3-productivity.html#some-examples"><i class="fa fa-check"></i><b>3.3.4</b> Some examples</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-productivity.html"><a href="3-productivity.html#more-unix-commands"><i class="fa fa-check"></i><b>3.3.5</b> More Unix commands</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-productivity.html"><a href="3-productivity.html#advanced-unix"><i class="fa fa-check"></i><b>3.3.6</b> Advanced Unix</a></li>
<li class="chapter" data-level="3.3.7" data-path="3-productivity.html"><a href="3-productivity.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>3.3.7</b> File manipulation in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-productivity.html"><a href="3-productivity.html#git"><i class="fa fa-check"></i><b>3.4</b> Git and GitHub</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-productivity.html"><a href="3-productivity.html#github-accounts"><i class="fa fa-check"></i><b>3.4.1</b> GitHub accounts</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-productivity.html"><a href="3-productivity.html#github-repos"><i class="fa fa-check"></i><b>3.4.2</b> GitHub repositories</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-productivity.html"><a href="3-productivity.html#git-overview"><i class="fa fa-check"></i><b>3.4.3</b> Overview of Git</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-git"><i class="fa fa-check"></i><b>3.4.4</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-productivity.html"><a href="3-productivity.html#r"><i class="fa fa-check"></i><b>3.5</b> R</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-projects"><i class="fa fa-check"></i><b>3.5.1</b> RStudio projects</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-productivity.html"><a href="3-productivity.html#r-markdown"><i class="fa fa-check"></i><b>3.5.2</b> R markdown</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-productivity.html"><a href="3-productivity.html#help-for-r"><i class="fa fa-check"></i><b>3.5.3</b> Help for R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-wrangling.html"><a href="4-wrangling.html"><i class="fa fa-check"></i><b>4</b> Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#piping"><i class="fa fa-check"></i><b>4.1</b> The pipe operator: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.2" data-path="4-wrangling.html"><a href="4-wrangling.html#filter"><i class="fa fa-check"></i><b>4.2</b> <code>filter</code> rows</a></li>
<li class="chapter" data-level="4.3" data-path="4-wrangling.html"><a href="4-wrangling.html#summarize"><i class="fa fa-check"></i><b>4.3</b> <code>summarize</code> variables</a></li>
<li class="chapter" data-level="4.4" data-path="4-wrangling.html"><a href="4-wrangling.html#groupby"><i class="fa fa-check"></i><b>4.4</b> <code>group_by</code> rows</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#grouping-by-more-than-one-variable"><i class="fa fa-check"></i><b>4.4.1</b> Grouping by more than one variable</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-wrangling.html"><a href="4-wrangling.html#mutate"><i class="fa fa-check"></i><b>4.5</b> <code>mutate</code> existing variables</a></li>
<li class="chapter" data-level="4.6" data-path="4-wrangling.html"><a href="4-wrangling.html#arrange"><i class="fa fa-check"></i><b>4.6</b> <code>arrange</code> and sort rows</a></li>
<li class="chapter" data-level="4.7" data-path="4-wrangling.html"><a href="4-wrangling.html#factors"><i class="fa fa-check"></i><b>4.7</b> Factors</a><ul>
<li class="chapter" data-level="4.7.1" data-path="4-wrangling.html"><a href="4-wrangling.html#the-forcats-package"><i class="fa fa-check"></i><b>4.7.1</b> The <strong>forcats</strong> package</a></li>
<li class="chapter" data-level="4.7.2" data-path="4-wrangling.html"><a href="4-wrangling.html#dropping-unused-levels"><i class="fa fa-check"></i><b>4.7.2</b> Dropping unused levels</a></li>
<li class="chapter" data-level="4.7.3" data-path="4-wrangling.html"><a href="4-wrangling.html#reorder-factors"><i class="fa fa-check"></i><b>4.7.3</b> Change order of the levels, principled</a></li>
<li class="chapter" data-level="4.7.4" data-path="4-wrangling.html"><a href="4-wrangling.html#change-order-of-the-levels-because-i-said-so"><i class="fa fa-check"></i><b>4.7.4</b> Change order of the levels, “because I said so”</a></li>
<li class="chapter" data-level="4.7.5" data-path="4-wrangling.html"><a href="4-wrangling.html#recode-the-levels"><i class="fa fa-check"></i><b>4.7.5</b> Recode the levels</a></li>
<li class="chapter" data-level="4.7.6" data-path="4-wrangling.html"><a href="4-wrangling.html#grow-a-factor"><i class="fa fa-check"></i><b>4.7.6</b> Grow a factor</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors"><i class="fa fa-check"></i><b>4.8</b> Character Vectors</a><ul>
<li class="chapter" data-level="4.8.1" data-path="4-wrangling.html"><a href="4-wrangling.html#manipulating-character-vectors"><i class="fa fa-check"></i><b>4.8.1</b> Manipulating character vectors</a></li>
<li class="chapter" data-level="4.8.2" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-resources"><i class="fa fa-check"></i><b>4.8.2</b> Regular expressions resources</a></li>
<li class="chapter" data-level="4.8.3" data-path="4-wrangling.html"><a href="4-wrangling.html#character-encoding-resources"><i class="fa fa-check"></i><b>4.8.3</b> Character encoding resources</a></li>
<li class="chapter" data-level="4.8.4" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors-that-live-in-a-data-frame"><i class="fa fa-check"></i><b>4.8.4</b> Character vectors that live in a data frame</a></li>
<li class="chapter" data-level="4.8.5" data-path="4-wrangling.html"><a href="4-wrangling.html#regex-free-string-manipulation-with-stringr-and-tidyr"><i class="fa fa-check"></i><b>4.8.5</b> Regex-free string manipulation with stringr and tidyr</a></li>
<li class="chapter" data-level="4.8.6" data-path="4-wrangling.html"><a href="4-wrangling.html#detect-or-filter-on-a-target-string"><i class="fa fa-check"></i><b>4.8.6</b> Detect or filter on a target string</a></li>
<li class="chapter" data-level="4.8.7" data-path="4-wrangling.html"><a href="4-wrangling.html#string-splitting-by-delimiter"><i class="fa fa-check"></i><b>4.8.7</b> String splitting by delimiter</a></li>
<li class="chapter" data-level="4.8.8" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-extraction-and-replacement-by-position"><i class="fa fa-check"></i><b>4.8.8</b> Substring extraction (and replacement) by position</a></li>
<li class="chapter" data-level="4.8.9" data-path="4-wrangling.html"><a href="4-wrangling.html#collapse-a-vector"><i class="fa fa-check"></i><b>4.8.9</b> Collapse a vector</a></li>
<li class="chapter" data-level="4.8.10" data-path="4-wrangling.html"><a href="4-wrangling.html#catenate-vectors"><i class="fa fa-check"></i><b>4.8.10</b> Create a character vector by catenating multiple vectors</a></li>
<li class="chapter" data-level="4.8.11" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-replacement"><i class="fa fa-check"></i><b>4.8.11</b> Substring replacement</a></li>
<li class="chapter" data-level="4.8.12" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-with-stringr"><i class="fa fa-check"></i><b>4.8.12</b> Regular expressions with stringr</a></li>
<li class="chapter" data-level="4.8.13" data-path="4-wrangling.html"><a href="4-wrangling.html#characters-with-special-meaning"><i class="fa fa-check"></i><b>4.8.13</b> Characters with special meaning</a></li>
<li class="chapter" data-level="4.8.14" data-path="4-wrangling.html"><a href="4-wrangling.html#character-classes"><i class="fa fa-check"></i><b>4.8.14</b> Character classes</a></li>
<li class="chapter" data-level="4.8.15" data-path="4-wrangling.html"><a href="4-wrangling.html#quantifiers"><i class="fa fa-check"></i><b>4.8.15</b> Quantifiers</a></li>
<li class="chapter" data-level="4.8.16" data-path="4-wrangling.html"><a href="4-wrangling.html#escaping"><i class="fa fa-check"></i><b>4.8.16</b> Escaping</a></li>
<li class="chapter" data-level="4.8.17" data-path="4-wrangling.html"><a href="4-wrangling.html#groups-and-backreferences"><i class="fa fa-check"></i><b>4.8.17</b> Groups and backreferences</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="4-wrangling.html"><a href="4-wrangling.html#combining-data"><i class="fa fa-check"></i><b>4.9</b> Combining Data</a><ul>
<li class="chapter" data-level="4.9.1" data-path="4-wrangling.html"><a href="4-wrangling.html#bind"><i class="fa fa-check"></i><b>4.9.1</b> Bind</a></li>
<li class="chapter" data-level="4.9.2" data-path="4-wrangling.html"><a href="4-wrangling.html#joins-in-dplyr"><i class="fa fa-check"></i><b>4.9.2</b> Joins in dplyr</a></li>
<li class="chapter" data-level="4.9.3" data-path="4-wrangling.html"><a href="4-wrangling.html#joining"><i class="fa fa-check"></i><b>4.9.3</b> Joining</a></li>
<li class="chapter" data-level="4.9.4" data-path="4-wrangling.html"><a href="4-wrangling.html#matching-key-variable-names"><i class="fa fa-check"></i><b>4.9.4</b> Matching “key” variable names</a></li>
<li class="chapter" data-level="4.9.5" data-path="4-wrangling.html"><a href="4-wrangling.html#diff-key"><i class="fa fa-check"></i><b>4.9.5</b> Different “key” variable names</a></li>
<li class="chapter" data-level="4.9.6" data-path="4-wrangling.html"><a href="4-wrangling.html#multiple-key-variables"><i class="fa fa-check"></i><b>4.9.6</b> Multiple “key” variables</a></li>
<li class="chapter" data-level="4.9.7" data-path="4-wrangling.html"><a href="4-wrangling.html#normal-forms"><i class="fa fa-check"></i><b>4.9.7</b> Normal forms</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="4-wrangling.html"><a href="4-wrangling.html#other-verbs"><i class="fa fa-check"></i><b>4.10</b> Other Verbs</a><ul>
<li class="chapter" data-level="4.10.1" data-path="4-wrangling.html"><a href="4-wrangling.html#select"><i class="fa fa-check"></i><b>4.10.1</b> <code>select</code> variables</a></li>
<li class="chapter" data-level="4.10.2" data-path="4-wrangling.html"><a href="4-wrangling.html#rename"><i class="fa fa-check"></i><b>4.10.2</b> <code>rename</code> variables</a></li>
<li class="chapter" data-level="4.10.3" data-path="4-wrangling.html"><a href="4-wrangling.html#top_n-values-of-a-variable"><i class="fa fa-check"></i><b>4.10.3</b> <code>top_n</code> values of a variable</a></li>
<li class="chapter" data-level="4.10.4" data-path="4-wrangling.html"><a href="4-wrangling.html#slice-and-pull-and"><i class="fa fa-check"></i><b>4.10.4</b> <code>slice</code> and <code>pull</code> and <code>[]</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="4-wrangling.html"><a href="4-wrangling.html#conclusion-2"><i class="fa fa-check"></i><b>4.11</b> Conclusion</a><ul>
<li class="chapter" data-level="4.11.1" data-path="4-wrangling.html"><a href="4-wrangling.html#summary-table-1"><i class="fa fa-check"></i><b>4.11.1</b> Summary table</a></li>
<li class="chapter" data-level="4.11.2" data-path="4-wrangling.html"><a href="4-wrangling.html#additional-resources-2"><i class="fa fa-check"></i><b>4.11.2</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-tidy.html"><a href="5-tidy.html"><i class="fa fa-check"></i><b>5</b> Tidy</a><ul>
<li class="chapter" data-level="5.1" data-path="5-tidy.html"><a href="5-tidy.html#csv"><i class="fa fa-check"></i><b>5.1</b> Importing data</a></li>
<li class="chapter" data-level="5.2" data-path="5-tidy.html"><a href="5-tidy.html#web-scraping"><i class="fa fa-check"></i><b>5.2</b> Web scraping</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-tidy.html"><a href="5-tidy.html#html"><i class="fa fa-check"></i><b>5.2.1</b> HTML</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-tidy.html"><a href="5-tidy.html#the-rvest-package"><i class="fa fa-check"></i><b>5.2.2</b> The rvest package</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-tidy.html"><a href="5-tidy.html#css-selectors"><i class="fa fa-check"></i><b>5.2.3</b> CSS selectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-tidy.html"><a href="5-tidy.html#json"><i class="fa fa-check"></i><b>5.2.4</b> JSON</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-tidy.html"><a href="5-tidy.html#tidy-data-ex"><i class="fa fa-check"></i><b>5.3</b> “Tidy” data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-tidy.html"><a href="5-tidy.html#tidy-definition"><i class="fa fa-check"></i><b>5.3.1</b> Definition of “tidy” data</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-tidy.html"><a href="5-tidy.html#converting-to-tidy-data"><i class="fa fa-check"></i><b>5.3.2</b> Converting to “tidy” data</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-tidy.html"><a href="5-tidy.html#nycflights13-package-1"><i class="fa fa-check"></i><b>5.3.3</b> <code>nycflights13</code> package</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-tidy.html"><a href="5-tidy.html#case-study-tidy"><i class="fa fa-check"></i><b>5.4</b> Case study: Democracy in Guatemala</a></li>
<li class="chapter" data-level="5.5" data-path="5-tidy.html"><a href="5-tidy.html#tidyverse-package"><i class="fa fa-check"></i><b>5.5</b> <code>tidyverse</code> package</a></li>
<li class="chapter" data-level="5.6" data-path="5-tidy.html"><a href="5-tidy.html#conclusion-3"><i class="fa fa-check"></i><b>5.6</b> Conclusion</a><ul>
<li class="chapter" data-level="5.6.1" data-path="5-tidy.html"><a href="5-tidy.html#additional-resources-3"><i class="fa fa-check"></i><b>5.6.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-functions.html"><a href="6-functions.html"><i class="fa fa-check"></i><b>6</b> Functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-functions.html"><a href="6-functions.html#part-1"><i class="fa fa-check"></i><b>6.1</b> Part 1</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works"><i class="fa fa-check"></i><b>6.1.1</b> Get something that works</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-functions.html"><a href="6-functions.html#skateboard-perfectly-formed-rear-view-mirror"><i class="fa fa-check"></i><b>6.1.2</b> Skateboard &gt;&gt; perfectly formed rear-view mirror</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function"><i class="fa fa-check"></i><b>6.1.3</b> Turn the working interactive code into a function</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-functions.html"><a href="6-functions.html#test-your-function"><i class="fa fa-check"></i><b>6.1.4</b> Test your function</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-functions.html"><a href="6-functions.html#check-the-validity-of-arguments"><i class="fa fa-check"></i><b>6.1.5</b> Check the validity of arguments</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next"><i class="fa fa-check"></i><b>6.1.6</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-functions.html"><a href="6-functions.html#part-2"><i class="fa fa-check"></i><b>6.2</b> Part 2</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-functions.html"><a href="6-functions.html#load-the-gapminder-data"><i class="fa fa-check"></i><b>6.2.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function"><i class="fa fa-check"></i><b>6.2.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-functions.html"><a href="6-functions.html#generalize-our-function-to-other-quantiles"><i class="fa fa-check"></i><b>6.2.3</b> Generalize our function to other quantiles</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works-again"><i class="fa fa-check"></i><b>6.2.4</b> Get something that works, again</a></li>
<li class="chapter" data-level="6.2.5" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function-again"><i class="fa fa-check"></i><b>6.2.5</b> Turn the working interactive code into a function, again</a></li>
<li class="chapter" data-level="6.2.6" data-path="6-functions.html"><a href="6-functions.html#argument-names-freedom-and-conventions"><i class="fa fa-check"></i><b>6.2.6</b> Argument names: freedom and conventions</a></li>
<li class="chapter" data-level="6.2.7" data-path="6-functions.html"><a href="6-functions.html#what-a-function-returns"><i class="fa fa-check"></i><b>6.2.7</b> What a function returns</a></li>
<li class="chapter" data-level="6.2.8" data-path="6-functions.html"><a href="6-functions.html#default-values-freedom-to-not-specify-the-arguments"><i class="fa fa-check"></i><b>6.2.8</b> Default values: freedom to NOT specify the arguments</a></li>
<li class="chapter" data-level="6.2.9" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next-1"><i class="fa fa-check"></i><b>6.2.9</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-functions.html"><a href="6-functions.html#part-3"><i class="fa fa-check"></i><b>6.3</b> Part 3</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-functions.html"><a href="6-functions.html#load-the-gapminder-data-1"><i class="fa fa-check"></i><b>6.3.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function-1"><i class="fa fa-check"></i><b>6.3.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-functions.html"><a href="6-functions.html#be-proactive-about-nas"><i class="fa fa-check"></i><b>6.3.3</b> Be proactive about <code>NA</code>s</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-functions.html"><a href="6-functions.html#the-useful-but-mysterious-...-argument"><i class="fa fa-check"></i><b>6.3.4</b> The useful but mysterious <code>...</code> argument</a></li>
<li class="chapter" data-level="6.3.5" data-path="6-functions.html"><a href="6-functions.html#use-testthat-for-formal-unit-tests"><i class="fa fa-check"></i><b>6.3.5</b> Use testthat for formal unit tests</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-functions.html"><a href="6-functions.html#list-columns-and-map_-functions"><i class="fa fa-check"></i><b>6.4</b> List columns and <code>map_*</code> functions</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-functions.html"><a href="6-functions.html#what-are-list-columns"><i class="fa fa-check"></i><b>6.4.1</b> What are list columns?</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-functions.html"><a href="6-functions.html#creating-list-columns-with-mutate"><i class="fa fa-check"></i><b>6.4.2</b> Creating list columns with <code>mutate()</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="6-functions.html"><a href="6-functions.html#map_-functions"><i class="fa fa-check"></i><b>6.4.3</b> <code>map_*</code> functions</a></li>
<li class="chapter" data-level="6.4.4" data-path="6-functions.html"><a href="6-functions.html#using-map_-functions-to-create-list-columns"><i class="fa fa-check"></i><b>6.4.4</b> Using <code>map_*</code> functions to create list columns</a></li>
<li class="chapter" data-level="6.4.5" data-path="6-functions.html"><a href="6-functions.html#practice-with-map_-functions-and-list-columns"><i class="fa fa-check"></i><b>6.4.5</b> Practice with <code>map_*</code> functions and list columns</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probability.html"><a href="7-probability.html"><i class="fa fa-check"></i><b>7</b> Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probability.html"><a href="7-probability.html#basicsOfProbability"><i class="fa fa-check"></i><b>7.1</b> Defining probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-probability.html"><a href="7-probability.html#intro-questions"><i class="fa fa-check"></i><b>7.1.1</b> Intro Questions</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-probability.html"><a href="7-probability.html#probability-1"><i class="fa fa-check"></i><b>7.1.2</b> Probability</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-probability.html"><a href="7-probability.html#disjoint-or-mutually-exclusive-outcomes"><i class="fa fa-check"></i><b>7.1.3</b> Disjoint or mutually exclusive outcomes</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-probability.html"><a href="7-probability.html#probabilities-when-events-are-not-disjoint"><i class="fa fa-check"></i><b>7.1.4</b> Probabilities when events are not disjoint</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-probability.html"><a href="7-probability.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.5</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.6" data-path="7-probability.html"><a href="7-probability.html#complement-of-an-event"><i class="fa fa-check"></i><b>7.1.6</b> Complement of an event</a></li>
<li class="chapter" data-level="7.1.7" data-path="7-probability.html"><a href="7-probability.html#probabilityIndependence"><i class="fa fa-check"></i><b>7.1.7</b> Independence</a></li>
<li class="chapter" data-level="7.1.8" data-path="7-probability.html"><a href="7-probability.html#conditionalProbabilitySection"><i class="fa fa-check"></i><b>7.1.8</b> Conditional probability</a></li>
<li class="chapter" data-level="7.1.9" data-path="7-probability.html"><a href="7-probability.html#marginalAndJointProbabilities"><i class="fa fa-check"></i><b>7.1.9</b> Marginal and joint probabilities</a></li>
<li class="chapter" data-level="7.1.10" data-path="7-probability.html"><a href="7-probability.html#defining-conditional-probability"><i class="fa fa-check"></i><b>7.1.10</b> Defining conditional probability</a></li>
<li class="chapter" data-level="7.1.11" data-path="7-probability.html"><a href="7-probability.html#smallpox-in-boston-1721"><i class="fa fa-check"></i><b>7.1.11</b> Smallpox in Boston, 1721</a></li>
<li class="chapter" data-level="7.1.12" data-path="7-probability.html"><a href="7-probability.html#general-multiplication-rule"><i class="fa fa-check"></i><b>7.1.12</b> General multiplication rule</a></li>
<li class="chapter" data-level="7.1.13" data-path="7-probability.html"><a href="7-probability.html#tree-diagrams"><i class="fa fa-check"></i><b>7.1.13</b> Tree diagrams</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-probability.html"><a href="7-probability.html#randomVariablesSection"><i class="fa fa-check"></i><b>7.2</b> Random variables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-probability.html"><a href="7-probability.html#expectation"><i class="fa fa-check"></i><b>7.2.1</b> Expectation</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-probability.html"><a href="7-probability.html#variability-in-random-variables"><i class="fa fa-check"></i><b>7.2.2</b> Variability in random variables</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-probability.html"><a href="7-probability.html#linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.2.3</b> Linear combinations of random variables</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-probability.html"><a href="7-probability.html#variability-in-linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.2.4</b> Variability in linear combinations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-probability.html"><a href="7-probability.html#appendixA"><i class="fa fa-check"></i><b>7.3</b> Statistical Background</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-probability.html"><a href="7-probability.html#appendix-stat-terms"><i class="fa fa-check"></i><b>7.3.1</b> Basic statistical terms</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-probability.html"><a href="7-probability.html#appendix-normal-curve"><i class="fa fa-check"></i><b>7.3.2</b> Normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html"><i class="fa fa-check"></i><b>8</b> Bayes’s Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conditional-probability"><i class="fa fa-check"></i><b>8.1</b> Conditional probability</a></li>
<li class="chapter" data-level="8.2" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conjoint-probability"><i class="fa fa-check"></i><b>8.2</b> Conjoint probability</a></li>
<li class="chapter" data-level="8.3" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-cookie-problem"><i class="fa fa-check"></i><b>8.3</b> The cookie problem</a></li>
<li class="chapter" data-level="8.4" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#bayess-theorem-1"><i class="fa fa-check"></i><b>8.4</b> Bayes’s Theorem</a></li>
<li class="chapter" data-level="8.5" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-diachronic-interpretation"><i class="fa fa-check"></i><b>8.5</b> The diachronic interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-mm-problem"><i class="fa fa-check"></i><b>8.6</b> The M&amp;M problem</a></li>
<li class="chapter" data-level="8.7" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>8.7</b> The Monty Hall problem</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-sampling.html"><a href="9-sampling.html"><i class="fa fa-check"></i><b>9</b> Sampling</a><ul>
<li class="chapter" data-level="" data-path="9-sampling.html"><a href="9-sampling.html#needed-packages-1"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="9.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-activity"><i class="fa fa-check"></i><b>9.1</b> Sampling bowl activity</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-sampling.html"><a href="9-sampling.html#what-proportion-of-this-bowls-balls-are-red"><i class="fa fa-check"></i><b>9.1.1</b> What proportion of this bowl’s balls are red?</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-shovel-once"><i class="fa fa-check"></i><b>9.1.2</b> Using the shovel once</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-sampling.html"><a href="9-sampling.html#student-shovels"><i class="fa fa-check"></i><b>9.1.3</b> Using the shovel 33 times</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-sampling.html"><a href="9-sampling.html#what-did-we-just-do"><i class="fa fa-check"></i><b>9.1.4</b> What did we just do?</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-simulation"><i class="fa fa-check"></i><b>9.2</b> Virtual sampling</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-once"><i class="fa fa-check"></i><b>9.2.1</b> Using the virtual shovel once</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-33-times"><i class="fa fa-check"></i><b>9.2.2</b> Using the virtual shovel 33 times</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-sampling.html"><a href="9-sampling.html#shovel-1000-times"><i class="fa fa-check"></i><b>9.2.3</b> Using the virtual shovel 1,000 times</a></li>
<li class="chapter" data-level="9.2.4" data-path="9-sampling.html"><a href="9-sampling.html#different-shovels"><i class="fa fa-check"></i><b>9.2.4</b> Using different shovels</a></li>
<li class="chapter" data-level="9.2.5" data-path="9-sampling.html"><a href="9-sampling.html#using-many-shovels-at-once"><i class="fa fa-check"></i><b>9.2.5</b> Using many shovels at once</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-sampling.html"><a href="9-sampling.html#sampling-framework"><i class="fa fa-check"></i><b>9.3</b> Sampling framework</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-sampling.html"><a href="9-sampling.html#terminology-and-notation"><i class="fa fa-check"></i><b>9.3.1</b> Terminology and notation</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-definitions"><i class="fa fa-check"></i><b>9.3.2</b> Statistical definitions</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-sampling.html"><a href="9-sampling.html#moral-of-the-story"><i class="fa fa-check"></i><b>9.3.3</b> The moral of the story</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-sampling.html"><a href="9-sampling.html#sampling-case-study"><i class="fa fa-check"></i><b>9.4</b> Case study: Polls</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-conclusion-central-limit-theorem"><i class="fa fa-check"></i><b>9.4.1</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9-sampling.html"><a href="9-sampling.html#conclusion-4"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html"><i class="fa fa-check"></i><b>10</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#needed-packages-2"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="10.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-tactile"><i class="fa fa-check"></i><b>10.1</b> Pennies activity</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-is-the-average-year-on-us-pennies-in-2019"><i class="fa fa-check"></i><b>10.1.1</b> What is the average year on US pennies in 2019?</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-once"><i class="fa fa-check"></i><b>10.1.2</b> Resampling once</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#student-resamples"><i class="fa fa-check"></i><b>10.1.3</b> Resampling 35 times</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-did-we-just-do-1"><i class="fa fa-check"></i><b>10.1.4</b> What did we just do?</a></li>
<li class="chapter" data-level="10.1.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-simulation"><i class="fa fa-check"></i><b>10.1.5</b> Virtually resampling once</a></li>
<li class="chapter" data-level="10.1.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-35-replicates"><i class="fa fa-check"></i><b>10.1.6</b> Virtually resampling 35 times</a></li>
<li class="chapter" data-level="10.1.7" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-1000-replicates"><i class="fa fa-check"></i><b>10.1.7</b> Virtually resampling 1,000 times</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build-up"><i class="fa fa-check"></i><b>10.2</b> Measuring uncertainty with confidence intervals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#percentile-method"><i class="fa fa-check"></i><b>10.2.1</b> Percentile method</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#se-method"><i class="fa fa-check"></i><b>10.2.2</b> Standard error method</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#one-prop-ci"><i class="fa fa-check"></i><b>10.2.3</b> Interpreting confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-width"><i class="fa fa-check"></i><b>10.3</b> Width of confidence intervals</a><ul>
<li class="chapter" data-level="10.3.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-confidence-level"><i class="fa fa-check"></i><b>10.3.1</b> Impact of confidence level</a></li>
<li class="chapter" data-level="10.3.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#fitting-multiple-models-using-map"><i class="fa fa-check"></i><b>10.3.2</b> Fitting multiple models using <code>map()</code></a></li>
<li class="chapter" data-level="10.3.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-sample-size"><i class="fa fa-check"></i><b>10.3.3</b> Impact of sample size</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut"><i class="fa fa-check"></i><b>10.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="10.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#case-study-two-prop-ci"><i class="fa fa-check"></i><b>10.5</b> Case study: Is yawning contagious?</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#mythbusters-study-data"><i class="fa fa-check"></i><b>10.5.1</b> <em>Mythbusters</em> study data</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#sampling-scenario"><i class="fa fa-check"></i><b>10.5.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="10.5.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build"><i class="fa fa-check"></i><b>10.5.3</b> Constructing the confidence interval</a></li>
<li class="chapter" data-level="10.5.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut-1"><i class="fa fa-check"></i><b>10.5.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-conclusion"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a><ul>
<li class="chapter" data-level="10.6.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-vs-sampling"><i class="fa fa-check"></i><b>10.6.1</b> Comparing bootstrap and sampling distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-regression.html"><a href="11-regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="" data-path="11-regression.html"><a href="11-regression.html#needed-packages-3"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="11.1" data-path="11-regression.html"><a href="11-regression.html#model1"><i class="fa fa-check"></i><b>11.1</b> Teaching evaluations: one numerical explanatory variable</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-regression.html"><a href="11-regression.html#model1EDA"><i class="fa fa-check"></i><b>11.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-regression.html"><a href="11-regression.html#model1table"><i class="fa fa-check"></i><b>11.1.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="11.1.3" data-path="11-regression.html"><a href="11-regression.html#interpreting-regression-coefficients"><i class="fa fa-check"></i><b>11.1.3</b> Interpreting regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-regression.html"><a href="11-regression.html#uncertainty-in-simple-linear-regressions"><i class="fa fa-check"></i><b>11.2</b> Uncertainty in simple linear regressions</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-regression.html"><a href="11-regression.html#using-lm-and-tidy-as-a-shortcut-2"><i class="fa fa-check"></i><b>11.2.1</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-regression.html"><a href="11-regression.html#model1points"><i class="fa fa-check"></i><b>11.2.2</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-regression.html"><a href="11-regression.html#model2"><i class="fa fa-check"></i><b>11.3</b> Life expectancy: one categorical explanatory variable</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-regression.html"><a href="11-regression.html#model2EDA"><i class="fa fa-check"></i><b>11.3.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-regression.html"><a href="11-regression.html#model2table"><i class="fa fa-check"></i><b>11.3.2</b> Linear regression</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-regression.html"><a href="11-regression.html#model2points"><i class="fa fa-check"></i><b>11.3.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-regression.html"><a href="11-regression.html#case-study-2018-gubernatorial-forecasts"><i class="fa fa-check"></i><b>11.4</b> Case study: 2018 gubernatorial forecasts</a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-regression.html"><a href="11-regression.html#fitting-multiple-models-using-map-1"><i class="fa fa-check"></i><b>11.4.1</b> Fitting multiple models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11-regression.html"><a href="11-regression.html#leastsquares"><i class="fa fa-check"></i><b>11.5</b> Appendix: Best-fitting line</a></li>
<li class="chapter" data-level="11.6" data-path="11-regression.html"><a href="11-regression.html#conclusion-5"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a><ul>
<li class="chapter" data-level="11.6.1" data-path="11-regression.html"><a href="11-regression.html#additional-resources-basic-regression"><i class="fa fa-check"></i><b>11.6.1</b> Additional resources</a></li>
<li class="chapter" data-level="11.6.2" data-path="11-regression.html"><a href="11-regression.html#whats-to-come"><i class="fa fa-check"></i><b>11.6.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#needed-packages-4"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="12.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4"><i class="fa fa-check"></i><b>12.1</b> Teaching evaluations revisited: one numerical and one categorical explanatory variable</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4EDA"><i class="fa fa-check"></i><b>12.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4interactiontable"><i class="fa fa-check"></i><b>12.1.2</b> Interaction model</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#interpreting-regression-coefficients-with-interactions"><i class="fa fa-check"></i><b>12.1.3</b> Interpreting regression coefficients with interactions</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4table"><i class="fa fa-check"></i><b>12.1.4</b> Parallel slopes model</a></li>
<li class="chapter" data-level="12.1.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4points"><i class="fa fa-check"></i><b>12.1.5</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3"><i class="fa fa-check"></i><b>12.2</b> Credit card debt: two numerical explanatory variables</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3EDA"><i class="fa fa-check"></i><b>12.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3table"><i class="fa fa-check"></i><b>12.2.2</b> Regression plane</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3points"><i class="fa fa-check"></i><b>12.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#seattle-house-prices"><i class="fa fa-check"></i><b>12.3</b> Case study: Seattle house prices</a><ul>
<li class="chapter" data-level="12.3.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-I"><i class="fa fa-check"></i><b>12.3.1</b> Exploratory data analysis: Part I</a></li>
<li class="chapter" data-level="12.3.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-II"><i class="fa fa-check"></i><b>12.3.2</b> Exploratory data analysis: Part II</a></li>
<li class="chapter" data-level="12.3.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-regression"><i class="fa fa-check"></i><b>12.3.3</b> Regression modeling</a></li>
<li class="chapter" data-level="12.3.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-making-predictions"><i class="fa fa-check"></i><b>12.3.4</b> Making predictions</a></li>
<li class="chapter" data-level="12.3.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#fitting-many-models-using-map"><i class="fa fa-check"></i><b>12.3.5</b> Fitting many models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#data-journalism"><i class="fa fa-check"></i><b>12.4</b> Case studies: Effective data storytelling</a><ul>
<li class="chapter" data-level="12.4.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#bechdel-test-for-hollywood-gender-representation"><i class="fa fa-check"></i><b>12.4.1</b> Bechdel test for Hollywood gender representation</a></li>
<li class="chapter" data-level="12.4.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#us-births-in-1999"><i class="fa fa-check"></i><b>12.4.2</b> US Births in 1999</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#appendix-related-topics"><i class="fa fa-check"></i><b>12.5</b> Appendix: Related topics</a><ul>
<li class="chapter" data-level="12.5.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model-selection"><i class="fa fa-check"></i><b>12.5.1</b> Model selection</a></li>
<li class="chapter" data-level="12.5.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#correlationcoefficient2"><i class="fa fa-check"></i><b>12.5.2</b> Correlation coefficient</a></li>
<li class="chapter" data-level="12.5.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#simpsonsparadox"><i class="fa fa-check"></i><b>12.5.3</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-classification.html"><a href="13-classification.html"><i class="fa fa-check"></i><b>13</b> Classification</a><ul>
<li class="chapter" data-level="" data-path="13-classification.html"><a href="13-classification.html#needed-packages-5"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="13.1" data-path="13-classification.html"><a href="13-classification.html#logistic-regression"><i class="fa fa-check"></i><b>13.1</b> Logistic regression</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-classification.html"><a href="13-classification.html#what-is-logistic-regression"><i class="fa fa-check"></i><b>13.1.1</b> What is logistic regression?</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-classification.html"><a href="13-classification.html#house-elections-exploratory-data-analysis"><i class="fa fa-check"></i><b>13.1.2</b> House elections: exploratory data analysis</a></li>
<li class="chapter" data-level="13.1.3" data-path="13-classification.html"><a href="13-classification.html#one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.3</b> One categorical explanatory variable</a></li>
<li class="chapter" data-level="13.1.4" data-path="13-classification.html"><a href="13-classification.html#observedfitted-values-and-residuals"><i class="fa fa-check"></i><b>13.1.4</b> Observed/fitted values and residuals</a></li>
<li class="chapter" data-level="13.1.5" data-path="13-classification.html"><a href="13-classification.html#one-numerical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.5</b> One numerical explanatory variable</a></li>
<li class="chapter" data-level="13.1.6" data-path="13-classification.html"><a href="13-classification.html#one-numerical-and-one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.6</b> One numerical and one categorical explanatory variable</a></li>
<li class="chapter" data-level="13.1.7" data-path="13-classification.html"><a href="13-classification.html#fitting-many-models-using-map-1"><i class="fa fa-check"></i><b>13.1.7</b> Fitting many models using <code>map()</code></a></li>
<li class="chapter" data-level="13.1.8" data-path="13-classification.html"><a href="13-classification.html#linear-regression-vs.logistic-regression"><i class="fa fa-check"></i><b>13.1.8</b> Linear regression vs. logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-classification.html"><a href="13-classification.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>13.2</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="13.2.1" data-path="13-classification.html"><a href="13-classification.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>13.2.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="13.2.2" data-path="13-classification.html"><a href="13-classification.html#cart-motivation"><i class="fa fa-check"></i><b>13.2.2</b> CART motivation</a></li>
<li class="chapter" data-level="13.2.3" data-path="13-classification.html"><a href="13-classification.html#regression-trees"><i class="fa fa-check"></i><b>13.2.3</b> Regression trees</a></li>
<li class="chapter" data-level="13.2.4" data-path="13-classification.html"><a href="13-classification.html#classification-decision-trees"><i class="fa fa-check"></i><b>13.2.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13-classification.html"><a href="13-classification.html#random-forests"><i class="fa fa-check"></i><b>13.3</b> Random forests</a><ul>
<li class="chapter" data-level="13.3.1" data-path="13-classification.html"><a href="13-classification.html#references"><i class="fa fa-check"></i><b>13.3.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-machine.html"><a href="14-machine.html"><i class="fa fa-check"></i><b>14</b> Machine Learning</a><ul>
<li class="chapter" data-level="14.1" data-path="14-machine.html"><a href="14-machine.html#notation"><i class="fa fa-check"></i><b>14.1</b> Notation</a></li>
<li class="chapter" data-level="14.2" data-path="14-machine.html"><a href="14-machine.html#an-example"><i class="fa fa-check"></i><b>14.2</b> An example</a></li>
<li class="chapter" data-level="14.3" data-path="14-machine.html"><a href="14-machine.html#evaluation-metrics"><i class="fa fa-check"></i><b>14.3</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14-machine.html"><a href="14-machine.html#training-and-test-sets"><i class="fa fa-check"></i><b>14.3.1</b> Training and test sets</a></li>
<li class="chapter" data-level="14.3.2" data-path="14-machine.html"><a href="14-machine.html#overall-accuracy"><i class="fa fa-check"></i><b>14.3.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="14.3.3" data-path="14-machine.html"><a href="14-machine.html#the-confusion-matrix"><i class="fa fa-check"></i><b>14.3.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="14.3.4" data-path="14-machine.html"><a href="14-machine.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>14.3.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="14.3.5" data-path="14-machine.html"><a href="14-machine.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>14.3.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="14.3.6" data-path="14-machine.html"><a href="14-machine.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>14.3.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="14.3.7" data-path="14-machine.html"><a href="14-machine.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>14.3.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="14.3.8" data-path="14-machine.html"><a href="14-machine.html#loss-function"><i class="fa fa-check"></i><b>14.3.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14-machine.html"><a href="14-machine.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>14.4</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-machine.html"><a href="14-machine.html#conditional-probabilities"><i class="fa fa-check"></i><b>14.4.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-machine.html"><a href="14-machine.html#conditional-expectations"><i class="fa fa-check"></i><b>14.4.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="14.4.3" data-path="14-machine.html"><a href="14-machine.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>14.4.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-machine.html"><a href="14-machine.html#two-or-seven"><i class="fa fa-check"></i><b>14.5</b> Case study: is it a 2 or a 7?</a></li>
<li class="chapter" data-level="14.6" data-path="14-machine.html"><a href="14-machine.html#cross-validation"><i class="fa fa-check"></i><b>14.6</b> Cross validation</a></li>
<li class="chapter" data-level="14.7" data-path="14-machine.html"><a href="14-machine.html#knn-cv-intro"><i class="fa fa-check"></i><b>14.7</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="14.7.1" data-path="14-machine.html"><a href="14-machine.html#over-training"><i class="fa fa-check"></i><b>14.7.1</b> Over-training</a></li>
<li class="chapter" data-level="14.7.2" data-path="14-machine.html"><a href="14-machine.html#over-smoothing"><i class="fa fa-check"></i><b>14.7.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="14.7.3" data-path="14-machine.html"><a href="14-machine.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>14.7.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="14-machine.html"><a href="14-machine.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>14.8</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="14.9" data-path="14-machine.html"><a href="14-machine.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>14.9</b> K-fold cross validation</a></li>
<li class="chapter" data-level="14.10" data-path="14-machine.html"><a href="14-machine.html#bootstrap"><i class="fa fa-check"></i><b>14.10</b> Bootstrap</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html"><i class="fa fa-check"></i><b>A</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="A.1" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#causal-effects"><i class="fa fa-check"></i><b>A.1</b> Causal effects</a></li>
<li class="chapter" data-level="A.2" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#potential-outcomes"><i class="fa fa-check"></i><b>A.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="A.3" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#no-causation-without-manipulation"><i class="fa fa-check"></i><b>A.3</b> No causation without manipulation</a></li>
<li class="chapter" data-level="A.4" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#average-treatment-effect"><i class="fa fa-check"></i><b>A.4</b> Average treatment effect</a></li>
<li class="chapter" data-level="A.5" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#stable-unit-treatment-value-assumption-sutva"><i class="fa fa-check"></i><b>A.5</b> Stable unit treatment value assumption (SUTVA)</a></li>
<li class="chapter" data-level="A.6" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>A.6</b> The fundamental problem of causal inference</a></li>
<li class="chapter" data-level="A.7" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-assignment-mechanism"><i class="fa fa-check"></i><b>A.7</b> The assignment mechanism</a></li>
<li class="chapter" data-level="A.8" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#permutation-tests"><i class="fa fa-check"></i><b>A.8</b> Permutation tests</a></li>
<li class="chapter" data-level="A.9" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#confounding-and-selection-bias"><i class="fa fa-check"></i><b>A.9</b> Confounding and selection bias</a></li>
<li class="chapter" data-level="A.10" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#internal-and-external-validity"><i class="fa fa-check"></i><b>A.10</b> Internal and external validity</a></li>
<li class="chapter" data-level="A.11" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#survey-research-and-external-validity"><i class="fa fa-check"></i><b>A.11</b> Survey research and external validity</a></li>
<li class="chapter" data-level="A.12" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#conclusion-6"><i class="fa fa-check"></i><b>A.12</b> Conclusion</a></li>
<li class="chapter" data-level="A.13" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#references-1"><i class="fa fa-check"></i><b>A.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-maps.html"><a href="B-maps.html"><i class="fa fa-check"></i><b>B</b> Maps</a><ul>
<li class="chapter" data-level="B.1" data-path="B-maps.html"><a href="B-maps.html#tidycensus"><i class="fa fa-check"></i><b>B.1</b> Tidycensus</a></li>
<li class="chapter" data-level="B.2" data-path="B-maps.html"><a href="B-maps.html#conceptual-introduction-to-mapping"><i class="fa fa-check"></i><b>B.2</b> Conceptual introduction to mapping</a><ul>
<li class="chapter" data-level="B.2.1" data-path="B-maps.html"><a href="B-maps.html#vector-versus-spatial-data"><i class="fa fa-check"></i><b>B.2.1</b> Vector versus spatial data</a></li>
<li class="chapter" data-level="B.2.2" data-path="B-maps.html"><a href="B-maps.html#sf-vs-sp"><i class="fa fa-check"></i><b>B.2.2</b> <strong>sf</strong> vs <strong>sp</strong></a></li>
<li class="chapter" data-level="B.2.3" data-path="B-maps.html"><a href="B-maps.html#shapefiles"><i class="fa fa-check"></i><b>B.2.3</b> Shapefiles</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="B-maps.html"><a href="B-maps.html#mapping-with-tidycensus-and-geom_sf"><i class="fa fa-check"></i><b>B.3</b> Mapping with <strong>tidycensus</strong> and <code>geom_sf()</code></a><ul>
<li class="chapter" data-level="B.3.1" data-path="B-maps.html"><a href="B-maps.html#making-maps-pretty"><i class="fa fa-check"></i><b>B.3.1</b> Making maps pretty</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-maps.html"><a href="B-maps.html#adding-back-alaska-and-hawaii"><i class="fa fa-check"></i><b>B.3.2</b> Adding back Alaska and Hawaii</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-maps.html"><a href="B-maps.html#faceting-maps"><i class="fa fa-check"></i><b>B.4</b> Faceting maps</a><ul>
<li class="chapter" data-level="B.4.1" data-path="B-maps.html"><a href="B-maps.html#transforming-and-mapping-the-data"><i class="fa fa-check"></i><b>B.4.1</b> Transforming and mapping the data</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="B-maps.html"><a href="B-maps.html#want-to-explore-further"><i class="fa fa-check"></i><b>B.5</b> Want to explore further?</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-animation.html"><a href="C-animation.html"><i class="fa fa-check"></i><b>C</b> Animation</a><ul>
<li class="chapter" data-level="C.1" data-path="C-animation.html"><a href="C-animation.html#gganimate-how-to-create-plots-with-beautiful-animation-in-r"><i class="fa fa-check"></i><b>C.1</b> gganimate: How to Create Plots with Beautiful Animation in R</a><ul>
<li class="chapter" data-level="C.1.1" data-path="C-animation.html"><a href="C-animation.html#prerequisites"><i class="fa fa-check"></i><b>C.1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="C.1.2" data-path="C-animation.html"><a href="C-animation.html#demo-dataset"><i class="fa fa-check"></i><b>C.1.2</b> Demo dataset</a></li>
<li class="chapter" data-level="C.1.3" data-path="C-animation.html"><a href="C-animation.html#static-plot"><i class="fa fa-check"></i><b>C.1.3</b> Static plot</a></li>
<li class="chapter" data-level="C.1.4" data-path="C-animation.html"><a href="C-animation.html#transition-through-distinct-states-in-time"><i class="fa fa-check"></i><b>C.1.4</b> Transition through distinct states in time</a></li>
<li class="chapter" data-level="C.1.5" data-path="C-animation.html"><a href="C-animation.html#reveal-data-along-a-given-dimension"><i class="fa fa-check"></i><b>C.1.5</b> Reveal data along a given dimension</a></li>
<li class="chapter" data-level="C.1.6" data-path="C-animation.html"><a href="C-animation.html#transition-between-several-distinct-stages-of-the-data"><i class="fa fa-check"></i><b>C.1.6</b> Transition between several distinct stages of the data</a></li>
<li class="chapter" data-level="C.1.7" data-path="C-animation.html"><a href="C-animation.html#read-more"><i class="fa fa-check"></i><b>C.1.7</b> Read more</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="C-animation.html"><a href="C-animation.html#how-to-save-your-animation"><i class="fa fa-check"></i><b>C.2</b> How to save your animation</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-shiny.html"><a href="D-shiny.html"><i class="fa fa-check"></i><b>D</b> Shiny</a><ul>
<li class="chapter" data-level="D.1" data-path="D-shiny.html"><a href="D-shiny.html#helpful-resources"><i class="fa fa-check"></i><b>D.1</b> Helpful Resources</a></li>
<li class="chapter" data-level="D.2" data-path="D-shiny.html"><a href="D-shiny.html#set-up-and-getting-started"><i class="fa fa-check"></i><b>D.2</b> Set Up and Getting Started</a></li>
<li class="chapter" data-level="D.3" data-path="D-shiny.html"><a href="D-shiny.html#building-your-basic-app"><i class="fa fa-check"></i><b>D.3</b> Building Your Basic App</a><ul>
<li class="chapter" data-level="D.3.1" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-basic-ui"><i class="fa fa-check"></i><b>D.3.1</b> Setting Up the Basic UI</a></li>
<li class="chapter" data-level="D.3.2" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-server"><i class="fa fa-check"></i><b>D.3.2</b> Setting up the Server</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="D-shiny.html"><a href="D-shiny.html#organization"><i class="fa fa-check"></i><b>D.4</b> Organization</a></li>
<li class="chapter" data-level="D.5" data-path="D-shiny.html"><a href="D-shiny.html#customizations"><i class="fa fa-check"></i><b>D.5</b> Customizations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Preceptor’s Primer for Bayesian Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Classification</h1>
<p><span class="math display">\[
\newcommand{\lik}{\operatorname{Lik}}
\newcommand{\Lik}{\operatorname{Lik}}
\]</span>
Many research questions have binary (yes/no or success/failure) responses:</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Are students with poor grades more likely to binge drink?</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Is exposure to a particular chemical associated with a cancer diagnosis?</li>
</ol></li>
</ul>
<p><strong>Binary responses</strong> take on only two values: success (<span class="math inline">\(Y=1\)</span>) or failure (<span class="math inline">\(Y=0\)</span>), Yes (<span class="math inline">\(Y=1\)</span>) or No (<span class="math inline">\(Y=0\)</span>), etc. Thus, examples (a) and (b) above would be considered to have binary responses (Does a student binge drink? Was a patient diagnosed with cancer?). Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success <span class="math inline">\(p\)</span> based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure.</p>
<p>In this chapter, we will look at three common techniques of <strong>classification</strong> of binary data. First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapter <a href="11-regression.html#regression">11</a> and <a href="12-multiple-regression.html#multiple-regression">12</a>. Second, we will consider classification and regression trees (CART). Finally, we will discuss random forests.</p>
<div id="needed-packages-5" class="section level3 unnumbered">
<h3>Needed packages</h3>
<p>Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section <a href="5-tidy.html#tidyverse-package">5.5</a> that loading the <strong>tidyverse</strong> package by running <code>library(tidyverse)</code> loads the following commonly used data science packages all at once:</p>
<ul>
<li><strong>ggplot2</strong> for data visualization</li>
<li><strong>dplyr</strong> for data wrangling</li>
<li><strong>tidyr</strong> for converting data to “tidy” format</li>
<li><strong>readr</strong> for importing spreadsheet data into R</li>
<li>As well as the more advanced <strong>purrr</strong>, <strong>tibble</strong>, <strong>stringr</strong>, and <strong>forcats</strong> packages</li>
</ul>
<p>If needed, read Section <a href="1-getting-started.html#packages">1.3</a> for information on how to install and load R packages.</p>
<div class="sourceCode" id="cb866"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb866-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb866-2" data-line-number="2"><span class="kw">library</span>(broom)</a>
<a class="sourceLine" id="cb866-3" data-line-number="3"><span class="kw">library</span>(skimr)</a>
<a class="sourceLine" id="cb866-4" data-line-number="4"><span class="kw">library</span>(fivethirtyeight)</a></code></pre></div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">13.1</span> Logistic regression</h2>
<div id="what-is-logistic-regression" class="section level3">
<h3><span class="header-section-number">13.1.1</span> What is logistic regression?</h3>
<p>Figure <a href="13-classification.html#fig:OLSlogistic">13.1</a> illustrates a data set with a binary (0 or 1) response (<span class="math inline">\(Y\)</span>) and a single continuous predictor (<span class="math inline">\(X\)</span>). The blue line is a linear regression to model the probability of a success (<span class="math inline">\(Y=1\)</span>) for a given value of <span class="math inline">\(X\)</span>. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1.</p>
<p>The red curve is the <em>logistic regression</em> curve. Note that its characteristic “S” shape always produces predicted probabilities between 0 and 1. Here is the formula for a logistic regression:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]</span></p>
<p>where the observed values <span class="math inline">\(Y_i \sim\)</span> Bernoulli with <span class="math inline">\(p=p_i\)</span> for a given set of predictors <span class="math inline">\(X\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:OLSlogistic"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/OLSlogistic-1.png" alt="Linear vs. logistic regression models for binary response data." width="60%" />
<p class="caption">
FIGURE 13.1: Linear vs. logistic regression models for binary response data.
</p>
</div>
<p>The mathematical function <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span> is called the <em>logit function</em> and it transforms variables from the space <span class="math inline">\((0, 1)\)</span> (like probabilities) to <span class="math inline">\((-\infty, \infty)\)</span>. The inverse of that function, the <em>logistic function</em>, is <span class="math inline">\(\left(\frac{e^x}{e^x + 1}\right)\)</span> and transforms variables from the space <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>. From that latter function’s name we get the terminology of <em>logistic regression</em>.</p>
</div>
<div id="house-elections-exploratory-data-analysis" class="section level3">
<h3><span class="header-section-number">13.1.2</span> House elections: exploratory data analysis</h3>
<p>What affects whether a Democrat or Republican wins a race in the U.S. House of Representatives? This is an example of a binary response: either a Democrat wins (and a Republican loses) or a Republican wins (and a Democrat loses).<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> In this section, we are going to consider several models predicting Democratic victory in House races. First, we will consider a single categorical variable as a predictor: the region that a district lies in (Midwest, Northeast, South, or West). Second, we will consider a single continuous variable (year). Finally, we fill fit a model that contains an interaction of the two.</p>
<p>The data on House election results from 1976 to 2018 can be found in the <code>house_results</code> data frame in <strong>politicaldata</strong> package. We’ll create a version of this data frame called <code>house_ch13</code> that creates a new column <code>dem_win</code> that notes for each state if the Democratic candidate in a congressional district received more votes than the other candidates. We’ll also join it with the <code>state_info</code> data frame in the <strong>fivethirtyeight</strong> package to add the <code>region</code> of each state.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb867-1" data-line-number="1"><span class="kw">library</span>(politicaldata)</a>
<a class="sourceLine" id="cb867-2" data-line-number="2"></a>
<a class="sourceLine" id="cb867-3" data-line-number="3">house_ch13 &lt;-<span class="st"> </span>house_results <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb867-4" data-line-number="4"><span class="st">  </span></a>
<a class="sourceLine" id="cb867-5" data-line-number="5"><span class="st">  </span><span class="co"># Create dem_win variable</span></a>
<a class="sourceLine" id="cb867-6" data-line-number="6"><span class="st">  </span></a>
<a class="sourceLine" id="cb867-7" data-line-number="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dem =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(dem), <span class="dv">0</span>, dem),</a>
<a class="sourceLine" id="cb867-8" data-line-number="8">         <span class="dt">other =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(other), <span class="dv">0</span>, other),</a>
<a class="sourceLine" id="cb867-9" data-line-number="9">         <span class="dt">rep =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(rep), <span class="dv">0</span>, rep),</a>
<a class="sourceLine" id="cb867-10" data-line-number="10">         <span class="dt">dem_win =</span> <span class="kw">ifelse</span>(dem <span class="op">&gt;</span><span class="st"> </span>rep <span class="op">&amp;</span><span class="st"> </span>dem <span class="op">&gt;</span><span class="st"> </span>other, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb867-11" data-line-number="11"><span class="st">  </span></a>
<a class="sourceLine" id="cb867-12" data-line-number="12"><span class="st">  </span><span class="co"># Rename to join with state_info</span></a>
<a class="sourceLine" id="cb867-13" data-line-number="13"><span class="st">  </span></a>
<a class="sourceLine" id="cb867-14" data-line-number="14"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">state_abbrev =</span> state_abb) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb867-15" data-line-number="15"><span class="st">  </span><span class="kw">left_join</span>(state_info) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb867-16" data-line-number="16"><span class="st">  </span><span class="kw">select</span>(region, state, district, year, dem_win)</a></code></pre></div>
<pre><code>Joining, by = &quot;state_abbrev&quot;</code></pre>
<p>Recall the three common steps in an exploratory data analysis we saw in Subsection <a href="11-regression.html#model1EDA">11.1.1</a>:</p>
<ol style="list-style-type: decimal">
<li>Looking at the raw data values.</li>
<li>Computing summary statistics.</li>
<li>Creating data visualizations.</li>
</ol>
<p>Let’s first look at the raw data values by either looking at <code>house_ch13</code> using RStudio’s spreadsheet viewer or by using the <code>glimpse()</code> function from the <strong>dplyr</strong> package:</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb869-1" data-line-number="1"><span class="kw">glimpse</span>(house_ch13)</a></code></pre></div>
<pre><code>Observations: 9,557
Variables: 5
$ region   &lt;chr&gt; &quot;West&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;…
$ state    &lt;chr&gt; &quot;Alaska&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaba…
$ district &lt;chr&gt; &quot;AK-AL&quot;, &quot;AL-01&quot;, &quot;AL-02&quot;, &quot;AL-03&quot;, &quot;AL-04&quot;, &quot;AL-05&quot;, &quot;AL-06…
$ year     &lt;dbl&gt; 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, …
$ dem_win  &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …</code></pre>
<p>Let’s also display a random sample of 5 rows of the 9,557 rows corresponding to different district-years. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb871-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb871-2" data-line-number="2"><span class="st">  </span><span class="kw">sample_n</span>(<span class="dt">size =</span> <span class="dv">5</span>)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-593">TABLE 13.1: </span>A random sample of 5 out of the 4,201 district-years
</caption>
<thead>
<tr>
<th style="text-align:left;">
region
</th>
<th style="text-align:left;">
state
</th>
<th style="text-align:left;">
district
</th>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
dem_win
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
West
</td>
<td style="text-align:left;">
California
</td>
<td style="text-align:left;">
CA-50
</td>
<td style="text-align:right;">
2012
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Northeast
</td>
<td style="text-align:left;">
New York
</td>
<td style="text-align:left;">
NY-27
</td>
<td style="text-align:right;">
1984
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Northeast
</td>
<td style="text-align:left;">
New York
</td>
<td style="text-align:left;">
NY-34
</td>
<td style="text-align:right;">
1978
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
South
</td>
<td style="text-align:left;">
Florida
</td>
<td style="text-align:left;">
FL-06
</td>
<td style="text-align:right;">
2008
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Midwest
</td>
<td style="text-align:left;">
Michigan
</td>
<td style="text-align:left;">
MI-15
</td>
<td style="text-align:right;">
1988
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
<p>Now that we’ve looked at the raw values in our <code>house_ch13</code> data frame and got a sense of the data, let’s compute summary statistics. As we’ve done in our exploratory data analyses before, let’s use the <code>skim()</code> function from the <code>skimr</code> package, being sure to only select() the variables of interest in our model:</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb872-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb872-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(dem_win, region, year) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb872-3" data-line-number="3"><span class="st">  </span><span class="kw">skim</span>()</a></code></pre></div>
<table style='width: auto;'
        class='table table-condensed'>
<caption>
<span id="tab:unnamed-chunk-594">TABLE 13.2: </span>Data summary
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
9557
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
character
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
empty
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:right;">
whitespace
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
region
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
dem_win
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.54
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1997.00
</td>
<td style="text-align:right;">
12.7
</td>
<td style="text-align:right;">
1976
</td>
<td style="text-align:right;">
1986
</td>
<td style="text-align:right;">
1998
</td>
<td style="text-align:right;">
2008
</td>
<td style="text-align:right;">
2018
</td>
<td style="text-align:left;">
▇▆▆▆▇
</td>
</tr>
</tbody>
</table>
<p>Observe that we have no missing data, that we have 9,557 observations, and that the mean of <code>dem_win</code> is 0.54, indicating that Democrats won 54% of the House elections in this period (1976–2018).</p>
<p>Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations.</p>
<p>For our categorical variable, we’ll look at histograms of <code>dem_win</code> faceted by <code>region</code>:</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb873-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb873-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dem_win)) <span class="op">+</span></a>
<a class="sourceLine" id="cb873-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb873-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Democratic victory percentage, 1976-2018&quot;</span>, </a>
<a class="sourceLine" id="cb873-5" data-line-number="5">       <span class="dt">y =</span> <span class="st">&quot;Number of districts&quot;</span>,</a>
<a class="sourceLine" id="cb873-6" data-line-number="6">       <span class="dt">title =</span> <span class="st">&quot;Histogram of distribution of Democratic victories by House district&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb873-7" data-line-number="7"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span></a>
<a class="sourceLine" id="cb873-8" data-line-number="8"><span class="st">  </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-595-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Wait! That doesn’t tell us very much, because our outcome variable only takes two values, 0 and 1. Let’s instead <code>group_by(district)</code> and <code>summarize()</code> to get a better sense of the distributions:</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb874-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(region, district) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-3" data-line-number="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">dem_win =</span> <span class="kw">mean</span>(dem_win)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-4" data-line-number="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dem_win)) <span class="op">+</span></a>
<a class="sourceLine" id="cb874-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb874-6" data-line-number="6"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Democratic victory percentage, 1976-2018&quot;</span>, </a>
<a class="sourceLine" id="cb874-7" data-line-number="7">       <span class="dt">y =</span> <span class="st">&quot;Number of districts&quot;</span>,</a>
<a class="sourceLine" id="cb874-8" data-line-number="8">       <span class="dt">title =</span> <span class="st">&quot;Histogram of distribution of Democratic victories by House district&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb874-9" data-line-number="9"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb874-10" data-line-number="10"><span class="st">  </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-596-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This is much more informative! We can see that the Midwest is highly bimodal, with many districts either electing Democrats for every year in this period or for none. The Northeast and West have many districts that always elect Democrats but few that never do. The South is the only region with a peak in the middle, indicating that there are many districts in the South that elected Democrats for about half the time during 1976-2018.</p>
<p>What happens if we create a scatterplot of our outcome variable <code>dem_win</code> and a continuous predictor, <code>year</code>?</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb875-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb875-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> dem_win)) <span class="op">+</span></a>
<a class="sourceLine" id="cb875-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb875-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Democratic Victory&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb875-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-597-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This is completely incomprehensible! When dealing with binary data, it is more helpful to construct an <em>empirical logit</em> plot instead of a regular scatterplot. The steps for constructing such a plot are as follows:</p>
<ol style="list-style-type: decimal">
<li><code>group_by</code> your continuous variable.</li>
<li><code>summarize</code> the percentage of successes in your outcome variable.</li>
<li>Calculate the <em>empirical logit</em> for each group, using the logit function: <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span></li>
<li>Plot the results.</li>
</ol>
<p>First, since we will use it frequently, let’s define a function <code>logit()</code> that performs the logit transformation:</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb876-1" data-line-number="1">logit &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="kw">log</span>(p <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))</a></code></pre></div>
<p>Next, let’s look at the empirical logit plot:</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb877-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb877-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb877-3" data-line-number="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">perc_dem_win =</span> <span class="kw">mean</span>(dem_win),</a>
<a class="sourceLine" id="cb877-4" data-line-number="4">            <span class="dt">emplogit =</span> <span class="kw">logit</span>(perc_dem_win)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb877-5" data-line-number="5"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> emplogit)) <span class="op">+</span></a>
<a class="sourceLine" id="cb877-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb877-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb877-8" data-line-number="8"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</a>
<a class="sourceLine" id="cb877-9" data-line-number="9">       <span class="dt">y =</span> <span class="st">&quot;Empirical logits&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb877-10" data-line-number="10"><span class="st">  </span><span class="kw">theme_classic</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-599-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Now we see that after the logit transformation, there is roughly a linear relationship between our outcome variable and our explanatory variable <code>year</code>. This means that a logistic regression model makes sense. Some of the most visually apparent outliers will be familiar to students of American politics: 1994 (the “Republican Revolution”), 2008 (Obama’s first election), and 2018. Yet in general it appears that over time the Democrats have performed worse in House elections.</p>
<p>We can follow the same steps to look at this relationship within Census regions (Midwest, Northeast, South and West):</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb878-1" data-line-number="1">house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb878-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(region, year) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb878-3" data-line-number="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">perc_dem_win =</span> <span class="kw">mean</span>(dem_win),</a>
<a class="sourceLine" id="cb878-4" data-line-number="4">            <span class="dt">emplogit =</span> <span class="kw">logit</span>(perc_dem_win)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb878-5" data-line-number="5"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> emplogit)) <span class="op">+</span></a>
<a class="sourceLine" id="cb878-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb878-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb878-8" data-line-number="8"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</a>
<a class="sourceLine" id="cb878-9" data-line-number="9">       <span class="dt">y =</span> <span class="st">&quot;Empirical logits&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb878-10" data-line-number="10"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span></a>
<a class="sourceLine" id="cb878-11" data-line-number="11"><span class="st">  </span><span class="kw">theme_classic</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-600-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We see roughly linear relationships after the logit transformation within-region as well, although the relationship looks more linear in the Midwest and South than in the Northeast and West. We see that the Democratic Party’s overall decline in House races is driven by the South and to a lesser extent the Midwest; Democratic performance has on average improved in the Northeast and West. The sharp negative slope in the South will not be surprising if one is familiar with the collapse of the “Solid South.”</p>
</div>
<div id="one-categorical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.3</span> One categorical explanatory variable</h3>
<p>Let’s start our modeling by predicting <code>dem_win</code> with a single categorical explanatory variable, <code>region</code>. As we’ll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression. In fact, we’ll follow the same basic steps:</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x, family, data)</code> function and save it in <code>house_region_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_region_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns.</li>
</ol>
<p>Note that the key difference is that instead of using <code>lm()</code>, we are now using <code>glm()</code>. <code>glm()</code> operates very similarly to <code>lm()</code>, but it has an additional argument: <code>family</code>. To run a logistic regression, we use <code>family = binomial</code>.</p>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb879-1" data-line-number="1">house_region_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>region, <span class="dt">family =</span> binomial <span class="dt">data =</span> house_ch13)</a>
<a class="sourceLine" id="cb879-2" data-line-number="2">house_region_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb879-3" data-line-number="3"><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb879-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-603">TABLE 13.3: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-0.076
</td>
<td style="text-align:right;">
-0.157
</td>
<td style="text-align:right;">
0.006
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast
</td>
<td style="text-align:right;">
0.657
</td>
<td style="text-align:right;">
0.534
</td>
<td style="text-align:right;">
0.780
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth
</td>
<td style="text-align:right;">
0.056
</td>
<td style="text-align:right;">
-0.050
</td>
<td style="text-align:right;">
0.163
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest
</td>
<td style="text-align:right;">
0.333
</td>
<td style="text-align:right;">
0.213
</td>
<td style="text-align:right;">
0.453
</td>
</tr>
</tbody>
</table>
<p>Recall that in the linear regression context, we interpreted the coefficients as follows: the intercept represented the mean for the omitted category, while the other coefficients all represented offsets from that value. We can’t use that intepretation here. Recall our logistic regression model equation:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]</span></p>
<p>Note that there’s a familiar term for what we are taking the log of on the left hand side:</p>
<p><span class="math display">\[\textrm{Odds} = \frac{\textrm{probability of success}}{\textrm{probability of failure}}=
\frac{p}{1-p}.\]</span></p>
<p>For example, there’s a 25% chance of flipping two heads in a row when flipping fair coins. In odds terms, the odds of flipping two heads in a row are <span class="math inline">\(\frac{0.25}{0.75} = \frac{1}{3}\)</span>. These are conventionally phrased in words in terms of the odds <em>against</em> success, such as “three to one odds against flipping two heads” or just “three to one odds.”</p>
<p>Thus, our equation can also be read as:</p>
<p><span class="math display">\[
\log(\textrm{Odds})=\beta_0+\beta_1X 
\]</span></p>
<p>Let’s say that <span class="math inline">\(X\)</span> is our categorical variable <code>region</code>, with the baseline category being the Midwest. Based on this model, the log odds of a Democratic victory when <span class="math inline">\(X = \textrm{Midwest}\)</span> is:
<span class="math display">\[
\log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) =\beta_0 = -0.076,
\]</span>
and the log odds when <span class="math inline">\(X = \textrm{South}\)</span> is:
<span class="math display">\[
\log\left(\frac{p_{South}}{1-p_{South}}\right)=\beta_0+\beta_{1,South} = -0.076 + 0.056 = -0.020.
\]</span></p>
<p>We can see that <span class="math inline">\(\beta_{1,South}\)</span> is the difference between the log odds of success when <span class="math inline">\(X = \textrm{South}\)</span> versus <span class="math inline">\(X = \textrm{Midwest}\)</span>. Using rules of logs:
<span class="math display">\[
\begin{aligned}
\beta_{1,South} &amp;= (\beta_0 + \beta_{1,South}) - \beta_0 \\ &amp;=
\log\left(\frac{p_{South}}{1-p_{South}}\right) - \log\left(\frac{p_{Midwest}}{1-p_{Midwest}}\right) \\ &amp;=
\log\left(\frac{p_{South}/(1-p_{South})}{p_{Midwest}/{(1-p_{Midwest})}}\right) \\ &amp;=
\log\left(\frac{\textrm{Odds}(South)}{\textrm{Odds}(Midwest)}\right)
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(e^{\log(x)} = x\)</span>, <span class="math inline">\(e^{\beta_{1,South}}\)</span> is the ratio of the odds of success when <span class="math inline">\(X = \textrm{South}\)</span> compared to <span class="math inline">\(X = \textrm{Midwest}\)</span>. In general, <strong>exponentiated coefficients in a logistic regression are odds ratios</strong>. A general interpretation of an odds ratio is the odds of success for group A compared to the odds of success for group B—how many times greater the odds of success are in group A compared to group B.</p>
<p>Note that the logistic regression model can also be re-written in a <strong>probability form</strong>:</p>
<p><span class="math display">\[
p_X=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
\]</span></p>
<p>Now that we understand that expontentiated coefficients in logistic regressions are odds ratios, we can use the argument <code>exponentiate = TRUE</code> in the <code>tidy()</code> function to exponentiate the coefficients for us:</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb880-1" data-line-number="1">house_region_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb880-2" data-line-number="2"><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb880-3" data-line-number="3">       <span class="dt">exponentiate =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb880-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-605">TABLE 13.4: </span>Logistic regression table, exponentiated coefficients
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.927
</td>
<td style="text-align:right;">
0.855
</td>
<td style="text-align:right;">
1.01
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast
</td>
<td style="text-align:right;">
1.929
</td>
<td style="text-align:right;">
1.706
</td>
<td style="text-align:right;">
2.18
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth
</td>
<td style="text-align:right;">
1.058
</td>
<td style="text-align:right;">
0.951
</td>
<td style="text-align:right;">
1.18
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest
</td>
<td style="text-align:right;">
1.395
</td>
<td style="text-align:right;">
1.238
</td>
<td style="text-align:right;">
1.57
</td>
</tr>
</tbody>
</table>
<p>This allows us to see that the odds of a Democratic victory in the Northeast are nearly twice as high as the odds of a Democratic victory in the Midwest, the reference category. The odds of a Democratic victory in the West are about 1.4 times higher than the odds of a Democratic victory in the Midwest. The odds of a Democratic victory in the South are similar to the odds of a Democratic victory in the Midwest.</p>
</div>
<div id="observedfitted-values-and-residuals" class="section level3">
<h3><span class="header-section-number">13.1.4</span> Observed/fitted values and residuals</h3>
<p>We have previously defined the following three concepts for a linear regression:</p>
<ol style="list-style-type: decimal">
<li>Observed values <span class="math inline">\(y\)</span>, or the observed value of the outcome variable</li>
<li>Fitted values <span class="math inline">\(\widehat{y}\)</span>, or the value on the regression line for a given <span class="math inline">\(x\)</span> value</li>
<li>Residuals <span class="math inline">\(y - \widehat{y}\)</span>, or the error between the observed value and the fitted value</li>
</ol>
<p>We obtained these values and other values using the <code>augment()</code> function from the <strong>broom</strong> package. Recall too that we used the <code>.se.fit</code> column to construct confidence intervals. We’ll see here how we can apply these same concepts to logistic regression.</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb881-1" data-line-number="1">regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb881-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb881-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb881-4" data-line-number="4">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb881-5" data-line-number="5"><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</a>
<a class="sourceLine" id="cb881-6" data-line-number="6">regression_points</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-607">TABLE 13.5: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.257
</td>
<td style="text-align:right;">
0.168
</td>
<td style="text-align:right;">
0.347
</td>
<td style="text-align:right;">
-1.29
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
</tbody>
</table>
<p>The syntax is the same, but the interpretation has to change, since the <code>.fitted</code>, <code>conf.low</code>, and <code>conf.high</code> columns are all on the logit scale. While we could try to interpret these values, <code>augment()</code> has the argument <code>type.predict = &quot;response&quot;</code> that allow us to present the results in terms of <strong>predicted probabilities</strong>:</p>
<div class="sourceCode" id="cb882"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb882-1" data-line-number="1">regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb882-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb882-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb882-4" data-line-number="4">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb882-5" data-line-number="5"><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</a>
<a class="sourceLine" id="cb882-6" data-line-number="6">regression_points</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-609">TABLE 13.6: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.564
</td>
<td style="text-align:right;">
0.542
</td>
<td style="text-align:right;">
0.586
</td>
<td style="text-align:right;">
-1.29
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
</tbody>
</table>
<p>Now each of the <code>.fitted</code> values is a <em>predicted probability</em> of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability.
You may be wondering how to interpret the residuals. The residuals reported by <code>augment()</code> for a logistic regression are called <em>deviance residuals</em>. A deviance residual can be calculated for each observation using:</p>
<p><span class="math display">\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the actual outcome and <span class="math inline">\(p_i\)</span> is the predicted probability from the logistic regression model.</p>
<p>The sum of the individual deviance residuals is referred to as the <strong>deviance</strong> or <strong>residual deviance</strong>. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.</p>
<p>However, you can also have <code>augment()</code> report residuals as differences between the observed outcome and the predicted probabilities by using <code>type.residuals = &quot;response&quot;</code>:</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb883-1" data-line-number="1">regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb883-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,</a>
<a class="sourceLine" id="cb883-3" data-line-number="3">          <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb883-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb883-5" data-line-number="5">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb883-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</a>
<a class="sourceLine" id="cb883-7" data-line-number="7">regression_points</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-611">TABLE 13.7: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.564
</td>
<td style="text-align:right;">
0.542
</td>
<td style="text-align:right;">
0.586
</td>
<td style="text-align:right;">
-0.564
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
</tbody>
</table>
<p>Now, the <code>.resid</code> value is the difference between the actual outcome (<code>dem_win</code>) and the predicted probability.</p>
</div>
<div id="one-numerical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.5</span> One numerical explanatory variable</h3>
<p>We’ll now predict <code>dem_win</code> with a single numerical explanatory variable, <code>year</code>.</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x, family, data)</code> function and save it in <code>house_year_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_year_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns. Recall that setting <code>exponentiate = TRUE</code> exponentiates the logistic regression coefficients, which can help with interpretation.</li>
</ol>
<div class="sourceCode" id="cb884"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb884-1" data-line-number="1">house_year_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>year, <span class="dt">family =</span> binomial <span class="dt">data =</span> house_ch13)</a>
<a class="sourceLine" id="cb884-2" data-line-number="2">house_year_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb884-3" data-line-number="3"><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb884-4" data-line-number="4">       <span class="dt">exponentiate =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb884-5" data-line-number="5"><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-614">TABLE 13.8: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
949444013670122.375
</td>
<td style="text-align:right;">
1603037644663.54
</td>
<td style="text-align:right;">
571921061250829760.000
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
0.983
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
0.986
</td>
</tr>
</tbody>
</table>
<p>How do we interpret the coefficients in this model? Since the <code>year</code> coefficient is less than 1, that means that each additional year <em>reduces</em> the odds of a Democratic victory. In fact, each additional year is associated with a 17% reduction in the odds of a Democratic victory.</p>
<p>Note that the very high number for the “intercept” suggests that the odds of a Democratic victory in year 0 are arbitrarily high – that is, the <em>probability</em> of a Democratic victory in year 0 is essentially 100%. But of course, there was no House of Representatives in year 0 (furthermore, there is no year 0)! This is a good lesson in making sure to know the limits of one’s models: just because something makes <em>mathematical</em> sense as an output of your model, doesn’t mean that it makes real-world sense for the phenomenon you are studying.</p>
<p>While <code>house_region_model</code> and <code>house_year_model</code> both tell us something interesting, we could learn more with an <em>interaction model</em> that includes both of our predictors.</p>
</div>
<div id="one-numerical-and-one-categorical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.6</span> One numerical and one categorical explanatory variable</h3>
<p>We’ll now predict <code>dem_win</code> with a two variable, <code>region</code> and <code>year</code>, as well as the interaction between the two</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x1 * x2, family, data)</code> function and save it in <code>house_interact_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_interact_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns. Recall that setting <code>exponentiate = TRUE</code> exponentiates the logistic regression coefficients, which can help with interpretation.</li>
</ol>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb885-1" data-line-number="1">house_interact_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>region <span class="op">*</span><span class="st"> </span>year, <span class="dt">family =</span> binomial <span class="dt">data =</span> house_ch13)</a>
<a class="sourceLine" id="cb885-2" data-line-number="2">house_interact_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb885-3" data-line-number="3"><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb885-4" data-line-number="4">       <span class="dt">exponentiate =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb885-5" data-line-number="5"><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-617">TABLE 13.9: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
6402005451242306.000
</td>
<td style="text-align:right;">
16134536152.644
</td>
<td style="text-align:right;">
2735712173971624951808.000
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth
</td>
<td style="text-align:right;">
457773687111729090939322368.000
</td>
<td style="text-align:right;">
11936399059160444928.000
</td>
<td style="text-align:right;">
18029227663025415916498767606448128.000
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
0.982
</td>
<td style="text-align:right;">
0.976
</td>
<td style="text-align:right;">
0.988
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast:year
</td>
<td style="text-align:right;">
1.029
</td>
<td style="text-align:right;">
1.019
</td>
<td style="text-align:right;">
1.039
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth:year
</td>
<td style="text-align:right;">
0.970
</td>
<td style="text-align:right;">
0.961
</td>
<td style="text-align:right;">
0.978
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest:year
</td>
<td style="text-align:right;">
1.026
</td>
<td style="text-align:right;">
1.017
</td>
<td style="text-align:right;">
1.036
</td>
</tr>
</tbody>
</table>
<p>Now we can see how the effect of <code>year</code> varies by <code>region</code>. While the passage of time is associated with more Democratic victories in the Northeast and the West, which we can see from the exponentiated coefficients on <code>regionNortheast:year</code> and <code>regionWest:year</code> being greater than 1, <code>year</code> is associated with declining Democratic fortunes in the Midwest (<code>year</code>) and the South (<code>regionSouth:year</code>).</p>
<p>Looking at predicted probabilities can also put this model in perspective. Let’s use <code>augment()</code> to generate the predictions. Remember that <code>type.predict = &quot;response&quot;</code> and <code>type.residuals = &quot;response&quot;</code> put the fitted values and the residuals on the probability scale.</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb886-1" data-line-number="1">regression_points &lt;-<span class="st"> </span>house_interact_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb886-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,</a>
<a class="sourceLine" id="cb886-3" data-line-number="3">          <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb886-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb886-5" data-line-number="5">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb886-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</a>
<a class="sourceLine" id="cb886-7" data-line-number="7">regression_points</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-619">TABLE 13.10: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.522
</td>
<td style="text-align:right;">
0.477
</td>
<td style="text-align:right;">
0.567
</td>
<td style="text-align:right;">
-0.522
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
</tbody>
</table>
<p>We can also use <code>augment</code> to make predictions for years that aren’t in our data. What would our model predict for the 2020 elections? We use the <code>newdata</code> argument in <code>augment()</code> to make these predictions.</p>
<div class="sourceCode" id="cb887"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb887-1" data-line-number="1">house_interact_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb887-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">tibble</span>(<span class="dt">year =</span> <span class="kw">rep</span>(<span class="dv">2020</span>, <span class="dv">4</span>),</a>
<a class="sourceLine" id="cb887-3" data-line-number="3">                           <span class="dt">region =</span> <span class="kw">c</span>(<span class="st">&quot;Midwest&quot;</span>, <span class="st">&quot;Northeast&quot;</span>, <span class="st">&quot;South&quot;</span>, <span class="st">&quot;West&quot;</span>)),</a>
<a class="sourceLine" id="cb887-4" data-line-number="4">          <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb887-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb887-6" data-line-number="6">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit)</a></code></pre></div>
<pre><code># A tibble: 4 x 6
   year region     .fitted   .se.fit conf.low conf.high
  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1  2020 Midwest   0.373821 0.0210871 0.331646  0.415995
2  2020 Northeast 0.697319 0.0218275 0.653664  0.740974
3  2020 South     0.247674 0.0142235 0.219227  0.276121
4  2020 West      0.604990 0.0217154 0.561559  0.648420</code></pre>
<p>These can easily be plotted using <code>ggplot()</code>:</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb889-1" data-line-number="1">house_interact_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb889-2" data-line-number="2"><span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">tibble</span>(<span class="dt">year =</span> <span class="kw">rep</span>(<span class="dv">2020</span>, <span class="dv">4</span>),</a>
<a class="sourceLine" id="cb889-3" data-line-number="3">                           <span class="dt">region =</span> <span class="kw">c</span>(<span class="st">&quot;Midwest&quot;</span>, <span class="st">&quot;Northeast&quot;</span>, <span class="st">&quot;South&quot;</span>, <span class="st">&quot;West&quot;</span>)),</a>
<a class="sourceLine" id="cb889-4" data-line-number="4">          <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb889-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</a>
<a class="sourceLine" id="cb889-6" data-line-number="6">         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb889-7" data-line-number="7"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .fitted,</a>
<a class="sourceLine" id="cb889-8" data-line-number="8">             <span class="dt">ymin =</span> conf.low,</a>
<a class="sourceLine" id="cb889-9" data-line-number="9">             <span class="dt">ymax =</span> conf.high,</a>
<a class="sourceLine" id="cb889-10" data-line-number="10">             <span class="dt">x =</span> region)) <span class="op">+</span></a>
<a class="sourceLine" id="cb889-11" data-line-number="11"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb889-12" data-line-number="12"><span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb889-13" data-line-number="13"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Predicted probability of a Democratic victory (2020)&quot;</span>,</a>
<a class="sourceLine" id="cb889-14" data-line-number="14">       <span class="dt">x =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb889-15" data-line-number="15"><span class="st">  </span><span class="kw">theme_classic</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-621-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
</div>
<div id="fitting-many-models-using-map-1" class="section level3">
<h3><span class="header-section-number">13.1.7</span> Fitting many models using <code>map()</code></h3>
<p>While it is interesting to see how Democrats perform by region over time, it would also be interesting to see how each state has changed in its partisan voting from 1976–2018. Have any seen particularly large increases (or decreases) in the probability of a Democratic candidate winning?</p>
<p>The code to do this is very similar to the code we used for the gubernatorial forecasts in Chapter <a href="11-regression.html#regression">11</a> and the Seattle house prices in Chapter <a href="12-multiple-regression.html#multiple-regression">12</a>. However, we will use <code>glm()</code> instead of <code>lm()</code>, and we will use the <code>exponentiate = TRUE</code> option for <code>tidy()</code>.</p>
<p>First, we’ll filter to the states that have at least 50 district-years in the dataset. Next, let’s use <code>map()</code> to learn about these districts:</p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb890-1" data-line-number="1">infreq_states &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>(state) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-3" data-line-number="3"><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&lt;</span><span class="st"> </span><span class="dv">50</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-4" data-line-number="4"><span class="st">  </span><span class="kw">pull</span>(state)</a>
<a class="sourceLine" id="cb890-5" data-line-number="5"></a>
<a class="sourceLine" id="cb890-6" data-line-number="6">house_state_model &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-7" data-line-number="7"><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="st"> </span>state <span class="op">%in%</span><span class="st">  </span>infreq_states) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-8" data-line-number="8"><span class="st">  </span><span class="kw">group_by</span>(state) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-9" data-line-number="9"><span class="st">  </span><span class="kw">nest</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb890-10" data-line-number="10"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>year, <span class="dt">family =</span> binomial, <span class="dt">data =</span> .)),</a>
<a class="sourceLine" id="cb890-11" data-line-number="11">         <span class="dt">reg_results =</span> <span class="kw">map</span>(mod, <span class="op">~</span><span class="st"> </span><span class="kw">tidy</span>(., <span class="dt">conf.int =</span> <span class="ot">TRUE</span>, <span class="dt">exponentiate =</span> <span class="ot">TRUE</span>)),</a>
<a class="sourceLine" id="cb890-12" data-line-number="12">         <span class="dt">year_coef =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(estimate)),</a>
<a class="sourceLine" id="cb890-13" data-line-number="13">         <span class="dt">year_low =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(conf.low)),</a>
<a class="sourceLine" id="cb890-14" data-line-number="14">         <span class="dt">year_high =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(conf.high)))</a>
<a class="sourceLine" id="cb890-15" data-line-number="15"></a>
<a class="sourceLine" id="cb890-16" data-line-number="16"><span class="kw">glimpse</span>(house_state_model)</a></code></pre></div>
<pre><code>Observations: 38
Variables: 7
Groups: state [38]
$ state       &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;Arizona&quot;, &quot;California&quot;, &quot;Colorado…
$ data        &lt;list&lt;df[,4]&gt;&gt; South, South, South, South, South, South, South,…
$ mod         &lt;list&gt; [&lt;134.3621, -0.0675, -3.85, -3.85, 1.35, 1.35, 1.35, -3.…
$ reg_results &lt;list&gt; [&lt;tbl_df[2 x 7]&gt;, &lt;tbl_df[2 x 7]&gt;, &lt;tbl_df[2 x 7]&gt;, &lt;tbl…
$ year_coef   &lt;dbl&gt; 0.935, 0.946, 1.021, 1.017, 0.997, 1.048, 0.965, 0.926, 0…
$ year_low    &lt;dbl&gt; 0.907, 0.910, 0.993, 1.007, 0.971, 1.015, 0.950, 0.904, 0…
$ year_high   &lt;dbl&gt; 0.962, 0.980, 1.051, 1.027, 1.024, 1.085, 0.979, 0.948, 1…</code></pre>
<p>The easiest way to see the results of these models is to plot the exponentiated coefficients:</p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb892-1" data-line-number="1">house_state_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb892-2" data-line-number="2"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb892-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">fct_reorder</span>(state, year_coef)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb892-4" data-line-number="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> state, <span class="dt">y =</span> year_coef, <span class="dt">ymin =</span> year_low, <span class="dt">ymax =</span> year_high)) <span class="op">+</span></a>
<a class="sourceLine" id="cb892-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb892-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb892-7" data-line-number="7"><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb892-8" data-line-number="8"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb892-9" data-line-number="9">       <span class="dt">y =</span> <span class="st">&quot;Exponentiated coefficients of year&quot;</span>,</a>
<a class="sourceLine" id="cb892-10" data-line-number="10">       <span class="dt">title =</span> <span class="st">&quot;Predicting Democratic victories in the U.S. House over time by state&quot;</span>,</a>
<a class="sourceLine" id="cb892-11" data-line-number="11">       <span class="dt">subtitle =</span> <span class="st">&quot;Logistic regression coefficients for year plotted by state&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb892-12" data-line-number="12"><span class="st">  </span><span class="kw">coord_flip</span>()</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-623-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Consistent with the account we saw when looking at the effect of <code>year</code> overall, there are more states where the odds of a Democratic victory have been decreasing by year than ones where they have been increasing.</p>
</div>
<div id="linear-regression-vs.logistic-regression" class="section level3">
<h3><span class="header-section-number">13.1.8</span> Linear regression vs. logistic regression</h3>
<p>Here’s a useful summary of the differences between linear regression and logistic regression:</p>
<p><span class="math display">\[
\underline{\textrm{Response}} \\
\mathbf{Linear\ Regression:}\textrm{ numeric} \\
\mathbf{Logistic\ Regression:}\textrm{ binary} \\
\textrm{ } \\
\underline{\textrm{Model Fitting}} \\
\mathbf{Linear\ Regression:}\ \mu=\beta_0+\beta_1x \textrm{ using }\texttt{lm()}\\
\mathbf{Logistic\ Regression:}\ \log\left(\frac{p}{1-p}\right)=\beta_0+\beta_1x \text{ using }\texttt{glm()}\\
\textrm{ } \\
\underline{\textrm{EDA}} \\
\mathbf{Linear\ Regression:}\textrm{ plot $X$ vs. $Y$; add line} \\
\mathbf{Logistic\ Regression:}\textrm{ find $\log(\textrm{odds})$ for several subgroups; plot vs. $X$} \\
\textrm{ } \\
\underline{\textrm{Interpreting Coefficients}} \\
\mathbf{Linear\ Regression:}\ \beta_1=\textrm{ change in }\mu_y\textrm{ for unit change in $X$} \\
\mathbf{Logistic\ Regression:}\ e^{\beta_1}=\textrm{ percent change in odds for unit change in $X$} 
\]</span></p>
</div>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">13.2</span> Classification and regression trees (CART)</h2>
<div id="the-curse-of-dimensionality" class="section level3">
<h3><span class="header-section-number">13.2.1</span> The curse of dimensionality</h3>
<p>We described how methods such as LDA and QDA are not meant to be used with many predictors <span class="math inline">\(p\)</span> because the number of parameters that we need to estimate becomes too large. For example, with the digits example <span class="math inline">\(p=784\)</span>, we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the <em>curse of dimensionality</em>. The <em>dimension</em> here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</p>
<p>For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-625-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to <span class="math inline">\(\sqrt{.10} \approx .316\)</span>:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-626-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>.
In general, to include 10% of the data in a case with <span class="math inline">\(p\)</span> dimensions, we need an interval with each side of size <span class="math inline">\(\sqrt[p]{.10}\)</span> of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb893-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb893-2" data-line-number="2">p &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></a>
<a class="sourceLine" id="cb893-3" data-line-number="3"><span class="kw">qplot</span>(p, <span class="fl">.1</span><span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>p), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-627-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</p>
<p>Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.</p>
</div>
<div id="cart-motivation" class="section level3">
<h3><span class="header-section-number">13.2.2</span> CART motivation</h3>
<p>To motivate this section, we will use a new dataset
that includes the breakdown of the composition of olive oil into 8 fatty acids:</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb894-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb894-2" data-line-number="2"><span class="kw">library</span>(dslabs)</a></code></pre></div>
<pre><code>
Attaching package: &#39;dslabs&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:gapminder&#39;:

    gapminder</code></pre>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb897-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;olive&quot;</span>)</a>
<a class="sourceLine" id="cb897-2" data-line-number="2"><span class="kw">names</span>(olive)</a></code></pre></div>
<pre><code> [1] &quot;region&quot;      &quot;area&quot;        &quot;palmitic&quot;    &quot;palmitoleic&quot; &quot;stearic&quot;    
 [6] &quot;oleic&quot;       &quot;linoleic&quot;    &quot;linolenic&quot;   &quot;arachidic&quot;   &quot;eicosenoic&quot; </code></pre>
<p>For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb899-1" data-line-number="1"><span class="kw">table</span>(olive<span class="op">$</span>region)</a></code></pre></div>
<pre><code>
Northern Italy       Sardinia Southern Italy 
           151             98            323 </code></pre>
<p>We remove the <code>area</code> column because we won’t use it as a predictor.</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb901-1" data-line-number="1">olive &lt;-<span class="st"> </span><span class="kw">select</span>(olive, <span class="op">-</span>area)</a></code></pre></div>
<p>Let’s very quickly try to predict the region using kNN:</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb902-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb902-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">train</span>(region <span class="op">~</span><span class="st"> </span>.,  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb902-3" data-line-number="3">             <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">2</span>)), </a>
<a class="sourceLine" id="cb902-4" data-line-number="4">             <span class="dt">data =</span> olive)</a>
<a class="sourceLine" id="cb902-5" data-line-number="5"><span class="kw">ggplot</span>(fit)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-631-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.</p>
<div class="sourceCode" id="cb903"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb903-1" data-line-number="1">olive <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(fatty_acid, percentage, <span class="op">-</span>region) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb903-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(region, percentage, <span class="dt">fill =</span> region)) <span class="op">+</span></a>
<a class="sourceLine" id="cb903-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb903-4" data-line-number="4"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>fatty_acid, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">4</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb903-5" data-line-number="5"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_blank</span>(), <span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-632-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.</p>
<div class="sourceCode" id="cb904"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb904-1" data-line-number="1">olive <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb904-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(eicosenoic, linoleic, <span class="dt">color =</span> region)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb904-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb904-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.065</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb904-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="fl">-0.2</span>, <span class="dt">y =</span> <span class="fl">10.54</span>, <span class="dt">xend =</span> <span class="fl">0.065</span>, <span class="dt">yend =</span> <span class="fl">10.54</span>, </a>
<a class="sourceLine" id="cb904-6" data-line-number="6">               <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-633-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>In Section <a href="#predictor-space"><strong>??</strong></a> we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye,
we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than <span class="math inline">\(10.535\)</span>, predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-634-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Decision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:</p>
<p><img src="images/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin: auto;" /></p>
<p>(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>.)</p>
<p>A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>.
Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by partitioning the predictors.</p>
</div>
<div id="regression-trees" class="section level3">
<h3><span class="header-section-number">13.2.3</span> Regression trees</h3>
<p>When the outcome is continuous, we call the method a <em>regression</em> tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation <span class="math inline">\(f(x) = \mbox{E}(Y | X = x)\)</span> with <span class="math inline">\(Y\)</span> the poll margin and <span class="math inline">\(x\)</span> the day.</p>
<div class="sourceCode" id="cb905"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb905-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;polls_2008&quot;</span>)</a>
<a class="sourceLine" id="cb905-2" data-line-number="2"><span class="kw">qplot</span>(day, margin, <span class="dt">data =</span> polls_<span class="dv">2008</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-636-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. A mathematical way to describe this is to say that we are partitioning the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations <span class="math inline">\(y_i\)</span> for which the associated predictor <span class="math inline">\(x_i\)</span> is also in <span class="math inline">\(R_j\)</span>.</p>
<p>But how do we decide on the partition <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and how do we choose <span class="math inline">\(J\)</span>? Here is where the algorithm gets a bit complicated.</p>
<p>Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later.</p>
<p>Once we select a partition <span class="math inline">\(\mathbf{x}\)</span> to split in order to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, which we will call <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>In our current example we only have one predictor, so we will always choose <span class="math inline">\(j=1\)</span>, but in general this will not be the case. Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning, we compute predictors by taking the average of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>But how do we pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>? Basically we find the pair that minimizes the residual sum of square (RSS):
<span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>This is then applied recursively to the new regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the <code>rpart</code> function in the <strong>rpart</strong> package.</p>
<div class="sourceCode" id="cb906"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb906-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb906-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>)</a></code></pre></div>
<p>Here, there is only one predictor. Thus we do not have to decide which predictor <span class="math inline">\(j\)</span> to split by, we simply have to decide what value <span class="math inline">\(s\)</span> we use to split. We can visually see where the splits were made:</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb907-1" data-line-number="1"><span class="kw">plot</span>(fit, <span class="dt">margin =</span> <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb907-2" data-line-number="2"><span class="kw">text</span>(fit, <span class="dt">cex =</span> <span class="fl">0.75</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-639-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate <span class="math inline">\(\hat{f}(x)\)</span> looks like this:</p>
<div class="sourceCode" id="cb908"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb908-1" data-line-number="1">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb908-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb908-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb908-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></a>
<a class="sourceLine" id="cb908-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-640-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made.</p>
<p>First we need to define the term <em>complexity parameter</em> (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the <em>complexity parameter</em> (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.</p>
<p>However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the <code>rpart</code> function is <code>minsplit</code> and the default is 20. The <code>rpart</code> implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is <code>minbucket</code> and defaults to <code>round(minsplit/3)</code>.</p>
<p>As expected, if we set <code>cp = 0</code> and <code>minsplit = 2</code>, then our prediction is as flexible as possible and our predictor is our original data:</p>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb909-1" data-line-number="1">fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>, </a>
<a class="sourceLine" id="cb909-2" data-line-number="2">             <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">minsplit =</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb909-3" data-line-number="3">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb909-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb909-5" data-line-number="5"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb909-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></a>
<a class="sourceLine" id="cb909-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-641-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Intuitively we know that this is not a good approach as it will generally result in over-training. These <code>cp</code>, <code>minsplit</code>, and <code>minbucket</code>, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.</p>
<p>So how do we pick these parameters? We can use cross validation, described in Chapter <a href="14-machine.html#cross-validation">14.6</a>, just like with any tuning parameter. Here is an example of using cross validation to chose cp.</p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb910-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb910-2" data-line-number="2">train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(margin <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb910-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb910-4" data-line-number="4">                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="fl">0.05</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</a>
<a class="sourceLine" id="cb910-5" data-line-number="5">                     <span class="dt">data =</span> polls_<span class="dv">2008</span>)</a>
<a class="sourceLine" id="cb910-6" data-line-number="6"><span class="kw">ggplot</span>(train_rpart)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-642-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>To see the resulting tree, we access the <code>finalModel</code> and plot it:</p>
<div class="sourceCode" id="cb911"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb911-1" data-line-number="1"><span class="kw">plot</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">margin =</span> <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb911-2" data-line-number="2"><span class="kw">text</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">cex =</span> <span class="fl">0.75</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-644-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>And because we only have one predictor, we can actually plot <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb912-1" data-line-number="1">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb912-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(train_rpart)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb912-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb912-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></a>
<a class="sourceLine" id="cb912-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-645-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Note that if we already have a tree and want to apply a higher cp value, we can use the <code>prune</code> function. We call this <em>pruning</em> a tree because we are snipping off partitions that do not meet a <code>cp</code> criterion. We previously created a tree that used a <code>cp = 0</code> and saved it to <code>fit</code>. We can prune it like this:</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb913-1" data-line-number="1">pruned_fit &lt;-<span class="st"> </span><span class="kw">prune</span>(fit, <span class="dt">cp =</span> <span class="fl">0.01</span>)</a></code></pre></div>
</div>
<div id="classification-decision-trees" class="section level3">
<h3><span class="header-section-number">13.2.4</span> Classification (decision) trees</h3>
<p>Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.</p>
<p>The first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).</p>
<p>The second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the <em>Gini Index</em> and <em>Entropy</em>.</p>
<p>In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The <em>Gini Index</em> is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define <span class="math inline">\(\hat{p}_{j,k}\)</span> as the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. The Gini Index is defined as</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.</p>
<p><em>Entropy</em> is a very similar quantity, defined as</p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<p>Let us look at how a classification tree performs on the digits example we examined before:</p>
<p>We can use this code to run the algorithm and plot the resulting tree:</p>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb914-1" data-line-number="1">train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb914-2" data-line-number="2">                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb914-3" data-line-number="3">                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</a>
<a class="sourceLine" id="cb914-4" data-line-number="4">                     <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</a>
<a class="sourceLine" id="cb914-5" data-line-number="5"><span class="kw">plot</span>(train_rpart)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-649-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>The accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods:</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb915-1" data-line-number="1">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_rpart, mnist_<span class="dv">27</span><span class="op">$</span>test)</a>
<a class="sourceLine" id="cb915-2" data-line-number="2"><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</a></code></pre></div>
<pre><code>Accuracy 
    0.82 </code></pre>
<p>The plot of the estimated conditional probability shows us the limitations of classification trees:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-651-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">13.3</span> Random forests</h2>
<!-- AR: don't bother doing one categorical, one numerical here -->
<p>Random forests are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<p>1. Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p>
<p>2. For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p>
<p>3. For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<p>1. Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p>
<p>2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p>
<p>To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to
the 2008 polls data. We will use the <code>randomForest</code> function in the <strong>randomForest</strong> package:</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb917-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb917-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">randomForest</span>(margin<span class="op">~</span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>) </a></code></pre></div>
<p>Note that if we apply the function <code>plot</code> to the resulting object, stored in <code>fit</code>, we see how the error rate of our algorithm changes as we add trees.</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb918-1" data-line-number="1">rafalib<span class="op">::</span><span class="kw">mypar</span>()</a>
<a class="sourceLine" id="cb918-2" data-line-number="2"><span class="kw">plot</span>(fit)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-654-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.</p>
<p>The resulting estimate for this random forest can be seen like this:</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb919-1" data-line-number="1">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb919-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> polls_<span class="dv">2008</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb919-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb919-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></a>
<a class="sourceLine" id="cb919-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-655-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of <span class="math inline">\(b\)</span> and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.</p>
<p><img src="images/rf.gif" width="100%" style="display: block; margin: auto;" /></p>
<p>Here is the random forest fit for our digits example based on two predictors:</p>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb920-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb920-2" data-line-number="2">train_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train)</a>
<a class="sourceLine" id="cb920-3" data-line-number="3"></a>
<a class="sourceLine" id="cb920-4" data-line-number="4"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf, mnist_<span class="dv">27</span><span class="op">$</span>test),</a>
<a class="sourceLine" id="cb920-5" data-line-number="5">                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</a></code></pre></div>
<pre><code>Accuracy 
    0.79 </code></pre>
<p>Here is what the conditional probabilities look like:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-658-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the <strong>caret</strong> package to optimize over the minimum node size. Because, this is not one of the parameters that the <strong>caret</strong> package optimizes by default we will write our own code:</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb922-1" data-line-number="1">nodesize &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">51</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb922-2" data-line-number="2">acc &lt;-<span class="st"> </span><span class="kw">sapply</span>(nodesize, <span class="cf">function</span>(ns){</a>
<a class="sourceLine" id="cb922-3" data-line-number="3">  <span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train,</a>
<a class="sourceLine" id="cb922-4" data-line-number="4">               <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">mtry =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb922-5" data-line-number="5">               <span class="dt">nodesize =</span> ns)<span class="op">$</span>results<span class="op">$</span>Accuracy</a>
<a class="sourceLine" id="cb922-6" data-line-number="6">})</a>
<a class="sourceLine" id="cb922-7" data-line-number="7"><span class="kw">qplot</span>(nodesize, acc)</a></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-659-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data.</p>
<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb923-1" data-line-number="1">train_rf_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train,</a>
<a class="sourceLine" id="cb923-2" data-line-number="2">                           <span class="dt">nodesize =</span> nodesize[<span class="kw">which.max</span>(acc)])</a>
<a class="sourceLine" id="cb923-3" data-line-number="3"></a>
<a class="sourceLine" id="cb923-4" data-line-number="4"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf_<span class="dv">2</span>, mnist_<span class="dv">27</span><span class="op">$</span>test),</a>
<a class="sourceLine" id="cb923-5" data-line-number="5">                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</a></code></pre></div>
<pre><code>Accuracy 
   0.815 </code></pre>
<p>The selected model improves accuracy and provides a smoother estimate.</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-661-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that we can avoid writing our own code by using other random forest implementations as described in the <strong>caret</strong> manual<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>.</p>
<p>Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine <em>variable importance</em>.
To define <em>variable importance</em> we count how often a predictor is used in the individual trees. You can learn more about <em>variable importance</em> in an advanced machine learning book<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>. The <strong>caret</strong> package includes the function <code>varImp</code> that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.</p>
<div id="references" class="section level3">
<h3><span class="header-section-number">13.3.1</span> References</h3>
<p>Centers for Disease Control and Prevention. 2009. “Youth Risk Behavior Survey Data.” <a href="http://www.cdc.gov/HealthyYouth/yrbs/index.htm" class="uri">http://www.cdc.gov/HealthyYouth/yrbs/index.htm</a>.</p>
<p>Grabe, Shelly, Janet Shibley Hyde, and L. Moniquee Ward. 2008. “The Role of the Media in Body Image Concerns Among Women: A Meta-Analysis of Experimental and Correlational Studies.” Pyschological Bulletin 134 (3): 460–76. <a href="doi:10.1037/0033-2909.134.3.460" class="uri">doi:10.1037/0033-2909.134.3.460</a>.</p>
<p>Martinsen, M, S Bratland-Sanda, A K Eriksson, and J Sundgot-Borgen. 2009. “Dieting to Win or to Be Thin? A Study of Dieting and Disordered Eating Among Adolescent Elite Athletes and Non-Athlete Controls.” British Journal of Sports Medicine 44 (1): 70–76. <a href="doi:10.1136/bjsm.2009.068668" class="uri">doi:10.1136/bjsm.2009.068668</a>.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>It is rare that third party candidates mount serious bids in U.S. House bids, so it isn’t a much of an oversimplication to think of the variable as binary.<a href="13-classification.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p><a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2" class="uri">https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2</a><a href="13-classification.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p><a href="http://topepo.github.io/caret/available-models.html" class="uri">http://topepo.github.io/caret/available-models.html</a><a href="13-classification.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a><a href="13-classification.html#fnref24" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-machine.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
