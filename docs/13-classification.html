<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science</title>
  <meta name="description" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="davidkane9/PPBDS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  
  
  

<meta name="author" content="David Kane" />


<meta name="date" content="2020-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12-multiple-regression.html"/>
<link rel="next" href="14-machine.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="forward.html"><a href="forward.html"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="warning.html"><a href="warning.html"><i class="fa fa-check"></i>Warning</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="1" data-path="1-getting-started.html"><a href="1-getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#r-rstudio"><i class="fa fa-check"></i><b>1.1</b> What are R and RStudio?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#installing"><i class="fa fa-check"></i><b>1.1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#using-r-via-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Using R via RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#code"><i class="fa fa-check"></i><b>1.2</b> How do I code in R?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-getting-started.html"><a href="1-getting-started.html#programming-concepts"><i class="fa fa-check"></i><b>1.2.1</b> Basic programming concepts and terminology</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-getting-started.html"><a href="1-getting-started.html#messages"><i class="fa fa-check"></i><b>1.2.2</b> Errors, warnings, and messages</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-getting-started.html"><a href="1-getting-started.html#tips-code"><i class="fa fa-check"></i><b>1.2.3</b> Tips on learning to code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-getting-started.html"><a href="1-getting-started.html#packages"><i class="fa fa-check"></i><b>1.3</b> What are R packages?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-getting-started.html"><a href="1-getting-started.html#package-installation"><i class="fa fa-check"></i><b>1.3.1</b> Package installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-getting-started.html"><a href="1-getting-started.html#package-loading"><i class="fa fa-check"></i><b>1.3.2</b> Package loading</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-getting-started.html"><a href="1-getting-started.html#package-use"><i class="fa fa-check"></i><b>1.3.3</b> Package use</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13"><i class="fa fa-check"></i><b>1.4</b> Explore your first datasets</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13-package"><i class="fa fa-check"></i><b>1.4.1</b> <code>nycflights13</code> package</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-getting-started.html"><a href="1-getting-started.html#flights-data-frame"><i class="fa fa-check"></i><b>1.4.2</b> <code>flights</code> data frame</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-getting-started.html"><a href="1-getting-started.html#exploredataframes"><i class="fa fa-check"></i><b>1.4.3</b> Exploring data frames</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-getting-started.html"><a href="1-getting-started.html#identification-vs-measurement-variables"><i class="fa fa-check"></i><b>1.4.4</b> Identification and measurement variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-getting-started.html"><a href="1-getting-started.html#help-files"><i class="fa fa-check"></i><b>1.4.5</b> Help files</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-getting-started.html"><a href="1-getting-started.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-getting-started.html"><a href="1-getting-started.html#additional-resources"><i class="fa fa-check"></i><b>1.5.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-viz.html"><a href="2-viz.html"><i class="fa fa-check"></i><b>2</b> Visualization</a><ul>
<li class="chapter" data-level="" data-path="2-viz.html"><a href="2-viz.html#needed-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="2.1" data-path="2-viz.html"><a href="2-viz.html#grammarofgraphics"><i class="fa fa-check"></i><b>2.1</b> The grammar of graphics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-viz.html"><a href="2-viz.html#components-of-the-grammar"><i class="fa fa-check"></i><b>2.1.1</b> Components of the grammar</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-viz.html"><a href="2-viz.html#gapminder"><i class="fa fa-check"></i><b>2.1.2</b> Gapminder data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-viz.html"><a href="2-viz.html#other-components"><i class="fa fa-check"></i><b>2.1.3</b> Other components</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-viz.html"><a href="2-viz.html#ggplot2-package"><i class="fa fa-check"></i><b>2.1.4</b> ggplot2 package</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-viz.html"><a href="2-viz.html#scatterplots"><i class="fa fa-check"></i><b>2.2</b> Scatterplots</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-viz.html"><a href="2-viz.html#geompoint"><i class="fa fa-check"></i><b>2.2.1</b> Scatterplots via <code>geom_point</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="2-viz.html"><a href="2-viz.html#overplotting"><i class="fa fa-check"></i><b>2.2.2</b> Overplotting</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-viz.html"><a href="2-viz.html#summary"><i class="fa fa-check"></i><b>2.2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-viz.html"><a href="2-viz.html#linegraphs"><i class="fa fa-check"></i><b>2.3</b> Linegraphs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-viz.html"><a href="2-viz.html#geomline"><i class="fa fa-check"></i><b>2.3.1</b> Linegraphs via <code>geom_line</code></a></li>
<li class="chapter" data-level="2.3.2" data-path="2-viz.html"><a href="2-viz.html#summary-1"><i class="fa fa-check"></i><b>2.3.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-viz.html"><a href="2-viz.html#histograms"><i class="fa fa-check"></i><b>2.4</b> Histograms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-viz.html"><a href="2-viz.html#geomhistogram"><i class="fa fa-check"></i><b>2.4.1</b> Histograms via <code>geom_histogram</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="2-viz.html"><a href="2-viz.html#adjustbins"><i class="fa fa-check"></i><b>2.4.2</b> Adjusting the bins</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-viz.html"><a href="2-viz.html#summary-2"><i class="fa fa-check"></i><b>2.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-viz.html"><a href="2-viz.html#facets"><i class="fa fa-check"></i><b>2.5</b> Facets</a></li>
<li class="chapter" data-level="2.6" data-path="2-viz.html"><a href="2-viz.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-viz.html"><a href="2-viz.html#geomboxplot"><i class="fa fa-check"></i><b>2.6.1</b> Boxplots via <code>geom_boxplot</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="2-viz.html"><a href="2-viz.html#summary-3"><i class="fa fa-check"></i><b>2.6.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-viz.html"><a href="2-viz.html#geombar"><i class="fa fa-check"></i><b>2.7</b> Barplots</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-viz.html"><a href="2-viz.html#barplots-via-geom_bar-or-geom_col"><i class="fa fa-check"></i><b>2.7.1</b> Barplots via <code>geom_bar</code> or <code>geom_col</code></a></li>
<li class="chapter" data-level="2.7.2" data-path="2-viz.html"><a href="2-viz.html#must-avoid-pie-charts"><i class="fa fa-check"></i><b>2.7.2</b> Must avoid pie charts!</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-viz.html"><a href="2-viz.html#two-categ-barplot"><i class="fa fa-check"></i><b>2.7.3</b> Two categorical variables</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-viz.html"><a href="2-viz.html#summary-4"><i class="fa fa-check"></i><b>2.7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="2-viz.html"><a href="2-viz.html#conclusion-1"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a><ul>
<li class="chapter" data-level="2.8.1" data-path="2-viz.html"><a href="2-viz.html#summary-table"><i class="fa fa-check"></i><b>2.8.1</b> Summary table</a></li>
<li class="chapter" data-level="2.8.2" data-path="2-viz.html"><a href="2-viz.html#function-argument-specification"><i class="fa fa-check"></i><b>2.8.2</b> Function argument specification</a></li>
<li class="chapter" data-level="2.8.3" data-path="2-viz.html"><a href="2-viz.html#additional-resources-1"><i class="fa fa-check"></i><b>2.8.3</b> Additional resources</a></li>
<li class="chapter" data-level="2.8.4" data-path="2-viz.html"><a href="2-viz.html#whats-to-come-3"><i class="fa fa-check"></i><b>2.8.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-productivity.html"><a href="3-productivity.html"><i class="fa fa-check"></i><b>3</b> Productivity</a><ul>
<li class="chapter" data-level="3.1" data-path="3-productivity.html"><a href="3-productivity.html#set-up"><i class="fa fa-check"></i><b>3.1</b> Set Up</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-mac"><i class="fa fa-check"></i><b>3.1.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>3.1.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>3.1.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-windows"><i class="fa fa-check"></i><b>3.1.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-productivity.html"><a href="3-productivity.html#unix"><i class="fa fa-check"></i><b>3.2</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-productivity.html"><a href="3-productivity.html#naming-convention"><i class="fa fa-check"></i><b>3.2.1</b> Naming convention</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-productivity.html"><a href="3-productivity.html#the-terminal"><i class="fa fa-check"></i><b>3.2.2</b> The terminal</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-productivity.html"><a href="3-productivity.html#filesystem"><i class="fa fa-check"></i><b>3.2.3</b> The filesystem</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-productivity.html"><a href="3-productivity.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>3.2.4</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-productivity.html"><a href="3-productivity.html#the-home-directory"><i class="fa fa-check"></i><b>3.2.5</b> The home directory</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-productivity.html"><a href="3-productivity.html#working-directory"><i class="fa fa-check"></i><b>3.2.6</b> Working directory</a></li>
<li class="chapter" data-level="3.2.7" data-path="3-productivity.html"><a href="3-productivity.html#paths"><i class="fa fa-check"></i><b>3.2.7</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-productivity.html"><a href="3-productivity.html#unix-commands"><i class="fa fa-check"></i><b>3.3</b> Unix commands</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-productivity.html"><a href="3-productivity.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>3.3.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-productivity.html"><a href="3-productivity.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>3.3.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-productivity.html"><a href="3-productivity.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>3.3.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-productivity.html"><a href="3-productivity.html#some-examples"><i class="fa fa-check"></i><b>3.3.4</b> Some examples</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-productivity.html"><a href="3-productivity.html#more-unix-commands"><i class="fa fa-check"></i><b>3.3.5</b> More Unix commands</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-productivity.html"><a href="3-productivity.html#advanced-unix"><i class="fa fa-check"></i><b>3.3.6</b> Advanced Unix</a></li>
<li class="chapter" data-level="3.3.7" data-path="3-productivity.html"><a href="3-productivity.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>3.3.7</b> File manipulation in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-productivity.html"><a href="3-productivity.html#git"><i class="fa fa-check"></i><b>3.4</b> Git and GitHub</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-productivity.html"><a href="3-productivity.html#github-accounts"><i class="fa fa-check"></i><b>3.4.1</b> GitHub accounts</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-productivity.html"><a href="3-productivity.html#github-repos"><i class="fa fa-check"></i><b>3.4.2</b> GitHub repositories</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-productivity.html"><a href="3-productivity.html#git-overview"><i class="fa fa-check"></i><b>3.4.3</b> Overview of Git</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-git"><i class="fa fa-check"></i><b>3.4.4</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-productivity.html"><a href="3-productivity.html#r"><i class="fa fa-check"></i><b>3.5</b> R</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-projects"><i class="fa fa-check"></i><b>3.5.1</b> RStudio projects</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-productivity.html"><a href="3-productivity.html#r-markdown"><i class="fa fa-check"></i><b>3.5.2</b> R markdown</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-productivity.html"><a href="3-productivity.html#help-for-r"><i class="fa fa-check"></i><b>3.5.3</b> Help for R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-wrangling.html"><a href="4-wrangling.html"><i class="fa fa-check"></i><b>4</b> Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#piping"><i class="fa fa-check"></i><b>4.1</b> The pipe operator: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.2" data-path="4-wrangling.html"><a href="4-wrangling.html#filter"><i class="fa fa-check"></i><b>4.2</b> <code>filter</code> rows</a></li>
<li class="chapter" data-level="4.3" data-path="4-wrangling.html"><a href="4-wrangling.html#summarize"><i class="fa fa-check"></i><b>4.3</b> <code>summarize</code> variables</a></li>
<li class="chapter" data-level="4.4" data-path="4-wrangling.html"><a href="4-wrangling.html#groupby"><i class="fa fa-check"></i><b>4.4</b> <code>group_by</code> rows</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#grouping-by-more-than-one-variable"><i class="fa fa-check"></i><b>4.4.1</b> Grouping by more than one variable</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-wrangling.html"><a href="4-wrangling.html#mutate"><i class="fa fa-check"></i><b>4.5</b> <code>mutate</code> existing variables</a></li>
<li class="chapter" data-level="4.6" data-path="4-wrangling.html"><a href="4-wrangling.html#arrange"><i class="fa fa-check"></i><b>4.6</b> <code>arrange</code> and sort rows</a></li>
<li class="chapter" data-level="4.7" data-path="4-wrangling.html"><a href="4-wrangling.html#factors"><i class="fa fa-check"></i><b>4.7</b> Factors</a><ul>
<li class="chapter" data-level="4.7.1" data-path="4-wrangling.html"><a href="4-wrangling.html#the-forcats-package"><i class="fa fa-check"></i><b>4.7.1</b> The <strong>forcats</strong> package</a></li>
<li class="chapter" data-level="4.7.2" data-path="4-wrangling.html"><a href="4-wrangling.html#dropping-unused-levels"><i class="fa fa-check"></i><b>4.7.2</b> Dropping unused levels</a></li>
<li class="chapter" data-level="4.7.3" data-path="4-wrangling.html"><a href="4-wrangling.html#reorder-factors"><i class="fa fa-check"></i><b>4.7.3</b> Change order of the levels, principled</a></li>
<li class="chapter" data-level="4.7.4" data-path="4-wrangling.html"><a href="4-wrangling.html#change-order-of-the-levels-because-i-said-so"><i class="fa fa-check"></i><b>4.7.4</b> Change order of the levels, “because I said so”</a></li>
<li class="chapter" data-level="4.7.5" data-path="4-wrangling.html"><a href="4-wrangling.html#recode-the-levels"><i class="fa fa-check"></i><b>4.7.5</b> Recode the levels</a></li>
<li class="chapter" data-level="4.7.6" data-path="4-wrangling.html"><a href="4-wrangling.html#grow-a-factor"><i class="fa fa-check"></i><b>4.7.6</b> Grow a factor</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors"><i class="fa fa-check"></i><b>4.8</b> Character Vectors</a><ul>
<li class="chapter" data-level="4.8.1" data-path="4-wrangling.html"><a href="4-wrangling.html#manipulating-character-vectors"><i class="fa fa-check"></i><b>4.8.1</b> Manipulating character vectors</a></li>
<li class="chapter" data-level="4.8.2" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-resources"><i class="fa fa-check"></i><b>4.8.2</b> Regular expressions resources</a></li>
<li class="chapter" data-level="4.8.3" data-path="4-wrangling.html"><a href="4-wrangling.html#character-encoding-resources"><i class="fa fa-check"></i><b>4.8.3</b> Character encoding resources</a></li>
<li class="chapter" data-level="4.8.4" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors-that-live-in-a-data-frame"><i class="fa fa-check"></i><b>4.8.4</b> Character vectors that live in a data frame</a></li>
<li class="chapter" data-level="4.8.5" data-path="4-wrangling.html"><a href="4-wrangling.html#regex-free-string-manipulation-with-stringr-and-tidyr"><i class="fa fa-check"></i><b>4.8.5</b> Regex-free string manipulation with stringr and tidyr</a></li>
<li class="chapter" data-level="4.8.6" data-path="4-wrangling.html"><a href="4-wrangling.html#detect-or-filter-on-a-target-string"><i class="fa fa-check"></i><b>4.8.6</b> Detect or filter on a target string</a></li>
<li class="chapter" data-level="4.8.7" data-path="4-wrangling.html"><a href="4-wrangling.html#string-splitting-by-delimiter"><i class="fa fa-check"></i><b>4.8.7</b> String splitting by delimiter</a></li>
<li class="chapter" data-level="4.8.8" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-extraction-and-replacement-by-position"><i class="fa fa-check"></i><b>4.8.8</b> Substring extraction (and replacement) by position</a></li>
<li class="chapter" data-level="4.8.9" data-path="4-wrangling.html"><a href="4-wrangling.html#collapse-a-vector"><i class="fa fa-check"></i><b>4.8.9</b> Collapse a vector</a></li>
<li class="chapter" data-level="4.8.10" data-path="4-wrangling.html"><a href="4-wrangling.html#catenate-vectors"><i class="fa fa-check"></i><b>4.8.10</b> Create a character vector by catenating multiple vectors</a></li>
<li class="chapter" data-level="4.8.11" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-replacement"><i class="fa fa-check"></i><b>4.8.11</b> Substring replacement</a></li>
<li class="chapter" data-level="4.8.12" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-with-stringr"><i class="fa fa-check"></i><b>4.8.12</b> Regular expressions with stringr</a></li>
<li class="chapter" data-level="4.8.13" data-path="4-wrangling.html"><a href="4-wrangling.html#characters-with-special-meaning"><i class="fa fa-check"></i><b>4.8.13</b> Characters with special meaning</a></li>
<li class="chapter" data-level="4.8.14" data-path="4-wrangling.html"><a href="4-wrangling.html#character-classes"><i class="fa fa-check"></i><b>4.8.14</b> Character classes</a></li>
<li class="chapter" data-level="4.8.15" data-path="4-wrangling.html"><a href="4-wrangling.html#quantifiers"><i class="fa fa-check"></i><b>4.8.15</b> Quantifiers</a></li>
<li class="chapter" data-level="4.8.16" data-path="4-wrangling.html"><a href="4-wrangling.html#escaping"><i class="fa fa-check"></i><b>4.8.16</b> Escaping</a></li>
<li class="chapter" data-level="4.8.17" data-path="4-wrangling.html"><a href="4-wrangling.html#groups-and-backreferences"><i class="fa fa-check"></i><b>4.8.17</b> Groups and backreferences</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="4-wrangling.html"><a href="4-wrangling.html#combining-data"><i class="fa fa-check"></i><b>4.9</b> Combining Data</a><ul>
<li class="chapter" data-level="4.9.1" data-path="4-wrangling.html"><a href="4-wrangling.html#bind"><i class="fa fa-check"></i><b>4.9.1</b> Bind</a></li>
<li class="chapter" data-level="4.9.2" data-path="4-wrangling.html"><a href="4-wrangling.html#joins-in-dplyr"><i class="fa fa-check"></i><b>4.9.2</b> Joins in dplyr</a></li>
<li class="chapter" data-level="4.9.3" data-path="4-wrangling.html"><a href="4-wrangling.html#joining"><i class="fa fa-check"></i><b>4.9.3</b> Joining</a></li>
<li class="chapter" data-level="4.9.4" data-path="4-wrangling.html"><a href="4-wrangling.html#matching-key-variable-names"><i class="fa fa-check"></i><b>4.9.4</b> Matching “key” variable names</a></li>
<li class="chapter" data-level="4.9.5" data-path="4-wrangling.html"><a href="4-wrangling.html#diff-key"><i class="fa fa-check"></i><b>4.9.5</b> Different “key” variable names</a></li>
<li class="chapter" data-level="4.9.6" data-path="4-wrangling.html"><a href="4-wrangling.html#multiple-key-variables"><i class="fa fa-check"></i><b>4.9.6</b> Multiple “key” variables</a></li>
<li class="chapter" data-level="4.9.7" data-path="4-wrangling.html"><a href="4-wrangling.html#normal-forms"><i class="fa fa-check"></i><b>4.9.7</b> Normal forms</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="4-wrangling.html"><a href="4-wrangling.html#other-verbs"><i class="fa fa-check"></i><b>4.10</b> Other Verbs</a><ul>
<li class="chapter" data-level="4.10.1" data-path="4-wrangling.html"><a href="4-wrangling.html#select"><i class="fa fa-check"></i><b>4.10.1</b> <code>select</code> variables</a></li>
<li class="chapter" data-level="4.10.2" data-path="4-wrangling.html"><a href="4-wrangling.html#rename"><i class="fa fa-check"></i><b>4.10.2</b> <code>rename</code> variables</a></li>
<li class="chapter" data-level="4.10.3" data-path="4-wrangling.html"><a href="4-wrangling.html#top_n-values-of-a-variable"><i class="fa fa-check"></i><b>4.10.3</b> <code>top_n</code> values of a variable</a></li>
<li class="chapter" data-level="4.10.4" data-path="4-wrangling.html"><a href="4-wrangling.html#slice-and-pull-and"><i class="fa fa-check"></i><b>4.10.4</b> <code>slice</code> and <code>pull</code> and <code>[]</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="4-wrangling.html"><a href="4-wrangling.html#conclusion-2"><i class="fa fa-check"></i><b>4.11</b> Conclusion</a><ul>
<li class="chapter" data-level="4.11.1" data-path="4-wrangling.html"><a href="4-wrangling.html#summary-table-1"><i class="fa fa-check"></i><b>4.11.1</b> Summary table</a></li>
<li class="chapter" data-level="4.11.2" data-path="4-wrangling.html"><a href="4-wrangling.html#additional-resources-2"><i class="fa fa-check"></i><b>4.11.2</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-tidy.html"><a href="5-tidy.html"><i class="fa fa-check"></i><b>5</b> Tidy</a><ul>
<li class="chapter" data-level="5.1" data-path="5-tidy.html"><a href="5-tidy.html#csv"><i class="fa fa-check"></i><b>5.1</b> Importing data</a></li>
<li class="chapter" data-level="5.2" data-path="5-tidy.html"><a href="5-tidy.html#web-scraping"><i class="fa fa-check"></i><b>5.2</b> Web scraping</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-tidy.html"><a href="5-tidy.html#html"><i class="fa fa-check"></i><b>5.2.1</b> HTML</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-tidy.html"><a href="5-tidy.html#the-rvest-package"><i class="fa fa-check"></i><b>5.2.2</b> The rvest package</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-tidy.html"><a href="5-tidy.html#css-selectors"><i class="fa fa-check"></i><b>5.2.3</b> CSS selectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-tidy.html"><a href="5-tidy.html#json"><i class="fa fa-check"></i><b>5.2.4</b> JSON</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-tidy.html"><a href="5-tidy.html#tidy-data-ex"><i class="fa fa-check"></i><b>5.3</b> “Tidy” data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-tidy.html"><a href="5-tidy.html#tidy-definition"><i class="fa fa-check"></i><b>5.3.1</b> Definition of “tidy” data</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-tidy.html"><a href="5-tidy.html#converting-to-tidy-data"><i class="fa fa-check"></i><b>5.3.2</b> Converting to “tidy” data</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-tidy.html"><a href="5-tidy.html#nycflights13-package-1"><i class="fa fa-check"></i><b>5.3.3</b> <code>nycflights13</code> package</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-tidy.html"><a href="5-tidy.html#case-study-tidy"><i class="fa fa-check"></i><b>5.4</b> Case study: Democracy in Guatemala</a></li>
<li class="chapter" data-level="5.5" data-path="5-tidy.html"><a href="5-tidy.html#tidyverse-package"><i class="fa fa-check"></i><b>5.5</b> <code>tidyverse</code> package</a></li>
<li class="chapter" data-level="5.6" data-path="5-tidy.html"><a href="5-tidy.html#conclusion-3"><i class="fa fa-check"></i><b>5.6</b> Conclusion</a><ul>
<li class="chapter" data-level="5.6.1" data-path="5-tidy.html"><a href="5-tidy.html#additional-resources-3"><i class="fa fa-check"></i><b>5.6.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-functions.html"><a href="6-functions.html"><i class="fa fa-check"></i><b>6</b> Functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-functions.html"><a href="6-functions.html#part-1"><i class="fa fa-check"></i><b>6.1</b> Part 1</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works"><i class="fa fa-check"></i><b>6.1.1</b> Get something that works</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-functions.html"><a href="6-functions.html#skateboard-perfectly-formed-rear-view-mirror"><i class="fa fa-check"></i><b>6.1.2</b> Skateboard &gt;&gt; perfectly formed rear-view mirror</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function"><i class="fa fa-check"></i><b>6.1.3</b> Turn the working interactive code into a function</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-functions.html"><a href="6-functions.html#test-your-function"><i class="fa fa-check"></i><b>6.1.4</b> Test your function</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-functions.html"><a href="6-functions.html#check-the-validity-of-arguments"><i class="fa fa-check"></i><b>6.1.5</b> Check the validity of arguments</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next"><i class="fa fa-check"></i><b>6.1.6</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-functions.html"><a href="6-functions.html#part-2"><i class="fa fa-check"></i><b>6.2</b> Part 2</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-functions.html"><a href="6-functions.html#resources"><i class="fa fa-check"></i><b>6.2.1</b> Resources</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-functions.html"><a href="6-functions.html#where-were-we-where-are-we-going"><i class="fa fa-check"></i><b>6.2.2</b> Where were we? Where are we going?</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function"><i class="fa fa-check"></i><b>6.2.3</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-functions.html"><a href="6-functions.html#generalize-our-function-to-other-quantiles"><i class="fa fa-check"></i><b>6.2.4</b> Generalize our function to other quantiles</a></li>
<li class="chapter" data-level="6.2.5" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works-again"><i class="fa fa-check"></i><b>6.2.5</b> Get something that works, again</a></li>
<li class="chapter" data-level="6.2.6" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function-again"><i class="fa fa-check"></i><b>6.2.6</b> Turn the working interactive code into a function, again</a></li>
<li class="chapter" data-level="6.2.7" data-path="6-functions.html"><a href="6-functions.html#argument-names-freedom-and-conventions"><i class="fa fa-check"></i><b>6.2.7</b> Argument names: freedom and conventions</a></li>
<li class="chapter" data-level="6.2.8" data-path="6-functions.html"><a href="6-functions.html#what-a-function-returns"><i class="fa fa-check"></i><b>6.2.8</b> What a function returns</a></li>
<li class="chapter" data-level="6.2.9" data-path="6-functions.html"><a href="6-functions.html#default-values-freedom-to-not-specify-the-arguments"><i class="fa fa-check"></i><b>6.2.9</b> Default values: freedom to NOT specify the arguments</a></li>
<li class="chapter" data-level="6.2.10" data-path="6-functions.html"><a href="6-functions.html#check-the-validity-of-arguments-again"><i class="fa fa-check"></i><b>6.2.10</b> Check the validity of arguments, again</a></li>
<li class="chapter" data-level="6.2.11" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next-1"><i class="fa fa-check"></i><b>6.2.11</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-functions.html"><a href="6-functions.html#part-3"><i class="fa fa-check"></i><b>6.3</b> Part 3</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-functions.html"><a href="6-functions.html#resources-1"><i class="fa fa-check"></i><b>6.3.1</b> Resources</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-functions.html"><a href="6-functions.html#where-were-we-where-are-we-going-1"><i class="fa fa-check"></i><b>6.3.2</b> Where were we? Where are we going?</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-functions.html"><a href="6-functions.html#load-the-gapminder-data"><i class="fa fa-check"></i><b>6.3.3</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function-1"><i class="fa fa-check"></i><b>6.3.4</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.3.5" data-path="6-functions.html"><a href="6-functions.html#be-proactive-about-nas"><i class="fa fa-check"></i><b>6.3.5</b> Be proactive about <code>NA</code>s</a></li>
<li class="chapter" data-level="6.3.6" data-path="6-functions.html"><a href="6-functions.html#the-useful-but-mysterious-...-argument"><i class="fa fa-check"></i><b>6.3.6</b> The useful but mysterious <code>...</code> argument</a></li>
<li class="chapter" data-level="6.3.7" data-path="6-functions.html"><a href="6-functions.html#use-testthat-for-formal-unit-tests"><i class="fa fa-check"></i><b>6.3.7</b> Use testthat for formal unit tests</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-functions.html"><a href="6-functions.html#list-columns-and-map_-functions"><i class="fa fa-check"></i><b>6.4</b> List columns and <code>map_*</code> functions</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-functions.html"><a href="6-functions.html#what-are-list-columns"><i class="fa fa-check"></i><b>6.4.1</b> What are list columns?</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-functions.html"><a href="6-functions.html#creating-list-columns-with-mutate"><i class="fa fa-check"></i><b>6.4.2</b> Creating list columns with <code>mutate()</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="6-functions.html"><a href="6-functions.html#map_-functions"><i class="fa fa-check"></i><b>6.4.3</b> <code>map_*</code> functions</a></li>
<li class="chapter" data-level="6.4.4" data-path="6-functions.html"><a href="6-functions.html#using-map_-functions-to-create-list-columns"><i class="fa fa-check"></i><b>6.4.4</b> Using <code>map_*</code> functions to create list columns</a></li>
<li class="chapter" data-level="6.4.5" data-path="6-functions.html"><a href="6-functions.html#practice-with-map_-functions-and-list-columns"><i class="fa fa-check"></i><b>6.4.5</b> Practice with <code>map_*</code> functions and list columns</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-functions.html"><a href="6-functions.html#resources-2"><i class="fa fa-check"></i><b>6.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probability.html"><a href="7-probability.html"><i class="fa fa-check"></i><b>7</b> Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probability.html"><a href="7-probability.html#basicsOfProbability"><i class="fa fa-check"></i><b>7.1</b> Defining probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-probability.html"><a href="7-probability.html#intro-questions"><i class="fa fa-check"></i><b>7.1.1</b> Intro Questions</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-probability.html"><a href="7-probability.html#probability-1"><i class="fa fa-check"></i><b>7.1.2</b> Probability</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-probability.html"><a href="7-probability.html#disjoint-or-mutually-exclusive-outcomes"><i class="fa fa-check"></i><b>7.1.3</b> Disjoint or mutually exclusive outcomes</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-probability.html"><a href="7-probability.html#probabilities-when-events-are-not-disjoint"><i class="fa fa-check"></i><b>7.1.4</b> Probabilities when events are not disjoint</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-probability.html"><a href="7-probability.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.5</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.6" data-path="7-probability.html"><a href="7-probability.html#complement-of-an-event"><i class="fa fa-check"></i><b>7.1.6</b> Complement of an event</a></li>
<li class="chapter" data-level="7.1.7" data-path="7-probability.html"><a href="7-probability.html#probabilityIndependence"><i class="fa fa-check"></i><b>7.1.7</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-probability.html"><a href="7-probability.html#conditionalProbabilitySection"><i class="fa fa-check"></i><b>7.2</b> Conditional probability</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-probability.html"><a href="7-probability.html#marginalAndJointProbabilities"><i class="fa fa-check"></i><b>7.2.1</b> Marginal and joint probabilities</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-probability.html"><a href="7-probability.html#defining-conditional-probability"><i class="fa fa-check"></i><b>7.2.2</b> Defining conditional probability</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-probability.html"><a href="7-probability.html#smallpox-in-boston-1721"><i class="fa fa-check"></i><b>7.2.3</b> Smallpox in Boston, 1721</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-probability.html"><a href="7-probability.html#general-multiplication-rule"><i class="fa fa-check"></i><b>7.2.4</b> General multiplication rule</a></li>
<li class="chapter" data-level="7.2.5" data-path="7-probability.html"><a href="7-probability.html#independence-considerations-in-conditional-probability"><i class="fa fa-check"></i><b>7.2.5</b> Independence considerations in conditional probability</a></li>
<li class="chapter" data-level="7.2.6" data-path="7-probability.html"><a href="7-probability.html#tree-diagrams"><i class="fa fa-check"></i><b>7.2.6</b> Tree diagrams</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-probability.html"><a href="7-probability.html#randomVariablesSection"><i class="fa fa-check"></i><b>7.3</b> Random variables</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-probability.html"><a href="7-probability.html#expectation"><i class="fa fa-check"></i><b>7.3.1</b> Expectation</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-probability.html"><a href="7-probability.html#variability-in-random-variables"><i class="fa fa-check"></i><b>7.3.2</b> Variability in random variables</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-probability.html"><a href="7-probability.html#linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.3.3</b> Linear combinations of random variables</a></li>
<li class="chapter" data-level="7.3.4" data-path="7-probability.html"><a href="7-probability.html#variability-in-linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.3.4</b> Variability in linear combinations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-probability.html"><a href="7-probability.html#appendixA"><i class="fa fa-check"></i><b>7.4</b> Statistical Background</a></li>
<li class="chapter" data-level="7.5" data-path="7-probability.html"><a href="7-probability.html#appendix-stat-terms"><i class="fa fa-check"></i><b>7.5</b> Basic statistical terms</a><ul>
<li class="chapter" data-level="7.5.1" data-path="7-probability.html"><a href="7-probability.html#mean"><i class="fa fa-check"></i><b>7.5.1</b> Mean</a></li>
<li class="chapter" data-level="7.5.2" data-path="7-probability.html"><a href="7-probability.html#median"><i class="fa fa-check"></i><b>7.5.2</b> Median</a></li>
<li class="chapter" data-level="7.5.3" data-path="7-probability.html"><a href="7-probability.html#standard-deviation"><i class="fa fa-check"></i><b>7.5.3</b> Standard deviation</a></li>
<li class="chapter" data-level="7.5.4" data-path="7-probability.html"><a href="7-probability.html#five-number-summary"><i class="fa fa-check"></i><b>7.5.4</b> Five-number summary</a></li>
<li class="chapter" data-level="7.5.5" data-path="7-probability.html"><a href="7-probability.html#distribution"><i class="fa fa-check"></i><b>7.5.5</b> Distribution</a></li>
<li class="chapter" data-level="7.5.6" data-path="7-probability.html"><a href="7-probability.html#outliers"><i class="fa fa-check"></i><b>7.5.6</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7-probability.html"><a href="7-probability.html#appendix-normal-curve"><i class="fa fa-check"></i><b>7.6</b> Normal distribution</a></li>
<li class="chapter" data-level="7.7" data-path="7-probability.html"><a href="7-probability.html#appendix-log10-transformations"><i class="fa fa-check"></i><b>7.7</b> log10 transformations</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html"><i class="fa fa-check"></i><b>8</b> Bayes’s Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conditional-probability"><i class="fa fa-check"></i><b>8.1</b> Conditional probability</a></li>
<li class="chapter" data-level="8.2" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conjoint-probability"><i class="fa fa-check"></i><b>8.2</b> Conjoint probability</a></li>
<li class="chapter" data-level="8.3" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-cookie-problem"><i class="fa fa-check"></i><b>8.3</b> The cookie problem</a></li>
<li class="chapter" data-level="8.4" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#bayess-theorem-1"><i class="fa fa-check"></i><b>8.4</b> Bayes’s Theorem</a></li>
<li class="chapter" data-level="8.5" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-diachronic-interpretation"><i class="fa fa-check"></i><b>8.5</b> The diachronic interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-mm-problem"><i class="fa fa-check"></i><b>8.6</b> The M&amp;M problem</a></li>
<li class="chapter" data-level="8.7" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>8.7</b> The Monty Hall problem</a></li>
<li class="chapter" data-level="8.8" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#discussion"><i class="fa fa-check"></i><b>8.8</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-sampling.html"><a href="9-sampling.html"><i class="fa fa-check"></i><b>9</b> Sampling</a><ul>
<li class="chapter" data-level="" data-path="9-sampling.html"><a href="9-sampling.html#needed-packages-1"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="9.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-activity"><i class="fa fa-check"></i><b>9.1</b> Sampling bowl activity</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-sampling.html"><a href="9-sampling.html#what-proportion-of-this-bowls-balls-are-red"><i class="fa fa-check"></i><b>9.1.1</b> What proportion of this bowl’s balls are red?</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-shovel-once"><i class="fa fa-check"></i><b>9.1.2</b> Using the shovel once</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-sampling.html"><a href="9-sampling.html#student-shovels"><i class="fa fa-check"></i><b>9.1.3</b> Using the shovel 33 times</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-sampling.html"><a href="9-sampling.html#what-did-we-just-do"><i class="fa fa-check"></i><b>9.1.4</b> What did we just do?</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-simulation"><i class="fa fa-check"></i><b>9.2</b> Virtual sampling</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-once"><i class="fa fa-check"></i><b>9.2.1</b> Using the virtual shovel once</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-33-times"><i class="fa fa-check"></i><b>9.2.2</b> Using the virtual shovel 33 times</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-sampling.html"><a href="9-sampling.html#shovel-1000-times"><i class="fa fa-check"></i><b>9.2.3</b> Using the virtual shovel 1000 times</a></li>
<li class="chapter" data-level="9.2.4" data-path="9-sampling.html"><a href="9-sampling.html#different-shovels"><i class="fa fa-check"></i><b>9.2.4</b> Using different shovels</a></li>
<li class="chapter" data-level="9.2.5" data-path="9-sampling.html"><a href="9-sampling.html#using-many-shovels-at-once"><i class="fa fa-check"></i><b>9.2.5</b> Using many shovels at once</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-sampling.html"><a href="9-sampling.html#sampling-framework"><i class="fa fa-check"></i><b>9.3</b> Sampling framework</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-sampling.html"><a href="9-sampling.html#terminology-and-notation"><i class="fa fa-check"></i><b>9.3.1</b> Terminology and notation</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-definitions"><i class="fa fa-check"></i><b>9.3.2</b> Statistical definitions</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-sampling.html"><a href="9-sampling.html#moral-of-the-story"><i class="fa fa-check"></i><b>9.3.3</b> The moral of the story</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-sampling.html"><a href="9-sampling.html#sampling-case-study"><i class="fa fa-check"></i><b>9.4</b> Case study: Polls</a></li>
<li class="chapter" data-level="9.5" data-path="9-sampling.html"><a href="9-sampling.html#sampling-conclusion"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-conclusion-central-limit-theorem"><i class="fa fa-check"></i><b>9.5.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="9.5.2" data-path="9-sampling.html"><a href="9-sampling.html#whats-to-come"><i class="fa fa-check"></i><b>9.5.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html"><i class="fa fa-check"></i><b>10</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#needed-packages-2"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="10.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-tactile"><i class="fa fa-check"></i><b>10.1</b> Pennies activity</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-is-the-average-year-on-us-pennies-in-2019"><i class="fa fa-check"></i><b>10.1.1</b> What is the average year on US pennies in 2019?</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-once"><i class="fa fa-check"></i><b>10.1.2</b> Resampling once</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#student-resamples"><i class="fa fa-check"></i><b>10.1.3</b> Resampling 35 times</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-did-we-just-do-1"><i class="fa fa-check"></i><b>10.1.4</b> What did we just do?</a></li>
<li class="chapter" data-level="10.1.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-simulation"><i class="fa fa-check"></i><b>10.1.5</b> Virtually resampling once</a></li>
<li class="chapter" data-level="10.1.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-35-replicates"><i class="fa fa-check"></i><b>10.1.6</b> Virtually resampling 35 times</a></li>
<li class="chapter" data-level="10.1.7" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-1000-replicates"><i class="fa fa-check"></i><b>10.1.7</b> Virtually resampling 1000 times</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build-up"><i class="fa fa-check"></i><b>10.2</b> Measuring uncertainty with confidence intervals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#percentile-method"><i class="fa fa-check"></i><b>10.2.1</b> Percentile method</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#se-method"><i class="fa fa-check"></i><b>10.2.2</b> Standard error method</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#one-prop-ci"><i class="fa fa-check"></i><b>10.2.3</b> Interpreting confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-width"><i class="fa fa-check"></i><b>10.3</b> Width of confidence intervals</a><ul>
<li class="chapter" data-level="10.3.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-confidence-level"><i class="fa fa-check"></i><b>10.3.1</b> Impact of confidence level</a></li>
<li class="chapter" data-level="10.3.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#fitting-multiple-models-using-map"><i class="fa fa-check"></i><b>10.3.2</b> Fitting multiple models using <code>map()</code></a></li>
<li class="chapter" data-level="10.3.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-sample-size"><i class="fa fa-check"></i><b>10.3.3</b> Impact of sample size</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut"><i class="fa fa-check"></i><b>10.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="10.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#case-study-two-prop-ci"><i class="fa fa-check"></i><b>10.5</b> Case study: Is yawning contagious?</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#mythbusters-study-data"><i class="fa fa-check"></i><b>10.5.1</b> <em>Mythbusters</em> study data</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#sampling-scenario"><i class="fa fa-check"></i><b>10.5.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="10.5.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build"><i class="fa fa-check"></i><b>10.5.3</b> Constructing the confidence interval</a></li>
<li class="chapter" data-level="10.5.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut-1"><i class="fa fa-check"></i><b>10.5.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-conclusion"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a><ul>
<li class="chapter" data-level="10.6.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-vs-sampling"><i class="fa fa-check"></i><b>10.6.1</b> Comparing bootstrap and sampling distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-regression.html"><a href="11-regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="" data-path="11-regression.html"><a href="11-regression.html#needed-packages-3"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="11.1" data-path="11-regression.html"><a href="11-regression.html#model1"><i class="fa fa-check"></i><b>11.1</b> Teaching evaluations: one numerical explanatory variable</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-regression.html"><a href="11-regression.html#model1EDA"><i class="fa fa-check"></i><b>11.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-regression.html"><a href="11-regression.html#model1table"><i class="fa fa-check"></i><b>11.1.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="11.1.3" data-path="11-regression.html"><a href="11-regression.html#interpreting-regression-coefficients"><i class="fa fa-check"></i><b>11.1.3</b> Interpreting regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-regression.html"><a href="11-regression.html#uncertainty-in-simple-linear-regressions"><i class="fa fa-check"></i><b>11.2</b> Uncertainty in simple linear regressions</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-regression.html"><a href="11-regression.html#interpreting-confidence-intervals"><i class="fa fa-check"></i><b>11.2.1</b> Interpreting confidence intervals</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-regression.html"><a href="11-regression.html#using-lm-and-tidy-as-a-shortcut-2"><i class="fa fa-check"></i><b>11.2.2</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="11.2.3" data-path="11-regression.html"><a href="11-regression.html#model1points"><i class="fa fa-check"></i><b>11.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-regression.html"><a href="11-regression.html#model2"><i class="fa fa-check"></i><b>11.3</b> Life expectancy: one categorical explanatory variable</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-regression.html"><a href="11-regression.html#model2EDA"><i class="fa fa-check"></i><b>11.3.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-regression.html"><a href="11-regression.html#model2table"><i class="fa fa-check"></i><b>11.3.2</b> Linear regression</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-regression.html"><a href="11-regression.html#model2points"><i class="fa fa-check"></i><b>11.3.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-regression.html"><a href="11-regression.html#fitting-multiple-models-using-map-1"><i class="fa fa-check"></i><b>11.4</b> Fitting multiple models using <code>map()</code></a></li>
<li class="chapter" data-level="11.5" data-path="11-regression.html"><a href="11-regression.html#leastsquares"><i class="fa fa-check"></i><b>11.5</b> Best-fitting line</a></li>
<li class="chapter" data-level="11.6" data-path="11-regression.html"><a href="11-regression.html#conclusion-4"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a><ul>
<li class="chapter" data-level="11.6.1" data-path="11-regression.html"><a href="11-regression.html#additional-resources-basic-regression"><i class="fa fa-check"></i><b>11.6.1</b> Additional resources</a></li>
<li class="chapter" data-level="11.6.2" data-path="11-regression.html"><a href="11-regression.html#whats-to-come-1"><i class="fa fa-check"></i><b>11.6.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#needed-packages-4"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="12.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4"><i class="fa fa-check"></i><b>12.1</b> One numerical and one categorical explanatory variable</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4EDA"><i class="fa fa-check"></i><b>12.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4interactiontable"><i class="fa fa-check"></i><b>12.1.2</b> Interaction model</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4table"><i class="fa fa-check"></i><b>12.1.3</b> Parallel slopes model</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4points"><i class="fa fa-check"></i><b>12.1.4</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3"><i class="fa fa-check"></i><b>12.2</b> Two numerical explanatory variables</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3EDA"><i class="fa fa-check"></i><b>12.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3table"><i class="fa fa-check"></i><b>12.2.2</b> Regression plane</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3points"><i class="fa fa-check"></i><b>12.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#related-topics"><i class="fa fa-check"></i><b>12.3</b> Related topics</a><ul>
<li class="chapter" data-level="12.3.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model-selection"><i class="fa fa-check"></i><b>12.3.1</b> Model selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#correlationcoefficient2"><i class="fa fa-check"></i><b>12.3.2</b> Correlation coefficient</a></li>
<li class="chapter" data-level="12.3.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#simpsonsparadox"><i class="fa fa-check"></i><b>12.3.3</b> Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#seattle-house-prices"><i class="fa fa-check"></i><b>12.4</b> Case study: Seattle house prices</a><ul>
<li class="chapter" data-level="12.4.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-I"><i class="fa fa-check"></i><b>12.4.1</b> Exploratory data analysis: Part I</a></li>
<li class="chapter" data-level="12.4.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-II"><i class="fa fa-check"></i><b>12.4.2</b> Exploratory data analysis: Part II</a></li>
<li class="chapter" data-level="12.4.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-regression"><i class="fa fa-check"></i><b>12.4.3</b> Regression modeling</a></li>
<li class="chapter" data-level="12.4.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-making-predictions"><i class="fa fa-check"></i><b>12.4.4</b> Making predictions</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#data-journalism"><i class="fa fa-check"></i><b>12.5</b> Case study: Effective data storytelling</a><ul>
<li class="chapter" data-level="12.5.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#bechdel-test-for-hollywood-gender-representation"><i class="fa fa-check"></i><b>12.5.1</b> Bechdel test for Hollywood gender representation</a></li>
<li class="chapter" data-level="12.5.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#us-births-in-1999"><i class="fa fa-check"></i><b>12.5.2</b> US Births in 1999</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-classification.html"><a href="13-classification.html"><i class="fa fa-check"></i><b>13</b> Classification</a><ul>
<li class="chapter" data-level="13.1" data-path="13-classification.html"><a href="13-classification.html#learning-objectives"><i class="fa fa-check"></i><b>13.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="13-classification.html"><a href="13-classification.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>13.2</b> Introduction to Logistic Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="13-classification.html"><a href="13-classification.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>13.2.1</b> Logistic Regression Assumptions</a></li>
<li class="chapter" data-level="13.2.2" data-path="13-classification.html"><a href="13-classification.html#a-graphical-look-at-logistic-regression"><i class="fa fa-check"></i><b>13.2.2</b> A Graphical Look at Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13-classification.html"><a href="13-classification.html#case-studies-overview"><i class="fa fa-check"></i><b>13.3</b> Case Studies Overview</a></li>
<li class="chapter" data-level="13.4" data-path="13-classification.html"><a href="13-classification.html#case-study-soccer-goalkeepers"><i class="fa fa-check"></i><b>13.4</b> Case Study: Soccer Goalkeepers</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-classification.html"><a href="13-classification.html#modeling-odds"><i class="fa fa-check"></i><b>13.4.1</b> Modeling Odds</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-classification.html"><a href="13-classification.html#logistic-regression-models-for-binomial-responses"><i class="fa fa-check"></i><b>13.4.2</b> Logistic Regression Models for Binomial Responses</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-classification.html"><a href="13-classification.html#theoretical-rationale-for-logistic-regression-models-optional"><i class="fa fa-check"></i><b>13.4.3</b> Theoretical rationale for logistic regression models (Optional)</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="13-classification.html"><a href="13-classification.html#case-study-reconstructing-alabama"><i class="fa fa-check"></i><b>13.5</b> Case Study: Reconstructing Alabama</a><ul>
<li class="chapter" data-level="13.5.1" data-path="13-classification.html"><a href="13-classification.html#data-organization"><i class="fa fa-check"></i><b>13.5.1</b> Data Organization</a></li>
<li class="chapter" data-level="13.5.2" data-path="13-classification.html"><a href="13-classification.html#exploratory-analyses"><i class="fa fa-check"></i><b>13.5.2</b> Exploratory Analyses</a></li>
<li class="chapter" data-level="13.5.3" data-path="13-classification.html"><a href="13-classification.html#initial-models"><i class="fa fa-check"></i><b>13.5.3</b> Initial Models</a></li>
<li class="chapter" data-level="13.5.4" data-path="13-classification.html"><a href="13-classification.html#sec-logisticInf"><i class="fa fa-check"></i><b>13.5.4</b> Tests for significance of model coefficients</a></li>
<li class="chapter" data-level="13.5.5" data-path="13-classification.html"><a href="13-classification.html#confidence-intervals-for-model-coefficients"><i class="fa fa-check"></i><b>13.5.5</b> Confidence intervals for model coefficients</a></li>
<li class="chapter" data-level="13.5.6" data-path="13-classification.html"><a href="13-classification.html#testing-for-goodness-of-fit"><i class="fa fa-check"></i><b>13.5.6</b> Testing for goodness of fit</a></li>
<li class="chapter" data-level="13.5.7" data-path="13-classification.html"><a href="13-classification.html#residuals-for-binomial-regression"><i class="fa fa-check"></i><b>13.5.7</b> Residuals for Binomial Regression</a></li>
<li class="chapter" data-level="13.5.8" data-path="13-classification.html"><a href="13-classification.html#sec-logOverdispersion"><i class="fa fa-check"></i><b>13.5.8</b> Overdispersion</a></li>
<li class="chapter" data-level="13.5.9" data-path="13-classification.html"><a href="13-classification.html#summary-5"><i class="fa fa-check"></i><b>13.5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13-classification.html"><a href="13-classification.html#least-squares-regression-vs.-logistic-regression"><i class="fa fa-check"></i><b>13.6</b> Least Squares Regression vs. Logistic Regression</a></li>
<li class="chapter" data-level="13.7" data-path="13-classification.html"><a href="13-classification.html#case-study-trying-to-lose-weight"><i class="fa fa-check"></i><b>13.7</b> Case Study: Trying to Lose Weight</a><ul>
<li class="chapter" data-level="13.7.1" data-path="13-classification.html"><a href="13-classification.html#data-organization-1"><i class="fa fa-check"></i><b>13.7.1</b> Data Organization</a></li>
<li class="chapter" data-level="13.7.2" data-path="13-classification.html"><a href="13-classification.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>13.7.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="13.7.3" data-path="13-classification.html"><a href="13-classification.html#initial-models-1"><i class="fa fa-check"></i><b>13.7.3</b> Initial Models</a></li>
<li class="chapter" data-level="13.7.4" data-path="13-classification.html"><a href="13-classification.html#drop-in-deviance-tests"><i class="fa fa-check"></i><b>13.7.4</b> Drop-in-deviance Tests</a></li>
<li class="chapter" data-level="13.7.5" data-path="13-classification.html"><a href="13-classification.html#model-discussion-and-summary"><i class="fa fa-check"></i><b>13.7.5</b> Model Discussion and Summary</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="13-classification.html"><a href="13-classification.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>13.8</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="13.8.1" data-path="13-classification.html"><a href="13-classification.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>13.8.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="13.8.2" data-path="13-classification.html"><a href="13-classification.html#cart-motivation"><i class="fa fa-check"></i><b>13.8.2</b> CART motivation</a></li>
<li class="chapter" data-level="13.8.3" data-path="13-classification.html"><a href="13-classification.html#regression-trees"><i class="fa fa-check"></i><b>13.8.3</b> Regression trees</a></li>
<li class="chapter" data-level="13.8.4" data-path="13-classification.html"><a href="13-classification.html#classification-decision-trees"><i class="fa fa-check"></i><b>13.8.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="13-classification.html"><a href="13-classification.html#random-forests"><i class="fa fa-check"></i><b>13.9</b> Random forests</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-machine.html"><a href="14-machine.html"><i class="fa fa-check"></i><b>14</b> Machine Learning</a><ul>
<li class="chapter" data-level="14.1" data-path="14-machine.html"><a href="14-machine.html#notation"><i class="fa fa-check"></i><b>14.1</b> Notation</a></li>
<li class="chapter" data-level="14.2" data-path="14-machine.html"><a href="14-machine.html#an-example"><i class="fa fa-check"></i><b>14.2</b> An example</a></li>
<li class="chapter" data-level="14.3" data-path="14-machine.html"><a href="14-machine.html#evaluation-metrics"><i class="fa fa-check"></i><b>14.3</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14-machine.html"><a href="14-machine.html#training-and-test-sets"><i class="fa fa-check"></i><b>14.3.1</b> Training and test sets</a></li>
<li class="chapter" data-level="14.3.2" data-path="14-machine.html"><a href="14-machine.html#overall-accuracy"><i class="fa fa-check"></i><b>14.3.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="14.3.3" data-path="14-machine.html"><a href="14-machine.html#the-confusion-matrix"><i class="fa fa-check"></i><b>14.3.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="14.3.4" data-path="14-machine.html"><a href="14-machine.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>14.3.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="14.3.5" data-path="14-machine.html"><a href="14-machine.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>14.3.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="14.3.6" data-path="14-machine.html"><a href="14-machine.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>14.3.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="14.3.7" data-path="14-machine.html"><a href="14-machine.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>14.3.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="14.3.8" data-path="14-machine.html"><a href="14-machine.html#loss-function"><i class="fa fa-check"></i><b>14.3.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14-machine.html"><a href="14-machine.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>14.4</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-machine.html"><a href="14-machine.html#conditional-probabilities"><i class="fa fa-check"></i><b>14.4.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-machine.html"><a href="14-machine.html#conditional-expectations"><i class="fa fa-check"></i><b>14.4.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="14.4.3" data-path="14-machine.html"><a href="14-machine.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>14.4.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-machine.html"><a href="14-machine.html#two-or-seven"><i class="fa fa-check"></i><b>14.5</b> Case study: is it a 2 or a 7?</a></li>
<li class="chapter" data-level="14.6" data-path="14-machine.html"><a href="14-machine.html#cross-validation"><i class="fa fa-check"></i><b>14.6</b> Cross validation</a></li>
<li class="chapter" data-level="14.7" data-path="14-machine.html"><a href="14-machine.html#knn-cv-intro"><i class="fa fa-check"></i><b>14.7</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="14.7.1" data-path="14-machine.html"><a href="14-machine.html#over-training"><i class="fa fa-check"></i><b>14.7.1</b> Over-training</a></li>
<li class="chapter" data-level="14.7.2" data-path="14-machine.html"><a href="14-machine.html#over-smoothing"><i class="fa fa-check"></i><b>14.7.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="14.7.3" data-path="14-machine.html"><a href="14-machine.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>14.7.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="14-machine.html"><a href="14-machine.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>14.8</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="14.9" data-path="14-machine.html"><a href="14-machine.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>14.9</b> K-fold cross validation</a></li>
<li class="chapter" data-level="14.10" data-path="14-machine.html"><a href="14-machine.html#bootstrap"><i class="fa fa-check"></i><b>14.10</b> Bootstrap</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html"><i class="fa fa-check"></i><b>A</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="A.1" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#causal-effects"><i class="fa fa-check"></i><b>A.1</b> Causal effects</a></li>
<li class="chapter" data-level="A.2" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#potential-outcomes"><i class="fa fa-check"></i><b>A.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="A.3" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#no-causation-without-manipulation"><i class="fa fa-check"></i><b>A.3</b> No causation without manipulation</a></li>
<li class="chapter" data-level="A.4" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#average-treatment-effect"><i class="fa fa-check"></i><b>A.4</b> Average treatment effect</a></li>
<li class="chapter" data-level="A.5" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#stable-unit-treatment-value-assumption-sutva"><i class="fa fa-check"></i><b>A.5</b> Stable unit treatment value assumption (SUTVA)</a></li>
<li class="chapter" data-level="A.6" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>A.6</b> The fundamental problem of causal inference</a></li>
<li class="chapter" data-level="A.7" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-assignment-mechanism"><i class="fa fa-check"></i><b>A.7</b> The assignment mechanism</a></li>
<li class="chapter" data-level="A.8" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#permutation-tests"><i class="fa fa-check"></i><b>A.8</b> Permutation tests</a></li>
<li class="chapter" data-level="A.9" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#confounding-and-selection-bias"><i class="fa fa-check"></i><b>A.9</b> Confounding and selection bias</a></li>
<li class="chapter" data-level="A.10" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#internal-and-external-validity"><i class="fa fa-check"></i><b>A.10</b> Internal and external validity</a></li>
<li class="chapter" data-level="A.11" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#survey-research-and-external-validity"><i class="fa fa-check"></i><b>A.11</b> Survey research and external validity</a></li>
<li class="chapter" data-level="A.12" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#conclusion-5"><i class="fa fa-check"></i><b>A.12</b> Conclusion</a></li>
<li class="chapter" data-level="A.13" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#references"><i class="fa fa-check"></i><b>A.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-maps.html"><a href="B-maps.html"><i class="fa fa-check"></i><b>B</b> Maps</a><ul>
<li class="chapter" data-level="B.1" data-path="B-maps.html"><a href="B-maps.html#tidycensus"><i class="fa fa-check"></i><b>B.1</b> Tidycensus</a></li>
<li class="chapter" data-level="B.2" data-path="B-maps.html"><a href="B-maps.html#conceptual-introduction-to-mapping"><i class="fa fa-check"></i><b>B.2</b> Conceptual introduction to mapping</a><ul>
<li class="chapter" data-level="B.2.1" data-path="B-maps.html"><a href="B-maps.html#vector-versus-spatial-data"><i class="fa fa-check"></i><b>B.2.1</b> Vector versus spatial data</a></li>
<li class="chapter" data-level="B.2.2" data-path="B-maps.html"><a href="B-maps.html#sf-vs-sp"><i class="fa fa-check"></i><b>B.2.2</b> <strong>sf</strong> vs <strong>sp</strong></a></li>
<li class="chapter" data-level="B.2.3" data-path="B-maps.html"><a href="B-maps.html#shapefiles"><i class="fa fa-check"></i><b>B.2.3</b> Shapefiles</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="B-maps.html"><a href="B-maps.html#mapping-with-tidycensus-and-geom_sf"><i class="fa fa-check"></i><b>B.3</b> Mapping with <strong>tidycensus</strong> and <code>geom_sf()</code></a><ul>
<li class="chapter" data-level="B.3.1" data-path="B-maps.html"><a href="B-maps.html#making-maps-pretty"><i class="fa fa-check"></i><b>B.3.1</b> Making maps pretty</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-maps.html"><a href="B-maps.html#adding-back-alaska-and-hawaii"><i class="fa fa-check"></i><b>B.3.2</b> Adding back Alaska and Hawaii</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-maps.html"><a href="B-maps.html#faceting-maps"><i class="fa fa-check"></i><b>B.4</b> Faceting maps</a><ul>
<li class="chapter" data-level="B.4.1" data-path="B-maps.html"><a href="B-maps.html#transforming-and-mapping-the-data"><i class="fa fa-check"></i><b>B.4.1</b> Transforming and mapping the data</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="B-maps.html"><a href="B-maps.html#want-to-explore-further"><i class="fa fa-check"></i><b>B.5</b> Want to explore further?</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-animation.html"><a href="C-animation.html"><i class="fa fa-check"></i><b>C</b> Animation</a><ul>
<li class="chapter" data-level="C.1" data-path="C-animation.html"><a href="C-animation.html#gganimate-how-to-create-plots-with-beautiful-animation-in-r"><i class="fa fa-check"></i><b>C.1</b> gganimate: How to Create Plots with Beautiful Animation in R</a><ul>
<li class="chapter" data-level="C.1.1" data-path="C-animation.html"><a href="C-animation.html#prerequisites"><i class="fa fa-check"></i><b>C.1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="C.1.2" data-path="C-animation.html"><a href="C-animation.html#demo-dataset"><i class="fa fa-check"></i><b>C.1.2</b> Demo dataset</a></li>
<li class="chapter" data-level="C.1.3" data-path="C-animation.html"><a href="C-animation.html#static-plot"><i class="fa fa-check"></i><b>C.1.3</b> Static plot</a></li>
<li class="chapter" data-level="C.1.4" data-path="C-animation.html"><a href="C-animation.html#transition-through-distinct-states-in-time"><i class="fa fa-check"></i><b>C.1.4</b> Transition through distinct states in time</a></li>
<li class="chapter" data-level="C.1.5" data-path="C-animation.html"><a href="C-animation.html#reveal-data-along-a-given-dimension"><i class="fa fa-check"></i><b>C.1.5</b> Reveal data along a given dimension</a></li>
<li class="chapter" data-level="C.1.6" data-path="C-animation.html"><a href="C-animation.html#transition-between-several-distinct-stages-of-the-data"><i class="fa fa-check"></i><b>C.1.6</b> Transition between several distinct stages of the data</a></li>
<li class="chapter" data-level="C.1.7" data-path="C-animation.html"><a href="C-animation.html#read-more"><i class="fa fa-check"></i><b>C.1.7</b> Read more</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="C-animation.html"><a href="C-animation.html#how-to-save-your-animation"><i class="fa fa-check"></i><b>C.2</b> How to save your animation</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-shiny.html"><a href="D-shiny.html"><i class="fa fa-check"></i><b>D</b> Shiny</a><ul>
<li class="chapter" data-level="D.1" data-path="D-shiny.html"><a href="D-shiny.html#helpful-resources"><i class="fa fa-check"></i><b>D.1</b> Helpful Resources</a></li>
<li class="chapter" data-level="D.2" data-path="D-shiny.html"><a href="D-shiny.html#set-up-and-getting-started"><i class="fa fa-check"></i><b>D.2</b> Set Up and Getting Started</a></li>
<li class="chapter" data-level="D.3" data-path="D-shiny.html"><a href="D-shiny.html#building-your-basic-app"><i class="fa fa-check"></i><b>D.3</b> Building Your Basic App</a><ul>
<li class="chapter" data-level="D.3.1" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-basic-ui"><i class="fa fa-check"></i><b>D.3.1</b> Setting Up the Basic UI</a></li>
<li class="chapter" data-level="D.3.2" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-server"><i class="fa fa-check"></i><b>D.3.2</b> Setting up the Server</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="D-shiny.html"><a href="D-shiny.html#organization"><i class="fa fa-check"></i><b>D.4</b> Organization</a></li>
<li class="chapter" data-level="D.5" data-path="D-shiny.html"><a href="D-shiny.html#customizations"><i class="fa fa-check"></i><b>D.5</b> Customizations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Preceptor’s Primer for Bayesian Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Classification</h1>
<p><span class="math display">\[
\newcommand{\lik}{\operatorname{Lik}}
\newcommand{\Lik}{\operatorname{Lik}}
\]</span></p>
<div id="learning-objectives" class="section level2">
<h2><span class="header-section-number">13.1</span> Learning Objectives</h2>
<ul>
<li>Identify a binomial random variable and assess the validity of the binomial assumptions.</li>
<li>Write a generalized linear model for binomial responses in two forms, one as a function of the logit and one as a function of <span class="math inline">\(p\)</span>.</li>
<li>Explain how fitting a logistic regression differs from fitting an ordinary least squares (OLS) regression model.</li>
<li>Interpret estimated coefficients in logistic regression.</li>
<li>Differentiate between logistic regression models with binary and binomial responses.</li>
<li>Use the residual deviance to compare models, to test for lack-of-fit when appropriate, and to check for unusual observations or needed transformations.</li>
</ul>
</div>
<div id="introduction-to-logistic-regression" class="section level2">
<h2><span class="header-section-number">13.2</span> Introduction to Logistic Regression</h2>
<p>Logistic regression is characterized by research questions with binary (yes/no or success/failure) or binomial (number of yesses or successes in <span class="math inline">\(n\)</span> trials) responses:</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Are students with poor grades more likely to binge drink?</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Is exposure to a particular chemical associated with a cancer diagnosis?</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Are the number of votes for a congressional candidate associated with the amount of campaign contributions?</li>
</ol></li>
</ul>
<p><strong>Binary Responses:</strong> Recall from Section <a href="#sec-binary"><strong>??</strong></a> that binary responses take on only two values: success (Y=1) or failure (Y=0), Yes (Y=1) or No (Y=0), etc. Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success <span class="math inline">\(p\)</span> based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure.</p>
<p>Examples (a) and (b) above would be considered to have binary responses (Does a student binge drink? Was a patient diagnosed with cancer?), assuming that we have a unique set of covariates for each individual student or patient.</p>
<p><strong>Binomial Responses:</strong> Also recall from Section <a href="#sec-binomial"><strong>??</strong></a> that binomial responses are number of successes in <span class="math inline">\(n\)</span> identical, independent trials with constant probability <span class="math inline">\(p\)</span> of success. A sequence of independent trials like this with the same probability of success is called a <strong>Bernoulli process</strong>. As with binary responses, our objective in modeling binomial responses is to quantify how the probability of success, <span class="math inline">\(p\)</span>, is associated with relevant covariates.</p>
<p>Example (c) above would be considered to have a binonial response, assuming we have vote totals at the congressional district level rather than information on individual voters.</p>
<div id="logistic-regression-assumptions" class="section level3">
<h3><span class="header-section-number">13.2.1</span> Logistic Regression Assumptions</h3>
<p>Much like OLS, using logistic regression to make inferences requires model assumptions.</p>
<ol style="list-style-type: decimal">
<li><strong>Binary Response</strong> The response variable is dichotomous (two possible responses) or the sum of dichotomous responses.</li>
<li><strong>Independence</strong> The observations must be independent of one another.</li>
<li><strong>Variance Structure</strong> By definition, the variance of a binomial random variable is <span class="math inline">\(np(1-p)\)</span>, so that variability is highest when <span class="math inline">\(p=.5\)</span>.</li>
<li><strong>Linearity</strong> The log of the odds ratio, log(<span class="math inline">\(\frac{p}{1-p}\)</span>), must be a linear function of x. This will be explained further in the context of the first case study.</li>
</ol>
</div>
<div id="a-graphical-look-at-logistic-regression" class="section level3">
<h3><span class="header-section-number">13.2.2</span> A Graphical Look at Logistic Regression</h3>
<div class="figure" style="text-align: center"><span id="fig:OLSlogistic"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/OLSlogistic-1.png" alt="Linear vs. Logistic regression models for binary response data." width="60%" />
<p class="caption">
FIGURE 13.1: Linear vs. Logistic regression models for binary response data.
</p>
</div>
<p>Figure <a href="13-classification.html#fig:OLSlogistic">13.1</a> illustrates a data set with a binary (0 or 1) response (Y) and a single continuous predictor (X). The blue line is a linear regression fit with OLS to model the probability of a success (Y=1) for a given value of X. With a binary response, the line doesn’t fit the data well, and it produces predicted probabilities below 0 and above 1. On the other hand, the logistic regression fit (red curve) with its typical “S” shape follows the data closely and always produces predicted probabilities between 0 and 1. For these and several other reasons detailed in this chapter, we will focus on the following model for logistic regression with binary or binomial responses:</p>
<p><span class="math display" id="eq:logisticReg">\[\begin{equation}
log(\frac{p_i}{1-p_i})=\beta_0+\beta_1 x_i
\tag{13.1}
\end{equation}\]</span>
where the observed values <span class="math inline">\(Y_i \sim\)</span> binomial with <span class="math inline">\(p=p_i\)</span> for a given <span class="math inline">\(x_i\)</span> and <span class="math inline">\(n=1\)</span> for binary responses.</p>
</div>
</div>
<div id="case-studies-overview" class="section level2">
<h2><span class="header-section-number">13.3</span> Case Studies Overview</h2>
<p>We consider three case studies in this chapter. The first two involve binomial responses (Soccer Goalkeepers and Reconstructing Alabama), while the last case uses a binary response (Trying to Lose Weight). Even though binary responses are much more common, their models have a very similar form to binomial responses, so the first two case studies will illustrate important principles that also apply to the binary case. Here are the statistical concepts you will encounter for each Case Study.</p>
<p>The soccer goalkeeper data can be written in the form of a 2 <span class="math inline">\(\times\)</span> 2 table. This example is used to describe some of the underlying theory for logistic regression. We demonstrate how binomial probability mass functions (pmfs) can be written in one parameter exponential family form, from which we can identify the canonical link as in Chapter <a href="#ch-glms"><strong>??</strong></a>. Using the canonical link, we write a Generalized Linear Model for binomial counts and determine corresponding MLEs for model coefficients. Interpretation of the estimated parameters involves a fundamental concept, the odds ratio.</p>
<p>The Reconstructing Alabama case study is another binomial example which introduces the notion of deviances, which are used to compare and assess models. We will check the assumptions of logistic regression using empirical logit plots and deviance residuals.</p>
<p>The last case study addresses why teens try to lose weight. Here the response is a binary variable which allows us to analyze individual level data. The analysis builds on concepts from the previous sections in the context of a random sample from CDC’s Youth Risk Behavior Survey (YRBS).</p>
</div>
<div id="case-study-soccer-goalkeepers" class="section level2">
<h2><span class="header-section-number">13.4</span> Case Study: Soccer Goalkeepers</h2>
<p>Does the probability of a save in a soccer match depend upon whether the goalkeeper’s team is behind or not? <span class="citation">(<span class="citeproc-not-found" data-reference-id="Roskes2011"><strong>???</strong></span>)</span> looked at penalty kicks in the men’s World Cup soccer championships from 1982 - 2010, and they assembled data on 204 penalty kicks during shootouts. The data for this study is summarized in Table <a href="13-classification.html#tab:table1chp6">13.1</a>.</p>
<table>
<caption>
<span id="tab:table1chp6">TABLE 13.1: </span>Soccer goalkeepers’ saves when their team is and is not behind. Source: Roskes et al. 2011 Psychological Science
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Saves
</th>
<th style="text-align:right;">
Scores
</th>
<th style="text-align:right;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Behind
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
24
</td>
</tr>
<tr>
<td style="text-align:left;">
Not Behind
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
141
</td>
<td style="text-align:right;">
180
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
204
</td>
</tr>
</tbody>
</table>
<div id="modeling-odds" class="section level3">
<h3><span class="header-section-number">13.4.1</span> Modeling Odds</h3>
<p>Odds are one way to quantify a goalkeeper’s performance. Here the odds that a goalkeeper makes a save when his team is behind is 2 to 22 or 0.09 to 1. Or equivalently, the odds that a goal is scored on a penalty kick is 22 to 2 or 11 to 1. An odds of 11 to 1 tells you that a shooter whose team is ahead will score 11 times for every 1 shot that the goalkeeper saves. When the goalkeeper’s team is not behind the odds a goal is scored is 141 to 39 or 3.61 to 1. We see that the odds of a goal scored on a penalty kick are better when the goalkeeper’s team is behind than when it is not behind (i.e., better odds of scoring for the shooter when the shooter’s team is ahead). We can compare these odds by calculating the <strong>odds ratio</strong>  (OR), 11/3.61 or 3.05, which tells us that the <em>odds</em> of a successful penalty kick are 3.05 times higher when the shooter’s team is leading.</p>
<p>In our example, it is also possible to estimate the probability of a goal, <span class="math inline">\(p\)</span>, for either circumstance. When the goalkeeper’s team is behind, the probability of a successful penalty kick is <span class="math inline">\(p\)</span> = 22/24 or 0.833. We can see that the ratio of the probability of a goal scored divided by the probability of no goal is <span class="math inline">\((22/24)/(2/24)=22/2\)</span> or 11, the odds we had calculated above. The same calculation can be made when the goalkeeper’s team is not behind. In general, we now have several ways of finding the odds of success under certain circumstances:
<span class="math display">\[\textrm{Odds} = \frac{\# \textrm{successes}}{\# \textrm{failures}}=
\frac{\# \textrm{successes}/n}{\# \textrm{failures}/n}=
\frac{p}{1-p}.\]</span></p>
</div>
<div id="logistic-regression-models-for-binomial-responses" class="section level3">
<h3><span class="header-section-number">13.4.2</span> Logistic Regression Models for Binomial Responses</h3>
<p>We would like to model the odds of success, however odds are strictly positive. Therefore, similar to modeling log(<span class="math inline">\(\lambda\)</span>) in Poisson regression, which allowed the response to take on values from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>, we will model the log(odds), the <strong>logit</strong>, in logistic regression. Logits will be suitable for modeling with a linear function of the predictors:<br />
<span class="math display" id="eq:binreg">\[\begin{equation}
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\tag{13.2}
 \end{equation}\]</span>
Models of this form are referred to as <strong>binomial regression models</strong>, or more generally as <strong>logistic regression models</strong>.  Here we provide intuition for using and interpreting logistic regression models, and then in the short optional section that follows, we present rationale for these models using GLM theory.</p>
<p>In our example we could define <span class="math inline">\(X=0\)</span> for not behind and <span class="math inline">\(X=1\)</span> for behind and fit the model:
<span class="math display" id="eq:logitXform">\[\begin{equation}
\log\left(\frac{p_X}{1-p_X}\right)=\beta_0 +\beta_1X
\tag{13.3}
\end{equation}\]</span>
where <span class="math inline">\(p_X\)</span> is the probability of a successful penalty kick given <span class="math inline">\(X\)</span>.</p>
<p>So, based on this model, the log odds of a successful penalty kick when the goalkeeper’s team is not behind is:
<span class="math display">\[
\log\left(\frac{p_0}{1-p_0}\right) =\beta_0 \nonumber,
\]</span>
and the log odds when the team is behind is:
<span class="math display">\[
\log\left(\frac{p_1}{1-p_1}\right)=\beta_0+\beta_1. \nonumber
\]</span></p>
<p>We can see that <span class="math inline">\(\beta_1\)</span> is the difference between the log odds of a successful penalty kick between games when the goalkeeper’s team is behind and games when the team is not behind. Using rules of logs:
<span class="math display" id="eq:ORform">\[\begin{equation}
\beta_1 = (\beta_0 + \beta_1) - \beta_0 = 
\log\left(\frac{p_1}{1-p_1}\right) - \log\left(\frac{p_0}{1-p_0}\right) =
\log\left(\frac{p_1/(1-p_1)}{p_0/{(1-p_0)}}\right).
\tag{13.4}
\end{equation}\]</span>
Thus <span class="math inline">\(e^{\beta_1}\)</span> is the ratio of the odds of scoring when the goalkeeper’s team is not behind compared to scoring when the team is behind. In general, <strong>exponentiated coefficients in logistic regression are odds ratios (OR)</strong>. A general interpretation of an OR is the odds of success for group A compared to the odds of success for group B—how many times greater the odds of success are in group A compared to group B.</p>
<p>The logit model (Equation <a href="13-classification.html#eq:logitXform">(13.3)</a>) can also be re-written in a <strong>probability form</strong>:
<span class="math display" id="eq:pXform">\[\begin{equation} 
p_X=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
\tag{13.5}
\end{equation}\]</span>
which can be re-written for games when the goalkeeper’s team is behind as:
<span class="math display" id="eq:pBehindform">\[\begin{equation} 
p_1=\frac{e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}  
\tag{13.6}
\end{equation}\]</span>
and for games when the goalkeeper’s team is not behind as:
<span class="math display" id="eq:pNotBehindform">\[\begin{equation} 
p_0=\frac{e^{\beta_0}}{1+e^{\beta_0}}
\tag{13.7}
\end{equation}\]</span></p>
<p>We use likelihood methods to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. As we had done in Chapter <a href="#ch-beyondmost"><strong>??</strong></a>, we can write the likelihood for this example in the following form:
<span class="math display">\[\Lik(p_1, p_0) = {28 \choose 22}p_1^{22}(1-p_1)^{2}
{180 \choose 141}p_0^{141}(1-p_0)^{39}\]</span></p>
<p>Our interest centers on estimating <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, not <span class="math inline">\(p_1\)</span> or <span class="math inline">\(p_0\)</span>. So we replace <span class="math inline">\(p_1\)</span> in the likelihood with an expression for <span class="math inline">\(p_1\)</span> in terms of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> as in Equation <a href="13-classification.html#eq:pBehindform">(13.6)</a>. Similarly, <span class="math inline">\(p_0\)</span> in Equation <a href="13-classification.html#eq:pNotBehindform">(13.7)</a> involves only <span class="math inline">\(\beta_0\)</span>. After removing constants, the new likelihood looks like:</p>
<p><span class="math display" id="eq:newlik">\[\begin{equation}
    \Lik(\beta_0,\beta_1) \propto \left( \frac{e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}\right)^{22}\left(1- \frac{e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}\right)^{2}
    \left(\frac{e^{\beta_0}}{1+e^{\beta_0}}\right)^{141}\left(1-\frac{e^{\beta_0}}{1+e^{\beta_0}}\right)^{39}
\tag{13.8}
\end{equation}\]</span></p>
<p>Now what? Fitting the model means finding estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, but familiar methods from calculus for maximizing the likelihood don’t work here. Instead, we consider all possible combinations of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. That is, we will pick that pair of values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that yield the largest likelihood for our data. Trial and error to find the best pair is tedious at best, but more efficient numerical methods are available. The MLEs for the coefficients in the soccer goalkeeper study are <span class="math inline">\(\hat{\beta_0}= 1.2852\)</span> and <span class="math inline">\(\hat{\beta_1}=1.1127\)</span>.</p>
<p>Exponentiating <span class="math inline">\(\hat{\beta_1}\)</span> provides an estimate of the odds ratio (the odds of scoring when the goalkeeper’s team is behind compared to the odds of scoring when the team is not behind) of 3.04, which is consistent with our calculations using the 2 <span class="math inline">\(\times\)</span> 2 table. We estimate that the odds of scoring when the goalkeeper’s team is behind is over 3 times that of when the team is not behind or, in other words, the odds a shooter is successful in a penalty kick shootout are 3.04 times higher when his team is leading.</p>
<p><strong>Time out for study discussion (Optional).</strong></p>
<ul>
<li><p>Discuss the following quote from the study abstract: “Because penalty takers shot at the two sides of the goal equally often, the goalkeepers’ right-oriented bias was dysfunctional, allowing more goals to be scored.”</p></li>
<li><p>Construct an argument for why the greater success observed when the goalkeeper’s team was behind might be better explained from the shooter’s perspective.</p></li>
</ul>
<p>Before we go on, you may be curious as to why there is <em>no error term</em> in our model statements for logistic or Poisson regression. One way to look at it is to consider that all models describe how observed values are generated. With the logistic model we assume that the observations are generated as a binomial random variables. Each observation or realization of <span class="math inline">\(Y\)</span> = number of successes in <span class="math inline">\(n\)</span> independent and identical trials with a probability of success on any one trial of <span class="math inline">\(p\)</span> is produced by <span class="math inline">\(Y \sim \textrm{Binomial}(n,p)\)</span>. So the randomness in this model is not introduced by an added error term but rather by appealing to a Binomial probability distribution, where variability depends only on <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> through <span class="math inline">\(\textrm{Var}(Y)=np(1-p)\)</span>, and where <span class="math inline">\(n\)</span> is usually considered fixed and <span class="math inline">\(p\)</span> the parameter of interest.</p>
</div>
<div id="theoretical-rationale-for-logistic-regression-models-optional" class="section level3">
<h3><span class="header-section-number">13.4.3</span> Theoretical rationale for logistic regression models (Optional)</h3>
<p>Recall from Chapter <a href="#ch-glms"><strong>??</strong></a> that generalized linear models (GLMs) are a way in which to model a variety of different types of responses. In this chapter, we apply the general results of GLMs to the specific application of binomial responses. Let <span class="math inline">\(Y\)</span> = the number scored out of <span class="math inline">\(n\)</span> penalty kicks. The parameter, <span class="math inline">\(p\)</span>, is the probability of a score on a single penalty kick. Recall that the theory of GLM is based on the unifying notion of the one-parameter exponential family form:
<span class="math display" id="eq:1expform">\[\begin{equation}
f(y;\theta)=e^{[a(y)b(\theta)+c(\theta)+d(y)]}
\tag{13.9}
\end{equation}\]</span>
To see that we can apply the general approach of GLMs to binomial responses, we first write an expression for the probability of a binomial response and then use a little algebra to rewrite it until we can demonstrate that it, too, can be written in one-parameter exponential family form with <span class="math inline">\(\theta = p\)</span>. This will provide a way in which to specify the canonical link and the form for the model. Additional theory allows us to deduce the mean, standard deviation, and more from this form.</p>
<p>If <span class="math inline">\(Y\)</span> follows a binomial distribution with <span class="math inline">\(n\)</span> trials and probability of success <span class="math inline">\(p\)</span>, we can write:
<span class="math display" id="eq:canlink">\[\begin{eqnarray}
P(Y=y)&amp;=&amp; \binom{n}{y}p^y(1-p)^{(n-y)} \\
      &amp;=&amp;e^{y\log(p) + (n-y)\log(1-p) + \log\binom{n}{y}}
\tag{13.10}
\end{eqnarray}\]</span>
However, this probability mass function is not quite in one parameter exponential family form. Note that there are two terms in the exponent which consist of a product of functions of <span class="math inline">\(y\)</span> and <span class="math inline">\(p\)</span>. So more simplification is in order:
<span class="math display" id="eq:opeff">\[\begin{equation}
P(Y=y) = e^{y\log\left(\frac{p}{1-p}\right) + n\log(1-p)+ \log\binom{n}{y}}
\tag{13.11}
\end{equation}\]</span>
Don’t forget to consider the support; we must make sure that the set of possible values for this response is not dependent upon <span class="math inline">\(p\)</span>. For fixed <span class="math inline">\(n\)</span> and any value of <span class="math inline">\(p\)</span>, <span class="math inline">\(0&lt;p&lt;1\)</span>, all integer values from <span class="math inline">\(0\)</span> to <span class="math inline">\(n\)</span> are possible, so the support is indeed independent of <span class="math inline">\(p\)</span>.</p>
<p>The one parameter exponential family form for binomial responses shows that the canonical link is <span class="math inline">\(\log\left(\frac{p}{1-p}\right)\)</span>. Thus, GLM theory suggests that constructing a model using the logit, the log odds of a score, as a linear function of covariates is a reasonable approach.</p>
</div>
</div>
<div id="case-study-reconstructing-alabama" class="section level2">
<h2><span class="header-section-number">13.5</span> Case Study: Reconstructing Alabama</h2>
<p>This case study demonstrates how wide ranging applications of statistics can be. Many would not associate statistics with historical research, but this case study shows that it can be done. US Census data from 1870 helped historian Michael Fitzgerald of St. Olaf College gain insight into important questions about how railroads were supported during the Reconstruction Era.</p>
<p>In a paper entitled “Reconstructing Alabama: Reconstruction Era Demographic and Statistical Research,” Ben Bayer performs an analysis of data from 1870 to explain influences on voting on referendums related to railroad subsidies <span class="citation">(<span class="citeproc-not-found" data-reference-id="Bayer2011"><strong>???</strong></span>)</span>. Positive votes are hypothesized to be inversely proportional to the distance a voter is from the proposed railroad, but the racial composition of a community (as measured by the percentage of blacks) is hypothesized to be associated with voting behavior as well. Separate analyses of three counties in Alabama—Hale, Clarke, and Dallas—were performed; we discuss Hale County here. This example differs from the soccer example in that it includes continuous covariates. Was voting on railroad referenda related to distance from the proposed railroad line and the racial composition of a community?</p>
<div id="data-organization" class="section level3">
<h3><span class="header-section-number">13.5.1</span> Data Organization</h3>
<p>The unit of observation for this data is a community in Hale County. We will focus on the following variables from <code>RR_Data_Hale.csv</code> collected for each community:</p>
<ul>
<li><p><code>YesVotes</code> = the number of “Yes” votes in favor of the proposed railroad line (our primary response variable)</p></li>
<li><p><code>NumVotes</code> = total number of votes cast in the election</p></li>
<li><p><code>pctBlack</code> = the percentage of blacks in the community</p></li>
<li><p><code>distance</code> = the distance, in miles, the proposed railroad is from the community</p></li>
</ul>
<table>
<caption>
<span id="tab:table2chp6">TABLE 13.2: </span>Sample of the data for the Hale County, Alabama, railroad subsidy vote.
</caption>
<thead>
<tr>
<th style="text-align:left;">
community
</th>
<th style="text-align:left;">
pctBlack
</th>
<th style="text-align:right;">
distance
</th>
<th style="text-align:right;">
YesVotes
</th>
<th style="text-align:right;">
NumVotes
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Carthage
</td>
<td style="text-align:left;">
58.4
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
61
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:left;">
Cederville
</td>
<td style="text-align:left;">
92.4
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
Greensboro
</td>
<td style="text-align:left;">
59.4
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1790
</td>
<td style="text-align:right;">
1804
</td>
</tr>
<tr>
<td style="text-align:left;">
Havana
</td>
<td style="text-align:left;">
58.4
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
68
</td>
</tr>
</tbody>
</table>
</div>
<div id="exploratory-analyses" class="section level3">
<h3><span class="header-section-number">13.5.2</span> Exploratory Analyses</h3>
<p>We first look at a coded scatterplot to see our data. Figure <a href="13-classification.html#fig:coded">13.2</a> portrays the relationship between <code>distance</code> and <code>pctBlack</code> coded by the <code>InFavor</code> status (whether a community supported the referendum with over 50% Yes votes). From this scatterplot, we can see that all of the communities in favor of the railroad referendum are over 55% black, and all of those opposed are 7 miles or farther from the proposed line. The overall percentage of voters in Hale County in favor of the railroad is 87.9%.</p>
<div class="figure" style="text-align: center"><span id="fig:coded"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/coded-1.png" alt=" Scatterplot of distance from a proposed rail line and percent black in the community coded by whether the community was in favor of the referendum or not." width="60%" />
<p class="caption">
FIGURE 13.2:  Scatterplot of distance from a proposed rail line and percent black in the community coded by whether the community was in favor of the referendum or not.
</p>
</div>
Recall that a model with two covariates has the form:
<span class="math display">\[\log(\textrm{odds}) = \log\left(\frac{p}{1-p}\right) = \beta_0+\beta_1X_1+\beta_2X_2.\]</span>
where <span class="math inline">\(p\)</span> is the proportion of Yes votes in a community. In logistic regression, we expect the logits to be a linear function of <span class="math inline">\(X\)</span>, the predictors. To assess the linearity assumption, we construct <strong>empirical logit plots</strong>, where “empirical” means “based on sample data.” Empirical logits are computed for each community by taking <span class="math inline">\(\log\left(\frac{\textrm{number of successes}}{\textrm{number of failures}}\right)\)</span>. In Figure <a href="13-classification.html#fig:emplogits">13.3</a>, we see that the plot of empirical logits versus distance produces a plot that looks linear, as needed for the logistic regression assumption. In contrast, the empirical logits by percent black reveal that Greensboro deviates quite a bit from the otherwise linear pattern; this suggests that Greensboro is an outlier and possibly an influential point. Greensboro has 99.2% voting yes with only 59.4% black.
<div class="figure" style="text-align: center"><span id="fig:emplogits"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/emplogits-1.png" alt="Empirical logit plots for the Railroad Referendum data." width="60%" />
<p class="caption">
FIGURE 13.3: Empirical logit plots for the Railroad Referendum data.
</p>
</div>
<p>In addition to examining how the response correlates with the predictors, it is a good idea to determine whether the predictors correlate with one another. Here the correlation between distance and percent black is negative and moderately strong with <span class="math inline">\(r = -0.49\)</span>. We’ll watch to see if the correlation affects the stability of our odds ratio estimates.</p>
</div>
<div id="initial-models" class="section level3">
<h3><span class="header-section-number">13.5.3</span> Initial Models</h3>
<p>The first model includes only one covariate, distance.</p>
<pre><code>glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance, 
    family = binomial, data = rrHale.df)

Coefficients:
            Estimate Std. Error z value            Pr(&gt;|z|)    
(Intercept)    3.309      0.113    29.2 &lt;0.0000000000000002 ***
distance      -0.288      0.013   -22.1 &lt;0.0000000000000002 ***
---
    Null deviance: 988.45  on 10  degrees of freedom
Residual deviance: 318.44  on  9  degrees of freedom
AIC: 360.7</code></pre>
<p>Our estimated binomial regression model is:
<span class="math display">\[\log\left(\frac{\hat{p}_i}{1-\hat{p}_i}\right)=3.309-0.288 (\textrm{distance}_i)\]</span>
where <span class="math inline">\(\hat{p}_i\)</span> is the estimated proportion of Yes votes in community <span class="math inline">\(i\)</span>. The estimated odds ratio for distance, that is the exponentiated coefficient for distance, in this model is <span class="math inline">\(e^{-0.288}=0.750\)</span>. It can be interpreted as follows: for each additional mile from the proposed railroad, the support (odds of a Yes vote) declines by 25.0%.</p>
<p>The covariate <code>pctBlack</code> is then added to the first model.</p>
<pre><code>glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + 
    pctBlack, family = binomial, data = rrHale.df)

Coefficients:
            Estimate Std. Error z value             Pr(&gt;|z|)    
(Intercept)   4.2220     0.2970   14.22 &lt; 0.0000000000000002 ***
distance     -0.2917     0.0131  -22.27 &lt; 0.0000000000000002 ***
pctBlack     -0.0132     0.0039   -3.39              0.00069 ***
---
    Null deviance: 988.45  on 10  degrees of freedom
Residual deviance: 307.22  on  8  degrees of freedom
AIC: 351.5</code></pre>
<p>Despite the somewhat strong negative correlation between percent black and distance, the estimated odds ratio for distance remains approximately the same in this new model (OR <span class="math inline">\(= e^{-0.29} = 0.747\)</span>); controlling for percent black does little to change our estimate of the effect of distance. For each additional mile from the proposed railroad, odds of a Yes vote declines by 25.3% after adjusting for the racial composition of a community. We also see that, for a fixed distance from the proposed railroad, the odds of a Yes vote declines by 1.3% (OR <span class="math inline">\(= e^{-.0132} = .987\)</span>) for each additional percent black in the community.</p>
</div>
<div id="sec-logisticInf" class="section level3">
<h3><span class="header-section-number">13.5.4</span> Tests for significance of model coefficients</h3>
<p>Do we have statistically significant evidence that support for the railroad referendum decreases with higher proportions of black residents in a community, after accounting for the distance a community is from the railroad line? As discussed in Section <a href="#sec-PoisInference"><strong>??</strong></a> with Poisson regression, there are two primary approaches to testing signficance of model coefficients: <strong>Drop-in-deviance test to compare models</strong> and <strong>Wald test for a single coefficient</strong>.</p>
<p>With our larger model given by <span class="math inline">\(\log\left(\frac{p_i}{1-p_i}\right) = \beta_0+\beta_1(\textrm{distance}_i)+\beta_2(\textrm{pctBlack}_i)\)</span>, the Wald test produces a highly significant p-value (<span class="math inline">\(Z=\frac{-0.0132}{0.0039}= -3.394\)</span>, <span class="math inline">\(p=.00069\)</span>) indicating significant evidience that support for the railroad referendum decreases with higher proportions of black residents in a community, after adjusting for the distance a community is from the railroad line.</p>
<p>The drop in deviance test would compare the larger model above to the reduced model <span class="math inline">\(\log\left(\frac{p_i}{1-p_i}\right) = \beta_0+\beta_1(\textrm{distance}_i)\)</span> by comparing residual deviances from the two models.</p>
<div class="sourceCode" id="cb840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb840-1"><a href="13-classification.html#cb840-1"></a><span class="kw">anova</span>(model.HaleD, model.HaleBD, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>Analysis of Deviance Table

Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance
Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack
  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    
1         9        318                         
2         8        307  1     11.2  0.00081 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The drop-in-deviance test statistic is <span class="math inline">\(318.44 - 307.22 = 11.22\)</span> on <span class="math inline">\(9 - 8 = 1\)</span> df, producing a p-value of .00081, in close agreement with the Wald test.</p>
<p>A third approach to determining significance of <span class="math inline">\(\beta_2\)</span> would be to generate a 95% confidence interval and then checking if 0 falls within the interval or, equivalently, if 1 falls within a 95% confidence interval for <span class="math inline">\(e^{\beta_2}.\)</span> The next section describes two approaches to producing a confidence interval for coefficients in logistic regression models.</p>
</div>
<div id="confidence-intervals-for-model-coefficients" class="section level3">
<h3><span class="header-section-number">13.5.5</span> Confidence intervals for model coefficients</h3>
<p>Since the Wald statistic follows a normal distribution with <span class="math inline">\(n\)</span> large, we could generate a Wald-type (normal-based) confidence interval for <span class="math inline">\(\beta_2\)</span> using:
<span class="math display">\[\hat\beta_2 \pm 1.96\cdot\textrm{SE}(\hat\beta_2)\]</span>
and then exponentiating endpoints if we prefer a confidence interval for the odds ratio <span class="math inline">\(e^{\beta_2}\)</span>. In this case,
<span class="math display">\[\begin{eqnarray}
95\% \textrm{ CI for } &amp;\beta_2 &amp; = &amp; \hat\beta_2 \pm 1.96 \cdot \textrm{SE}(\hat\beta_2) \\
                       &amp;        &amp; = &amp; -0.0132 \pm 1.96 \cdot 0.0039 \\
                       &amp;        &amp; = &amp; -0.0132 \pm 0.00764 \\
                       &amp;        &amp; = &amp; (-0.0208, -0.0056) \\
95\% \textrm{ CI for } &amp;e^{\beta_2} &amp; = &amp; (e^{-0.0208}, e^{-0.0056}) \\
                       &amp;            &amp; = &amp; (.979, .994) \\
95\% \textrm{ CI for } &amp;e^{10\beta_2} &amp; = &amp; (e^{-0.208}, e^{-0.056}) \\
                       &amp;               &amp; = &amp; (.812, .946)
\end{eqnarray}\]</span>
Thus, we can be 95% confident that a 10% increase in the proportion of black residents is associated with a 5.4% to 18.8% decrease in the odds of a Yes vote for the railroad referendum. This same relationship could be expressed as between a 0.6% and a 2.1% decrease in odds for each 1% increase in the black population, or between a 5.7% (<span class="math inline">\(1/e^{-.056}\)</span>) and a 23.1% (<span class="math inline">\(1/e^{-.208}\)</span>) increase in odds for each 10% decrease in the black population, after adjusting for distance. Of course, with <span class="math inline">\(n=11\)</span>, we should be cautious about relying on a Wald-type interval in this example.</p>
<p>Another approach available in R is the <strong>profile likelihood method</strong>, similar to Chapter <a href="#ch-poissonreg"><strong>??</strong></a>.</p>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb842-1"><a href="13-classification.html#cb842-1"></a><span class="kw">exp</span>(<span class="kw">confint</span>(model.HaleBD))</span></code></pre></div>
<pre><code>             2.5 %  97.5 %
(Intercept) 38.228 122.612
distance     0.728   0.766
pctBlack     0.979   0.994</code></pre>
<p>In the model with <code>distance</code> and <code>pctBlack</code>, the profile likelihood 95% confidence interval for <span class="math inline">\(e^{\beta_2}\)</span> is (.979, .994), which is approximately equal to the Wald-based interval despite the small sample size. We can also confirm the statistically significant association between percent black and odds of voting Yes (after controlling for distance), because 1 is not a plausible value of <span class="math inline">\(e^{\beta_2}\)</span> (where an odds ratio of 1 would imply that the odds of voting Yes do not change with percent black).</p>
</div>
<div id="testing-for-goodness-of-fit" class="section level3">
<h3><span class="header-section-number">13.5.6</span> Testing for goodness of fit</h3>
<p>As in Section <a href="#sec-PoisGOF"><strong>??</strong></a>, we can evaluate the goodness of fit for our model by comparing the residual deviance (307.22) to a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n-p\)</span> (8) degrees of freedom.</p>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb844-1"><a href="13-classification.html#cb844-1"></a><span class="dv">1</span><span class="op">-</span><span class="kw">pchisq</span>(<span class="fl">307.2173</span>, <span class="dv">8</span>)</span></code></pre></div>
<pre><code>[1] 0</code></pre>
<p>The model with <code>pctBlack</code> and <code>distance</code> has statistically significant evidence of lack of fit (<span class="math inline">\(p&lt;.001\)</span>).</p>
<p>Similar to the Poisson regression models, this lack of fit could result from (a) missing covariates, (b) outliers, or (c) overdispersion. We will first attempt to address (a) by fitting a model with an interaction between distance and percent black, to determine whether the effect of racial composition differs based on how far a community is from the proposed railroad.</p>
<pre><code>glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + 
    pctBlack + distance:pctBlack, family = binomial, data = rrHale.df)

Coefficients:
                   Estimate Std. Error z value             Pr(&gt;|z|)    
(Intercept)        7.550902   0.638370   11.83 &lt; 0.0000000000000002 ***
distance          -0.614005   0.057381  -10.70 &lt; 0.0000000000000002 ***
pctBlack          -0.064731   0.009172   -7.06      0.0000000000017 ***
distance:pctBlack  0.005367   0.000898    5.97      0.0000000023207 ***
---
    Null deviance: 988.45  on 10  degrees of freedom
Residual deviance: 274.23  on  7  degrees of freedom
AIC: 320.5</code></pre>
<pre><code>Analysis of Deviance Table

Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack
Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + 
    distance:pctBlack
  Resid. Df Resid. Dev Df Deviance     Pr(&gt;Chi)    
1         8        307                             
2         7        274  1       33 0.0000000093 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We have statistically significant evidence (Wald test: <span class="math inline">\(Z = 5.974, p&lt;.001\)</span>; Drop in deviance test: <span class="math inline">\(\chi^2=32.984, p&lt;.001\)</span>) that the effect of the proportion of the community that is black on the odds of voting Yes depends on the distance of the community from the proposed railroad.</p>
<p>To interpret the interaction coefficient in context, we will compare two cases: one where a community is right on the proposed railroad (<code>distance</code> = 0), and the other where the community is 15 miles away (<code>distance</code> = 15). The significant interaction implies that the effect of <code>pctBlack</code> should differ in these two cases. In the first case, the coefficient for <code>pctBlack</code> is -0.0647, while in the second case, the relevant coefficient is <span class="math inline">\(-0.0647+15(.00537) = 0.0158\)</span>. Thus, for a community right on the proposed railroad, a 1% increase in percent black is associated with a 6.3% (<span class="math inline">\(e^{-.0647}=.937\)</span>) decrease in the odds of voting Yes, while for a community 15 miles away, a 1% increase in percent black is associated with a (<span class="math inline">\(e^{.0158}=1.016\)</span>) 1.6% <em>increase</em> in the odds of voting Yes. A significant interaction term doesn’t always imply a change in the direction of the association, but it does here.</p>
<p>Because our interaction model still exhibits lack of fit (residual deviance of 274.23 on just 7 df), and because we have used the covariates at our disposal, we will assess this model for potential outliers and overdispersion by examining the model’s residuals.</p>
</div>
<div id="residuals-for-binomial-regression" class="section level3">
<h3><span class="header-section-number">13.5.7</span> Residuals for Binomial Regression</h3>
<p>With OLS, residuals were used to assess model assumptions and identify outliers. For binomial regression, two different types of residuals are typically used. One residual, the <strong>Pearson residual</strong>, has a form similar to that used with OLS. Specifically, the Pearson residual is calculated using:
<span class="math display" id="eq:pearsonBinom">\[\begin{equation}
\textrm{Pearson residual}_i = \frac{\textrm{actual count}-\textrm{predicted count}}{\textrm{SD of count}} =
\frac{Y_i-m_i\hat{p_i}}{\sqrt{m_i\hat{p_i}(1-\hat{p_i})}}
\tag{13.12}
\end{equation}\]</span>
where <span class="math inline">\(m_i\)</span> is the number of trials for the <span class="math inline">\(i^{th}\)</span> observation and <span class="math inline">\(\hat{p}_i\)</span> is the estimated probability of success for that same observation.</p>
<p>A <strong>deviance residual</strong> is an alternative residual for binomial regression based on the discrepancy between the observed values and those estimated using the likelihood.
A deviance residual can be calculated for each observation using:
<span class="math display" id="eq:devianceBinom">\[\begin{equation}
\textrm{d}_i = 
\textrm{sign}(Y_i-m_i\hat{p_i})\sqrt{2[Y_i \log\left(\frac{Y_i}{m_i \hat{p_i}}\right)+
(m_i - Y_i) \log\left(\frac{m_i - Y_i}{m_i - m_i \hat{p_i}}\right)]}
\tag{13.13}
\end{equation}\]</span></p>
<p>When the number of trials is large for all of the observations and the models are appropriate, both sets of residuals should follow a standard normal distribution.</p>
<p>The sum of the individual deviance residuals is referred to as the <strong>deviance</strong> or <strong>residual deviance</strong>. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred. In the case of binomial regression, when the denominators, <span class="math inline">\(m_i\)</span>, are large and a model fits, the residual deviance follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n-p\)</span> degrees of freedom (the residual degrees of freedom). Thus for a good fitting model the residual deviance should be approximately equal to its corresponding degrees of freedom. When binomial data meets these conditions, the deviance can be used for a goodness-of-fit test. The p-value for lack-of-fit is the proportion of values from a <span class="math inline">\(\chi_{n-p}^2\)</span> that are greater than the observed residual deviance.</p>
<p>We begin a residual analysis of our interaction model by plotting the residuals against the fitted values in Figure <a href="13-classification.html#fig:resid">13.4</a>. This kind of plot for binomial regression would produce two linear trends with similar negative slopes if there were equal sample sizes <span class="math inline">\(m_i\)</span> for each observation.</p>
<div class="figure" style="text-align: center"><span id="fig:resid"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/resid-1.png" alt="Fitted values by residuals for the interaction model for the Railroad Referendum data." width="60%" />
<p class="caption">
FIGURE 13.4: Fitted values by residuals for the interaction model for the Railroad Referendum data.
</p>
</div>
<p>From this residual plot, Greensboro does not stand out as an outlier. If it did, we could remove Greensboro and refit our interaction model, checking to see if model coefficients changed in a noticeable way. Instead, we will continue to include Greensboro in our modeling efforts. Because the large residual deviance cannot be explained by outliers, and given we have included all of the covariates at hand as well as an interaction term, the observed binomial counts are likely overdispersed. This means that they exhibit more variation than the model would suggest, and we must consider ways to handle this overdispersion.</p>
</div>
<div id="sec-logOverdispersion" class="section level3">
<h3><span class="header-section-number">13.5.8</span> Overdispersion</h3>
<p>Similarly to Poisson regression we can adjust for overdispersion in binomial regression. With overdispersion there is <strong>extra-binomial variation</strong>, so the actual variance will be greater than the variance of a binomial variable, <span class="math inline">\(np(1-p)\)</span>. One way to adjust for overdispersion is to estimate a multiplier (dispersion parameter), <span class="math inline">\(\hat{\phi}\)</span>, for the variance that will inflate it and reflect the reduction in the amount of information we would otherwise have with independent observations. We used a similar approach to adjust for overdispersion in a Poisson regression model in Section <a href="#sec-overdispPois"><strong>??</strong></a>, and we will use the same estimate here: <span class="math inline">\(\hat\phi=\frac{\sum(\textrm{Pearson residuals})^2}{n-p}\)</span>.</p>
<p>When overdispersion is adjusted for in this way, we can no longer use maximum likelihood to fit our regression model; instead we use a quasi-likelihood approach. Quasi-likelihood is similar to likelihood-based inference, but because the model uses the dispersion parameter, it is not a binomial model with a true likelihood. R offers quasi-likelihood as an option when model fitting. The quasi-likelihood approach will yield the same coefficient point estimates as maximum likelihood; however, the variances will be larger in the presence of overdispersion (assuming <span class="math inline">\(\phi&gt;1\)</span>). We will see other ways in which to deal with overdispersion and clusters in the remaining chapters in the book, but the following describes how overdispersion is accounted for using <span class="math inline">\(\hat{\phi}\)</span>:</p>
<p align="center">
<strong>Summary: Accounting for Overdispersion</strong>
</p>
<ul>
<li>Use the dispersion parameter <span class="math inline">\(\hat\phi=\frac{\sum(\textrm{Pearson residuals})^2}{n-p}\)</span> to inflate standard errors of model coefficients</li>
<li>Wald test statistics: multiply the standard errors by <span class="math inline">\(\sqrt{\hat{\phi}}\)</span> so that <span class="math inline">\(\textrm{SE}_\textrm{Q}(\hat\beta)=\sqrt{\hat\phi}\cdot\textrm{SE}(\hat\beta)\)</span> and conduct tests using the <span class="math inline">\(t\)</span>-distribution</li>
<li>CIs use the adjusted standard errors and multiplier based on <span class="math inline">\(t\)</span>, so they are thereby wider: <span class="math inline">\(\hat\beta \pm t_{n-p} \cdot \textrm{SE}_\textrm{Q}(\hat\beta)\)</span></li>
<li>Drop-in-deviance test statistic comparing Model 1 (larger model with <span class="math inline">\(p\)</span> parameters) to Model 2 (smaller model with <span class="math inline">\(q&lt;p\)</span> parameters) =
<span class="math display" id="eq:drop12">\[\begin{equation}
 F = \frac{1}{\hat\phi} \cdot \frac{D_2 - D_1}{p-q}
 \tag{13.14}
 \end{equation}\]</span>
where <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> are the residual deviances for models 1 and 2 respectively and <span class="math inline">\(p-q\)</span> is the difference in the number of parameters for the two models. Note that both <span class="math inline">\(D_2-D_1\)</span> and <span class="math inline">\(p-q\)</span> are positive. This test statistic is compared to an F-distribution with <span class="math inline">\(p-q\)</span> and <span class="math inline">\(n-p\)</span> degrees of freedom.</li>
</ul>
<p>Output for a model which adjusts our interaction model for overdispersion appears below, where <span class="math inline">\(\hat{\phi}=51.6\)</span> is used to adjust the standard errors for the coefficients and the drop-in-deviance tests during model building. Standard errors will be inflated by a factor of <span class="math inline">\(\sqrt{51.6}=7.2\)</span>. As a results, there are no significant terms in the adjusted interaction model below.</p>
<pre><code>glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + 
    pctBlack + distance:pctBlack, family = quasibinomial, data = rrHale.df)

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)        7.55090    4.58546    1.65     0.14
distance          -0.61401    0.41217   -1.49     0.18
pctBlack          -0.06473    0.06589   -0.98     0.36
distance:pctBlack  0.00537    0.00645    0.83     0.43

(Dispersion parameter for quasibinomial family taken to be 51.6)

    Null deviance: 988.45  on 10  degrees of freedom
Residual deviance: 274.23  on  7  degrees of freedom</code></pre>
<p>We therefore remove the interaction term and refit the model, adjusting for the extra-binomial variation that still exists.</p>
<pre><code>glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + 
    pctBlack, family = quasibinomial, data = rrHale.df)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)   4.2220     1.9903    2.12    0.067 .
distance     -0.2917     0.0878   -3.32    0.010 *
pctBlack     -0.0132     0.0261   -0.51    0.626  
---
(Dispersion parameter for quasibinomial family taken to be 44.9)

    Null deviance: 988.45  on 10  degrees of freedom
Residual deviance: 307.22  on  8  degrees of freedom</code></pre>
<p>By removing the interaction term and using the overdispersion parameter, we see that distance is significantly associated with support, but percent black is no longer significant after adjusting for distance.</p>
<p>Because quasi-likelihood methods do not change estimated coefficients, we still estimate a 25% decline <span class="math inline">\((1-e^{-0.292})\)</span> in support for each additional mile from the proposed railroad (odds ratio of .75).</p>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb850-1"><a href="13-classification.html#cb850-1"></a><span class="kw">exp</span>(<span class="kw">confint</span>(model.HaleBDq))</span></code></pre></div>
<pre><code>            2.5 %   97.5 %
(Intercept) 1.361 5006.722
distance    0.609    0.871
pctBlack    0.937    1.044</code></pre>
<p>While we previously found a 95% confidence interval for the odds ratio of (.728, .766), our confidence interval is now much wider: (.609, .871). Appropriately accounting for overdispersion has changed both the significance of certain terms and the precision of our coefficient estimates.</p>
</div>
<div id="summary-5" class="section level3">
<h3><span class="header-section-number">13.5.9</span> Summary</h3>
<p>We began by fitting a logistic regression model with <code>distance</code> alone. Then we added covariate <code>pctBlack</code>, and the Wald-type test and the drop-in-deviance provided strong support for the addition of <code>pctBlack</code> to the model. The model with <code>distance</code> and <code>pctBlack</code> had a large residual deviance suggesting an ill-fitted model. When we looked at the residuals, we saw that Greensboro is an extreme observation. Models without Greensboro were fitted and compared to our initial models. Seeing no appreciable improvement or differences with Greensboro removed, we left it in the model. There remained a large residual deviance so we attempted to account for it by using an estimated dispersion parameter similar to Section <a href="#sec-overdispPois"><strong>??</strong></a> with Poisson regression. The final model included distance and percent black, although percent black was no longer significant after adjusting for overdispersion.</p>
</div>
</div>
<div id="least-squares-regression-vs.-logistic-regression" class="section level2">
<h2><span class="header-section-number">13.6</span> Least Squares Regression vs. Logistic Regression</h2>
<p><span class="math display">\[
\underline{\textrm{Response}} \\
\mathbf{OLS:}\textrm{ normal} \\
\mathbf{Binomial\ Regression:}\textrm{ number of successes in n trials} \\
\textrm{ } \\
\underline{\textrm{Variance}} \\
\mathbf{OLS:}\textrm{ Equal for each level of}\ X \\
\mathbf{Binomial\ Regression:}\ np(1-p)\textrm{ for each level of}\ X \\
\textrm{ } \\
\underline{\textrm{Model Fitting}} \\
\mathbf{OLS:}\ \mu=\beta_0+\beta_1x \textrm{ using Least Squares}\\
\mathbf{Binomial\ Regression:}\ \log\left(\frac{p}{1-p}\right)=\beta_0+\beta_1x \textrm{ using Maximum Likelihood}\\
\textrm{ } \\
\underline{\textrm{EDA}} \\
\mathbf{OLS:}\textrm{ plot $X$ vs. $Y$; add line} \\
\mathbf{Binomial\ Regression:}\textrm{ find $\log(\textrm{odds})$ for several subgroups; plot vs. $X$} \\
\textrm{ } \\
\underline{\textrm{Comparing Models}} \\
\mathbf{OLS:}\textrm{ extra sum of squares F-tests; AIC/BIC} \\
\mathbf{Binomial\ Regression:}\textrm{ Drop in Deviance tests; AIC/BIC} \\
\textrm{ } \\
\underline{\textrm{Interpreting Coefficients}} \\
\mathbf{OLS:}\ \beta_1=\textrm{ change in }\mu_y\textrm{ for unit change in $X$} \\
\mathbf{Binomial\ Regression:}\ e^{\beta_1}=\textrm{ percent change in odds for unit change in $X$} 
\]</span></p>
</div>
<div id="case-study-trying-to-lose-weight" class="section level2">
<h2><span class="header-section-number">13.7</span> Case Study: Trying to Lose Weight</h2>
<p>The final case study uses individual-specific information so that our response, rather than the number of successes out of some number of trials, is simply a binary variable taking on values of 0 or 1 (for failure/success, no/yes, etc.). This type of problem—<strong>binary logistic regression</strong>—is exceedingly common in practice. Here we examine characteristics of young people who are trying to lose weight. The prevalence of obesity among US youth suggests that wanting to lose weight is sensible and desirable for some young people such as those with a high body mass index (BMI). On the flip side, there are young people who do not need to lose weight but make ill-advised attempts to do so nonetheless. A multitude of studies on weight loss focus specifically on youth and propose a variety of motivations for the young wanting to lose weight; athletics and the media are two commonly cited sources of motivation for losing weight for young people.</p>
<p>Sports have been implicated as a reason for young people wanting to shed pounds, but not all studies are consistent with this idea. For example, a study by <span class="citation">(<span class="citeproc-not-found" data-reference-id="Martinsen2009"><strong>???</strong></span>)</span> reported that, despite preconceptions to the contrary, there was a higher rate of self-reported eating disorders among controls (non-elite athletes) as opposed to elite athletes. Interestingly, the kind of sport was not found to be a factor, as participants in leanness sports (for example, distance running, swimming, gymnastics, dance, and diving) did not differ in the proportion with eating disorders when compared to those in non-leanness sports. So, in our analysis, we will not make a distinction between different sports.</p>
<p>Other studies suggest that mass media is the culprit. They argue that students’ exposure to unrealistically thin celebrities may provide unhealthy motivation for some, particularly young women, to try to slim down. An examination and analysis of a large number of related studies (referred to as a <strong>meta-analysis</strong>) <span class="citation">(<span class="citeproc-not-found" data-reference-id="Grabe2008"><strong>???</strong></span>)</span> found a strong relationship between exposure to mass media and the amount of time that adolescents spend talking about what they see in the media, deciphering what it means, and figuring out how they can be more like the celebrities.</p>
<p>We are interested in the following questions: Are the odds that young females report trying to lose weight greater that the odds that males do? Is increasing BMI is associated with an interest in losing weight, regardless of sex? Does sports participation increase the desire to lose weight? Is media exposure is associated with more interest in losing weight?</p>
<p>We have sample of 500 teens from data collected in 2009 through the U.S. Youth Risk Behavior Surveillance System (YRBSS) <span class="citation">(<span class="citeproc-not-found" data-reference-id="YRBS2009"><strong>???</strong></span>)</span>. The YRBSS is an annual national school-based survey conducted by the Centers for Disease Control and Prevention (CDC) and state, territorial, and local education and health agencies and tribal governments. More information on this survey can be found <a href="http://www.cdc.gov/HealthyYouth/yrbs/index.htm">here</a>.</p>
<div id="data-organization-1" class="section level3">
<h3><span class="header-section-number">13.7.1</span> Data Organization</h3>
<p>Here are the three questions from the YRBSS we use for our investigation:</p>
<p>Q66. Which of the following are you trying to do about your weight?</p>
<ul>
<li>A. Lose weight</li>
<li>B. Gain weight</li>
<li>C. Stay the same weight</li>
<li>D. I am not trying to do anything about my weight</li>
</ul>
<p>Q81. On an average school day, how many hours do you watch TV?</p>
<ul>
<li>A. I do not watch TV on an average school day</li>
<li>B. Less than 1 hour per day</li>
<li>C. 1 hour per day</li>
<li>D. 2 hours per day</li>
<li>E. 3 hours per day</li>
<li>F. 4 hours per day</li>
<li>G. 5 or more hours per day</li>
</ul>
<p>Q84. During the past 12 months, on how many sports teams did you play? (Include any teams run by your school or community groups.)</p>
<ul>
<li>A. 0 teams</li>
<li>B. 1 team</li>
<li>C. 2 teams</li>
<li>D. 3 or more teams</li>
</ul>
<p>Answers to Q66 are used to define our response variable: Y = 1 corresponds to “(A) trying to lose weight”, while Y = 0 corresponds to the other non-missing values. Q84 provides information on students’ sports participation and is treated as numerical, 0 through 3, with 3 representing 3 or more. As a proxy for media exposure we use answers to Q81 as numerical values 0, 0.5, 1, 2, 3, 4, and 5, with 5 representing 5 or more. Media exposure and sports participation are also considered as categorical factors, that is, as variables with distinct levels which can be denoted by indicator variables as opposed to their numerical values.</p>
<p>BMI is included in this study as the percentile for a given BMI for members of the same sex. This facilitates comparisons when modeling with males and females. We will use the terms <em>BMI</em> and <em>BMI percentile</em> interchangeably with the understanding that we are always referring to the percentile.</p>
<p>With our sample, we use only the cases that include all of the data for these four questions. This is referred to as a <strong>complete case analysis</strong>. That brings our sample of 500 to 426. There are limitations of complete case analyses that we address in the Discussion.</p>
</div>
<div id="exploratory-data-analysis" class="section level3">
<h3><span class="header-section-number">13.7.2</span> Exploratory Data Analysis</h3>
<p>Nearly half (44.5%) of our sample of 426 youth report that they are trying to lose weight, 49.9% percent of the sample are females, and 56.6% play on one or more sports teams. 9.3 percent report that they do not watch any TV on school days, whereas another 9.3% watched 5 or more hours each day. Interestingly, the median BMI percentile for our 426 youth is 70. The most dramatic difference in the proportions of those who are trying to lose weight is by sex; 58% of the females want to lose weight in contrast to only 31% of the males (see Figure <a href="13-classification.html#fig:mosaicsexlose">13.5</a>). This provides strong support for the inclusion of a sex term in every model considered.</p>
<div class="figure" style="text-align: center"><span id="fig:mosaicsexlose"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/mosaicsexlose-1.png" alt="Weight loss plans vs. Sex" width="60%" />
<p class="caption">
FIGURE 13.5: Weight loss plans vs. Sex
</p>
</div>
<table>
<caption>
<span id="tab:table3chp6">TABLE 13.3: </span>Mean BMI percentile by sex and desire to lose weight.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Sex
</th>
<th style="text-align:left;">
Weight loss status
</th>
<th style="text-align:left;">
mean BMI percentile
</th>
<th style="text-align:left;">
SD
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Female
</td>
<td style="text-align:left;">
No weight loss
</td>
<td style="text-align:left;">
48.2
</td>
<td style="text-align:left;">
26.2
</td>
<td style="text-align:right;">
90
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Lose weight
</td>
<td style="text-align:left;">
76.3
</td>
<td style="text-align:left;">
19.5
</td>
<td style="text-align:right;">
124
</td>
</tr>
<tr>
<td style="text-align:left;">
Male
</td>
<td style="text-align:left;">
No weight loss
</td>
<td style="text-align:left;">
54.9
</td>
<td style="text-align:left;">
28.6
</td>
<td style="text-align:right;">
148
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Lose weight
</td>
<td style="text-align:left;">
84.4
</td>
<td style="text-align:left;">
17.7
</td>
<td style="text-align:right;">
67
</td>
</tr>
</tbody>
</table>
<p>Table <a href="13-classification.html#tab:table3chp6">13.3</a> displays the mean BMI of those wanting and not wanting to lose weight for males and females. The mean BMI is greater for those trying to lose weight compared to those not trying to lose weight, regardless of sex. The size of the difference is remarkably similar for the two sexes.</p>
If we consider including a BMI term in our model(s), the logit should be linearly related to BMI. We can investigate this assumption by constructing an empirical logit plot. In order to calculate empirical logits, we first divide our data by sex. Within each sex, we generate 10 groups of equal sizes, the first holding the bottom 10% in BMI percentile for that sex, the second holding the next lowest 10%, etc.. Within each group, we calculate the proporton, <span class="math inline">\(\hat{p}\)</span> that reported wanting to lose weight, and then the empirical log odds, <span class="math inline">\(log(\frac{\hat{p}}{1-\hat{p}})\)</span>, that a young person in that group wants to lose weight.
<div class="figure" style="text-align: center"><span id="fig:logitBMIsex"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/logitBMIsex-1.png" alt="Empirical logits of trying to lose weight by BMI and Sex." width="60%" />
<p class="caption">
FIGURE 13.6: Empirical logits of trying to lose weight by BMI and Sex.
</p>
</div>
<p>Figure <a href="13-classification.html#fig:logitBMIsex">13.6</a> presents the empirical logits for the BMI intervals by sex. Both males and females exhibit an increasing linear trend on the logit scale indicating that increasing BMI is associated a greater desire to lose weight and that modeling log odds as a linear function of BMI is reasonable. The slope for the females appears to be similar to the slope for males, so we do not need to consider an interaction term between BMI and sex in the model.</p>
<div class="figure" style="text-align: center"><span id="fig:mosaicsexsports"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/mosaicsexsports-1.png" alt="Weight loss plans vs. sex and sports participation" width="60%" />
<p class="caption">
FIGURE 13.7: Weight loss plans vs. sex and sports participation
</p>
</div>
<p>Out of those who play sports, 43% want to lose weight whereas 46% want to lose weight among those who do not play sports. Figure <a href="13-classification.html#fig:mosaicsexsports">13.7</a> compares the proportion of respondents who want to lose weight by their sex and sport participation. The data suggest that sports participation is associated with the same or even a slightly lower desire to lose weight, contrary to what had originally been hypothesized. While the overall levels of those wanting to lose weight differ considerably between the sexes, the differences between those in and out of sports within sex appear to be very small. A term for sports participation or number of teams will be considered, but there is not compelling evidence that an interaction term will be needed.</p>
<p>It was posited that increased exposure to media, here measured as hours of TV daily, is associated with increased desire to lose weight, particularly for females. Overall, the percentage who want to lose weight ranges from 35% of those watching 5 hours of TV per day to 52% among those watching 3 hours daily. There is minimal variation in the proportion wanting to lose weight with both sexes combined. However, we are more interested in differences between the sexes (see Figure <a href="13-classification.html#fig:mediaXsex">13.8</a>). We create empirical logits using the proportion of students trying to lose weight for each level of hours spent watching daily and look at the trends in the logits separately for males and females. From Figure <a href="13-classification.html#fig:logitmediasex">13.9</a>, there does not appear to be a linear relationship for males or females.</p>
<div class="figure" style="text-align: center"><span id="fig:mediaXsex"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/mediaXsex-1.png" alt="Weight loss plans vs. daily hours of TV and sex." width="60%" />
<p class="caption">
FIGURE 13.8: Weight loss plans vs. daily hours of TV and sex.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:logitmediasex"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/logitmediasex-1.png" alt="Empirical logits for the odds of trying to lose weight by TV watching and sex." width="60%" />
<p class="caption">
FIGURE 13.9: Empirical logits for the odds of trying to lose weight by TV watching and sex.
</p>
</div>
</div>
<div id="initial-models-1" class="section level3">
<h3><span class="header-section-number">13.7.3</span> Initial Models</h3>
<p>Our strategy for modeling is to use our questions of interest and what we have learned in the exploratory data analysis. For each model we interpret the coefficient of interest, look at the corresponding Wald test and, as a final step, compare the deviances for the different models we considered.</p>
<p>We first use a model where sex is our only predictor.</p>
<pre><code>glm(formula = lose.wt.01 ~ female, family = binomial, data = risk2009)

Coefficients:
            Estimate Std. Error z value  Pr(&gt;|z|)    
(Intercept)   -0.597      0.148   -4.04 0.0000532 ***
female         0.944      0.198    4.77 0.0000018 ***
---
    Null deviance: 607.92  on 438  degrees of freedom
Residual deviance: 584.45  on 437  degrees of freedom
AIC: 588.5</code></pre>
<p>Our estimated binomial regression model is:
<span class="math display">\[\log\left(\frac{{p}}{1-{p}}\right)=-0.79+1.11 (\textrm{female})\]</span>
where <span class="math inline">\({p}\)</span> is the estimated proportion of youth wanting to lose weight. We can interpret the coefficient on female by exponentiating <span class="math inline">\(e^{1.1130} = 3.04\)</span> (95% CI = <span class="math inline">\((2.05, 4.54)\)</span>) indicating that the odds of a female trying to lose weight is over three times the odds of a male trying to lose weight (<span class="math inline">\(Z=5.506\)</span>, <span class="math inline">\(p=3.67e-08\)</span>). We retain sex in the model and consider adding the BMI percentile:</p>
<pre><code>glm(formula = lose.wt.01 ~ female + bmipct, family = binomial, 
    data = risk2009)

Coefficients:
            Estimate Std. Error z value             Pr(&gt;|z|)    
(Intercept) -3.40051    0.40554   -8.39 &lt; 0.0000000000000002 ***
female       1.35923    0.23198    5.86  0.00000000464976492 ***
bmipct       0.03900    0.00482    8.09  0.00000000000000061 ***
---
    Null deviance: 607.92  on 438  degrees of freedom
Residual deviance: 498.64  on 436  degrees of freedom
AIC: 504.6</code></pre>
<p>We see that there is statistically significant evidence (<span class="math inline">\(Z=9.067, p&lt;.001\)</span>) that BMI is positively associated with the odds of trying to lose weight, after controlling for sex. Clearly BMI percentile belongs in the model with sex.</p>
<p>Our estimated binomial regression model is:
<span class="math display">\[\log\left(\frac{{p}}{1-{p}}\right)= -4.46+1.54\textrm{female}+0.051\textrm{bmipct}\]</span></p>
<p>To interpret the coefficient on <code>bmipct</code>, we will consider a 10 unit increase in <code>bmipct</code>. Because <span class="math inline">\(e^{10*0.0509}=1.664\)</span>, then there is an estimated 66.4% increase in the odds of wanting to lose weight for each additional 10 percentile points of BMI for members of the same sex. Just as we had done in other multiple regression models we need to interpret our coefficient <em>given that the other variables remain constant</em>. An interaction term for BMI by sex was tested (not shown) and it was not significant (<span class="math inline">\(Z=-0.405\)</span>, <span class="math inline">\(p=0.6856\)</span>), so the effect of BMI does not differ by sex.</p>
<p>We next add <code>sport</code> to our model. Sports participation was considered for inclusion in the model in three ways: an indicator of sports participation (0 = no teams, 1 = one or more teams), treating the number of teams (0, 1, 2, or 3) as numeric, and treating the number of teams as a factor. The models below treat sports participation using an indicator variable, but all three models produced similar results.</p>
<pre><code>glm(formula = lose.wt.01 ~ female + bmipct + sport, family = binomial, 
    data = risk2009)

Coefficients:
            Estimate Std. Error z value            Pr(&gt;|z|)    
(Intercept) -3.40264    0.43028   -7.91 0.00000000000000262 ***
female       1.35958    0.23318    5.83 0.00000000552187757 ***
bmipct       0.03900    0.00482    8.08 0.00000000000000063 ***
sportSports  0.00325    0.21933    0.01                0.99    

    Null deviance: 607.92  on 438  degrees of freedom
Residual deviance: 498.64  on 435  degrees of freedom</code></pre>
<pre><code>glm(formula = lose.wt.01 ~ female + bmipct + sport + female:sport + 
    bmipct:sport, family = binomial, data = risk2009)

Coefficients:
                   Estimate Std. Error z value    Pr(&gt;|z|)    
(Intercept)        -3.04792    0.56322   -5.41 0.000000062 ***
female              1.13005    0.33724    3.35     0.00081 ***
bmipct              0.03582    0.00663    5.40 0.000000067 ***
sportSports        -0.69916    0.81496   -0.86     0.39094    
female:sportSports  0.44009    0.46892    0.94     0.34798    
bmipct:sportSports  0.00669    0.00967    0.69     0.48896    

    Null deviance: 607.92  on 438  degrees of freedom
Residual deviance: 497.59  on 433  degrees of freedom</code></pre>
<p>Sports teams were not significant in any of these models, nor were interaction terms (sex by sports) and (bmipct by sports). As a result, sports participation was no longer considered for inclusion in the model.</p>
<p>We last look at adding <code>media</code> to our model.</p>
<pre><code>glm(formula = lose.wt.01 ~ bmipct + female + media, family = binomial, 
    data = risk2009)

Coefficients:
            Estimate Std. Error z value            Pr(&gt;|z|)    
(Intercept) -3.35140    0.41414   -8.09 0.00000000000000059 ***
bmipct       0.03942    0.00489    8.06 0.00000000000000077 ***
female       1.37105    0.23322    5.88 0.00000000413223676 ***
media       -0.03990    0.06999   -0.57                0.57    
---
    Null deviance: 607.92  on 438  degrees of freedom
Residual deviance: 498.31  on 435  degrees of freedom
AIC: 506.3</code></pre>
<p>Media is not a statistically significant term (<span class="math inline">\(p=0.456\)</span>, <span class="math inline">\(Z=-0.745\)</span>). However, because our interest centers on how media may affect attempts to lose weight and how its effect might be different for females and males, we fit a model with a media term and a sex by media interaction term (not shown). Neither term was statistically significant, so we have no support in our data that media exposure as measured by hours spent watching TV is associated with the odds a teen is trying to lose weight after accounting for sex and BMI.</p>
</div>
<div id="drop-in-deviance-tests" class="section level3">
<h3><span class="header-section-number">13.7.4</span> Drop-in-deviance Tests</h3>
<pre><code>Analysis of Deviance Table

Model 1: lose.wt.01 ~ female
Model 2: lose.wt.01 ~ female + bmipct
Model 3: lose.wt.01 ~ female + bmipct + sport
Model 4: lose.wt.01 ~ bmipct + female + media
  Resid. Df Resid. Dev Df Deviance            Pr(&gt;Chi)    
1       437        584                                    
2       436        499  1     85.8 &lt;0.0000000000000002 ***
3       435        499  1      0.0                0.99    
4       435        498  0      0.3                        
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>       df AIC
model1  2 588
model2  3 505
model3  4 507
model4  4 506</code></pre>
<p>Comparing models using differences in deviances requires that the models be <strong>nested</strong>, meaning each smaller model is a simplified version of the larger model. In our case, Models 1, 2, and 4 are nested, as are Models 1, 2, and 3, but Models 3 and 4 cannot be compared using a drop-in-deviance test.</p>
<p>There is a large drop-in-deviance adding BMI to the model with sex (Model 1 to Model 2, 123.13), which is clearly statistically significant when compared to a <span class="math inline">\(\chi^2\)</span> distribution with 1 df. The drop-in-deviance for adding an indicator variable for sports to the model with sex and BMI is only 434.88 - 434.75 = 0.13. There is a difference of a single parameter, so the drop-in-deviance would be compared to a <span class="math inline">\(\chi^2\)</span> distribution with 1 df. The resulting <span class="math inline">\(p\)</span>-value is very large (.7188) suggesting that adding an indicator for sports is not helpful once we’ve already accounted for BMI and sex. For comparing Models 3 and 4, one approach is to look at the AIC. In this case, the AIC is (barely) smaller for the model with media, providing evidence that the latter model is slightly preferable.</p>
</div>
<div id="model-discussion-and-summary" class="section level3">
<h3><span class="header-section-number">13.7.5</span> Model Discussion and Summary</h3>
<p>We found that the odds of wanting to lose weight are considerably greater for females compared to males. In addition, respondents with greater BMI percentiles express a greater desire to lose weight for members of the same sex. Regardless of sex or BMI percentile, sports participation and TV watching are not associated with different odds for wanting to lose weight.</p>
<p>A limitation of this analysis is that we used complete cases in place of a method of imputing responses or modeling missingness. This reduced our sample from 500 to 429, and it may have introduced bias. For example, if respondents who watch a lot of TV were unwilling to reveal as much, and if they differed with respect to their desire to lose weight from those respondents who reported watching little TV, our inferences regarding the relationship between lots of TV and desire to lose weight may be biased.</p>
<p>Other limitations may result from definitions. Trying to lose weight is self-reported and may not correlate with any action undertaken to do so. The number of sports teams may not accurately reflect sports related pressures to lose weight. For example, elite athletes may focus on a single sport and be subject to greater pressures whereas athletes who casually participate in three sports may not feel any pressure to lose weight. Hours spent watching TV is not likely to encompass the totality of media exposure, particularly because exposure to celebrities occurs often online. Furthermore, this analysis does not explore in any detail maladaptions—inappropriate motivations for wanting to lose weight. For example, we did not focus our study on subsets of respondents with low BMI who are attempting to lose weight.</p>
<p>It would be instructive to use data science methodologies to explore the entire data set of 16,000 instead of sampling 500. However, the types of exploration and models used here could translate to the larger sample size.</p>
<p>Finally a limitation may be introduced as a result of the acknowledged variation in the administration of the YRBSS. States and local authorities are allowed to administer the survey as they see fit, which at times results in significant variation in sample selection and response.</p>
</div>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">13.8</span> Classification and regression trees (CART)</h2>
<div id="the-curse-of-dimensionality" class="section level3">
<h3><span class="header-section-number">13.8.1</span> The curse of dimensionality</h3>
<p>We described how methods such as LDA and QDA are not meant to be used with many predictors <span class="math inline">\(p\)</span> because the number of parameters that we need to estimate becomes too large. For example, with the digits example <span class="math inline">\(p=784\)</span>, we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the <em>curse of dimensionality</em>. The <em>dimension</em> here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</p>
<p>For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-585-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to <span class="math inline">\(\sqrt{.10} \approx .316\)</span>:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-586-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>.
In general, to include 10% of the data in a case with <span class="math inline">\(p\)</span> dimensions, we need an interval with each side of size <span class="math inline">\(\sqrt[p]{.10}\)</span> of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.</p>
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb859-1"><a href="13-classification.html#cb859-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb859-2"><a href="13-classification.html#cb859-2"></a>p &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></span>
<span id="cb859-3"><a href="13-classification.html#cb859-3"></a><span class="kw">qplot</span>(p, <span class="fl">.1</span><span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>p), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-587-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</p>
<p>Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.</p>
</div>
<div id="cart-motivation" class="section level3">
<h3><span class="header-section-number">13.8.2</span> CART motivation</h3>
<p>To motivate this section, we will use a new dataset
that includes the breakdown of the composition of olive oil into 8 fatty acids:</p>
<div class="sourceCode" id="cb860"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb860-1"><a href="13-classification.html#cb860-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb860-2"><a href="13-classification.html#cb860-2"></a><span class="kw">library</span>(dslabs)</span></code></pre></div>
<pre><code>
Attaching package: &#39;dslabs&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:gapminder&#39;:

    gapminder</code></pre>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb863-1"><a href="13-classification.html#cb863-1"></a><span class="kw">data</span>(<span class="st">&quot;olive&quot;</span>)</span>
<span id="cb863-2"><a href="13-classification.html#cb863-2"></a><span class="kw">names</span>(olive)</span></code></pre></div>
<pre><code> [1] &quot;region&quot;      &quot;area&quot;        &quot;palmitic&quot;    &quot;palmitoleic&quot; &quot;stearic&quot;    
 [6] &quot;oleic&quot;       &quot;linoleic&quot;    &quot;linolenic&quot;   &quot;arachidic&quot;   &quot;eicosenoic&quot; </code></pre>
<p>For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.</p>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb865-1"><a href="13-classification.html#cb865-1"></a><span class="kw">table</span>(olive<span class="op">$</span>region)</span></code></pre></div>
<pre><code>
Northern Italy       Sardinia Southern Italy 
           151             98            323 </code></pre>
<p>We remove the <code>area</code> column because we won’t use it as a predictor.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb867-1"><a href="13-classification.html#cb867-1"></a>olive &lt;-<span class="st"> </span><span class="kw">select</span>(olive, <span class="op">-</span>area)</span></code></pre></div>
<p>Let’s very quickly try to predict the region using kNN:</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb868-1"><a href="13-classification.html#cb868-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb868-2"><a href="13-classification.html#cb868-2"></a>fit &lt;-<span class="st"> </span><span class="kw">train</span>(region <span class="op">~</span><span class="st"> </span>.,  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </span>
<span id="cb868-3"><a href="13-classification.html#cb868-3"></a>             <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">2</span>)), </span>
<span id="cb868-4"><a href="13-classification.html#cb868-4"></a>             <span class="dt">data =</span> olive)</span>
<span id="cb868-5"><a href="13-classification.html#cb868-5"></a><span class="kw">ggplot</span>(fit)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-591-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb869-1"><a href="13-classification.html#cb869-1"></a>olive <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(fatty_acid, percentage, <span class="op">-</span>region) <span class="op">%&gt;%</span></span>
<span id="cb869-2"><a href="13-classification.html#cb869-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(region, percentage, <span class="dt">fill =</span> region)) <span class="op">+</span></span>
<span id="cb869-3"><a href="13-classification.html#cb869-3"></a><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span></span>
<span id="cb869-4"><a href="13-classification.html#cb869-4"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>fatty_acid, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb869-5"><a href="13-classification.html#cb869-5"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_blank</span>(), <span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-592-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.</p>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb870-1"><a href="13-classification.html#cb870-1"></a>olive <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb870-2"><a href="13-classification.html#cb870-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(eicosenoic, linoleic, <span class="dt">color =</span> region)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb870-3"><a href="13-classification.html#cb870-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb870-4"><a href="13-classification.html#cb870-4"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.065</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb870-5"><a href="13-classification.html#cb870-5"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="fl">-0.2</span>, <span class="dt">y =</span> <span class="fl">10.54</span>, <span class="dt">xend =</span> <span class="fl">0.065</span>, <span class="dt">yend =</span> <span class="fl">10.54</span>, </span>
<span id="cb870-6"><a href="13-classification.html#cb870-6"></a>               <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-593-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>In Section <a href="#predictor-space"><strong>??</strong></a> we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye,
we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than <span class="math inline">\(10.535\)</span>, predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-594-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Decision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:</p>
<p><img src="images/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin: auto;" /></p>
<p>(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>.)</p>
<p>A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>.
Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by partitioning the predictors.</p>
</div>
<div id="regression-trees" class="section level3">
<h3><span class="header-section-number">13.8.3</span> Regression trees</h3>
<p>When the outcome is continuous, we call the method a <em>regression</em> tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation <span class="math inline">\(f(x) = \mbox{E}(Y | X = x)\)</span> with <span class="math inline">\(Y\)</span> the poll margin and <span class="math inline">\(x\)</span> the day.</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb871-1"><a href="13-classification.html#cb871-1"></a><span class="kw">data</span>(<span class="st">&quot;polls_2008&quot;</span>)</span>
<span id="cb871-2"><a href="13-classification.html#cb871-2"></a><span class="kw">qplot</span>(day, margin, <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-596-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. A mathematical way to describe this is to say that we are partitioning the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations <span class="math inline">\(y_i\)</span> for which the associated predictor <span class="math inline">\(x_i\)</span> is also in <span class="math inline">\(R_j\)</span>.</p>
<p>But how do we decide on the partition <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and how do we choose <span class="math inline">\(J\)</span>? Here is where the algorithm gets a bit complicated.</p>
<p>Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later.</p>
<p>Once we select a partition <span class="math inline">\(\mathbf{x}\)</span> to split in order to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, which we will call <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>In our current example we only have one predictor, so we will always choose <span class="math inline">\(j=1\)</span>, but in general this will not be the case. Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning, we compute predictors by taking the average of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>But how do we pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>? Basically we find the pair that minimizes the residual sum of square (RSS):
<span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>This is then applied recursively to the new regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the <code>rpart</code> function in the <strong>rpart</strong> package.</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb872-1"><a href="13-classification.html#cb872-1"></a><span class="kw">library</span>(rpart)</span>
<span id="cb872-2"><a href="13-classification.html#cb872-2"></a>fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span></code></pre></div>
<p>Here, there is only one predictor. Thus we do not have to decide which predictor <span class="math inline">\(j\)</span> to split by, we simply have to decide what value <span class="math inline">\(s\)</span> we use to split. We can visually see where the splits were made:</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb873-1"><a href="13-classification.html#cb873-1"></a><span class="kw">plot</span>(fit, <span class="dt">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb873-2"><a href="13-classification.html#cb873-2"></a><span class="kw">text</span>(fit, <span class="dt">cex =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-599-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate <span class="math inline">\(\hat{f}(x)\)</span> looks like this:</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb874-1"><a href="13-classification.html#cb874-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb874-2"><a href="13-classification.html#cb874-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb874-3"><a href="13-classification.html#cb874-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb874-4"><a href="13-classification.html#cb874-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb874-5"><a href="13-classification.html#cb874-5"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-600-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made.</p>
<p>First we need to define the term <em>complexity parameter</em> (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the <em>complexity parameter</em> (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.</p>
<p>However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the <code>rpart</code> function is <code>minsplit</code> and the default is 20. The <code>rpart</code> implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is <code>minbucket</code> and defaults to <code>round(minsplit/3)</code>.</p>
<p>As expected, if we set <code>cp = 0</code> and <code>minsplit = 2</code>, then our prediction is as flexible as possible and our predictor is our original data:</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb875-1"><a href="13-classification.html#cb875-1"></a>fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>, </span>
<span id="cb875-2"><a href="13-classification.html#cb875-2"></a>             <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb875-3"><a href="13-classification.html#cb875-3"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb875-4"><a href="13-classification.html#cb875-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb875-5"><a href="13-classification.html#cb875-5"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb875-6"><a href="13-classification.html#cb875-6"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb875-7"><a href="13-classification.html#cb875-7"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-601-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Intuitively we know that this is not a good approach as it will generally result in over-training. These <code>cp</code>, <code>minsplit</code>, and <code>minbucket</code>, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.</p>
<p>So how do we pick these parameters? We can use cross validation, described in Chapter <a href="14-machine.html#cross-validation">14.6</a>, just like with any tuning parameter. Here is an example of using cross validation to chose cp.</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb876-1"><a href="13-classification.html#cb876-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb876-2"><a href="13-classification.html#cb876-2"></a>train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(margin <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb876-3"><a href="13-classification.html#cb876-3"></a>                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb876-4"><a href="13-classification.html#cb876-4"></a>                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="fl">0.05</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</span>
<span id="cb876-5"><a href="13-classification.html#cb876-5"></a>                     <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span>
<span id="cb876-6"><a href="13-classification.html#cb876-6"></a><span class="kw">ggplot</span>(train_rpart)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-602-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>To see the resulting tree, we access the <code>finalModel</code> and plot it:</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb877-1"><a href="13-classification.html#cb877-1"></a><span class="kw">plot</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb877-2"><a href="13-classification.html#cb877-2"></a><span class="kw">text</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">cex =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-604-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>And because we only have one predictor, we can actually plot <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb878-1"><a href="13-classification.html#cb878-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb878-2"><a href="13-classification.html#cb878-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(train_rpart)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb878-3"><a href="13-classification.html#cb878-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb878-4"><a href="13-classification.html#cb878-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb878-5"><a href="13-classification.html#cb878-5"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-605-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Note that if we already have a tree and want to apply a higher cp value, we can use the <code>prune</code> function. We call this <em>pruning</em> a tree because we are snipping off partitions that do not meet a <code>cp</code> criterion. We previously created a tree that used a <code>cp = 0</code> and saved it to <code>fit</code>. We can prune it like this:</p>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb879-1"><a href="13-classification.html#cb879-1"></a>pruned_fit &lt;-<span class="st"> </span><span class="kw">prune</span>(fit, <span class="dt">cp =</span> <span class="fl">0.01</span>)</span></code></pre></div>
</div>
<div id="classification-decision-trees" class="section level3">
<h3><span class="header-section-number">13.8.4</span> Classification (decision) trees</h3>
<p>Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.</p>
<p>The first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).</p>
<p>The second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the <em>Gini Index</em> and <em>Entropy</em>.</p>
<p>In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The <em>Gini Index</em> is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define <span class="math inline">\(\hat{p}_{j,k}\)</span> as the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. The Gini Index is defined as</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.</p>
<p><em>Entropy</em> is a very similar quantity, defined as</p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<p>Let us look at how a classification tree performs on the digits example we examined before:</p>
<p>We can use this code to run the algorithm and plot the resulting tree:</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb880-1"><a href="13-classification.html#cb880-1"></a>train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb880-2"><a href="13-classification.html#cb880-2"></a>                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb880-3"><a href="13-classification.html#cb880-3"></a>                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</span>
<span id="cb880-4"><a href="13-classification.html#cb880-4"></a>                     <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</span>
<span id="cb880-5"><a href="13-classification.html#cb880-5"></a><span class="kw">plot</span>(train_rpart)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-609-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>The accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods:</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb881-1"><a href="13-classification.html#cb881-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_rpart, mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb881-2"><a href="13-classification.html#cb881-2"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span></code></pre></div>
<pre><code>Accuracy 
    0.82 </code></pre>
<p>The plot of the estimated conditional probability shows us the limitations of classification trees:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-611-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">13.9</span> Random forests</h2>
<p>Random forests are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<p>1. Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p>
<p>2. For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p>
<p>3. For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<p>1. Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p>
<p>2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p>
<p>To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to
the 2008 polls data. We will use the <code>randomForest</code> function in the <strong>randomForest</strong> package:</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="13-classification.html#cb883-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb883-2"><a href="13-classification.html#cb883-2"></a>fit &lt;-<span class="st"> </span><span class="kw">randomForest</span>(margin<span class="op">~</span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>) </span></code></pre></div>
<p>Note that if we apply the function <code>plot</code> to the resulting object, stored in <code>fit</code>, we see how the error rate of our algorithm changes as we add trees.</p>
<div class="sourceCode" id="cb884"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb884-1"><a href="13-classification.html#cb884-1"></a>rafalib<span class="op">::</span><span class="kw">mypar</span>()</span>
<span id="cb884-2"><a href="13-classification.html#cb884-2"></a><span class="kw">plot</span>(fit)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-614-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.</p>
<p>The resulting estimate for this random forest can be seen like this:</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb885-1"><a href="13-classification.html#cb885-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></span>
<span id="cb885-2"><a href="13-classification.html#cb885-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> polls_<span class="dv">2008</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb885-3"><a href="13-classification.html#cb885-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb885-4"><a href="13-classification.html#cb885-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb885-5"><a href="13-classification.html#cb885-5"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-615-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of <span class="math inline">\(b\)</span> and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.</p>
<p><img src="images/rf.gif" width="100%" style="display: block; margin: auto;" /></p>
<p>Here is the random forest fit for our digits example based on two predictors:</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb886-1"><a href="13-classification.html#cb886-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb886-2"><a href="13-classification.html#cb886-2"></a>train_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train)</span>
<span id="cb886-3"><a href="13-classification.html#cb886-3"></a></span>
<span id="cb886-4"><a href="13-classification.html#cb886-4"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf, mnist_<span class="dv">27</span><span class="op">$</span>test),</span>
<span id="cb886-5"><a href="13-classification.html#cb886-5"></a>                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span></code></pre></div>
<pre><code>Accuracy 
    0.79 </code></pre>
<p>Here is what the conditional probabilities look like:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-618-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the <strong>caret</strong> package to optimize over the minimum node size. Because, this is not one of the parameters that the <strong>caret</strong> package optimizes by default we will write our own code:</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb888-1"><a href="13-classification.html#cb888-1"></a>nodesize &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">51</span>, <span class="dv">10</span>)</span>
<span id="cb888-2"><a href="13-classification.html#cb888-2"></a>acc &lt;-<span class="st"> </span><span class="kw">sapply</span>(nodesize, <span class="cf">function</span>(ns){</span>
<span id="cb888-3"><a href="13-classification.html#cb888-3"></a>  <span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train,</span>
<span id="cb888-4"><a href="13-classification.html#cb888-4"></a>               <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">mtry =</span> <span class="dv">2</span>),</span>
<span id="cb888-5"><a href="13-classification.html#cb888-5"></a>               <span class="dt">nodesize =</span> ns)<span class="op">$</span>results<span class="op">$</span>Accuracy</span>
<span id="cb888-6"><a href="13-classification.html#cb888-6"></a>})</span>
<span id="cb888-7"><a href="13-classification.html#cb888-7"></a><span class="kw">qplot</span>(nodesize, acc)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-619-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="13-classification.html#cb889-1"></a>train_rf_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train,</span>
<span id="cb889-2"><a href="13-classification.html#cb889-2"></a>                           <span class="dt">nodesize =</span> nodesize[<span class="kw">which.max</span>(acc)])</span>
<span id="cb889-3"><a href="13-classification.html#cb889-3"></a></span>
<span id="cb889-4"><a href="13-classification.html#cb889-4"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf_<span class="dv">2</span>, mnist_<span class="dv">27</span><span class="op">$</span>test),</span>
<span id="cb889-5"><a href="13-classification.html#cb889-5"></a>                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span></code></pre></div>
<pre><code>Accuracy 
   0.815 </code></pre>
<p>The selected model improves accuracy and provides a smoother estimate.</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-621-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that we can avoid writing our own code by using other random forest implementations as described in the <strong>caret</strong> manual<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>.</p>
<p>Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine <em>variable importance</em>.
To define <em>variable importance</em> we count how often a predictor is used in the individual trees. You can learn more about <em>variable importance</em> in an advanced machine learning book<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>. The <strong>caret</strong> package includes the function <code>varImp</code> that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="23">
<li id="fn23"><p><a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2" class="uri">https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2</a><a href="13-classification.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p><a href="http://topepo.github.io/caret/available-models.html" class="uri">http://topepo.github.io/caret/available-models.html</a><a href="13-classification.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a><a href="13-classification.html#fnref25" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-machine.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
