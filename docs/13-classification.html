<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science</title>
  <meta name="description" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="davidkane9/PPBDS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Classification | Preceptor’s Primer for Bayesian Data Science" />
  
  
  

<meta name="author" content="David Kane" />


<meta name="date" content="2020-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12-multiple-regression.html"/>
<link rel="next" href="14-machine-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="forward.html"><a href="forward.html"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="warning.html"><a href="warning.html"><i class="fa fa-check"></i>Warning</a></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="1" data-path="1-getting-started.html"><a href="1-getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#r-rstudio"><i class="fa fa-check"></i><b>1.1</b> What are R and RStudio?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-getting-started.html"><a href="1-getting-started.html#installing"><i class="fa fa-check"></i><b>1.1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#using-r-via-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Using R via RStudio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-getting-started.html"><a href="1-getting-started.html#code"><i class="fa fa-check"></i><b>1.2</b> How do I code in R?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-getting-started.html"><a href="1-getting-started.html#programming-concepts"><i class="fa fa-check"></i><b>1.2.1</b> Basic programming concepts and terminology</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-getting-started.html"><a href="1-getting-started.html#messages"><i class="fa fa-check"></i><b>1.2.2</b> Errors, warnings, and messages</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-getting-started.html"><a href="1-getting-started.html#tips-code"><i class="fa fa-check"></i><b>1.2.3</b> Tips on learning to code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-getting-started.html"><a href="1-getting-started.html#packages"><i class="fa fa-check"></i><b>1.3</b> What are R packages?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-getting-started.html"><a href="1-getting-started.html#package-installation"><i class="fa fa-check"></i><b>1.3.1</b> Package installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-getting-started.html"><a href="1-getting-started.html#package-loading"><i class="fa fa-check"></i><b>1.3.2</b> Package loading</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-getting-started.html"><a href="1-getting-started.html#package-use"><i class="fa fa-check"></i><b>1.3.3</b> Package use</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13"><i class="fa fa-check"></i><b>1.4</b> Explore your first datasets</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-getting-started.html"><a href="1-getting-started.html#nycflights13-package"><i class="fa fa-check"></i><b>1.4.1</b> <code>nycflights13</code> package</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-getting-started.html"><a href="1-getting-started.html#flights-data-frame"><i class="fa fa-check"></i><b>1.4.2</b> <code>flights</code> data frame</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-getting-started.html"><a href="1-getting-started.html#exploredataframes"><i class="fa fa-check"></i><b>1.4.3</b> Exploring data frames</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-getting-started.html"><a href="1-getting-started.html#identification-vs-measurement-variables"><i class="fa fa-check"></i><b>1.4.4</b> Identification and measurement variables</a></li>
<li class="chapter" data-level="1.4.5" data-path="1-getting-started.html"><a href="1-getting-started.html#help-files"><i class="fa fa-check"></i><b>1.4.5</b> Help files</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-getting-started.html"><a href="1-getting-started.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-getting-started.html"><a href="1-getting-started.html#additional-resources"><i class="fa fa-check"></i><b>1.5.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-viz.html"><a href="2-viz.html"><i class="fa fa-check"></i><b>2</b> Visualization</a><ul>
<li class="chapter" data-level="" data-path="2-viz.html"><a href="2-viz.html#needed-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="2.1" data-path="2-viz.html"><a href="2-viz.html#grammarofgraphics"><i class="fa fa-check"></i><b>2.1</b> The grammar of graphics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-viz.html"><a href="2-viz.html#components-of-the-grammar"><i class="fa fa-check"></i><b>2.1.1</b> Components of the grammar</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-viz.html"><a href="2-viz.html#gapminder"><i class="fa fa-check"></i><b>2.1.2</b> Gapminder data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-viz.html"><a href="2-viz.html#other-components"><i class="fa fa-check"></i><b>2.1.3</b> Other components</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-viz.html"><a href="2-viz.html#ggplot2-package"><i class="fa fa-check"></i><b>2.1.4</b> ggplot2 package</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-viz.html"><a href="2-viz.html#scatterplots"><i class="fa fa-check"></i><b>2.2</b> Scatterplots</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-viz.html"><a href="2-viz.html#geompoint"><i class="fa fa-check"></i><b>2.2.1</b> Scatterplots via <code>geom_point</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="2-viz.html"><a href="2-viz.html#overplotting"><i class="fa fa-check"></i><b>2.2.2</b> Overplotting</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-viz.html"><a href="2-viz.html#summary"><i class="fa fa-check"></i><b>2.2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-viz.html"><a href="2-viz.html#linegraphs"><i class="fa fa-check"></i><b>2.3</b> Linegraphs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-viz.html"><a href="2-viz.html#geomline"><i class="fa fa-check"></i><b>2.3.1</b> Linegraphs via <code>geom_line</code></a></li>
<li class="chapter" data-level="2.3.2" data-path="2-viz.html"><a href="2-viz.html#summary-1"><i class="fa fa-check"></i><b>2.3.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-viz.html"><a href="2-viz.html#histograms"><i class="fa fa-check"></i><b>2.4</b> Histograms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-viz.html"><a href="2-viz.html#geomhistogram"><i class="fa fa-check"></i><b>2.4.1</b> Histograms via <code>geom_histogram</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="2-viz.html"><a href="2-viz.html#adjustbins"><i class="fa fa-check"></i><b>2.4.2</b> Adjusting the bins</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-viz.html"><a href="2-viz.html#summary-2"><i class="fa fa-check"></i><b>2.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-viz.html"><a href="2-viz.html#facets"><i class="fa fa-check"></i><b>2.5</b> Facets</a></li>
<li class="chapter" data-level="2.6" data-path="2-viz.html"><a href="2-viz.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-viz.html"><a href="2-viz.html#geomboxplot"><i class="fa fa-check"></i><b>2.6.1</b> Boxplots via <code>geom_boxplot</code></a></li>
<li class="chapter" data-level="2.6.2" data-path="2-viz.html"><a href="2-viz.html#summary-3"><i class="fa fa-check"></i><b>2.6.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2-viz.html"><a href="2-viz.html#geombar"><i class="fa fa-check"></i><b>2.7</b> Barplots</a><ul>
<li class="chapter" data-level="2.7.1" data-path="2-viz.html"><a href="2-viz.html#barplots-via-geom_bar-or-geom_col"><i class="fa fa-check"></i><b>2.7.1</b> Barplots via <code>geom_bar</code> or <code>geom_col</code></a></li>
<li class="chapter" data-level="2.7.2" data-path="2-viz.html"><a href="2-viz.html#must-avoid-pie-charts"><i class="fa fa-check"></i><b>2.7.2</b> Must avoid pie charts!</a></li>
<li class="chapter" data-level="2.7.3" data-path="2-viz.html"><a href="2-viz.html#two-categ-barplot"><i class="fa fa-check"></i><b>2.7.3</b> Two categorical variables</a></li>
<li class="chapter" data-level="2.7.4" data-path="2-viz.html"><a href="2-viz.html#summary-4"><i class="fa fa-check"></i><b>2.7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="2-viz.html"><a href="2-viz.html#conclusion-1"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a><ul>
<li class="chapter" data-level="2.8.1" data-path="2-viz.html"><a href="2-viz.html#summary-table"><i class="fa fa-check"></i><b>2.8.1</b> Summary table</a></li>
<li class="chapter" data-level="2.8.2" data-path="2-viz.html"><a href="2-viz.html#function-argument-specification"><i class="fa fa-check"></i><b>2.8.2</b> Function argument specification</a></li>
<li class="chapter" data-level="2.8.3" data-path="2-viz.html"><a href="2-viz.html#additional-resources-1"><i class="fa fa-check"></i><b>2.8.3</b> Additional resources</a></li>
<li class="chapter" data-level="2.8.4" data-path="2-viz.html"><a href="2-viz.html#whats-to-come-3"><i class="fa fa-check"></i><b>2.8.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-productivity.html"><a href="3-productivity.html"><i class="fa fa-check"></i><b>3</b> Productivity</a><ul>
<li class="chapter" data-level="3.1" data-path="3-productivity.html"><a href="3-productivity.html#set-up"><i class="fa fa-check"></i><b>3.1</b> Set Up</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-mac"><i class="fa fa-check"></i><b>3.1.1</b> Accessing the terminal on a Mac</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>3.1.2</b> Installing Git on the Mac</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-productivity.html"><a href="3-productivity.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>3.1.3</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-productivity.html"><a href="3-productivity.html#terminal-on-windows"><i class="fa fa-check"></i><b>3.1.4</b> Accessing the terminal on Windows</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-productivity.html"><a href="3-productivity.html#unix"><i class="fa fa-check"></i><b>3.2</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-productivity.html"><a href="3-productivity.html#naming-convention"><i class="fa fa-check"></i><b>3.2.1</b> Naming convention</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-productivity.html"><a href="3-productivity.html#the-terminal"><i class="fa fa-check"></i><b>3.2.2</b> The terminal</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-productivity.html"><a href="3-productivity.html#filesystem"><i class="fa fa-check"></i><b>3.2.3</b> The filesystem</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-productivity.html"><a href="3-productivity.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>3.2.4</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-productivity.html"><a href="3-productivity.html#the-home-directory"><i class="fa fa-check"></i><b>3.2.5</b> The home directory</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-productivity.html"><a href="3-productivity.html#working-directory"><i class="fa fa-check"></i><b>3.2.6</b> Working directory</a></li>
<li class="chapter" data-level="3.2.7" data-path="3-productivity.html"><a href="3-productivity.html#paths"><i class="fa fa-check"></i><b>3.2.7</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-productivity.html"><a href="3-productivity.html#unix-commands"><i class="fa fa-check"></i><b>3.3</b> Unix commands</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-productivity.html"><a href="3-productivity.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>3.3.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-productivity.html"><a href="3-productivity.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>3.3.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-productivity.html"><a href="3-productivity.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>3.3.3</b> <code>cd</code>: navigating the filesystem by changing directories</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-productivity.html"><a href="3-productivity.html#some-examples"><i class="fa fa-check"></i><b>3.3.4</b> Some examples</a></li>
<li class="chapter" data-level="3.3.5" data-path="3-productivity.html"><a href="3-productivity.html#more-unix-commands"><i class="fa fa-check"></i><b>3.3.5</b> More Unix commands</a></li>
<li class="chapter" data-level="3.3.6" data-path="3-productivity.html"><a href="3-productivity.html#advanced-unix"><i class="fa fa-check"></i><b>3.3.6</b> Advanced Unix</a></li>
<li class="chapter" data-level="3.3.7" data-path="3-productivity.html"><a href="3-productivity.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>3.3.7</b> File manipulation in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-productivity.html"><a href="3-productivity.html#git"><i class="fa fa-check"></i><b>3.4</b> Git and GitHub</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-productivity.html"><a href="3-productivity.html#github-accounts"><i class="fa fa-check"></i><b>3.4.1</b> GitHub accounts</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-productivity.html"><a href="3-productivity.html#github-repos"><i class="fa fa-check"></i><b>3.4.2</b> GitHub repositories</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-productivity.html"><a href="3-productivity.html#git-overview"><i class="fa fa-check"></i><b>3.4.3</b> Overview of Git</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-git"><i class="fa fa-check"></i><b>3.4.4</b> Using Git and GitHub in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-productivity.html"><a href="3-productivity.html#r"><i class="fa fa-check"></i><b>3.5</b> R</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-productivity.html"><a href="3-productivity.html#rstudio-projects"><i class="fa fa-check"></i><b>3.5.1</b> RStudio projects</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-productivity.html"><a href="3-productivity.html#r-markdown"><i class="fa fa-check"></i><b>3.5.2</b> R markdown</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-productivity.html"><a href="3-productivity.html#help-for-r"><i class="fa fa-check"></i><b>3.5.3</b> Help for R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-wrangling.html"><a href="4-wrangling.html"><i class="fa fa-check"></i><b>4</b> Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#piping"><i class="fa fa-check"></i><b>4.1</b> The pipe operator: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.2" data-path="4-wrangling.html"><a href="4-wrangling.html#filter"><i class="fa fa-check"></i><b>4.2</b> <code>filter</code> rows</a></li>
<li class="chapter" data-level="4.3" data-path="4-wrangling.html"><a href="4-wrangling.html#summarize"><i class="fa fa-check"></i><b>4.3</b> <code>summarize</code> variables</a></li>
<li class="chapter" data-level="4.4" data-path="4-wrangling.html"><a href="4-wrangling.html#groupby"><i class="fa fa-check"></i><b>4.4</b> <code>group_by</code> rows</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-wrangling.html"><a href="4-wrangling.html#grouping-by-more-than-one-variable"><i class="fa fa-check"></i><b>4.4.1</b> Grouping by more than one variable</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-wrangling.html"><a href="4-wrangling.html#mutate"><i class="fa fa-check"></i><b>4.5</b> <code>mutate</code> existing variables</a></li>
<li class="chapter" data-level="4.6" data-path="4-wrangling.html"><a href="4-wrangling.html#arrange"><i class="fa fa-check"></i><b>4.6</b> <code>arrange</code> and sort rows</a></li>
<li class="chapter" data-level="4.7" data-path="4-wrangling.html"><a href="4-wrangling.html#factors"><i class="fa fa-check"></i><b>4.7</b> Factors</a><ul>
<li class="chapter" data-level="4.7.1" data-path="4-wrangling.html"><a href="4-wrangling.html#the-forcats-package"><i class="fa fa-check"></i><b>4.7.1</b> The <strong>forcats</strong> package</a></li>
<li class="chapter" data-level="4.7.2" data-path="4-wrangling.html"><a href="4-wrangling.html#dropping-unused-levels"><i class="fa fa-check"></i><b>4.7.2</b> Dropping unused levels</a></li>
<li class="chapter" data-level="4.7.3" data-path="4-wrangling.html"><a href="4-wrangling.html#reorder-factors"><i class="fa fa-check"></i><b>4.7.3</b> Change order of the levels, principled</a></li>
<li class="chapter" data-level="4.7.4" data-path="4-wrangling.html"><a href="4-wrangling.html#change-order-of-the-levels-because-i-said-so"><i class="fa fa-check"></i><b>4.7.4</b> Change order of the levels, “because I said so”</a></li>
<li class="chapter" data-level="4.7.5" data-path="4-wrangling.html"><a href="4-wrangling.html#recode-the-levels"><i class="fa fa-check"></i><b>4.7.5</b> Recode the levels</a></li>
<li class="chapter" data-level="4.7.6" data-path="4-wrangling.html"><a href="4-wrangling.html#grow-a-factor"><i class="fa fa-check"></i><b>4.7.6</b> Grow a factor</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors"><i class="fa fa-check"></i><b>4.8</b> Character Vectors</a><ul>
<li class="chapter" data-level="4.8.1" data-path="4-wrangling.html"><a href="4-wrangling.html#manipulating-character-vectors"><i class="fa fa-check"></i><b>4.8.1</b> Manipulating character vectors</a></li>
<li class="chapter" data-level="4.8.2" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-resources"><i class="fa fa-check"></i><b>4.8.2</b> Regular expressions resources</a></li>
<li class="chapter" data-level="4.8.3" data-path="4-wrangling.html"><a href="4-wrangling.html#character-encoding-resources"><i class="fa fa-check"></i><b>4.8.3</b> Character encoding resources</a></li>
<li class="chapter" data-level="4.8.4" data-path="4-wrangling.html"><a href="4-wrangling.html#character-vectors-that-live-in-a-data-frame"><i class="fa fa-check"></i><b>4.8.4</b> Character vectors that live in a data frame</a></li>
<li class="chapter" data-level="4.8.5" data-path="4-wrangling.html"><a href="4-wrangling.html#regex-free-string-manipulation-with-stringr-and-tidyr"><i class="fa fa-check"></i><b>4.8.5</b> Regex-free string manipulation with stringr and tidyr</a></li>
<li class="chapter" data-level="4.8.6" data-path="4-wrangling.html"><a href="4-wrangling.html#detect-or-filter-on-a-target-string"><i class="fa fa-check"></i><b>4.8.6</b> Detect or filter on a target string</a></li>
<li class="chapter" data-level="4.8.7" data-path="4-wrangling.html"><a href="4-wrangling.html#string-splitting-by-delimiter"><i class="fa fa-check"></i><b>4.8.7</b> String splitting by delimiter</a></li>
<li class="chapter" data-level="4.8.8" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-extraction-and-replacement-by-position"><i class="fa fa-check"></i><b>4.8.8</b> Substring extraction (and replacement) by position</a></li>
<li class="chapter" data-level="4.8.9" data-path="4-wrangling.html"><a href="4-wrangling.html#collapse-a-vector"><i class="fa fa-check"></i><b>4.8.9</b> Collapse a vector</a></li>
<li class="chapter" data-level="4.8.10" data-path="4-wrangling.html"><a href="4-wrangling.html#catenate-vectors"><i class="fa fa-check"></i><b>4.8.10</b> Create a character vector by catenating multiple vectors</a></li>
<li class="chapter" data-level="4.8.11" data-path="4-wrangling.html"><a href="4-wrangling.html#substring-replacement"><i class="fa fa-check"></i><b>4.8.11</b> Substring replacement</a></li>
<li class="chapter" data-level="4.8.12" data-path="4-wrangling.html"><a href="4-wrangling.html#regular-expressions-with-stringr"><i class="fa fa-check"></i><b>4.8.12</b> Regular expressions with stringr</a></li>
<li class="chapter" data-level="4.8.13" data-path="4-wrangling.html"><a href="4-wrangling.html#characters-with-special-meaning"><i class="fa fa-check"></i><b>4.8.13</b> Characters with special meaning</a></li>
<li class="chapter" data-level="4.8.14" data-path="4-wrangling.html"><a href="4-wrangling.html#character-classes"><i class="fa fa-check"></i><b>4.8.14</b> Character classes</a></li>
<li class="chapter" data-level="4.8.15" data-path="4-wrangling.html"><a href="4-wrangling.html#quantifiers"><i class="fa fa-check"></i><b>4.8.15</b> Quantifiers</a></li>
<li class="chapter" data-level="4.8.16" data-path="4-wrangling.html"><a href="4-wrangling.html#escaping"><i class="fa fa-check"></i><b>4.8.16</b> Escaping</a></li>
<li class="chapter" data-level="4.8.17" data-path="4-wrangling.html"><a href="4-wrangling.html#groups-and-backreferences"><i class="fa fa-check"></i><b>4.8.17</b> Groups and backreferences</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="4-wrangling.html"><a href="4-wrangling.html#combining-data"><i class="fa fa-check"></i><b>4.9</b> Combining Data</a><ul>
<li class="chapter" data-level="4.9.1" data-path="4-wrangling.html"><a href="4-wrangling.html#bind"><i class="fa fa-check"></i><b>4.9.1</b> Bind</a></li>
<li class="chapter" data-level="4.9.2" data-path="4-wrangling.html"><a href="4-wrangling.html#joins-in-dplyr"><i class="fa fa-check"></i><b>4.9.2</b> Joins in dplyr</a></li>
<li class="chapter" data-level="4.9.3" data-path="4-wrangling.html"><a href="4-wrangling.html#joining"><i class="fa fa-check"></i><b>4.9.3</b> Joining</a></li>
<li class="chapter" data-level="4.9.4" data-path="4-wrangling.html"><a href="4-wrangling.html#matching-key-variable-names"><i class="fa fa-check"></i><b>4.9.4</b> Matching “key” variable names</a></li>
<li class="chapter" data-level="4.9.5" data-path="4-wrangling.html"><a href="4-wrangling.html#diff-key"><i class="fa fa-check"></i><b>4.9.5</b> Different “key” variable names</a></li>
<li class="chapter" data-level="4.9.6" data-path="4-wrangling.html"><a href="4-wrangling.html#multiple-key-variables"><i class="fa fa-check"></i><b>4.9.6</b> Multiple “key” variables</a></li>
<li class="chapter" data-level="4.9.7" data-path="4-wrangling.html"><a href="4-wrangling.html#normal-forms"><i class="fa fa-check"></i><b>4.9.7</b> Normal forms</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="4-wrangling.html"><a href="4-wrangling.html#other-verbs"><i class="fa fa-check"></i><b>4.10</b> Other Verbs</a><ul>
<li class="chapter" data-level="4.10.1" data-path="4-wrangling.html"><a href="4-wrangling.html#select"><i class="fa fa-check"></i><b>4.10.1</b> <code>select</code> variables</a></li>
<li class="chapter" data-level="4.10.2" data-path="4-wrangling.html"><a href="4-wrangling.html#rename"><i class="fa fa-check"></i><b>4.10.2</b> <code>rename</code> variables</a></li>
<li class="chapter" data-level="4.10.3" data-path="4-wrangling.html"><a href="4-wrangling.html#top_n-values-of-a-variable"><i class="fa fa-check"></i><b>4.10.3</b> <code>top_n</code> values of a variable</a></li>
<li class="chapter" data-level="4.10.4" data-path="4-wrangling.html"><a href="4-wrangling.html#slice-and-pull-and"><i class="fa fa-check"></i><b>4.10.4</b> <code>slice</code> and <code>pull</code> and <code>[]</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="4-wrangling.html"><a href="4-wrangling.html#conclusion-2"><i class="fa fa-check"></i><b>4.11</b> Conclusion</a><ul>
<li class="chapter" data-level="4.11.1" data-path="4-wrangling.html"><a href="4-wrangling.html#summary-table-1"><i class="fa fa-check"></i><b>4.11.1</b> Summary table</a></li>
<li class="chapter" data-level="4.11.2" data-path="4-wrangling.html"><a href="4-wrangling.html#additional-resources-2"><i class="fa fa-check"></i><b>4.11.2</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-tidy.html"><a href="5-tidy.html"><i class="fa fa-check"></i><b>5</b> Tidy</a><ul>
<li class="chapter" data-level="5.1" data-path="5-tidy.html"><a href="5-tidy.html#csv"><i class="fa fa-check"></i><b>5.1</b> Importing data</a></li>
<li class="chapter" data-level="5.2" data-path="5-tidy.html"><a href="5-tidy.html#web-scraping"><i class="fa fa-check"></i><b>5.2</b> Web scraping</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-tidy.html"><a href="5-tidy.html#html"><i class="fa fa-check"></i><b>5.2.1</b> HTML</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-tidy.html"><a href="5-tidy.html#the-rvest-package"><i class="fa fa-check"></i><b>5.2.2</b> The rvest package</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-tidy.html"><a href="5-tidy.html#css-selectors"><i class="fa fa-check"></i><b>5.2.3</b> CSS selectors</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-tidy.html"><a href="5-tidy.html#json"><i class="fa fa-check"></i><b>5.2.4</b> JSON</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-tidy.html"><a href="5-tidy.html#tidy-data-ex"><i class="fa fa-check"></i><b>5.3</b> “Tidy” data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-tidy.html"><a href="5-tidy.html#tidy-definition"><i class="fa fa-check"></i><b>5.3.1</b> Definition of “tidy” data</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-tidy.html"><a href="5-tidy.html#converting-to-tidy-data"><i class="fa fa-check"></i><b>5.3.2</b> Converting to “tidy” data</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-tidy.html"><a href="5-tidy.html#nycflights13-package-1"><i class="fa fa-check"></i><b>5.3.3</b> <code>nycflights13</code> package</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-tidy.html"><a href="5-tidy.html#case-study-tidy"><i class="fa fa-check"></i><b>5.4</b> Case study: Democracy in Guatemala</a></li>
<li class="chapter" data-level="5.5" data-path="5-tidy.html"><a href="5-tidy.html#tidyverse-package"><i class="fa fa-check"></i><b>5.5</b> <code>tidyverse</code> package</a></li>
<li class="chapter" data-level="5.6" data-path="5-tidy.html"><a href="5-tidy.html#conclusion-3"><i class="fa fa-check"></i><b>5.6</b> Conclusion</a><ul>
<li class="chapter" data-level="5.6.1" data-path="5-tidy.html"><a href="5-tidy.html#additional-resources-3"><i class="fa fa-check"></i><b>5.6.1</b> Additional resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-functions.html"><a href="6-functions.html"><i class="fa fa-check"></i><b>6</b> Functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-functions.html"><a href="6-functions.html#part-1"><i class="fa fa-check"></i><b>6.1</b> Part 1</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works"><i class="fa fa-check"></i><b>6.1.1</b> Get something that works</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-functions.html"><a href="6-functions.html#skateboard-perfectly-formed-rear-view-mirror"><i class="fa fa-check"></i><b>6.1.2</b> Skateboard &gt;&gt; perfectly formed rear-view mirror</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function"><i class="fa fa-check"></i><b>6.1.3</b> Turn the working interactive code into a function</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-functions.html"><a href="6-functions.html#test-your-function"><i class="fa fa-check"></i><b>6.1.4</b> Test your function</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-functions.html"><a href="6-functions.html#check-the-validity-of-arguments"><i class="fa fa-check"></i><b>6.1.5</b> Check the validity of arguments</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next"><i class="fa fa-check"></i><b>6.1.6</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-functions.html"><a href="6-functions.html#part-2"><i class="fa fa-check"></i><b>6.2</b> Part 2</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-functions.html"><a href="6-functions.html#load-the-gapminder-data"><i class="fa fa-check"></i><b>6.2.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function"><i class="fa fa-check"></i><b>6.2.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-functions.html"><a href="6-functions.html#generalize-our-function-to-other-quantiles"><i class="fa fa-check"></i><b>6.2.3</b> Generalize our function to other quantiles</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-functions.html"><a href="6-functions.html#get-something-that-works-again"><i class="fa fa-check"></i><b>6.2.4</b> Get something that works, again</a></li>
<li class="chapter" data-level="6.2.5" data-path="6-functions.html"><a href="6-functions.html#turn-the-working-interactive-code-into-a-function-again"><i class="fa fa-check"></i><b>6.2.5</b> Turn the working interactive code into a function, again</a></li>
<li class="chapter" data-level="6.2.6" data-path="6-functions.html"><a href="6-functions.html#argument-names-freedom-and-conventions"><i class="fa fa-check"></i><b>6.2.6</b> Argument names: freedom and conventions</a></li>
<li class="chapter" data-level="6.2.7" data-path="6-functions.html"><a href="6-functions.html#what-a-function-returns"><i class="fa fa-check"></i><b>6.2.7</b> What a function returns</a></li>
<li class="chapter" data-level="6.2.8" data-path="6-functions.html"><a href="6-functions.html#default-values-freedom-to-not-specify-the-arguments"><i class="fa fa-check"></i><b>6.2.8</b> Default values: freedom to NOT specify the arguments</a></li>
<li class="chapter" data-level="6.2.9" data-path="6-functions.html"><a href="6-functions.html#wrap-up-and-whats-next-1"><i class="fa fa-check"></i><b>6.2.9</b> Wrap-up and what’s next?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-functions.html"><a href="6-functions.html#part-3"><i class="fa fa-check"></i><b>6.3</b> Part 3</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-functions.html"><a href="6-functions.html#load-the-gapminder-data-1"><i class="fa fa-check"></i><b>6.3.1</b> Load the Gapminder data</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-functions.html"><a href="6-functions.html#restore-our-max-minus-min-function-1"><i class="fa fa-check"></i><b>6.3.2</b> Restore our max minus min function</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-functions.html"><a href="6-functions.html#be-proactive-about-nas"><i class="fa fa-check"></i><b>6.3.3</b> Be proactive about <code>NA</code>s</a></li>
<li class="chapter" data-level="6.3.4" data-path="6-functions.html"><a href="6-functions.html#the-useful-but-mysterious-...-argument"><i class="fa fa-check"></i><b>6.3.4</b> The useful but mysterious <code>...</code> argument</a></li>
<li class="chapter" data-level="6.3.5" data-path="6-functions.html"><a href="6-functions.html#use-testthat-for-formal-unit-tests"><i class="fa fa-check"></i><b>6.3.5</b> Use testthat for formal unit tests</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-functions.html"><a href="6-functions.html#list-columns-and-map_-functions"><i class="fa fa-check"></i><b>6.4</b> List columns and <code>map_*</code> functions</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-functions.html"><a href="6-functions.html#what-are-list-columns"><i class="fa fa-check"></i><b>6.4.1</b> What are list columns?</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-functions.html"><a href="6-functions.html#creating-list-columns-with-mutate"><i class="fa fa-check"></i><b>6.4.2</b> Creating list columns with <code>mutate()</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="6-functions.html"><a href="6-functions.html#map_-functions"><i class="fa fa-check"></i><b>6.4.3</b> <code>map_*</code> functions</a></li>
<li class="chapter" data-level="6.4.4" data-path="6-functions.html"><a href="6-functions.html#using-map_-functions-to-create-list-columns"><i class="fa fa-check"></i><b>6.4.4</b> Using <code>map_*</code> functions to create list columns</a></li>
<li class="chapter" data-level="6.4.5" data-path="6-functions.html"><a href="6-functions.html#practice-with-map_-functions-and-list-columns"><i class="fa fa-check"></i><b>6.4.5</b> Practice with <code>map_*</code> functions and list columns</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probability.html"><a href="7-probability.html"><i class="fa fa-check"></i><b>7</b> Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probability.html"><a href="7-probability.html#basicsOfProbability"><i class="fa fa-check"></i><b>7.1</b> Defining probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-probability.html"><a href="7-probability.html#intro-questions"><i class="fa fa-check"></i><b>7.1.1</b> Intro Questions</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-probability.html"><a href="7-probability.html#probability-1"><i class="fa fa-check"></i><b>7.1.2</b> Probability</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-probability.html"><a href="7-probability.html#disjoint-or-mutually-exclusive-outcomes"><i class="fa fa-check"></i><b>7.1.3</b> Disjoint or mutually exclusive outcomes</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-probability.html"><a href="7-probability.html#probabilities-when-events-are-not-disjoint"><i class="fa fa-check"></i><b>7.1.4</b> Probabilities when events are not disjoint</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-probability.html"><a href="7-probability.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.5</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.6" data-path="7-probability.html"><a href="7-probability.html#complement-of-an-event"><i class="fa fa-check"></i><b>7.1.6</b> Complement of an event</a></li>
<li class="chapter" data-level="7.1.7" data-path="7-probability.html"><a href="7-probability.html#probabilityIndependence"><i class="fa fa-check"></i><b>7.1.7</b> Independence</a></li>
<li class="chapter" data-level="7.1.8" data-path="7-probability.html"><a href="7-probability.html#conditionalProbabilitySection"><i class="fa fa-check"></i><b>7.1.8</b> Conditional probability</a></li>
<li class="chapter" data-level="7.1.9" data-path="7-probability.html"><a href="7-probability.html#marginalAndJointProbabilities"><i class="fa fa-check"></i><b>7.1.9</b> Marginal and joint probabilities</a></li>
<li class="chapter" data-level="7.1.10" data-path="7-probability.html"><a href="7-probability.html#defining-conditional-probability"><i class="fa fa-check"></i><b>7.1.10</b> Defining conditional probability</a></li>
<li class="chapter" data-level="7.1.11" data-path="7-probability.html"><a href="7-probability.html#smallpox-in-boston-1721"><i class="fa fa-check"></i><b>7.1.11</b> Smallpox in Boston, 1721</a></li>
<li class="chapter" data-level="7.1.12" data-path="7-probability.html"><a href="7-probability.html#general-multiplication-rule"><i class="fa fa-check"></i><b>7.1.12</b> General multiplication rule</a></li>
<li class="chapter" data-level="7.1.13" data-path="7-probability.html"><a href="7-probability.html#tree-diagrams"><i class="fa fa-check"></i><b>7.1.13</b> Tree diagrams</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-probability.html"><a href="7-probability.html#randomVariablesSection"><i class="fa fa-check"></i><b>7.2</b> Random variables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-probability.html"><a href="7-probability.html#expectation"><i class="fa fa-check"></i><b>7.2.1</b> Expectation</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-probability.html"><a href="7-probability.html#variability-in-random-variables"><i class="fa fa-check"></i><b>7.2.2</b> Variability in random variables</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-probability.html"><a href="7-probability.html#linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.2.3</b> Linear combinations of random variables</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-probability.html"><a href="7-probability.html#variability-in-linear-combinations-of-random-variables"><i class="fa fa-check"></i><b>7.2.4</b> Variability in linear combinations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-probability.html"><a href="7-probability.html#appendixA"><i class="fa fa-check"></i><b>7.3</b> Statistical Background</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-probability.html"><a href="7-probability.html#appendix-stat-terms"><i class="fa fa-check"></i><b>7.3.1</b> Basic statistical terms</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-probability.html"><a href="7-probability.html#appendix-normal-curve"><i class="fa fa-check"></i><b>7.3.2</b> Normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html"><i class="fa fa-check"></i><b>8</b> Bayes’s Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conditional-probability"><i class="fa fa-check"></i><b>8.1</b> Conditional probability</a></li>
<li class="chapter" data-level="8.2" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#conjoint-probability"><i class="fa fa-check"></i><b>8.2</b> Conjoint probability</a></li>
<li class="chapter" data-level="8.3" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-cookie-problem"><i class="fa fa-check"></i><b>8.3</b> The cookie problem</a></li>
<li class="chapter" data-level="8.4" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#bayess-theorem-1"><i class="fa fa-check"></i><b>8.4</b> Bayes’s Theorem</a></li>
<li class="chapter" data-level="8.5" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-diachronic-interpretation"><i class="fa fa-check"></i><b>8.5</b> The diachronic interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-mm-problem"><i class="fa fa-check"></i><b>8.6</b> The M&amp;M problem</a></li>
<li class="chapter" data-level="8.7" data-path="8-bayess-theorem.html"><a href="8-bayess-theorem.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>8.7</b> The Monty Hall problem</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-sampling.html"><a href="9-sampling.html"><i class="fa fa-check"></i><b>9</b> Sampling</a><ul>
<li class="chapter" data-level="" data-path="9-sampling.html"><a href="9-sampling.html#needed-packages-1"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="9.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-activity"><i class="fa fa-check"></i><b>9.1</b> Sampling urn activity</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-sampling.html"><a href="9-sampling.html#what-proportion-of-this-urns-balls-are-red"><i class="fa fa-check"></i><b>9.1.1</b> What proportion of this urn’s balls are red?</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-shovel-once"><i class="fa fa-check"></i><b>9.1.2</b> Using the shovel once</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-sampling.html"><a href="9-sampling.html#student-shovels"><i class="fa fa-check"></i><b>9.1.3</b> Using the shovel 33 times</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-sampling.html"><a href="9-sampling.html#what-did-we-just-do"><i class="fa fa-check"></i><b>9.1.4</b> What did we just do?</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-simulation"><i class="fa fa-check"></i><b>9.2</b> Virtual sampling</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-once"><i class="fa fa-check"></i><b>9.2.1</b> Using the virtual shovel once</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-sampling.html"><a href="9-sampling.html#using-the-virtual-shovel-33-times"><i class="fa fa-check"></i><b>9.2.2</b> Using the virtual shovel 33 times</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-sampling.html"><a href="9-sampling.html#shovel-1000-times"><i class="fa fa-check"></i><b>9.2.3</b> Using the virtual shovel 1,000 times</a></li>
<li class="chapter" data-level="9.2.4" data-path="9-sampling.html"><a href="9-sampling.html#different-shovels"><i class="fa fa-check"></i><b>9.2.4</b> Using different shovels</a></li>
<li class="chapter" data-level="9.2.5" data-path="9-sampling.html"><a href="9-sampling.html#using-many-shovels-at-once"><i class="fa fa-check"></i><b>9.2.5</b> Using many shovels at once</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-sampling.html"><a href="9-sampling.html#sampling-framework"><i class="fa fa-check"></i><b>9.3</b> Sampling framework</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-sampling.html"><a href="9-sampling.html#terminology-and-notation"><i class="fa fa-check"></i><b>9.3.1</b> Terminology and notation</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-sampling.html"><a href="9-sampling.html#sampling-definitions"><i class="fa fa-check"></i><b>9.3.2</b> Statistical definitions</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-sampling.html"><a href="9-sampling.html#moral-of-the-story"><i class="fa fa-check"></i><b>9.3.3</b> The moral of the story</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-sampling.html"><a href="9-sampling.html#sampling-case-study"><i class="fa fa-check"></i><b>9.4</b> Case study: Polls</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9-sampling.html"><a href="9-sampling.html#sampling-conclusion-central-limit-theorem"><i class="fa fa-check"></i><b>9.4.1</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9-sampling.html"><a href="9-sampling.html#conclusion-4"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html"><i class="fa fa-check"></i><b>10</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#needed-packages-2"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="10.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-tactile"><i class="fa fa-check"></i><b>10.1</b> Pennies activity</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-is-the-average-year-on-us-pennies-in-2019"><i class="fa fa-check"></i><b>10.1.1</b> What is the average year on US pennies in 2019?</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-once"><i class="fa fa-check"></i><b>10.1.2</b> Resampling once</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#student-resamples"><i class="fa fa-check"></i><b>10.1.3</b> Resampling 35 times</a></li>
<li class="chapter" data-level="10.1.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#what-did-we-just-do-1"><i class="fa fa-check"></i><b>10.1.4</b> What did we just do?</a></li>
<li class="chapter" data-level="10.1.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#resampling-simulation"><i class="fa fa-check"></i><b>10.1.5</b> Virtually resampling once</a></li>
<li class="chapter" data-level="10.1.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-35-replicates"><i class="fa fa-check"></i><b>10.1.6</b> Virtually resampling 35 times</a></li>
<li class="chapter" data-level="10.1.7" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-1000-replicates"><i class="fa fa-check"></i><b>10.1.7</b> Virtually resampling 1,000 times</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build-up"><i class="fa fa-check"></i><b>10.2</b> Measuring uncertainty with confidence intervals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#percentile-method"><i class="fa fa-check"></i><b>10.2.1</b> Percentile method</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#se-method"><i class="fa fa-check"></i><b>10.2.2</b> Standard error method</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#one-prop-ci"><i class="fa fa-check"></i><b>10.2.3</b> Interpreting confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-width"><i class="fa fa-check"></i><b>10.3</b> Width of confidence intervals</a><ul>
<li class="chapter" data-level="10.3.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-confidence-level"><i class="fa fa-check"></i><b>10.3.1</b> Impact of confidence level</a></li>
<li class="chapter" data-level="10.3.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#fitting-multiple-models-using-map"><i class="fa fa-check"></i><b>10.3.2</b> Fitting multiple models using <code>map()</code></a></li>
<li class="chapter" data-level="10.3.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#impact-of-sample-size"><i class="fa fa-check"></i><b>10.3.3</b> Impact of sample size</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut"><i class="fa fa-check"></i><b>10.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="10.5" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#case-study-two-prop-ci"><i class="fa fa-check"></i><b>10.5</b> Case study: Is yawning contagious?</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#mythbusters-study-data"><i class="fa fa-check"></i><b>10.5.1</b> <em>Mythbusters</em> study data</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#sampling-scenario"><i class="fa fa-check"></i><b>10.5.2</b> Sampling scenario</a></li>
<li class="chapter" data-level="10.5.3" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-build"><i class="fa fa-check"></i><b>10.5.3</b> Constructing the confidence interval</a></li>
<li class="chapter" data-level="10.5.4" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#using-lm-and-tidy-as-a-shortcut-1"><i class="fa fa-check"></i><b>10.5.4</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#ci-conclusion"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a><ul>
<li class="chapter" data-level="10.6.1" data-path="10-confidence-intervals.html"><a href="10-confidence-intervals.html#bootstrap-vs-sampling"><i class="fa fa-check"></i><b>10.6.1</b> Comparing bootstrap and sampling distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-regression.html"><a href="11-regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="" data-path="11-regression.html"><a href="11-regression.html#needed-packages-3"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="11.1" data-path="11-regression.html"><a href="11-regression.html#model1"><i class="fa fa-check"></i><b>11.1</b> Teaching evaluations: one numerical explanatory variable</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-regression.html"><a href="11-regression.html#model1EDA"><i class="fa fa-check"></i><b>11.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-regression.html"><a href="11-regression.html#model1table"><i class="fa fa-check"></i><b>11.1.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="11.1.3" data-path="11-regression.html"><a href="11-regression.html#interpreting-regression-coefficients"><i class="fa fa-check"></i><b>11.1.3</b> Interpreting regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-regression.html"><a href="11-regression.html#uncertainty-in-simple-linear-regressions"><i class="fa fa-check"></i><b>11.2</b> Uncertainty in simple linear regressions</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-regression.html"><a href="11-regression.html#using-lm-and-tidy-as-a-shortcut-2"><i class="fa fa-check"></i><b>11.2.1</b> Using <code>lm()</code> and <code>tidy()</code> as a shortcut</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-regression.html"><a href="11-regression.html#model1points"><i class="fa fa-check"></i><b>11.2.2</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-regression.html"><a href="11-regression.html#model2"><i class="fa fa-check"></i><b>11.3</b> Life expectancy: one categorical explanatory variable</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-regression.html"><a href="11-regression.html#model2EDA"><i class="fa fa-check"></i><b>11.3.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-regression.html"><a href="11-regression.html#model2table"><i class="fa fa-check"></i><b>11.3.2</b> Linear regression</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-regression.html"><a href="11-regression.html#model2points"><i class="fa fa-check"></i><b>11.3.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-regression.html"><a href="11-regression.html#case-study-2018-gubernatorial-forecasts"><i class="fa fa-check"></i><b>11.4</b> Case study: 2018 gubernatorial forecasts</a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-regression.html"><a href="11-regression.html#fitting-multiple-models-using-map-1"><i class="fa fa-check"></i><b>11.4.1</b> Fitting multiple models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11-regression.html"><a href="11-regression.html#leastsquares"><i class="fa fa-check"></i><b>11.5</b> Appendix: Best-fitting line</a></li>
<li class="chapter" data-level="11.6" data-path="11-regression.html"><a href="11-regression.html#conclusion-5"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a><ul>
<li class="chapter" data-level="11.6.1" data-path="11-regression.html"><a href="11-regression.html#additional-resources-basic-regression"><i class="fa fa-check"></i><b>11.6.1</b> Additional resources</a></li>
<li class="chapter" data-level="11.6.2" data-path="11-regression.html"><a href="11-regression.html#whats-to-come"><i class="fa fa-check"></i><b>11.6.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#needed-packages-4"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="12.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4"><i class="fa fa-check"></i><b>12.1</b> Teaching evaluations revisited: one numerical and one categorical explanatory variable</a><ul>
<li class="chapter" data-level="12.1.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4EDA"><i class="fa fa-check"></i><b>12.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.1.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4interactiontable"><i class="fa fa-check"></i><b>12.1.2</b> Interaction model</a></li>
<li class="chapter" data-level="12.1.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#interpreting-regression-coefficients-with-interactions"><i class="fa fa-check"></i><b>12.1.3</b> Interpreting regression coefficients with interactions</a></li>
<li class="chapter" data-level="12.1.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4table"><i class="fa fa-check"></i><b>12.1.4</b> Parallel slopes model</a></li>
<li class="chapter" data-level="12.1.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model4points"><i class="fa fa-check"></i><b>12.1.5</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3"><i class="fa fa-check"></i><b>12.2</b> Credit card debt: two numerical explanatory variables</a><ul>
<li class="chapter" data-level="12.2.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3EDA"><i class="fa fa-check"></i><b>12.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3table"><i class="fa fa-check"></i><b>12.2.2</b> Regression plane</a></li>
<li class="chapter" data-level="12.2.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model3points"><i class="fa fa-check"></i><b>12.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#seattle-house-prices"><i class="fa fa-check"></i><b>12.3</b> Case study: Seattle house prices</a><ul>
<li class="chapter" data-level="12.3.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-I"><i class="fa fa-check"></i><b>12.3.1</b> Exploratory data analysis: Part I</a></li>
<li class="chapter" data-level="12.3.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-EDA-II"><i class="fa fa-check"></i><b>12.3.2</b> Exploratory data analysis: Part II</a></li>
<li class="chapter" data-level="12.3.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-regression"><i class="fa fa-check"></i><b>12.3.3</b> Regression modeling</a></li>
<li class="chapter" data-level="12.3.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#house-prices-making-predictions"><i class="fa fa-check"></i><b>12.3.4</b> Making predictions</a></li>
<li class="chapter" data-level="12.3.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#fitting-many-models-using-map"><i class="fa fa-check"></i><b>12.3.5</b> Fitting many models using <code>map()</code></a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#data-journalism"><i class="fa fa-check"></i><b>12.4</b> Case studies: Effective data storytelling</a><ul>
<li class="chapter" data-level="12.4.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#bechdel-test-for-hollywood-gender-representation"><i class="fa fa-check"></i><b>12.4.1</b> Bechdel test for Hollywood gender representation</a></li>
<li class="chapter" data-level="12.4.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#us-births-in-1999"><i class="fa fa-check"></i><b>12.4.2</b> US Births in 1999</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#appendix-related-topics"><i class="fa fa-check"></i><b>12.5</b> Appendix: Related topics</a><ul>
<li class="chapter" data-level="12.5.1" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#model-selection"><i class="fa fa-check"></i><b>12.5.1</b> Model selection</a></li>
<li class="chapter" data-level="12.5.2" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#correlationcoefficient2"><i class="fa fa-check"></i><b>12.5.2</b> Correlation coefficient</a></li>
<li class="chapter" data-level="12.5.3" data-path="12-multiple-regression.html"><a href="12-multiple-regression.html#simpsonsparadox"><i class="fa fa-check"></i><b>12.5.3</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-classification.html"><a href="13-classification.html"><i class="fa fa-check"></i><b>13</b> Classification</a><ul>
<li class="chapter" data-level="" data-path="13-classification.html"><a href="13-classification.html#needed-packages-5"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="13.1" data-path="13-classification.html"><a href="13-classification.html#logistic-regression"><i class="fa fa-check"></i><b>13.1</b> Logistic regression</a><ul>
<li class="chapter" data-level="13.1.1" data-path="13-classification.html"><a href="13-classification.html#what-is-logistic-regression"><i class="fa fa-check"></i><b>13.1.1</b> What is logistic regression?</a></li>
<li class="chapter" data-level="13.1.2" data-path="13-classification.html"><a href="13-classification.html#house-elections-exploratory-data-analysis"><i class="fa fa-check"></i><b>13.1.2</b> House elections: exploratory data analysis</a></li>
<li class="chapter" data-level="13.1.3" data-path="13-classification.html"><a href="13-classification.html#one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.3</b> One categorical explanatory variable</a></li>
<li class="chapter" data-level="13.1.4" data-path="13-classification.html"><a href="13-classification.html#observedfitted-values-and-residuals"><i class="fa fa-check"></i><b>13.1.4</b> Observed/fitted values and residuals</a></li>
<li class="chapter" data-level="13.1.5" data-path="13-classification.html"><a href="13-classification.html#one-numerical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.5</b> One numerical explanatory variable</a></li>
<li class="chapter" data-level="13.1.6" data-path="13-classification.html"><a href="13-classification.html#one-numerical-and-one-categorical-explanatory-variable"><i class="fa fa-check"></i><b>13.1.6</b> One numerical and one categorical explanatory variable</a></li>
<li class="chapter" data-level="13.1.7" data-path="13-classification.html"><a href="13-classification.html#fitting-many-models-using-map-1"><i class="fa fa-check"></i><b>13.1.7</b> Fitting many models using <code>map()</code></a></li>
<li class="chapter" data-level="13.1.8" data-path="13-classification.html"><a href="13-classification.html#professional-models"><i class="fa fa-check"></i><b>13.1.8</b> Professional models</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="13-classification.html"><a href="13-classification.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>13.2</b> Classification and regression trees (CART)</a><ul>
<li class="chapter" data-level="13.2.1" data-path="13-classification.html"><a href="13-classification.html#what-is-cart"><i class="fa fa-check"></i><b>13.2.1</b> What is CART?</a></li>
<li class="chapter" data-level="13.2.2" data-path="13-classification.html"><a href="13-classification.html#one-categorical-explanatory-variable-1"><i class="fa fa-check"></i><b>13.2.2</b> One categorical explanatory variable</a></li>
<li class="chapter" data-level="13.2.3" data-path="13-classification.html"><a href="13-classification.html#one-numerical-explanatory-variable-1"><i class="fa fa-check"></i><b>13.2.3</b> One numerical explanatory variable</a></li>
<li class="chapter" data-level="13.2.4" data-path="13-classification.html"><a href="13-classification.html#multiple-explanatory-variables"><i class="fa fa-check"></i><b>13.2.4</b> Multiple explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13-classification.html"><a href="13-classification.html#random-forests"><i class="fa fa-check"></i><b>13.3</b> Random forests</a><ul>
<li class="chapter" data-level="13.3.1" data-path="13-classification.html"><a href="13-classification.html#what-are-random-forests"><i class="fa fa-check"></i><b>13.3.1</b> What are random forests?</a></li>
<li class="chapter" data-level="13.3.2" data-path="13-classification.html"><a href="13-classification.html#fitting-random-forests"><i class="fa fa-check"></i><b>13.3.2</b> Fitting random forests</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="13-classification.html"><a href="13-classification.html#comparing-the-three-approaches"><i class="fa fa-check"></i><b>13.4</b> Comparing the three approaches</a><ul>
<li class="chapter" data-level="13.4.1" data-path="13-classification.html"><a href="13-classification.html#is-accuracy-the-right-measure"><i class="fa fa-check"></i><b>13.4.1</b> Is accuracy the right measure?</a></li>
<li class="chapter" data-level="13.4.2" data-path="13-classification.html"><a href="13-classification.html#modeling-for-prediction-vs.-explanation"><i class="fa fa-check"></i><b>13.4.2</b> Modeling for prediction vs. explanation</a></li>
<li class="chapter" data-level="13.4.3" data-path="13-classification.html"><a href="13-classification.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>13.4.3</b> Out-of-sample predictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-machine-learning.html"><a href="14-machine-learning.html"><i class="fa fa-check"></i><b>14</b> Machine Learning</a><ul>
<li class="chapter" data-level="14.1" data-path="14-machine-learning.html"><a href="14-machine-learning.html#notation"><i class="fa fa-check"></i><b>14.1</b> Notation</a></li>
<li class="chapter" data-level="14.2" data-path="14-machine-learning.html"><a href="14-machine-learning.html#an-example"><i class="fa fa-check"></i><b>14.2</b> An example</a></li>
<li class="chapter" data-level="14.3" data-path="14-machine-learning.html"><a href="14-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>14.3</b> Evaluation metrics</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14-machine-learning.html"><a href="14-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>14.3.1</b> Training and test sets</a></li>
<li class="chapter" data-level="14.3.2" data-path="14-machine-learning.html"><a href="14-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>14.3.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="14.3.3" data-path="14-machine-learning.html"><a href="14-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>14.3.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="14.3.4" data-path="14-machine-learning.html"><a href="14-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>14.3.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="14.3.5" data-path="14-machine-learning.html"><a href="14-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>14.3.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="14.3.6" data-path="14-machine-learning.html"><a href="14-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>14.3.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="14.3.7" data-path="14-machine-learning.html"><a href="14-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>14.3.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="14.3.8" data-path="14-machine-learning.html"><a href="14-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>14.3.8</b> The loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14-machine-learning.html"><a href="14-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>14.4</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="14.4.1" data-path="14-machine-learning.html"><a href="14-machine-learning.html#conditional-probabilities"><i class="fa fa-check"></i><b>14.4.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="14.4.2" data-path="14-machine-learning.html"><a href="14-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>14.4.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="14.4.3" data-path="14-machine-learning.html"><a href="14-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>14.4.3</b> Conditional expectation minimizes squared loss function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="14-machine-learning.html"><a href="14-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>14.5</b> Case study: is it a 2 or a 7?</a></li>
<li class="chapter" data-level="14.6" data-path="14-machine-learning.html"><a href="14-machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>14.6</b> Cross validation</a></li>
<li class="chapter" data-level="14.7" data-path="14-machine-learning.html"><a href="14-machine-learning.html#knn-cv-intro"><i class="fa fa-check"></i><b>14.7</b> Motivation with k-nearest neighbors</a><ul>
<li class="chapter" data-level="14.7.1" data-path="14-machine-learning.html"><a href="14-machine-learning.html#over-training"><i class="fa fa-check"></i><b>14.7.1</b> Over-training</a></li>
<li class="chapter" data-level="14.7.2" data-path="14-machine-learning.html"><a href="14-machine-learning.html#over-smoothing"><i class="fa fa-check"></i><b>14.7.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="14.7.3" data-path="14-machine-learning.html"><a href="14-machine-learning.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>14.7.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="14-machine-learning.html"><a href="14-machine-learning.html#mathematical-description-of-cross-validation"><i class="fa fa-check"></i><b>14.8</b> Mathematical description of cross validation</a></li>
<li class="chapter" data-level="14.9" data-path="14-machine-learning.html"><a href="14-machine-learning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>14.9</b> K-fold cross validation</a></li>
<li class="chapter" data-level="14.10" data-path="14-machine-learning.html"><a href="14-machine-learning.html#bootstrap"><i class="fa fa-check"></i><b>14.10</b> Bootstrap</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html"><i class="fa fa-check"></i><b>A</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="A.1" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#causal-effects"><i class="fa fa-check"></i><b>A.1</b> Causal effects</a></li>
<li class="chapter" data-level="A.2" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#potential-outcomes"><i class="fa fa-check"></i><b>A.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="A.3" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#no-causation-without-manipulation"><i class="fa fa-check"></i><b>A.3</b> No causation without manipulation</a></li>
<li class="chapter" data-level="A.4" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#average-treatment-effect"><i class="fa fa-check"></i><b>A.4</b> Average treatment effect</a></li>
<li class="chapter" data-level="A.5" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#stable-unit-treatment-value-assumption-sutva"><i class="fa fa-check"></i><b>A.5</b> Stable unit treatment value assumption (SUTVA)</a></li>
<li class="chapter" data-level="A.6" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>A.6</b> The fundamental problem of causal inference</a></li>
<li class="chapter" data-level="A.7" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#the-assignment-mechanism"><i class="fa fa-check"></i><b>A.7</b> The assignment mechanism</a></li>
<li class="chapter" data-level="A.8" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#permutation-tests"><i class="fa fa-check"></i><b>A.8</b> Permutation tests</a></li>
<li class="chapter" data-level="A.9" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#confounding-and-selection-bias"><i class="fa fa-check"></i><b>A.9</b> Confounding and selection bias</a></li>
<li class="chapter" data-level="A.10" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#internal-and-external-validity"><i class="fa fa-check"></i><b>A.10</b> Internal and external validity</a></li>
<li class="chapter" data-level="A.11" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#survey-research-and-external-validity"><i class="fa fa-check"></i><b>A.11</b> Survey research and external validity</a></li>
<li class="chapter" data-level="A.12" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#conclusion-6"><i class="fa fa-check"></i><b>A.12</b> Conclusion</a></li>
<li class="chapter" data-level="A.13" data-path="A-rubin-causal-model.html"><a href="A-rubin-causal-model.html#references"><i class="fa fa-check"></i><b>A.13</b> References</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-maps.html"><a href="B-maps.html"><i class="fa fa-check"></i><b>B</b> Maps</a><ul>
<li class="chapter" data-level="B.1" data-path="B-maps.html"><a href="B-maps.html#tidycensus"><i class="fa fa-check"></i><b>B.1</b> Tidycensus</a></li>
<li class="chapter" data-level="B.2" data-path="B-maps.html"><a href="B-maps.html#conceptual-introduction-to-mapping"><i class="fa fa-check"></i><b>B.2</b> Conceptual introduction to mapping</a><ul>
<li class="chapter" data-level="B.2.1" data-path="B-maps.html"><a href="B-maps.html#vector-versus-spatial-data"><i class="fa fa-check"></i><b>B.2.1</b> Vector versus spatial data</a></li>
<li class="chapter" data-level="B.2.2" data-path="B-maps.html"><a href="B-maps.html#sf-vs-sp"><i class="fa fa-check"></i><b>B.2.2</b> <strong>sf</strong> vs <strong>sp</strong></a></li>
<li class="chapter" data-level="B.2.3" data-path="B-maps.html"><a href="B-maps.html#shapefiles"><i class="fa fa-check"></i><b>B.2.3</b> Shapefiles</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="B-maps.html"><a href="B-maps.html#mapping-with-tidycensus-and-geom_sf"><i class="fa fa-check"></i><b>B.3</b> Mapping with <strong>tidycensus</strong> and <code>geom_sf()</code></a><ul>
<li class="chapter" data-level="B.3.1" data-path="B-maps.html"><a href="B-maps.html#making-maps-pretty"><i class="fa fa-check"></i><b>B.3.1</b> Making maps pretty</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-maps.html"><a href="B-maps.html#adding-back-alaska-and-hawaii"><i class="fa fa-check"></i><b>B.3.2</b> Adding back Alaska and Hawaii</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-maps.html"><a href="B-maps.html#faceting-maps"><i class="fa fa-check"></i><b>B.4</b> Faceting maps</a><ul>
<li class="chapter" data-level="B.4.1" data-path="B-maps.html"><a href="B-maps.html#transforming-and-mapping-the-data"><i class="fa fa-check"></i><b>B.4.1</b> Transforming and mapping the data</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="B-maps.html"><a href="B-maps.html#want-to-explore-further"><i class="fa fa-check"></i><b>B.5</b> Want to explore further?</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-animation.html"><a href="C-animation.html"><i class="fa fa-check"></i><b>C</b> Animation</a><ul>
<li class="chapter" data-level="C.1" data-path="C-animation.html"><a href="C-animation.html#gganimate-how-to-create-plots-with-beautiful-animation-in-r"><i class="fa fa-check"></i><b>C.1</b> gganimate: How to Create Plots with Beautiful Animation in R</a><ul>
<li class="chapter" data-level="C.1.1" data-path="C-animation.html"><a href="C-animation.html#prerequisites"><i class="fa fa-check"></i><b>C.1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="C.1.2" data-path="C-animation.html"><a href="C-animation.html#demo-dataset"><i class="fa fa-check"></i><b>C.1.2</b> Demo dataset</a></li>
<li class="chapter" data-level="C.1.3" data-path="C-animation.html"><a href="C-animation.html#static-plot"><i class="fa fa-check"></i><b>C.1.3</b> Static plot</a></li>
<li class="chapter" data-level="C.1.4" data-path="C-animation.html"><a href="C-animation.html#transition-through-distinct-states-in-time"><i class="fa fa-check"></i><b>C.1.4</b> Transition through distinct states in time</a></li>
<li class="chapter" data-level="C.1.5" data-path="C-animation.html"><a href="C-animation.html#reveal-data-along-a-given-dimension"><i class="fa fa-check"></i><b>C.1.5</b> Reveal data along a given dimension</a></li>
<li class="chapter" data-level="C.1.6" data-path="C-animation.html"><a href="C-animation.html#transition-between-several-distinct-stages-of-the-data"><i class="fa fa-check"></i><b>C.1.6</b> Transition between several distinct stages of the data</a></li>
<li class="chapter" data-level="C.1.7" data-path="C-animation.html"><a href="C-animation.html#read-more"><i class="fa fa-check"></i><b>C.1.7</b> Read more</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="C-animation.html"><a href="C-animation.html#how-to-save-your-animation"><i class="fa fa-check"></i><b>C.2</b> How to save your animation</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-shiny.html"><a href="D-shiny.html"><i class="fa fa-check"></i><b>D</b> Shiny</a><ul>
<li class="chapter" data-level="D.1" data-path="D-shiny.html"><a href="D-shiny.html#helpful-resources"><i class="fa fa-check"></i><b>D.1</b> Helpful Resources</a></li>
<li class="chapter" data-level="D.2" data-path="D-shiny.html"><a href="D-shiny.html#set-up-and-getting-started"><i class="fa fa-check"></i><b>D.2</b> Set Up and Getting Started</a></li>
<li class="chapter" data-level="D.3" data-path="D-shiny.html"><a href="D-shiny.html#building-your-basic-app"><i class="fa fa-check"></i><b>D.3</b> Building Your Basic App</a><ul>
<li class="chapter" data-level="D.3.1" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-basic-ui"><i class="fa fa-check"></i><b>D.3.1</b> Setting Up the Basic UI</a></li>
<li class="chapter" data-level="D.3.2" data-path="D-shiny.html"><a href="D-shiny.html#setting-up-the-server"><i class="fa fa-check"></i><b>D.3.2</b> Setting up the Server</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="D-shiny.html"><a href="D-shiny.html#organization"><i class="fa fa-check"></i><b>D.4</b> Organization</a></li>
<li class="chapter" data-level="D.5" data-path="D-shiny.html"><a href="D-shiny.html#customizations"><i class="fa fa-check"></i><b>D.5</b> Customizations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Preceptor’s Primer for Bayesian Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Classification</h1>
<p><span class="math display">\[
\newcommand{\lik}{\operatorname{Lik}}
\newcommand{\Lik}{\operatorname{Lik}}
\]</span>
Many research questions have binary (yes/no or success/failure) responses:</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Are students with poor grades more likely to binge drink?</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Is exposure to a particular chemical associated with a cancer diagnosis?</li>
</ol></li>
</ul>
<p><strong>Binary responses</strong> take on only two values: success (<span class="math inline">\(Y=1\)</span>) or failure (<span class="math inline">\(Y=0\)</span>), Yes (<span class="math inline">\(Y=1\)</span>) or No (<span class="math inline">\(Y=0\)</span>), etc. Thus, examples (a) and (b) above would be considered to have binary responses (Does a student binge drink? Was a patient diagnosed with cancer?). Binary responses are ubiquitous; they are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success <span class="math inline">\(p\)</span> based on a set of covariates, although sometimes we wish to use those covariates to classify a future observation as a success or a failure.</p>
<p>In this chapter, we will look at three common techniques of <strong>classification</strong> of binary data. First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapter <a href="11-regression.html#regression">11</a> and <a href="12-multiple-regression.html#multiple-regression">12</a>. Second, we will consider classification and regression trees (CART). Finally, we will discuss random forests.</p>
<div id="needed-packages-5" class="section level3 unnumbered">
<h3>Needed packages</h3>
<p>Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section <a href="5-tidy.html#tidyverse-package">5.5</a> that loading the <strong>tidyverse</strong> package by running <code>library(tidyverse)</code> loads the following commonly used data science packages all at once:</p>
<ul>
<li><strong>ggplot2</strong> for data visualization</li>
<li><strong>dplyr</strong> for data wrangling</li>
<li><strong>tidyr</strong> for converting data to “tidy” format</li>
<li><strong>readr</strong> for importing spreadsheet data into R</li>
<li>As well as the more advanced <strong>purrr</strong>, <strong>tibble</strong>, <strong>stringr</strong>, and <strong>forcats</strong> packages</li>
</ul>
<p>If needed, read Section <a href="1-getting-started.html#packages">1.3</a> for information on how to install and load R packages.</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb872-1"><a href="13-classification.html#cb872-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb872-2"><a href="13-classification.html#cb872-2"></a><span class="kw">library</span>(broom)</span>
<span id="cb872-3"><a href="13-classification.html#cb872-3"></a><span class="kw">library</span>(skimr)</span>
<span id="cb872-4"><a href="13-classification.html#cb872-4"></a><span class="kw">library</span>(fivethirtyeight)</span></code></pre></div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">13.1</span> Logistic regression</h2>
<div id="what-is-logistic-regression" class="section level3">
<h3><span class="header-section-number">13.1.1</span> What is logistic regression?</h3>
<p>Figure <a href="13-classification.html#fig:OLSlogistic">13.1</a> illustrates a data set with a binary (0 or 1) response (<span class="math inline">\(Y\)</span>) and a single continuous predictor (<span class="math inline">\(X\)</span>). The blue line is a linear regression to model the probability of a success (<span class="math inline">\(Y=1\)</span>) for a given value of <span class="math inline">\(X\)</span>. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1.</p>
<p>The red curve is the <em>logistic regression</em> curve. Note that its characteristic “S” shape always produces predicted probabilities between 0 and 1. Here is the formula for a logistic regression:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]</span></p>
<p>where the observed values <span class="math inline">\(Y_i \sim\)</span> Bernoulli with <span class="math inline">\(p=p_i\)</span> for a given set of predictors <span class="math inline">\(X\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:OLSlogistic"></span>
<img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/OLSlogistic-1.png" alt="Linear vs. logistic regression models for binary response data." width="60%" />
<p class="caption">
FIGURE 13.1: Linear vs. logistic regression models for binary response data.
</p>
</div>
<p>The mathematical function <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span> is called the <em>logit function</em> and it transforms variables from the space <span class="math inline">\((0, 1)\)</span> (like probabilities) to <span class="math inline">\((-\infty, \infty)\)</span>. The inverse of that function, the <em>standard logistic function</em>, is <span class="math inline">\(\frac{1}{1 + e^{-x}})\)</span> and transforms variables from the space <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>. From that latter function’s name we get the terminology of <em>logistic regression</em>.</p>
</div>
<div id="house-elections-exploratory-data-analysis" class="section level3">
<h3><span class="header-section-number">13.1.2</span> House elections: exploratory data analysis</h3>
<p>What affects whether a Democrat or Republican wins a race in the U.S. House of Representatives? This is an example of a binary response: either a Democrat wins (and a Republican loses) or a Republican wins (and a Democrat loses).<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> In this section, we are going to consider several models predicting Democratic victory in House races. First, we will consider a single categorical variable as a predictor: the region that a district lies in (Midwest, Northeast, South, or West). Second, we will consider a single continuous variable (year). Finally, we fill fit a model that contains an interaction of the two.</p>
<p>The data on House election results from 1976 to 2018 can be found in the <code>house_results</code> data frame in <strong>politicaldata</strong> package. We’ll create a version of this data frame called <code>house_ch13</code> that creates a new column <code>dem_win</code> that notes for each state if the Democratic candidate in a congressional district received more votes than the other candidates. We’ll also join it with the <code>state_info</code> data frame in the <strong>fivethirtyeight</strong> package to add the <code>region</code> of each state.</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb873-1"><a href="13-classification.html#cb873-1"></a><span class="kw">library</span>(politicaldata)</span>
<span id="cb873-2"><a href="13-classification.html#cb873-2"></a></span>
<span id="cb873-3"><a href="13-classification.html#cb873-3"></a>house_ch13 &lt;-<span class="st"> </span>house_results <span class="op">%&gt;%</span></span>
<span id="cb873-4"><a href="13-classification.html#cb873-4"></a><span class="st">  </span></span>
<span id="cb873-5"><a href="13-classification.html#cb873-5"></a><span class="st">  </span><span class="co"># Create dem_win variable</span></span>
<span id="cb873-6"><a href="13-classification.html#cb873-6"></a><span class="st">  </span></span>
<span id="cb873-7"><a href="13-classification.html#cb873-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dem =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(dem), <span class="dv">0</span>, dem),</span>
<span id="cb873-8"><a href="13-classification.html#cb873-8"></a>         <span class="dt">other =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(other), <span class="dv">0</span>, other),</span>
<span id="cb873-9"><a href="13-classification.html#cb873-9"></a>         <span class="dt">rep =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(rep), <span class="dv">0</span>, rep),</span>
<span id="cb873-10"><a href="13-classification.html#cb873-10"></a>         <span class="dt">dem_win =</span> <span class="kw">ifelse</span>(dem <span class="op">&gt;</span><span class="st"> </span>rep <span class="op">&amp;</span><span class="st"> </span>dem <span class="op">&gt;</span><span class="st"> </span>other, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb873-11"><a href="13-classification.html#cb873-11"></a><span class="st">  </span></span>
<span id="cb873-12"><a href="13-classification.html#cb873-12"></a><span class="st">  </span><span class="co"># Rename to join with state_info</span></span>
<span id="cb873-13"><a href="13-classification.html#cb873-13"></a><span class="st">  </span></span>
<span id="cb873-14"><a href="13-classification.html#cb873-14"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">state_abbrev =</span> state_abb) <span class="op">%&gt;%</span></span>
<span id="cb873-15"><a href="13-classification.html#cb873-15"></a><span class="st">  </span><span class="kw">left_join</span>(state_info) <span class="op">%&gt;%</span></span>
<span id="cb873-16"><a href="13-classification.html#cb873-16"></a><span class="st">  </span><span class="kw">select</span>(region, state, district, year, dem_win)</span></code></pre></div>
<pre><code>Joining, by = &quot;state_abbrev&quot;</code></pre>
<p>Recall the three common steps in an exploratory data analysis we saw in Subsection <a href="11-regression.html#model1EDA">11.1.1</a>:</p>
<ol style="list-style-type: decimal">
<li>Looking at the raw data values.</li>
<li>Computing summary statistics.</li>
<li>Creating data visualizations.</li>
</ol>
<p>Let’s first look at the raw data values by either looking at <code>house_ch13</code> using RStudio’s spreadsheet viewer or by using the <code>glimpse()</code> function from the <strong>dplyr</strong> package:</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb875-1"><a href="13-classification.html#cb875-1"></a><span class="kw">glimpse</span>(house_ch13)</span></code></pre></div>
<pre><code>Observations: 9,557
Variables: 5
$ region   &lt;chr&gt; &quot;West&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;…
$ state    &lt;chr&gt; &quot;Alaska&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaba…
$ district &lt;chr&gt; &quot;AK-AL&quot;, &quot;AL-01&quot;, &quot;AL-02&quot;, &quot;AL-03&quot;, &quot;AL-04&quot;, &quot;AL-05&quot;, &quot;AL-06…
$ year     &lt;dbl&gt; 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, …
$ dem_win  &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …</code></pre>
<p>Let’s also display a random sample of 5 rows of the 9,557 rows corresponding to different district-years. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb877-1"><a href="13-classification.html#cb877-1"></a>house_ch13 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb877-2"><a href="13-classification.html#cb877-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dt">size =</span> <span class="dv">5</span>)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-596">TABLE 13.1: </span>A random sample of 5 out of the 4,201 district-years
</caption>
<thead>
<tr>
<th style="text-align:left;">
region
</th>
<th style="text-align:left;">
state
</th>
<th style="text-align:left;">
district
</th>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
dem_win
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
West
</td>
<td style="text-align:left;">
California
</td>
<td style="text-align:left;">
CA-50
</td>
<td style="text-align:right;">
2012
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Northeast
</td>
<td style="text-align:left;">
New York
</td>
<td style="text-align:left;">
NY-27
</td>
<td style="text-align:right;">
1984
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Northeast
</td>
<td style="text-align:left;">
New York
</td>
<td style="text-align:left;">
NY-34
</td>
<td style="text-align:right;">
1978
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
South
</td>
<td style="text-align:left;">
Florida
</td>
<td style="text-align:left;">
FL-06
</td>
<td style="text-align:right;">
2008
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Midwest
</td>
<td style="text-align:left;">
Michigan
</td>
<td style="text-align:left;">
MI-15
</td>
<td style="text-align:right;">
1988
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
<p>Now that we’ve looked at the raw values in our <code>house_ch13</code> data frame and got a sense of the data, let’s compute summary statistics. As we’ve done in our exploratory data analyses before, let’s use the <code>skim()</code> function from the <code>skimr</code> package, being sure to only select() the variables of interest in our model:</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb878-1"><a href="13-classification.html#cb878-1"></a>house_ch13 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb878-2"><a href="13-classification.html#cb878-2"></a><span class="st">  </span><span class="kw">select</span>(dem_win, region, year) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb878-3"><a href="13-classification.html#cb878-3"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<table style='width: auto;'
        class='table table-condensed'>
<caption>
<span id="tab:unnamed-chunk-597">TABLE 13.2: </span>Data summary
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
9557
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
character
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
empty
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:right;">
whitespace
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
region
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
dem_win
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.54
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1997.00
</td>
<td style="text-align:right;">
12.7
</td>
<td style="text-align:right;">
1976
</td>
<td style="text-align:right;">
1986
</td>
<td style="text-align:right;">
1998
</td>
<td style="text-align:right;">
2008
</td>
<td style="text-align:right;">
2018
</td>
<td style="text-align:left;">
▇▆▆▆▇
</td>
</tr>
</tbody>
</table>
<p>Observe that we have no missing data, that we have 9,557 observations, and that the mean of <code>dem_win</code> is 0.54, indicating that Democrats won 54% of the House elections in this period (1976–2018).</p>
<p>Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations.</p>
<p>For our categorical variable, we’ll look at histograms of <code>dem_win</code> faceted by <code>region</code>:</p>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb879-1"><a href="13-classification.html#cb879-1"></a>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb879-2"><a href="13-classification.html#cb879-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dem_win)) <span class="op">+</span></span>
<span id="cb879-3"><a href="13-classification.html#cb879-3"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb879-4"><a href="13-classification.html#cb879-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Democratic victory percentage, 1976-2018&quot;</span>, </span>
<span id="cb879-5"><a href="13-classification.html#cb879-5"></a>       <span class="dt">y =</span> <span class="st">&quot;Number of districts&quot;</span>,</span>
<span id="cb879-6"><a href="13-classification.html#cb879-6"></a>       <span class="dt">title =</span> <span class="st">&quot;Histogram of distribution of Democratic victories by House district&quot;</span>) <span class="op">+</span></span>
<span id="cb879-7"><a href="13-classification.html#cb879-7"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span></span>
<span id="cb879-8"><a href="13-classification.html#cb879-8"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-598-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Wait! That doesn’t tell us very much, because our outcome variable only takes two values, 0 and 1. Let’s instead <code>group_by(district)</code> and <code>summarize()</code> to get a better sense of the distributions:</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb880-1"><a href="13-classification.html#cb880-1"></a>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb880-2"><a href="13-classification.html#cb880-2"></a><span class="st">  </span><span class="kw">group_by</span>(region, district) <span class="op">%&gt;%</span></span>
<span id="cb880-3"><a href="13-classification.html#cb880-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">dem_win =</span> <span class="kw">mean</span>(dem_win)) <span class="op">%&gt;%</span></span>
<span id="cb880-4"><a href="13-classification.html#cb880-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dem_win)) <span class="op">+</span></span>
<span id="cb880-5"><a href="13-classification.html#cb880-5"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb880-6"><a href="13-classification.html#cb880-6"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Democratic victory percentage, 1976-2018&quot;</span>, </span>
<span id="cb880-7"><a href="13-classification.html#cb880-7"></a>       <span class="dt">y =</span> <span class="st">&quot;Number of districts&quot;</span>,</span>
<span id="cb880-8"><a href="13-classification.html#cb880-8"></a>       <span class="dt">title =</span> <span class="st">&quot;Histogram of distribution of Democratic victories by House district&quot;</span>) <span class="op">+</span></span>
<span id="cb880-9"><a href="13-classification.html#cb880-9"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span><span class="st"> </span></span>
<span id="cb880-10"><a href="13-classification.html#cb880-10"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-599-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This is much more informative! We can see that the Midwest is highly bimodal, with many districts either electing Democrats for every year in this period or for none. The Northeast and West have many districts that always elect Democrats but few that never do. The South is the only region with a peak in the middle, indicating that there are many districts in the South that elected Democrats for about half the time during 1976-2018.</p>
<p>What happens if we create a scatterplot of our outcome variable <code>dem_win</code> and a continuous predictor, <code>year</code>?</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb881-1"><a href="13-classification.html#cb881-1"></a>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb881-2"><a href="13-classification.html#cb881-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> dem_win)) <span class="op">+</span></span>
<span id="cb881-3"><a href="13-classification.html#cb881-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb881-4"><a href="13-classification.html#cb881-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Democratic Victory&quot;</span>) <span class="op">+</span></span>
<span id="cb881-5"><a href="13-classification.html#cb881-5"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-600-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This is completely incomprehensible! When dealing with binary data, it is more helpful to construct an <em>empirical logit</em> plot instead of a regular scatterplot. The steps for constructing such a plot are as follows:</p>
<ol style="list-style-type: decimal">
<li><code>group_by</code> your continuous variable.</li>
<li><code>summarize</code> the percentage of successes in your outcome variable.</li>
<li>Calculate the <em>empirical logit</em> for each group, using the logit function: <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span></li>
<li>Plot the results.</li>
</ol>
<p>The logit function exists in base R as <code>qlogis()</code>. Let’s look at the empirical logit plot:</p>
<div class="sourceCode" id="cb882"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb882-1"><a href="13-classification.html#cb882-1"></a>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb882-2"><a href="13-classification.html#cb882-2"></a><span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span></span>
<span id="cb882-3"><a href="13-classification.html#cb882-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">perc_dem_win =</span> <span class="kw">mean</span>(dem_win),</span>
<span id="cb882-4"><a href="13-classification.html#cb882-4"></a>            <span class="dt">emplogit =</span> <span class="kw">qlogis</span>(perc_dem_win)) <span class="op">%&gt;%</span></span>
<span id="cb882-5"><a href="13-classification.html#cb882-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> emplogit)) <span class="op">+</span></span>
<span id="cb882-6"><a href="13-classification.html#cb882-6"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb882-7"><a href="13-classification.html#cb882-7"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb882-8"><a href="13-classification.html#cb882-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</span>
<span id="cb882-9"><a href="13-classification.html#cb882-9"></a>       <span class="dt">y =</span> <span class="st">&quot;Empirical logits&quot;</span>) <span class="op">+</span></span>
<span id="cb882-10"><a href="13-classification.html#cb882-10"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-601-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Now we see that after the logit transformation, there is roughly a linear relationship between our outcome variable and our explanatory variable <code>year</code>. This means that a logistic regression model makes sense. Some of the most visually apparent outliers will be familiar to students of American politics: 1994 (the “Republican Revolution”), 2008 (Obama’s first election), and 2018. Yet in general it appears that over time the Democrats have performed worse in House elections.</p>
<p>We can follow the same steps to look at this relationship within Census regions (Midwest, Northeast, South and West):</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="13-classification.html#cb883-1"></a>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb883-2"><a href="13-classification.html#cb883-2"></a><span class="st">  </span><span class="kw">group_by</span>(region, year) <span class="op">%&gt;%</span></span>
<span id="cb883-3"><a href="13-classification.html#cb883-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">perc_dem_win =</span> <span class="kw">mean</span>(dem_win),</span>
<span id="cb883-4"><a href="13-classification.html#cb883-4"></a>            <span class="dt">emplogit =</span> <span class="kw">qlogis</span>(perc_dem_win)) <span class="op">%&gt;%</span></span>
<span id="cb883-5"><a href="13-classification.html#cb883-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> emplogit)) <span class="op">+</span></span>
<span id="cb883-6"><a href="13-classification.html#cb883-6"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb883-7"><a href="13-classification.html#cb883-7"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb883-8"><a href="13-classification.html#cb883-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</span>
<span id="cb883-9"><a href="13-classification.html#cb883-9"></a>       <span class="dt">y =</span> <span class="st">&quot;Empirical logits&quot;</span>) <span class="op">+</span></span>
<span id="cb883-10"><a href="13-classification.html#cb883-10"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>region) <span class="op">+</span></span>
<span id="cb883-11"><a href="13-classification.html#cb883-11"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-602-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We see roughly linear relationships after the logit transformation within-region as well, although the relationship looks more linear in the Midwest and South than in the Northeast and West. We see that the Democratic Party’s overall decline in House races is driven by the South and to a lesser extent the Midwest; Democratic performance has on average improved in the Northeast and West. The sharp negative slope in the South will not be surprising if one is familiar with the collapse of the “Solid South.”</p>
</div>
<div id="one-categorical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.3</span> One categorical explanatory variable</h3>
<p>Let’s start our modeling by predicting <code>dem_win</code> with a single categorical explanatory variable. We’ll start with a binary variable for whether an observation is in the South and we will progress to the categorical variable <code>region</code>. As we’ll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression. In fact, we’ll follow the same basic steps:</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x, family, data)</code> function and save it in <code>house_region_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_region_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns.</li>
</ol>
<p>Note that the key difference is that instead of using <code>lm()</code>, we are now using <code>glm()</code>. <code>glm()</code> operates very similarly to <code>lm()</code>, but it has an additional argument: <code>family</code>. To run a logistic regression, we use <code>family = binomial</code>.</p>
<p>Let’s create a binary variable for whether an observation is in the South:</p>
<div class="sourceCode" id="cb884"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb884-1"><a href="13-classification.html#cb884-1"></a>house_ch13 &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb884-2"><a href="13-classification.html#cb884-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">south =</span> <span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(region <span class="op">==</span><span class="st"> &quot;South&quot;</span>))</span></code></pre></div>
<p>Next, let’s fit a model and <code>tidy()</code> it:</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb885-1"><a href="13-classification.html#cb885-1"></a>house_south_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>south, <span class="dt">family =</span> binomial, <span class="dt">data =</span> house_ch13)</span>
<span id="cb885-2"><a href="13-classification.html#cb885-2"></a>house_south_model <span class="op">%&gt;%</span></span>
<span id="cb885-3"><a href="13-classification.html#cb885-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb885-4"><a href="13-classification.html#cb885-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-606">TABLE 13.3: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.230
</td>
</tr>
<tr>
<td style="text-align:left;">
south
</td>
<td style="text-align:right;">
-0.249
</td>
</tr>
</tbody>
</table>
<p>How can we interpret the coefficients? Unlike linear regressions, these coefficients aren’t directly interpretable. Recall our logistic regression model equation:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]</span></p>
<p>A one-unit change in <span class="math inline">\(X\)</span> thus is associated with a one-unit change in <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span>, where <span class="math inline">\(p\)</span> is the predicted probability of success. It is hard to understand intuitively what this means.</p>
<p>However, with only two possible values for the predictors (<code>south</code> = 0 or <code>south</code> = 1), we can directly calculate all the possible values of <span class="math inline">\(p\)</span> this model by using the <em>standard logistic function</em>:</p>
<p><span class="math display">\[
p = \frac{1}{1 + e^{-(\beta_0+\beta_1X)}} 
\]</span></p>
<p>This can easily be done using the <code>plogis()</code> function in R.</p>
<p>Thus, for our example, we have:</p>
<p><span class="math display">\[
p_{dem\_win} = \frac{1}{1 + e^{-(0.230 - 0.249 \times south)}} 
\]</span></p>
<p>Therefore, the predicted probability for an observation in the South is <span class="math inline">\(\frac{1}{1 + e^{-(0.230 - 0.249)}}\)</span> (0.495) and the predicted probability for an observation outside the South is <span class="math inline">\(\frac{1}{1 + e^{-(0.230)}}\)</span> (0.557) a difference of -0.062. Thus, an observation being the South is associated with a -0.062 decrease in the predicted probability of a Democratic victory.</p>
<p>Note that you could also estimate this value through the so-called <strong>“divide by 4” rule</strong>: <span class="math inline">\(-0.249 / 4 \approx 0.062\)</span>. We’ll discuss this further when we talk about interpreting the coefficiences of continuous predictors in logistic regression models.</p>
<p>Let’s now estimate the uncertainty around our coefficients. We’ll start by showing how you can do this through the bootstrap. We’ll begin by loading the <strong>infer</strong> package. We’re going to use the function <code>rep_sample_n()</code> to resample from our data 1,000 times:</p>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb886-1"><a href="13-classification.html#cb886-1"></a><span class="kw">library</span>(infer)</span>
<span id="cb886-2"><a href="13-classification.html#cb886-2"></a></span>
<span id="cb886-3"><a href="13-classification.html#cb886-3"></a>x &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb886-4"><a href="13-classification.html#cb886-4"></a><span class="st">  </span><span class="kw">select</span>(dem_win, south) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb886-5"><a href="13-classification.html#cb886-5"></a><span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="kw">nrow</span>(house_ch13), <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">reps =</span> <span class="dv">1000</span>)</span>
<span id="cb886-6"><a href="13-classification.html#cb886-6"></a></span>
<span id="cb886-7"><a href="13-classification.html#cb886-7"></a>x</span></code></pre></div>
<pre><code># A tibble: 9,557,000 x 3
# Groups:   replicate [1,000]
   replicate dem_win south
       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;
 1         1       1     1
 2         1       1     0
 3         1       1     1
 4         1       1     0
 5         1       0     1
 6         1       0     1
 7         1       0     0
 8         1       0     1
 9         1       0     1
10         1       1     0
# … with 9,556,990 more rows</code></pre>
<p>For each <code>replicate</code>, we have 9557 resamples of <code>dem_win</code> and <code>south</code>.</p>
<p>Next, we are going to <code>nest()</code> our data by <code>replicate</code>:</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb888-1"><a href="13-classification.html#cb888-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb888-2"><a href="13-classification.html#cb888-2"></a><span class="st">  </span><span class="kw">group_by</span>(replicate) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb888-3"><a href="13-classification.html#cb888-3"></a><span class="st">  </span><span class="kw">nest</span>()</span>
<span id="cb888-4"><a href="13-classification.html#cb888-4"></a></span>
<span id="cb888-5"><a href="13-classification.html#cb888-5"></a>x</span></code></pre></div>
<pre><code># A tibble: 1,000 x 2
# Groups:   replicate [1,000]
   replicate data                
       &lt;int&gt; &lt;list&gt;              
 1         1 &lt;tibble [9,557 × 2]&gt;
 2         2 &lt;tibble [9,557 × 2]&gt;
 3         3 &lt;tibble [9,557 × 2]&gt;
 4         4 &lt;tibble [9,557 × 2]&gt;
 5         5 &lt;tibble [9,557 × 2]&gt;
 6         6 &lt;tibble [9,557 × 2]&gt;
 7         7 &lt;tibble [9,557 × 2]&gt;
 8         8 &lt;tibble [9,557 × 2]&gt;
 9         9 &lt;tibble [9,557 × 2]&gt;
10        10 &lt;tibble [9,557 × 2]&gt;
# … with 990 more rows</code></pre>
<p>After grouping by <code>replicate</code> and using <code>nest()</code>, we now have a dataset of 1,000 rows and a list column named <code>data</code>. Each element of <code>data</code> is a tibble consisting of one of the resampled datasets we created using <code>rep_sample_n()</code>.</p>
<p>Now, we can use <code>map()</code> to run our logistic regression for each dataset:</p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb890-1"><a href="13-classification.html#cb890-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb890-2"><a href="13-classification.html#cb890-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>south, <span class="dt">family =</span> binomial, <span class="dt">data =</span> .)))</span>
<span id="cb890-3"><a href="13-classification.html#cb890-3"></a></span>
<span id="cb890-4"><a href="13-classification.html#cb890-4"></a>x</span></code></pre></div>
<pre><code># A tibble: 1,000 x 3
# Groups:   replicate [1,000]
   replicate data                 mod   
       &lt;int&gt; &lt;list&gt;               &lt;list&gt;
 1         1 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 2         2 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 3         3 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 4         4 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 5         5 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 6         6 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 7         7 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 8         8 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
 9         9 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
10        10 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt; 
# … with 990 more rows</code></pre>
<p>Now we have a new list column, <code>mod</code>, that contains the model objects created by <code>glm()</code>. We will now want to <code>tidy()</code> the object created by <code>glm()</code>:</p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb892-1"><a href="13-classification.html#cb892-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb892-2"><a href="13-classification.html#cb892-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">reg_results =</span> <span class="kw">map</span>(mod, <span class="op">~</span><span class="st"> </span><span class="kw">tidy</span>(.)))</span>
<span id="cb892-3"><a href="13-classification.html#cb892-3"></a></span>
<span id="cb892-4"><a href="13-classification.html#cb892-4"></a>x</span></code></pre></div>
<pre><code># A tibble: 1,000 x 4
# Groups:   replicate [1,000]
   replicate data                 mod    reg_results     
       &lt;int&gt; &lt;list&gt;               &lt;list&gt; &lt;list&gt;          
 1         1 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 2         2 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 3         3 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 4         4 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 5         5 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 6         6 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 7         7 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 8         8 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
 9         9 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
10        10 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt;
# … with 990 more rows</code></pre>
<p><code>tidy()</code> stores the coefficients in the <code>estimate</code> column, with each coefficient named in the <code>term</code> column. Thus, if we <code>filter()</code> by <code>term</code> and <code>pull(estimate)</code>, we can get the logistic regression coefficient for each bootstrap sample:</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb894-1"><a href="13-classification.html#cb894-1"></a>x &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span></span>
<span id="cb894-2"><a href="13-classification.html#cb894-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">disp_coef =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;south&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(estimate)))</span>
<span id="cb894-3"><a href="13-classification.html#cb894-3"></a></span>
<span id="cb894-4"><a href="13-classification.html#cb894-4"></a>x</span></code></pre></div>
<pre><code># A tibble: 1,000 x 5
# Groups:   replicate [1,000]
   replicate data                 mod    reg_results      disp_coef
       &lt;int&gt; &lt;list&gt;               &lt;list&gt; &lt;list&gt;               &lt;dbl&gt;
 1         1 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.251803
 2         2 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.289411
 3         3 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.203216
 4         4 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.220194
 5         5 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.209159
 6         6 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.203081
 7         7 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.217606
 8         8 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.313245
 9         9 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.252358
10        10 &lt;tibble [9,557 × 2]&gt; &lt;glm&gt;  &lt;tibble [2 × 5]&gt; -0.224379
# … with 990 more rows</code></pre>
<p>Now that we have 1,000 estimates from our bootstrap samples, we can construct a percentile-based confidence interval easily:</p>
<div class="sourceCode" id="cb896"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb896-1"><a href="13-classification.html#cb896-1"></a>x <span class="op">%&gt;%</span></span>
<span id="cb896-2"><a href="13-classification.html#cb896-2"></a><span class="st"> </span><span class="kw">pull</span>(disp_coef) <span class="op">%&gt;%</span></span>
<span id="cb896-3"><a href="13-classification.html#cb896-3"></a><span class="st"> </span><span class="kw">quantile</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>  2.5%    50%  97.5% 
-0.333 -0.249 -0.166 </code></pre>
<p>As we’ve seen before, these are very similar to what we observe using <code>tidy(conf.int = TRUE)</code>:</p>
<div class="sourceCode" id="cb898"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb898-1"><a href="13-classification.html#cb898-1"></a>house_south_model <span class="op">%&gt;%</span></span>
<span id="cb898-2"><a href="13-classification.html#cb898-2"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb898-3"><a href="13-classification.html#cb898-3"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-614">TABLE 13.4: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.230
</td>
<td style="text-align:right;">
0.181
</td>
<td style="text-align:right;">
0.280
</td>
</tr>
<tr>
<td style="text-align:left;">
south
</td>
<td style="text-align:right;">
-0.249
</td>
<td style="text-align:right;">
-0.334
</td>
<td style="text-align:right;">
-0.164
</td>
</tr>
</tbody>
</table>
<p>Thus, from now on, we’ll just use <code>tidy(conf.int = TRUE)</code> when working with <code>glm()</code>, just as we did with <code>lm()</code>.</p>
<p>Let’s try running a slightly more complicated version of the above model, where we use <code>region</code> as a predictor instead of <code>south</code>.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="13-classification.html#cb899-1"></a>house_region_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>region, <span class="dt">family =</span> binomial, <span class="dt">data =</span> house_ch13)</span>
<span id="cb899-2"><a href="13-classification.html#cb899-2"></a>house_region_model <span class="op">%&gt;%</span></span>
<span id="cb899-3"><a href="13-classification.html#cb899-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb899-4"><a href="13-classification.html#cb899-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-617">TABLE 13.5: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-0.076
</td>
<td style="text-align:right;">
-0.157
</td>
<td style="text-align:right;">
0.006
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast
</td>
<td style="text-align:right;">
0.657
</td>
<td style="text-align:right;">
0.534
</td>
<td style="text-align:right;">
0.780
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth
</td>
<td style="text-align:right;">
0.056
</td>
<td style="text-align:right;">
-0.050
</td>
<td style="text-align:right;">
0.163
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest
</td>
<td style="text-align:right;">
0.333
</td>
<td style="text-align:right;">
0.213
</td>
<td style="text-align:right;">
0.453
</td>
</tr>
</tbody>
</table>
<p>The intercept here is the omitted category, the Midwest. We can calculate the predicted probabilities for each category:</p>
<ul>
<li>Midwest: <span class="math inline">\(\frac{1}{1 + e^{-(-0.076)}} = 0.481\)</span></li>
<li>Northeast: <span class="math inline">\(\frac{1}{1 + e^{-(-0.076 + 0.657)}} = 0.641\)</span></li>
<li>South: <span class="math inline">\(\frac{1}{1 + e^{-(-0.076 + 0.056)}} = 0.495\)</span></li>
<li>West: <span class="math inline">\(\frac{1}{1 + e^{-(-0.076 + 0.333)}} = 0.564\)</span></li>
</ul>
<p>We can then interpret the effect of moving from one category to another. For example, the effect of being in the Northeast as opposed to the Midwest is 0.16; the predicted probability of a Democratic victory in the Northeast is 0.16 greater than in the Midwest. Note too that we could have obtained this through the “divide by 4” rule: <span class="math inline">\(0.657 / 4 \approx 0.16\)</span>.</p>
</div>
<div id="observedfitted-values-and-residuals" class="section level3">
<h3><span class="header-section-number">13.1.4</span> Observed/fitted values and residuals</h3>
<p>We have previously defined the following three concepts for a linear regression:</p>
<ol style="list-style-type: decimal">
<li>Observed values <span class="math inline">\(y\)</span>, or the observed value of the outcome variable</li>
<li>Fitted values <span class="math inline">\(\widehat{y}\)</span>, or the value on the regression line for a given <span class="math inline">\(x\)</span> value</li>
<li>Residuals <span class="math inline">\(y - \widehat{y}\)</span>, or the error between the observed value and the fitted value</li>
</ol>
<p>We obtained these values and other values using the <code>augment()</code> function from the <strong>broom</strong> package. Recall too that we used the <code>.se.fit</code> column to construct confidence intervals. We’ll see here how we can apply these same concepts to logistic regression.</p>
<div class="sourceCode" id="cb900"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb900-1"><a href="13-classification.html#cb900-1"></a>regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></span>
<span id="cb900-2"><a href="13-classification.html#cb900-2"></a><span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span></span>
<span id="cb900-3"><a href="13-classification.html#cb900-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb900-4"><a href="13-classification.html#cb900-4"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb900-5"><a href="13-classification.html#cb900-5"></a><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</span>
<span id="cb900-6"><a href="13-classification.html#cb900-6"></a>regression_points</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-619">TABLE 13.6: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.257
</td>
<td style="text-align:right;">
0.168
</td>
<td style="text-align:right;">
0.347
</td>
<td style="text-align:right;">
-1.29
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.051
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
</tbody>
</table>
<p>The syntax is the same, but the interpretation has to change, since the <code>.fitted</code>, <code>conf.low</code>, and <code>conf.high</code> columns are all on the logit scale. While we could try to interpret these values, <code>augment()</code> has the argument <code>type.predict = "response"</code> that allow us to present the results in terms of <em>predicted probabilities</em>:</p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb901-1"><a href="13-classification.html#cb901-1"></a>regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></span>
<span id="cb901-2"><a href="13-classification.html#cb901-2"></a><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb901-3"><a href="13-classification.html#cb901-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb901-4"><a href="13-classification.html#cb901-4"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb901-5"><a href="13-classification.html#cb901-5"></a><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</span>
<span id="cb901-6"><a href="13-classification.html#cb901-6"></a>regression_points</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-621">TABLE 13.7: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.564
</td>
<td style="text-align:right;">
0.542
</td>
<td style="text-align:right;">
0.586
</td>
<td style="text-align:right;">
-1.29
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-1.17
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
1.19
</td>
</tr>
</tbody>
</table>
<p>Now each of the <code>.fitted</code> values is a <em>predicted probability</em> of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability.</p>
<p>You may be wondering how to interpret the residuals. The residuals reported by <code>augment()</code> for a logistic regression are called <em>deviance residuals</em>. A deviance residual can be calculated for each observation using:</p>
<p><span class="math display">\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the actual outcome and <span class="math inline">\(p_i\)</span> is the predicted probability from the logistic regression model.</p>
<p>The sum of the individual deviance residuals is referred to as the <strong>deviance</strong> or <strong>residual deviance</strong>. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.</p>
<p>However, you can also have <code>augment()</code> report residuals as differences between the observed outcome and the predicted probabilities by using <code>type.residuals = "response"</code>:</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb902-1"><a href="13-classification.html#cb902-1"></a>regression_points &lt;-<span class="st"> </span>house_region_model <span class="op">%&gt;%</span></span>
<span id="cb902-2"><a href="13-classification.html#cb902-2"></a><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,</span>
<span id="cb902-3"><a href="13-classification.html#cb902-3"></a>          <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb902-4"><a href="13-classification.html#cb902-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb902-5"><a href="13-classification.html#cb902-5"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb902-6"><a href="13-classification.html#cb902-6"></a><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</span>
<span id="cb902-7"><a href="13-classification.html#cb902-7"></a>regression_points</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-623">TABLE 13.8: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.564
</td>
<td style="text-align:right;">
0.542
</td>
<td style="text-align:right;">
0.586
</td>
<td style="text-align:right;">
-0.564
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
-0.495
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.495
</td>
<td style="text-align:right;">
0.478
</td>
<td style="text-align:right;">
0.513
</td>
<td style="text-align:right;">
0.505
</td>
</tr>
</tbody>
</table>
<p>Now, the <code>.resid</code> value is the difference between the actual outcome (<code>dem_win</code>) and the predicted probability.</p>
</div>
<div id="one-numerical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.5</span> One numerical explanatory variable</h3>
<p>We’ll now predict <code>dem_win</code> with a single numerical explanatory variable, <code>year</code>.</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x, family, data)</code> function and save it in <code>house_year_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_year_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns.</li>
</ol>
<div class="sourceCode" id="cb903"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb903-1"><a href="13-classification.html#cb903-1"></a>house_year_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>year, <span class="dt">family =</span> binomial, <span class="dt">data =</span> house_ch13)</span>
<span id="cb903-2"><a href="13-classification.html#cb903-2"></a>house_year_model <span class="op">%&gt;%</span></span>
<span id="cb903-3"><a href="13-classification.html#cb903-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb903-4"><a href="13-classification.html#cb903-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-626">TABLE 13.9: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
34.487
</td>
<td style="text-align:right;">
28.10
</td>
<td style="text-align:right;">
40.888
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
-0.017
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
-0.014
</td>
</tr>
</tbody>
</table>
<p>How do we interpret the coefficients in this model? Since the <code>year</code> coefficient is negative, that means that each additional year is associated with a reduction in the chances of a Democratic victory.</p>
<p>If we wanted to learn the predicted probabilities for any given value of <code>year</code>, we can plug in our values of <code>year</code> into the standard logistic function, like so:</p>
<p><span class="math display">\[
p_{dem\_win} = \frac{1}{1 + e^{-(34.487 - 0.017 \times year)}} 
\]</span></p>
<p>Note that since this is not a linear function, a one-unit change in <code>year</code> will be associated with various one-unit changes in <code>dem_win</code>, depending on what <code>year</code> you are starting from. Recall the figure we used to start the chapter:</p>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-627-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>A linear regression line (in blue) has a constant slope, which means that no matter what <span class="math inline">\(x\)</span> you start with, the effect of going from <span class="math inline">\(x\)</span> to <span class="math inline">\(x + 1\)</span> on <span class="math inline">\(y\)</span> is the same number.</p>
<p>However, take a look at the logistic regression curve (in red). The value of the slope for very high or very low values of <span class="math inline">\(x\)</span> is smaller (approaching 0 as <span class="math inline">\(x\)</span> tends to negative or positive infinity), while the slope in the middle of the curve is highest. The steepest part of the curve corresponds to that part of the curve where the predicted probability equals 0.5. That is, the effect of a one-unit change in <span class="math inline">\(x\)</span> is the highest when the predicted probability for that <span class="math inline">\(x\)</span> is close to 0.5 and smallest when the predicted probability for that <span class="math inline">\(x\)</span> is close to 0 or 1.</p>
<p>You can always use R to calculate the predicted probabilities for any value of <span class="math inline">\(x\)</span> and thus calculate the effect of moving from a particular <span class="math inline">\(x\)</span> to <span class="math inline">\(x + 1\)</span>. But this can get complicated. In particular, once you start employing logistic regression with multiple predictors, the effect of a one-unit change in a predictor <span class="math inline">\(x\)</span> depends not only on <span class="math inline">\(x\)</span>, but on the values of all the other predictors in your model! You can always plug in all the coefficients and values of your predictors into the logistic function to calculate predicted probabilities, but if you don’t do that, how can you interpret the coefficients?</p>
<p>Here is where we can use the <em>divide by 4 rule</em> that we discussed before. A logistic regression coefficient divided by 4 is the effect of that variable at the steepest part of the logistic regression curve, which, as we saw, corresponds to where the predicted probability is 0.5.</p>
<p>Therefore, you can divide a logistic regression coefficient by 4 to get an upper bound on the effect a one-unit change in that predictor will have on the predicted probability of your outcome. In this case, the approximation tells us that each additional year is associated with about a <span class="math inline">\(-0.004\)</span> decrease in the predicted probability of a Democratic victory. Since the predicted probability of a Democratic victory in this model never strays far from 0.5, this is a pretty good approximation.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<p>While <code>house_region_model</code> and <code>house_year_model</code> both tell us something interesting, we could learn more with an <em>interaction model</em> that includes both of our predictors.</p>
</div>
<div id="one-numerical-and-one-categorical-explanatory-variable" class="section level3">
<h3><span class="header-section-number">13.1.6</span> One numerical and one categorical explanatory variable</h3>
<p>We’ll now predict <code>dem_win</code> with a two variable, <code>region</code> and <code>year</code>, as well as the interaction between the two</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the logistic regression model using the <code>glm(y ~ x1 * x2, family, data)</code> function and save it in <code>house_interact_model</code>.</li>
<li>We get the regression table by applying the <code>tidy()</code> function from the <strong>broom</strong> package to <code>house_interact_model</code>. We’ll print the <code>term</code>, <code>estimate</code>, <code>conf.low</code>, and <code>conf.high</code> columns.</li>
</ol>
<div class="sourceCode" id="cb904"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb904-1"><a href="13-classification.html#cb904-1"></a>house_interact_model &lt;-<span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>region <span class="op">*</span><span class="st"> </span>year, <span class="dt">family =</span> binomial <span class="dt">data =</span> house_ch13)</span>
<span id="cb904-2"><a href="13-classification.html#cb904-2"></a>house_interact_model <span class="op">%&gt;%</span></span>
<span id="cb904-3"><a href="13-classification.html#cb904-3"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb904-4"><a href="13-classification.html#cb904-4"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-630">TABLE 13.10: </span>Logistic regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
36.395
</td>
<td style="text-align:right;">
23.504
</td>
<td style="text-align:right;">
49.361
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast
</td>
<td style="text-align:right;">
-56.548
</td>
<td style="text-align:right;">
-76.099
</td>
<td style="text-align:right;">
-37.066
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth
</td>
<td style="text-align:right;">
61.388
</td>
<td style="text-align:right;">
43.926
</td>
<td style="text-align:right;">
78.877
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest
</td>
<td style="text-align:right;">
-51.520
</td>
<td style="text-align:right;">
-70.633
</td>
<td style="text-align:right;">
-32.457
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
-0.018
</td>
<td style="text-align:right;">
-0.025
</td>
<td style="text-align:right;">
-0.012
</td>
</tr>
<tr>
<td style="text-align:left;">
regionNortheast:year
</td>
<td style="text-align:right;">
0.029
</td>
<td style="text-align:right;">
0.019
</td>
<td style="text-align:right;">
0.038
</td>
</tr>
<tr>
<td style="text-align:left;">
regionSouth:year
</td>
<td style="text-align:right;">
-0.031
</td>
<td style="text-align:right;">
-0.039
</td>
<td style="text-align:right;">
-0.022
</td>
</tr>
<tr>
<td style="text-align:left;">
regionWest:year
</td>
<td style="text-align:right;">
0.026
</td>
<td style="text-align:right;">
0.016
</td>
<td style="text-align:right;">
0.036
</td>
</tr>
</tbody>
</table>
<p>Now we can see how the effect of <code>year</code> varies by <code>region</code>. While the passage of time is associated with more Democratic victories in the Northeast and the West, <code>year</code> is associated with declining Democratic fortunes in the Midwest (<code>year</code>) and the South (<code>regionSouth:year</code>).</p>
<p>Looking at predicted probabilities can also put this model in perspective. Let’s use <code>augment()</code> to generate the predictions. Remember that <code>type.predict = "response"</code> and <code>type.residuals = "response"</code> put the fitted values and the residuals on the probability scale.</p>
<div class="sourceCode" id="cb905"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb905-1"><a href="13-classification.html#cb905-1"></a>regression_points &lt;-<span class="st"> </span>house_interact_model <span class="op">%&gt;%</span></span>
<span id="cb905-2"><a href="13-classification.html#cb905-2"></a><span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,</span>
<span id="cb905-3"><a href="13-classification.html#cb905-3"></a>          <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb905-4"><a href="13-classification.html#cb905-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb905-5"><a href="13-classification.html#cb905-5"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb905-6"><a href="13-classification.html#cb905-6"></a><span class="st">  </span><span class="kw">select</span>(dem_win, region, .fitted, conf.low, conf.high, .resid)</span>
<span id="cb905-7"><a href="13-classification.html#cb905-7"></a>regression_points</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-632">TABLE 13.11: </span>Regression points (First 10 out of 4,201 district-years)
</caption>
<thead>
<tr>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:left;">
region
</th>
<th style="text-align:right;">
.fitted
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
.resid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
0.522
</td>
<td style="text-align:right;">
0.477
</td>
<td style="text-align:right;">
0.567
</td>
<td style="text-align:right;">
-0.522
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
-0.739
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
0.739
</td>
<td style="text-align:right;">
0.711
</td>
<td style="text-align:right;">
0.768
</td>
<td style="text-align:right;">
0.261
</td>
</tr>
</tbody>
</table>
<p>We can also use <code>augment</code> to make predictions for years that aren’t in our data. What would our model predict for the 2020 elections? We use the <code>newdata</code> argument in <code>augment()</code> to make these predictions.</p>
<div class="sourceCode" id="cb906"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb906-1"><a href="13-classification.html#cb906-1"></a>house_interact_model <span class="op">%&gt;%</span></span>
<span id="cb906-2"><a href="13-classification.html#cb906-2"></a><span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">tibble</span>(<span class="dt">year =</span> <span class="kw">rep</span>(<span class="dv">2020</span>, <span class="dv">4</span>),</span>
<span id="cb906-3"><a href="13-classification.html#cb906-3"></a>                           <span class="dt">region =</span> <span class="kw">c</span>(<span class="st">&quot;Midwest&quot;</span>, <span class="st">&quot;Northeast&quot;</span>, <span class="st">&quot;South&quot;</span>, <span class="st">&quot;West&quot;</span>)),</span>
<span id="cb906-4"><a href="13-classification.html#cb906-4"></a>          <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb906-5"><a href="13-classification.html#cb906-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb906-6"><a href="13-classification.html#cb906-6"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit)</span></code></pre></div>
<pre><code># A tibble: 4 x 6
   year region     .fitted   .se.fit conf.low conf.high
  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1  2020 Midwest   0.373821 0.0210871 0.331646  0.415995
2  2020 Northeast 0.697319 0.0218275 0.653664  0.740974
3  2020 South     0.247674 0.0142235 0.219227  0.276121
4  2020 West      0.604990 0.0217154 0.561559  0.648420</code></pre>
<p>These can easily be plotted using <code>ggplot()</code>:</p>
<div class="sourceCode" id="cb908"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb908-1"><a href="13-classification.html#cb908-1"></a>house_interact_model <span class="op">%&gt;%</span></span>
<span id="cb908-2"><a href="13-classification.html#cb908-2"></a><span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> <span class="kw">tibble</span>(<span class="dt">year =</span> <span class="kw">rep</span>(<span class="dv">2020</span>, <span class="dv">4</span>),</span>
<span id="cb908-3"><a href="13-classification.html#cb908-3"></a>                           <span class="dt">region =</span> <span class="kw">c</span>(<span class="st">&quot;Midwest&quot;</span>, <span class="st">&quot;Northeast&quot;</span>, <span class="st">&quot;South&quot;</span>, <span class="st">&quot;West&quot;</span>)),</span>
<span id="cb908-4"><a href="13-classification.html#cb908-4"></a>          <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb908-5"><a href="13-classification.html#cb908-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">conf.low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit,</span>
<span id="cb908-6"><a href="13-classification.html#cb908-6"></a>         <span class="dt">conf.high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>.se.fit) <span class="op">%&gt;%</span></span>
<span id="cb908-7"><a href="13-classification.html#cb908-7"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .fitted,</span>
<span id="cb908-8"><a href="13-classification.html#cb908-8"></a>             <span class="dt">ymin =</span> conf.low,</span>
<span id="cb908-9"><a href="13-classification.html#cb908-9"></a>             <span class="dt">ymax =</span> conf.high,</span>
<span id="cb908-10"><a href="13-classification.html#cb908-10"></a>             <span class="dt">x =</span> region)) <span class="op">+</span></span>
<span id="cb908-11"><a href="13-classification.html#cb908-11"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb908-12"><a href="13-classification.html#cb908-12"></a><span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span></span>
<span id="cb908-13"><a href="13-classification.html#cb908-13"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Predicted probability of a Democratic victory (2020)&quot;</span>,</span>
<span id="cb908-14"><a href="13-classification.html#cb908-14"></a>       <span class="dt">x =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb908-15"><a href="13-classification.html#cb908-15"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-634-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
</div>
<div id="fitting-many-models-using-map-1" class="section level3">
<h3><span class="header-section-number">13.1.7</span> Fitting many models using <code>map()</code></h3>
<p>While it is interesting to see how Democrats perform by region over time, it would also be interesting to see how each state has changed in its partisan voting from 1976–2018. Have any seen particularly large increases (or decreases) in the probability of a Democratic candidate winning?</p>
<p>The code to do this is very similar to the code we used for the gubernatorial forecasts in Chapter <a href="11-regression.html#regression">11</a> and the Seattle house prices in Chapter <a href="12-multiple-regression.html#multiple-regression">12</a>. However, we will use <code>glm()</code> instead of <code>lm()</code>.</p>
<p>First, we’ll filter to the states that have at least 50 district-years in the dataset. Next, let’s use <code>map()</code> to learn about these districts:</p>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb909-1"><a href="13-classification.html#cb909-1"></a>infreq_states &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb909-2"><a href="13-classification.html#cb909-2"></a><span class="st">  </span><span class="kw">count</span>(state) <span class="op">%&gt;%</span></span>
<span id="cb909-3"><a href="13-classification.html#cb909-3"></a><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&lt;</span><span class="st"> </span><span class="dv">50</span>) <span class="op">%&gt;%</span></span>
<span id="cb909-4"><a href="13-classification.html#cb909-4"></a><span class="st">  </span><span class="kw">pull</span>(state)</span>
<span id="cb909-5"><a href="13-classification.html#cb909-5"></a></span>
<span id="cb909-6"><a href="13-classification.html#cb909-6"></a>house_state_model &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb909-7"><a href="13-classification.html#cb909-7"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="st"> </span>state <span class="op">%in%</span><span class="st">  </span>infreq_states) <span class="op">%&gt;%</span></span>
<span id="cb909-8"><a href="13-classification.html#cb909-8"></a><span class="st">  </span><span class="kw">group_by</span>(state) <span class="op">%&gt;%</span></span>
<span id="cb909-9"><a href="13-classification.html#cb909-9"></a><span class="st">  </span><span class="kw">nest</span>() <span class="op">%&gt;%</span></span>
<span id="cb909-10"><a href="13-classification.html#cb909-10"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(dem_win <span class="op">~</span><span class="st"> </span>year, <span class="dt">family =</span> binomial, <span class="dt">data =</span> .)),</span>
<span id="cb909-11"><a href="13-classification.html#cb909-11"></a>         <span class="dt">reg_results =</span> <span class="kw">map</span>(mod, <span class="op">~</span><span class="st"> </span><span class="kw">tidy</span>(., <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)),</span>
<span id="cb909-12"><a href="13-classification.html#cb909-12"></a>         <span class="dt">year_coef =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(estimate)),</span>
<span id="cb909-13"><a href="13-classification.html#cb909-13"></a>         <span class="dt">year_low =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(conf.low)),</span>
<span id="cb909-14"><a href="13-classification.html#cb909-14"></a>         <span class="dt">year_high =</span> <span class="kw">map_dbl</span>(reg_results, <span class="op">~</span><span class="st"> </span><span class="kw">filter</span>(., term <span class="op">==</span><span class="st"> &quot;year&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(conf.high)))</span>
<span id="cb909-15"><a href="13-classification.html#cb909-15"></a></span>
<span id="cb909-16"><a href="13-classification.html#cb909-16"></a><span class="kw">glimpse</span>(house_state_model)</span></code></pre></div>
<pre><code>Observations: 38
Variables: 7
Groups: state [38]
$ state       &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;Arizona&quot;, &quot;California&quot;, &quot;Colorado…
$ data        &lt;list&gt; [&lt;tbl_df[154 x 5]&gt;, &lt;tbl_df[88 x 5]&gt;, &lt;tbl_df[143 x 5]&gt;,…
$ mod         &lt;list&gt; [&lt;134.3621, -0.0675, -3.85, -3.85, 1.35, 1.35, 1.35, -3.…
$ reg_results &lt;list&gt; [&lt;tbl_df[2 x 7]&gt;, &lt;tbl_df[2 x 7]&gt;, &lt;tbl_df[2 x 7]&gt;, &lt;tbl…
$ year_coef   &lt;dbl&gt; -0.06747, -0.05540, 0.02080, 0.01670, -0.00268, 0.04724, …
$ year_low    &lt;dbl&gt; -0.09811, -0.09382, -0.00711, 0.00683, -0.02954, 0.01515,…
$ year_high   &lt;dbl&gt; -0.039188, -0.020154, 0.049793, 0.026642, 0.024115, 0.081…</code></pre>
<p>The easiest way to see the results of these models is to plot the coefficients:</p>
<div class="sourceCode" id="cb911"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb911-1"><a href="13-classification.html#cb911-1"></a>house_state_model <span class="op">%&gt;%</span></span>
<span id="cb911-2"><a href="13-classification.html#cb911-2"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb911-3"><a href="13-classification.html#cb911-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">fct_reorder</span>(state, year_coef)) <span class="op">%&gt;%</span></span>
<span id="cb911-4"><a href="13-classification.html#cb911-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> state, <span class="dt">y =</span> year_coef, <span class="dt">ymin =</span> year_low, <span class="dt">ymax =</span> year_high)) <span class="op">+</span></span>
<span id="cb911-5"><a href="13-classification.html#cb911-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb911-6"><a href="13-classification.html#cb911-6"></a><span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span></span>
<span id="cb911-7"><a href="13-classification.html#cb911-7"></a><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb911-8"><a href="13-classification.html#cb911-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb911-9"><a href="13-classification.html#cb911-9"></a>       <span class="dt">y =</span> <span class="st">&quot;Coefficients of </span><span class="ch">\&quot;</span><span class="st">year</span><span class="ch">\&quot;</span><span class="st">&quot;</span>,</span>
<span id="cb911-10"><a href="13-classification.html#cb911-10"></a>       <span class="dt">title =</span> <span class="st">&quot;Predicting Democratic victories in the U.S. House over time by state&quot;</span>,</span>
<span id="cb911-11"><a href="13-classification.html#cb911-11"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Logistic regression coefficients for year plotted by state&quot;</span>) <span class="op">+</span></span>
<span id="cb911-12"><a href="13-classification.html#cb911-12"></a><span class="st">  </span><span class="kw">coord_flip</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-636-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Consistent with the account we saw when looking at the effect of <code>year</code> overall, there are more states where the odds of a Democratic victory have been decreasing by year than ones where they have been increasing.</p>
</div>
<div id="professional-models" class="section level3">
<h3><span class="header-section-number">13.1.8</span> Professional models</h3>
<p>So far, we have been fitting linear regressions and logistic regressions using <code>lm()</code> and <code>glm()</code>. While these functions are well-known and easy to use, what if we wanted to fit another model? We’d have to learn a new function, which may have new syntax. It’d be much easier if we could use the same syntax for every model we fit.</p>
<p>A collection of packages that helps address this issue is called <strong>tidymodels</strong>:</p>
<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb912-1"><a href="13-classification.html#cb912-1"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
<p><strong>tidymodels</strong> includes many packages, but we’ll start by showing how to use <strong>parsnip</strong> to fit a logistic regression.</p>
<p>First, in the <strong>tidymodels</strong> workflow, we have to save the <em>model specification</em>. We do that using two functions: <code>logistic_reg()</code> and <code>set_engine()</code>.</p>
<div class="sourceCode" id="cb913"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb913-1"><a href="13-classification.html#cb913-1"></a>logistic_mod &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb913-2"><a href="13-classification.html#cb913-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;glm&quot;</span>)</span></code></pre></div>
<p><code>logistic_reg()</code> says that we want to fit a logistic regression, and <code>set_engine("glm")</code> specifies that we want to do it using <code>glm()</code>. Behind the scenes, <strong>parsnip</strong> uses many other packages to fit its models, but by unifying the syntax, it means that you don’t have to memorize how a lot of different functions work.</p>
<p>Note that our new object, <code>logistic_mod</code>, doesn’t contain our data or a formula. In order actually to fit our model, we need to feed <code>logistic_mod</code> to a function called <code>fit()</code>. <code>fit()</code> is the general purpose function in <strong>parsnip</strong> for fitting any model specification. It takes as its first argument the model specification, but otherwise it operates similarly to <code>lm()</code> and <code>glm()</code>:</p>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb914-1"><a href="13-classification.html#cb914-1"></a>logistic_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(logistic_mod,</span>
<span id="cb914-2"><a href="13-classification.html#cb914-2"></a>                    <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>region <span class="op">*</span><span class="st"> </span>year,</span>
<span id="cb914-3"><a href="13-classification.html#cb914-3"></a>                    <span class="dt">data =</span> house_ch13)</span></code></pre></div>
<p>(We have to wrap <code>dem_win</code> in <code>factor()</code>, because <code>fit()</code> is more careful than <code>glm()</code> in requiring that classification models actually have categorical outcomes.)</p>
<p>One we have fit the model, how can we use it? The <code>glm</code> object is still stored in <code>logistic_fit$fit</code>, so we can access that and use <code>tidy()</code>, just like we did before:</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb915-1"><a href="13-classification.html#cb915-1"></a>logistic_fit<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb915-2"><a href="13-classification.html#cb915-2"></a><span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb915-3"><a href="13-classification.html#cb915-3"></a><span class="st">  </span><span class="kw">select</span>(term, estimate, conf.low, conf.high)</span></code></pre></div>
<pre><code># A tibble: 8 x 4
  term                    estimate    conf.low   conf.high
  &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
1 (Intercept)           36.3954     23.5042     49.3607   
2 regionNortheast      -56.5479    -76.0992    -37.0664   
3 regionSouth           61.3884     43.9261     78.8773   
4 regionWest           -51.5197    -70.6330    -32.4566   
5 year                  -0.0182729  -0.0247690  -0.0118141
6 regionNortheast:year   0.0286625   0.0189008   0.0384600
7 regionSouth:year      -0.0306850  -0.0394437  -0.0219394
8 regionWest:year        0.0259712   0.0164257   0.0355423</code></pre>
<p>As you can see, this generates the same results as when we used <code>glm()</code> directly.</p>
<p>We’ll use <strong>tidymodels</strong> when introducing CART and random forests in this chapter and machine learning in the next chapter. While the models will change, the basic code structure will be very similar to how we fit the logistic regression above.</p>
</div>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">13.2</span> Classification and regression trees (CART)</h2>
<div id="what-is-cart" class="section level3">
<h3><span class="header-section-number">13.2.1</span> What is CART?</h3>
<p>We have learned how to fit models for binary responses using logistic regression. However, logistic regression is just one of many methods we can use to model binary responses. CART is another approach, which we’ll learn about in this section. In the next section, we’ll learn about random forests.</p>
<p>A <strong>tree</strong> is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>. Decision trees predict an outcome variable <span class="math inline">\(Y\)</span> by <em>partitioning</em> the predictors.</p>
<p>Decision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:</p>
<p><img src="images/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin: auto;" /></p>
<p>(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>.)</p>
<p>Here, the binary outcome is whether a patient is “High Risk” or “Low Risk.” We have three predictors: minimum systolic blood pressure over the initial 24-hour period, age, and presence of sinus tachycardia. The tree presents a series of yes or no questions that allow us to use the predictors to classify a patient’s risk level.</p>
<p><strong>Classification trees</strong>, or decision trees, are used in prediction problems where the outcome is categorical. (When the outcome is numerical, they are called <strong>regression trees</strong>; hence the acronym <strong>CART</strong>, standing for Classification and Regression Trees.) The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. A mathematical way to describe this is to say that we are partitioning the <em>predictor space</em> into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, we estimate <span class="math inline">\(f(x)\)</span> with the class that is the most common among the data within the partition for which the associated predictor <span class="math inline">\(x_i\)</span> is also in <span class="math inline">\(R_j\)</span>.</p>
<p>But how do we decide on which partitions to make (<span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>) and how do we choose <span class="math inline">\(J\)</span>, the total number of partitions? Here is where the algorithm gets a bit complicated.</p>
<p>Classification trees create partitions recursively. We start the algorithm with one partition, the entire predictor space (i.e., every observation is classified as 0 or 1). But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. (We will describe how we decide when to stop later.)</p>
<p>Once we select a partition <span class="math inline">\(\mathbf{x}\)</span> to split in order to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, which we will call <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span> (or if <span class="math inline">\(x_j\)</span> falls into a particular category <span class="math inline">\(s\)</span>, if the predictor <span class="math inline">\(j\)</span> is categorical):</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>But how do we pick the predictor <span class="math inline">\(j\)</span> and the value <span class="math inline">\(s\)</span>? One of the more popular ways for categorical data is the <em>Gini Index</em>.</p>
<p>In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The <em>Gini Index</em> is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define <span class="math inline">\(\hat{p}_{j,k}\)</span> as the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. The Gini Index is defined as</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since <span class="math inline">\(\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0\)</span> for all <span class="math inline">\(k\)</span>.</p>
<p>Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>But when do we stop partitioning? Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data. However, our model may therefore perform worse when exposed to new data (this problem is called <em>overfitting</em>). To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the <em>complexity parameter</em> (<span class="math inline">\(c_p\)</span>). The measure of fit must improve by a factor of <span class="math inline">\(c_p\)</span> for the new partition to be added. Large values of <span class="math inline">\(c_p\)</span> will therefore force the algorithm to stop earlier which results in fewer nodes.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.</p>
</div>
<div id="one-categorical-explanatory-variable-1" class="section level3">
<h3><span class="header-section-number">13.2.2</span> One categorical explanatory variable</h3>
<p>To create classification trees, we’ll use the <code>decision_tree()</code> model specification and the <code>"rpart"</code> engine. The syntax is very similar to when we used <code>logistic_reg()</code>. Note that our binary response variable has to be a factor, just like with <code>logistic_reg()</code>.</p>
<div class="sourceCode" id="cb917"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb917-1"><a href="13-classification.html#cb917-1"></a>tree_mod &lt;-<span class="st"> </span><span class="kw">decision_tree</span>() <span class="op">%&gt;%</span></span>
<span id="cb917-2"><a href="13-classification.html#cb917-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb917-3"><a href="13-classification.html#cb917-3"></a>             <span class="dt">model =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb917-4"><a href="13-classification.html#cb917-4"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p>(Note that we added the argument <code>model = TRUE</code> to <code>set_engine()</code>. This saves the model frame, which we will need to avoid a warning when we plot the trees later.)</p>
<p>The function <code>set_mode()</code> wasn’t necessary when we did logistic regression. Here it clarifies that we want a <em>classification</em> tree rather than a <em>regression</em> tree.</p>
<p>Now that we have the object <code>tree_mod</code>, we can use <code>fit()</code> in the <strong>parsnip</strong> package. We’ll start by predicting <code>dem_win</code> with <code>region</code>:</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="13-classification.html#cb918-1"></a>house_region_tree &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb918-2"><a href="13-classification.html#cb918-2"></a>                         <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>region, </span>
<span id="cb918-3"><a href="13-classification.html#cb918-3"></a>                         <span class="dt">data =</span> house_ch13)</span></code></pre></div>
<p>See how when using <strong>tidymodels</strong>, this is exactly the same as how we would fit a logistic regression, but with our model specification saved in <code>tree_mod</code> rather than the model specification we saved in <code>logistic_fit</code>.</p>
<p>What was the result of our tree?</p>
<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb919-1"><a href="13-classification.html#cb919-1"></a>house_region_tree</span></code></pre></div>
<pre><code>parsnip model object

Fit time:  28ms 
n= 9557 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 9557 4430 1 (0.464 0.536)  
  2) region=Midwest,South 5589 2740 0 (0.511 0.489) *
  3) region=Northeast,West 3968 1580 1 (0.398 0.602) *</code></pre>
<p>It’s not especially helpful to look at the results of a tree as text. In order to visualize the tree, we’ll use the <code>prp()</code> function in the <strong>rpart.plot</strong> package. Remember that the model object is stored in <code>house_region_tree$fit</code>.</p>
<div class="sourceCode" id="cb921"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb921-1"><a href="13-classification.html#cb921-1"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb921-2"><a href="13-classification.html#cb921-2"></a></span>
<span id="cb921-3"><a href="13-classification.html#cb921-3"></a>house_region_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb921-4"><a href="13-classification.html#cb921-4"></a><span class="st">  </span><span class="kw">prp</span>(<span class="dt">extra =</span> <span class="dv">6</span>, <span class="dt">varlen =</span> <span class="dv">0</span>, <span class="dt">faclen =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-646-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>The arguments <code>varlen = 0</code> and <code>faclen = 0</code> ensure that the full variable names and factor levels are printed. The argument <code>extra = 6</code> shows the proportion of “yes” outcomes within a given partition. Since we’ll be using these same arguments throughout the chapter, we’ll create a new function that calls <code>prp()</code> but with these options as defaults:</p>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="13-classification.html#cb922-1"></a>prp_ch13 &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) <span class="kw">prp</span>(x, <span class="dt">extra =</span> <span class="dv">6</span>, <span class="dt">varlen =</span> <span class="dv">0</span>, <span class="dt">faclen =</span> <span class="dv">0</span>, ...)</span>
<span id="cb922-2"><a href="13-classification.html#cb922-2"></a></span>
<span id="cb922-3"><a href="13-classification.html#cb922-3"></a>house_region_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb922-4"><a href="13-classification.html#cb922-4"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-647-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>How do we interpret this tree? Here we have two partitions, based on whether an observation is in the Midwest or South or not in the Midwest or South (i.e., in the Northeast or West). If an observation is in the Midwest or South, the tree classifies the observation as a 0, a Democratic loss. The “0.49” means that 49% of the observations in this node were Democratic wins. If an observation is not in the Midwest or South, the algorithm classifies the observation as a 1, a Democratic win; 60% of the observations in this node were Democratic wins.</p>
<p>As you can see, the algorithm is very simple when you have one categorical explanatory variable: it just classifies every observation based on the most common response per category. Take a look at the following table:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Region
</th>
<th style="text-align:right;">
Democratic Win Percentage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Midwest
</td>
<td style="text-align:right;">
48.1
</td>
</tr>
<tr>
<td style="text-align:left;">
Northeast
</td>
<td style="text-align:right;">
64.1
</td>
</tr>
<tr>
<td style="text-align:left;">
South
</td>
<td style="text-align:right;">
49.5
</td>
</tr>
<tr>
<td style="text-align:left;">
West
</td>
<td style="text-align:right;">
56.4
</td>
</tr>
</tbody>
</table>
<p>The two partitions just divide the observations by region into a) those regions where Democrats won a majority of the elections in the data and b) those where they lost a majority of the elections.</p>
</div>
<div id="one-numerical-explanatory-variable-1" class="section level3">
<h3><span class="header-section-number">13.2.3</span> One numerical explanatory variable</h3>
<p>Once we have created our model specification <code>tree_mod</code>, it is easy to use it to fit new models with different formulae and data. We can use the same approach to create a classification tree predicting <code>dem_win</code> with <code>year</code>:</p>
<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb923-1"><a href="13-classification.html#cb923-1"></a>house_year_tree &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb923-2"><a href="13-classification.html#cb923-2"></a>                       <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>year,</span>
<span id="cb923-3"><a href="13-classification.html#cb923-3"></a>                       <span class="dt">data =</span> house_ch13)</span>
<span id="cb923-4"><a href="13-classification.html#cb923-4"></a></span>
<span id="cb923-5"><a href="13-classification.html#cb923-5"></a>house_year_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb923-6"><a href="13-classification.html#cb923-6"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-649-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Now, the algorithm creates cutpoints in the <code>year</code> variable in order to classify observations. This tree will classify observations before 1993 (the 1992 election cycle or earlier, since House elections only occur in even-numbered years) or between 2005 and 2009 (the 2006 and 2008 cycles) as Democratic victories and all other observations as Democratic losses.</p>
<p>Eagle-eyed observers will notice that this is simply classifying every year based on whether the Democrats won a majority in that year, with one exception — observations in 2018 are predicted to be Democratic losses. Why is that? Recall that the algorithm uses a <em>complexity parameter</em>, <span class="math inline">\(c_p\)</span>, to avoid overfitting. The default value of <span class="math inline">\(c_p\)</span> in <code>rpart()</code> (the engine we are using) is 0.01; setting it to 0 allows one to see the maximum number of partitions that the algorithm will do.</p>
<p>How can we change <span class="math inline">\(c_p\)</span>? We don’t need to create an entirely new model specification. Rather, we can use the <code>update()</code> function to change <span class="math inline">\(c_p\)</span> while leaving everything else about <code>tree_mod</code> the same. All we need to do is specify the parameter we want to change, which here is called <code>cost_complexity</code>:</p>
<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb924-1"><a href="13-classification.html#cb924-1"></a>house_year_tree_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">fit</span>(<span class="kw">update</span>(tree_mod, <span class="dt">cost_complexity =</span> <span class="dv">0</span>),</span>
<span id="cb924-2"><a href="13-classification.html#cb924-2"></a>                         <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>year,</span>
<span id="cb924-3"><a href="13-classification.html#cb924-3"></a>                         <span class="dt">data =</span> house_ch13)</span></code></pre></div>
<p>We can then look at the new tree and see what changed:</p>
<div class="sourceCode" id="cb925"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb925-1"><a href="13-classification.html#cb925-1"></a>house_year_tree_<span class="dv">0</span><span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb925-2"><a href="13-classification.html#cb925-2"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-651-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Now there is an additional partition that classifies observations where <code>year</code> <span class="math inline">\(\geq 2017\)</span> as 1.</p>
<p>But which tree is better? We can answer that question using the <code>tune_grid()</code> function in the <strong>tune</strong> package, included as part of <strong>tidymodels</strong>. The basic idea is that we want to test out various values of <span class="math inline">\(c_p\)</span>. and select the model that performs best. We will provide code on how to decide which model is best by calculating the accuracy on 25 bootstrapped samples of our data. (We wouldn’t want to calculate the accuracy on our full data, because a lower value of <span class="math inline">\(c_p\)</span> will always perform best on any particular dataset, but the resulting model may not perform well on out-of-sample observations; this is an example of a general phenomenon called <em>overfitting</em>.)</p>
<p>First, we need to update the model specification to note that we are tuning the <code>cost_complexity</code> parameter. We can do that using the <code>update()</code> function and setting <code>cost_complexity = tune()</code>:</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb926-1"><a href="13-classification.html#cb926-1"></a>tree_mod_tune &lt;-<span class="st"> </span>tree_mod <span class="op">%&gt;%</span></span>
<span id="cb926-2"><a href="13-classification.html#cb926-2"></a><span class="st">  </span><span class="kw">update</span>(<span class="dt">cost_complexity =</span> <span class="kw">tune</span>())</span></code></pre></div>
<p>Next, we need to create our bootstrapped data. We can do that using <strong>tidymodels</strong> with the <code>bootstraps()</code> function in the <strong>rsample</strong> package. We will first call <code>set.seed()</code> in order to make sure our results are replicable, given the random nature of the bootstrap process.</p>
<div class="sourceCode" id="cb927"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb927-1"><a href="13-classification.html#cb927-1"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb927-2"><a href="13-classification.html#cb927-2"></a>house_boots &lt;-<span class="st"> </span><span class="kw">bootstraps</span>(house_ch13)</span></code></pre></div>
<p>By default, the <code>bootstraps()</code> function generates 25 samples. You can modify this with the <code>times</code> argument.</p>
<p>Next, we must set the values of <code>cost_complexity</code> that we will test. We can select random values using the <code>grid_random()</code> function; by default, it selects 5, although you can adjust that with the <code>size</code> option. We’ll save the result in <code>cost_grid</code>.</p>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb928-1"><a href="13-classification.html#cb928-1"></a>cost_grid &lt;-<span class="st"> </span><span class="kw">grid_random</span>(<span class="kw">cost_complexity</span>())</span>
<span id="cb928-2"><a href="13-classification.html#cb928-2"></a>cost_grid</span></code></pre></div>
<pre><code># A tibble: 5 x 1
  cost_complexity
            &lt;dbl&gt;
1    0.000198792 
2    0.0000668238
3    0.00503690  
4    0.00733769  
5    0.00133459  </code></pre>
<p>Now we have the elements necessary to run <code>tune_grid</code>. The key arguments are our formula (here, <code>factor(dem_win) ~ year</code>), the <code>model</code> (<code>tree_mod_tune</code>), the <code>resamples</code> (<code>house_boots</code>), and the grid (<code>cost_grid</code>). We’ll save the result as <code>tree_res</code>:</p>
<div class="sourceCode" id="cb930"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb930-1"><a href="13-classification.html#cb930-1"></a>tree_res &lt;-<span class="st"> </span><span class="kw">tune_grid</span>(<span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>year,</span>
<span id="cb930-2"><a href="13-classification.html#cb930-2"></a>                      <span class="dt">model =</span> tree_mod_tune,</span>
<span id="cb930-3"><a href="13-classification.html#cb930-3"></a>                      <span class="dt">resamples =</span> house_boots,</span>
<span id="cb930-4"><a href="13-classification.html#cb930-4"></a>                      <span class="dt">grid =</span> cost_grid)</span></code></pre></div>
<p>Now we need to know what value of <code>cost_complexity</code> performed the best. For this, we use the function <code>select_best()</code>. We need to supply an argument called <code>metric</code> to <code>select_best()</code>; for this example, we’ll choose <code>"accuracy"</code>, which is just the percentage of observations correctly classified.</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="13-classification.html#cb931-1"></a>best_cost &lt;-<span class="st"> </span><span class="kw">select_best</span>(tree_res, <span class="dt">metric =</span> <span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb931-2"><a href="13-classification.html#cb931-2"></a>best_cost</span></code></pre></div>
<pre><code># A tibble: 1 x 1
  cost_complexity
            &lt;dbl&gt;
1     0.000198792</code></pre>
<p>We can see from the output that 0 performed best on our bootstrap samples. We can visualize these results using <code>autoplot()</code>:</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb933-1"><a href="13-classification.html#cb933-1"></a><span class="kw">autoplot</span>(tree_res, <span class="dt">metric =</span> <span class="st">&quot;accuracy&quot;</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-657-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>All we have left to do is to <code>update()</code> our model specification based on <code>best_cost</code> and fit the results:</p>
<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb934-1"><a href="13-classification.html#cb934-1"></a>house_year_tree_best &lt;-<span class="st"> </span><span class="kw">fit</span>(<span class="kw">update</span>(tree_mod, <span class="dt">cost_complexity =</span> best_cost),</span>
<span id="cb934-2"><a href="13-classification.html#cb934-2"></a>                       <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>year,</span>
<span id="cb934-3"><a href="13-classification.html#cb934-3"></a>                       <span class="dt">data =</span> house_ch13)</span></code></pre></div>
<p>Let’s look at the tree, saved in <code>house_year_tree_best$fit</code>:</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb935-1"><a href="13-classification.html#cb935-1"></a>house_year_tree_best<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb935-2"><a href="13-classification.html#cb935-2"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-659-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>With the value of <code>cost_complexity</code> chosen by <code>tune_grid()</code>, we retain the partition based on <code>year &lt; 2017</code>.</p>
</div>
<div id="multiple-explanatory-variables" class="section level3">
<h3><span class="header-section-number">13.2.4</span> Multiple explanatory variables</h3>
<p>What if we wanted to predict <code>dem_win</code> based on <code>region</code> and <code>year</code>? The process is similar to what we’ve seen before. We’ll fit a new model called <code>region_year_tree</code> using our model specification <code>tree_mod</code>.</p>
<div class="sourceCode" id="cb936"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb936-1"><a href="13-classification.html#cb936-1"></a>region_year_tree &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb936-2"><a href="13-classification.html#cb936-2"></a>                        <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>region <span class="op">+</span><span class="st"> </span>year,</span>
<span id="cb936-3"><a href="13-classification.html#cb936-3"></a>                        <span class="dt">data =</span> house_ch13)</span>
<span id="cb936-4"><a href="13-classification.html#cb936-4"></a></span>
<span id="cb936-5"><a href="13-classification.html#cb936-5"></a>region_year_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb936-6"><a href="13-classification.html#cb936-6"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-660-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This tree classifies observations in the Northeast and West as a Democratic win and observations in the Midwest and South as a Democratic win before 1993 and a Democratic loss afterward.</p>
<p>In the section on logistic regression, we used <code>map()</code> to fit many models to visualize the effect of <code>year</code> by <code>state</code>. When fitting classification trees, it is easy just to add <code>state</code> as a predictor and allow the complexity parameter to determine whether we ought to partition further states by <code>year</code>. In fact, unlike with <code>lm()</code> and <code>glm()</code>, we can include both <code>region</code> and <code>state</code> in the model, and the algorithm will decide whether it wants to use the additional information provided by <code>state</code> or just create partitions based on <code>region</code>. Let’s take a look:</p>
<div class="sourceCode" id="cb937"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb937-1"><a href="13-classification.html#cb937-1"></a>state_year_tree &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb937-2"><a href="13-classification.html#cb937-2"></a>                       <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>region <span class="op">+</span><span class="st"> </span>state <span class="op">+</span><span class="st"> </span>year,</span>
<span id="cb937-3"><a href="13-classification.html#cb937-3"></a>                       <span class="dt">data =</span> house_ch13)</span>
<span id="cb937-4"><a href="13-classification.html#cb937-4"></a></span>
<span id="cb937-5"><a href="13-classification.html#cb937-5"></a>state_year_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb937-6"><a href="13-classification.html#cb937-6"></a><span class="st">  </span><span class="kw">prp_ch13</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-661-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>Unfortunately, this is very difficult to read, since <code>prp()</code> tries to plot all the state names on one line. The [documentation][<a href="http://www.milbo.org/doc/prp.pdf" class="uri">http://www.milbo.org/doc/prp.pdf</a>] gives example code on how to wrap the factor labels across multiple lines, which we will adapt:</p>
<div class="sourceCode" id="cb938"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb938-1"><a href="13-classification.html#cb938-1"></a>split.fun &lt;-<span class="st"> </span><span class="cf">function</span>(x, labs, digits, varlen, faclen) {</span>
<span id="cb938-2"><a href="13-classification.html#cb938-2"></a>  </span>
<span id="cb938-3"><a href="13-classification.html#cb938-3"></a>  <span class="co"># Replace commas with spaces (needed for strwrap)</span></span>
<span id="cb938-4"><a href="13-classification.html#cb938-4"></a>  </span>
<span id="cb938-5"><a href="13-classification.html#cb938-5"></a>  labs &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;,&quot;</span>, <span class="st">&quot; &quot;</span>, labs)</span>
<span id="cb938-6"><a href="13-classification.html#cb938-6"></a>  </span>
<span id="cb938-7"><a href="13-classification.html#cb938-7"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(labs)) {</span>
<span id="cb938-8"><a href="13-classification.html#cb938-8"></a>    </span>
<span id="cb938-9"><a href="13-classification.html#cb938-9"></a>    <span class="co"># Split labs[i] into multiple lines</span></span>
<span id="cb938-10"><a href="13-classification.html#cb938-10"></a>    </span>
<span id="cb938-11"><a href="13-classification.html#cb938-11"></a>    labs[i] &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="kw">strwrap</span>(labs[i], <span class="dt">width =</span> <span class="dv">50</span>), <span class="dt">collapse =</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb938-12"><a href="13-classification.html#cb938-12"></a>  }</span>
<span id="cb938-13"><a href="13-classification.html#cb938-13"></a>  </span>
<span id="cb938-14"><a href="13-classification.html#cb938-14"></a>  labs</span>
<span id="cb938-15"><a href="13-classification.html#cb938-15"></a>}</span></code></pre></div>
<p>Now we need to apply <code>split.fun</code> to <code>prp()</code>; we’ll also set <code>faclen = 2</code> in order to abbreviate the state names:</p>
<div class="sourceCode" id="cb939"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb939-1"><a href="13-classification.html#cb939-1"></a>state_year_tree<span class="op">$</span>fit <span class="op">%&gt;%</span></span>
<span id="cb939-2"><a href="13-classification.html#cb939-2"></a><span class="st">  </span><span class="kw">prp</span>(<span class="dt">split.fun =</span> split.fun,</span>
<span id="cb939-3"><a href="13-classification.html#cb939-3"></a>      <span class="dt">faclen =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-663-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>This experience shows a tradeoff when working with decision trees: as they get more complex, they become more accurate but also harder to read.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">13.3</span> Random forests</h2>
<div id="what-are-random-forests" class="section level3">
<h3><span class="header-section-number">13.3.1</span> What are random forests?</h3>
<p>Random forests are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<!-- AR: do we explain training set/test set before this? -->
<p>1. Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p>
<p>2. For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p>
<p>3. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<p>1. Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p>
<p>2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p>
</div>
<div id="fitting-random-forests" class="section level3">
<h3><span class="header-section-number">13.3.2</span> Fitting random forests</h3>
<p>We will demonstrate by fitting a random forest to the House elections data, predicting <code>dem_win</code> with <code>region</code>, <code>state</code>, and <code>year</code>. We will use the <code>rand_forest()</code> function to create our model specification, setting the engine to <code>"randomForest"</code> and the mode to <code>"classification"</code>.</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb940-1"><a href="13-classification.html#cb940-1"></a>forest_mod &lt;-<span class="st"> </span><span class="kw">rand_forest</span>() <span class="op">%&gt;%</span></span>
<span id="cb940-2"><a href="13-classification.html#cb940-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;randomForest&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb940-3"><a href="13-classification.html#cb940-3"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb940-4"><a href="13-classification.html#cb940-4"></a></span>
<span id="cb940-5"><a href="13-classification.html#cb940-5"></a>house_forest &lt;-<span class="st"> </span><span class="kw">fit</span>(forest_mod,</span>
<span id="cb940-6"><a href="13-classification.html#cb940-6"></a>                    <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>region <span class="op">+</span><span class="st"> </span>state <span class="op">+</span><span class="st"> </span>year,</span>
<span id="cb940-7"><a href="13-classification.html#cb940-7"></a>                    <span class="dt">data =</span> house_ch13)</span>
<span id="cb940-8"><a href="13-classification.html#cb940-8"></a></span>
<span id="cb940-9"><a href="13-classification.html#cb940-9"></a>house_forest</span></code></pre></div>
<pre><code>parsnip model object

Fit time:  17.8s 

Call:
 randomForest(x = as.data.frame(x), y = y) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 36%
Confusion matrix:
     0    1 class.error
0 2349 2084       0.470
1 1356 3768       0.265</code></pre>
<p>We see under “OOB estimate of error rate” that this model has an error rate of 36% (or, looking at it the other way, an accuracy of 64%). We can see how the error rate of our algorithm changes as we add trees by looking at <code>house_forest$fit$err.rate[, "OOB"]</code>. By default, <code>randomForest()</code> (the engine we specified) grows 500 trees.</p>
<div class="sourceCode" id="cb942"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb942-1"><a href="13-classification.html#cb942-1"></a><span class="kw">tibble</span>(<span class="st">`</span><span class="dt">Error rate</span><span class="st">`</span> =<span class="st"> </span>house_forest<span class="op">$</span>fit<span class="op">$</span>err.rate[, <span class="st">&quot;OOB&quot;</span>],</span>
<span id="cb942-2"><a href="13-classification.html#cb942-2"></a>       <span class="dt">Trees =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="op">%&gt;%</span></span>
<span id="cb942-3"><a href="13-classification.html#cb942-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Trees, <span class="dt">y =</span> <span class="st">`</span><span class="dt">Error rate</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb942-4"><a href="13-classification.html#cb942-4"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb942-5"><a href="13-classification.html#cb942-5"></a><span class="st">  </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="Preceptor%E2%80%99s-Primer-for-Bayesian-Data-Science_files/figure-html/unnamed-chunk-665-1.png" width="\textwidth" style="display: block; margin: auto;" /></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 300 trees where accuracy stabilizes.</p>
<p>Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability—we don’t get anything like the coefficients from a logistic regression or the single tree from CART.</p>
<!-- AR: Cutting this; not sure how to do this in tidymodels -->
<!-- An approach that helps with interpretability is to examine _variable importance_. To define _variable importance_ we count how often a predictor is used in the individual trees. You can learn more about _variable importance_ in an advanced machine learning book.^[https://web.stanford.edu/~hastie/Papers/ESLII.pdf] The __caret__ package includes the function `varImp` that extracts variable importance from any model in which the calculation is implemented.  -->
<!-- ```{r} -->
<!-- caret::varImp(house_forest) -->
<!-- ``` -->
<!-- Thus, we can see that the state an observation is in plays the biggest role in our forest, followed by year and finally by region. -->
</div>
</div>
<div id="comparing-the-three-approaches" class="section level2">
<h2><span class="header-section-number">13.4</span> Comparing the three approaches</h2>
<p>We’ve explored three ways to model binary responses in this chapter. Which one should you use?</p>
<p>This isn’t as straightforward a question as it may seem. First, we need some measure of which model is “better.” The most obvious way to compare models for binary data is simply to look at the predicted outcomes from the model and compare those predicted outcomes to the actual outcomes. The percentage of outcomes correctly classified by the model is called the model’s <em>accuracy</em>.</p>
<p>Let’s compare the accuracy of the following models in predicting Democratic victories in House elections:</p>
<ul>
<li>Predicting a Democratic win (the modal outcome in our data) for every observation</li>
<li>A logistic regression predicting <code>dem_win</code> with <code>state</code>, <code>year</code>, and the interaction of <code>state</code> and <code>year</code></li>
<li>A linear regression with the same predictors as the logistic regression</li>
<li>A classification tree using <code>state</code> and <code>year</code></li>
<li>A random forest using <code>state</code> and <code>year</code></li>
</ul>
<p>Those last two models we already have saved as <code>state_year_tree</code> and <code>house_forest</code> respectively. We’ll create the logistic and linear regressions and save them as <code>house_logistic</code> and <code>house_linear</code>:</p>
<div class="sourceCode" id="cb943"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb943-1"><a href="13-classification.html#cb943-1"></a>house_logistic &lt;-<span class="st"> </span><span class="kw">fit</span>(logistic_mod,</span>
<span id="cb943-2"><a href="13-classification.html#cb943-2"></a>                      <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>state <span class="op">*</span><span class="st"> </span>year,</span>
<span id="cb943-3"><a href="13-classification.html#cb943-3"></a>                      <span class="dt">data =</span> house_ch13)</span>
<span id="cb943-4"><a href="13-classification.html#cb943-4"></a></span>
<span id="cb943-5"><a href="13-classification.html#cb943-5"></a>house_linear &lt;-<span class="st"> </span><span class="kw">fit</span>(<span class="kw">linear_reg</span>(),</span>
<span id="cb943-6"><a href="13-classification.html#cb943-6"></a>                    dem_win <span class="op">~</span><span class="st"> </span>state <span class="op">*</span><span class="st"> </span>year,</span>
<span id="cb943-7"><a href="13-classification.html#cb943-7"></a>                    <span class="dt">data =</span> house_ch13)</span></code></pre></div>
<p>Note that fitting a linear regression is as simple as using the model specification <code>linear_reg()</code>, which by default uses <code>lm()</code> as its engine.</p>
<p>How can we extract the predictions for each of these models? <strong>tidymodels</strong> provides a unified function, <code>predict()</code>. The first argument of the function is the fitted model object. It also requires the argument <code>new_data</code>, which is the dataset on which you will be making your predictions. Here, we are making predictions on our original data (<code>house_ch13</code>). For example:</p>
<div class="sourceCode" id="cb944"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb944-1"><a href="13-classification.html#cb944-1"></a><span class="kw">predict</span>(house_logistic, <span class="dt">new_data =</span> house_ch13)</span></code></pre></div>
<pre><code># A tibble: 9,557 x 1
   .pred_class
   &lt;fct&gt;      
 1 0          
 2 1          
 3 1          
 4 1          
 5 1          
 6 1          
 7 1          
 8 1          
 9 1          
10 1          
# … with 9,547 more rows</code></pre>
<p>The result is a tibble with the predicted outcomes stored in the column <code>.pred_class</code>.</p>
<p>For the linear model, the result is going to be a number, not a class:</p>
<div class="sourceCode" id="cb946"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb946-1"><a href="13-classification.html#cb946-1"></a><span class="kw">predict</span>(house_linear, <span class="dt">new_data =</span> house_ch13)</span></code></pre></div>
<pre><code># A tibble: 9,557 x 1
         .pred
         &lt;dbl&gt;
 1 1.00222e-11
 2 7.34613e- 1
 3 7.34613e- 1
 4 7.34613e- 1
 5 7.34613e- 1
 6 7.34613e- 1
 7 7.34613e- 1
 8 7.34613e- 1
 9 7.82609e- 1
10 7.82609e- 1
# … with 9,547 more rows</code></pre>
<p>We can interpret this as a predicted probability and classify responses <span class="math inline">\(&gt; 0.5\)</span> as <span class="math inline">\(1\)</span> and <span class="math inline">\(&lt; 0.5\)</span> as <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb948"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb948-1"><a href="13-classification.html#cb948-1"></a><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">predict</span>(house_linear, <span class="dt">new_data =</span> house_ch13) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>)</span></code></pre></div>
<p>Let’s put it all together in a tibble called <code>house_preds</code>:</p>
<div class="sourceCode" id="cb949"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb949-1"><a href="13-classification.html#cb949-1"></a>house_preds &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span></span>
<span id="cb949-2"><a href="13-classification.html#cb949-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modal =</span> <span class="dv">1</span>,</span>
<span id="cb949-3"><a href="13-classification.html#cb949-3"></a>         <span class="dt">logistic =</span> <span class="kw">predict</span>(house_logistic, <span class="dt">new_data =</span> house_ch13) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb949-4"><a href="13-classification.html#cb949-4"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class),</span>
<span id="cb949-5"><a href="13-classification.html#cb949-5"></a>         <span class="dt">linear =</span> <span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">predict</span>(house_linear, <span class="dt">new_data =</span> house_ch13) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>),</span>
<span id="cb949-6"><a href="13-classification.html#cb949-6"></a>         <span class="dt">tree =</span> <span class="kw">predict</span>(state_year_tree, <span class="dt">new_data =</span> house_ch13) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb949-7"><a href="13-classification.html#cb949-7"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class),</span>
<span id="cb949-8"><a href="13-classification.html#cb949-8"></a>         <span class="dt">forest =</span> <span class="kw">predict</span>(house_forest, <span class="dt">new_data =</span> house_ch13) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb949-9"><a href="13-classification.html#cb949-9"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class))</span></code></pre></div>
<p>We’ll start by looking at five random rows of <code>house_preds</code>.</p>
<div class="sourceCode" id="cb950"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb950-1"><a href="13-classification.html#cb950-1"></a>house_preds <span class="op">%&gt;%</span></span>
<span id="cb950-2"><a href="13-classification.html#cb950-2"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>region, <span class="op">-</span>state) <span class="op">%&gt;%</span></span>
<span id="cb950-3"><a href="13-classification.html#cb950-3"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:left;">
district
</th>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
dem_win
</th>
<th style="text-align:right;">
south
</th>
<th style="text-align:right;">
modal
</th>
<th style="text-align:left;">
logistic
</th>
<th style="text-align:right;">
linear
</th>
<th style="text-align:left;">
tree
</th>
<th style="text-align:left;">
forest
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
NC-12
</td>
<td style="text-align:right;">
2014
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
PA-17
</td>
<td style="text-align:right;">
2002
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
SC-02
</td>
<td style="text-align:right;">
2010
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
NY-29
</td>
<td style="text-align:right;">
1984
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
PA-15
</td>
<td style="text-align:right;">
1990
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
<p>In these five districts, the Democrats won two and therefore our <code>modal</code> prediction, which always predicts a Democratic win, classifies 40% of the observations correctly. For these districts, the logistic, linear, and classification tree models all classify SC-02 in 2010 correctly as a Democratic loss and misclassify the remaining districts, for an accuracy of 20%. Finally, the random forest model classifies SC-02 in 2010 correctly as a Democratic loss and also correctly classifies PA-17 in 2002 as a Democratic win, for an accuracy of 40%.</p>
<p>Now let’s calculate the overall accuracy of our five models. We simply need to calculate the proportion of observations where the predicted value equals <code>dem_win</code>:</p>
<div class="sourceCode" id="cb951"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb951-1"><a href="13-classification.html#cb951-1"></a><span class="kw">tibble</span>(<span class="dt">modal =</span> <span class="kw">mean</span>(house_preds<span class="op">$</span>modal <span class="op">==</span><span class="st"> </span>house_preds<span class="op">$</span>dem_win),</span>
<span id="cb951-2"><a href="13-classification.html#cb951-2"></a>       <span class="dt">logistic =</span> <span class="kw">mean</span>(house_preds<span class="op">$</span>logistic <span class="op">==</span><span class="st"> </span>house_preds<span class="op">$</span>dem_win),</span>
<span id="cb951-3"><a href="13-classification.html#cb951-3"></a>       <span class="dt">linear =</span> <span class="kw">mean</span>(house_preds<span class="op">$</span>linear <span class="op">==</span><span class="st"> </span>house_preds<span class="op">$</span>dem_win),</span>
<span id="cb951-4"><a href="13-classification.html#cb951-4"></a>       <span class="dt">tree =</span> <span class="kw">mean</span>(house_preds<span class="op">$</span>tree <span class="op">==</span><span class="st"> </span>house_preds<span class="op">$</span>dem_win),</span>
<span id="cb951-5"><a href="13-classification.html#cb951-5"></a>       <span class="dt">forest =</span> <span class="kw">mean</span>(house_preds<span class="op">$</span>forest <span class="op">==</span><span class="st"> </span>house_preds<span class="op">$</span>dem_win))</span></code></pre></div>
<pre><code># A tibble: 1 x 5
     modal logistic   linear     tree   forest
     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
1 0.536152 0.649681 0.649785 0.642356 0.645391</code></pre>
<p>Here we see that all our models perform better than simply picking the modal outcome, but there isn’t much variation in accuracy of the four more sophisticated models.</p>
<p>However, when selecting a model, we may care about more than just accuracy. There are three concerns in particular that we’ll address here:</p>
<ol style="list-style-type: decimal">
<li>Is accuracy the right measure?</li>
<li>Modeling for prediction vs. explanation</li>
<li>Out-of-sample predictions</li>
</ol>
<div id="is-accuracy-the-right-measure" class="section level3">
<h3><span class="header-section-number">13.4.1</span> Is accuracy the right measure?</h3>
<p>We’ve been looking at the <em>accuracy</em> of each model. However, we can come up with more granular measures. Every prediction can be classified into one of the following four categories:</p>
<ul>
<li>True positive (classified as a <code>1</code> and actually a <code>1</code>)</li>
<li>True negative (classified as a <code>0</code> and actually a <code>0</code>)</li>
<li>False positive (classified as a <code>1</code> but actually a <code>0</code>)</li>
<li>False negative (classified as a <code>0</code> but actually a <code>1</code>)</li>
</ul>
<p>Accuracy thus is <span class="math inline">\((\text{True positives } + \text{ True negatives})/n\)</span>. When using accuracy as a metric, therefore, we are implicitly assuming that false positives and false negatives are equally bad. In the context of election forecasting, that is probably right, since we probably only care about how many seats we accurately predict.</p>
<p>However, there are other scenarios where a false negative and a false positive can have very different costs associated with them, such as when evaluating medical interventions. (A false negative may mean that a person who actually has a disease receives no treatment, whereas a false positive may mean that a healthy person receives a medical intervention; which is worse depends on the disease and the side effects of the treatment.) Therefore, there are many metrics beyond accuracy that may be relevant for different applications, such as <em>sensitivity</em> (the proportion of actual <span class="math inline">\(1\)</span>s classified as <span class="math inline">\(1\)</span>) and <em>specificity</em> (the proportion of actual <span class="math inline">\(0\)</span>s classified as <span class="math inline">\(0\)</span>).</p>
</div>
<div id="modeling-for-prediction-vs.-explanation" class="section level3">
<h3><span class="header-section-number">13.4.2</span> Modeling for prediction vs. explanation</h3>
<p>All things being equal, we would prefer a model with more predictive power to one with less. However, we have emphasized that models may serve different purposes. Some focus on attempting to predict an outcome. Others focus on learning about a <em>causal effect</em>. When one is interested in identifying a causal effect, we may not always prefer the model that has the greatest predictive power.</p>
<p>Why not? One concern is <em>interpretability</em>. We want to have a sense of the magnitude of the causal effect and our uncertainty about that magnitude; this is much easier with models such as linear and logistic regressions that produce coefficients.</p>
<p>Another concern is <em>post-treatment bias</em>. For example, let’s say that we were interested in the effect of a hypertension drug on all-cause mortality. We cannot use blood pressure measurements taken after the drug was administered as a predictor if we are interested in the effects of the drug on mortality, even though such a variable will surely improve our predictions, since that variable is affected by our treatment.</p>
</div>
<div id="out-of-sample-predictions" class="section level3">
<h3><span class="header-section-number">13.4.3</span> Out-of-sample predictions</h3>
<p>We saw that our models predicted 64%-65% of the House races from 1976–2018 accurately using <code>state</code> and <code>year</code> as predictors. However, these models were fitted on the same data that we used to evaluate their predictions. These are called <em>in-sample</em> predictions. A more strenuous test of a model’s performance is whether it can generate good predictions on data that were not used in fitting the model. For election forecasting, for example, we are much more interested in forecasting the 2020 election results (which we don’t know) than the 1976–2018 election results (which we know). The 2020 predictions are an example of <em>out-of-sample</em> predictions.</p>
<p>While we can’t evaluate our models’ performance on the 2020 elections (yet!), we can take a look at the process by fitting our logistic, classification tree, and random forest models on the data we have pre-2018 and then evaluating their accuracy on the 2018 election results. Let’s start by creating tibbles of our pre-2018 and 2018 data:</p>
<div class="sourceCode" id="cb953"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb953-1"><a href="13-classification.html#cb953-1"></a>house_pre_<span class="dv">2018</span> &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(year <span class="op">!=</span><span class="st"> </span><span class="dv">2018</span>)</span>
<span id="cb953-2"><a href="13-classification.html#cb953-2"></a>house_<span class="dv">2018</span> &lt;-<span class="st"> </span>house_ch13 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2018</span>)</span></code></pre></div>
<p>Next, we’ll fit the models on the pre-2018 data:</p>
<div class="sourceCode" id="cb954"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb954-1"><a href="13-classification.html#cb954-1"></a>house_logistic_pre &lt;-<span class="st"> </span><span class="kw">fit</span>(logistic_mod,</span>
<span id="cb954-2"><a href="13-classification.html#cb954-2"></a>                           <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>state <span class="op">*</span><span class="st"> </span>year,</span>
<span id="cb954-3"><a href="13-classification.html#cb954-3"></a>                           <span class="dt">data =</span> house_pre_<span class="dv">2018</span>)</span>
<span id="cb954-4"><a href="13-classification.html#cb954-4"></a></span>
<span id="cb954-5"><a href="13-classification.html#cb954-5"></a>house_tree_pre &lt;-<span class="st"> </span><span class="kw">fit</span>(tree_mod,</span>
<span id="cb954-6"><a href="13-classification.html#cb954-6"></a>                       <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>state <span class="op">+</span><span class="st"> </span>year,</span>
<span id="cb954-7"><a href="13-classification.html#cb954-7"></a>                       <span class="dt">data =</span> house_pre_<span class="dv">2018</span>)</span>
<span id="cb954-8"><a href="13-classification.html#cb954-8"></a></span>
<span id="cb954-9"><a href="13-classification.html#cb954-9"></a>house_forest_pre &lt;-<span class="st"> </span><span class="kw">fit</span>(forest_mod,</span>
<span id="cb954-10"><a href="13-classification.html#cb954-10"></a>                         <span class="kw">factor</span>(dem_win) <span class="op">~</span><span class="st"> </span>state <span class="op">+</span><span class="st"> </span>year,</span>
<span id="cb954-11"><a href="13-classification.html#cb954-11"></a>                         <span class="dt">data =</span> house_pre_<span class="dv">2018</span>)</span></code></pre></div>
<p>We’ll then generate predictions using <code>new_data = house_2018</code>:</p>
<div class="sourceCode" id="cb955"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb955-1"><a href="13-classification.html#cb955-1"></a>house_<span class="dv">2018</span>_preds &lt;-<span class="st"> </span>house_<span class="dv">2018</span> <span class="op">%&gt;%</span></span>
<span id="cb955-2"><a href="13-classification.html#cb955-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logistic =</span> <span class="kw">predict</span>(house_logistic_pre, <span class="dt">new_data =</span> house_<span class="dv">2018</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb955-3"><a href="13-classification.html#cb955-3"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class),</span>
<span id="cb955-4"><a href="13-classification.html#cb955-4"></a>         <span class="dt">tree =</span> <span class="kw">predict</span>(house_tree_pre, <span class="dt">new_data =</span> house_<span class="dv">2018</span>)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb955-5"><a href="13-classification.html#cb955-5"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class),</span>
<span id="cb955-6"><a href="13-classification.html#cb955-6"></a>         <span class="dt">forest =</span> <span class="kw">predict</span>(house_forest_pre, <span class="dt">new_data =</span> house_<span class="dv">2018</span>)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb955-7"><a href="13-classification.html#cb955-7"></a><span class="st">           </span><span class="kw">pull</span>(.pred_class))</span></code></pre></div>
<p>Finally, let’s take a look at the accuracy of the three models:</p>
<div class="sourceCode" id="cb956"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb956-1"><a href="13-classification.html#cb956-1"></a><span class="kw">tibble</span>(<span class="dt">logistic =</span> <span class="kw">mean</span>(house_<span class="dv">2018</span>_preds<span class="op">$</span>logistic <span class="op">==</span><span class="st"> </span>house_<span class="dv">2018</span>_preds<span class="op">$</span>dem_win),</span>
<span id="cb956-2"><a href="13-classification.html#cb956-2"></a>       <span class="dt">tree =</span> <span class="kw">mean</span>(house_<span class="dv">2018</span>_preds<span class="op">$</span>tree <span class="op">==</span><span class="st"> </span>house_<span class="dv">2018</span>_preds<span class="op">$</span>dem_win),</span>
<span id="cb956-3"><a href="13-classification.html#cb956-3"></a>       <span class="dt">forest =</span> <span class="kw">mean</span>(house_<span class="dv">2018</span>_preds<span class="op">$</span>forest <span class="op">==</span><span class="st"> </span>house_<span class="dv">2018</span>_preds<span class="op">$</span>dem_win))</span></code></pre></div>
<pre><code># A tibble: 1 x 3
  logistic     tree   forest
     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
1 0.696552 0.655172 0.721839</code></pre>
<p>Here we see that when fitting the models on pre-2018 data and testing them on 2018 data, the random forest specification performs the best and the classification tree the worse, with the logistic regression in-between.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="20">
<li id="fn20"><p>It is rare that third party candidates mount serious bids in U.S. House elections, so it isn’t a much of an oversimplication to think of the variable as binary.<a href="13-classification.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>Indeed, it is almost identical to the coefficient you would obtain in a linear regression–try it out!<a href="13-classification.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p><a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2" class="uri">https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2</a><a href="13-classification.html#fnref22" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-machine-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
