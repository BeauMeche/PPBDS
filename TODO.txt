# Next meeting.

# Weekly Priorities

Feb 17: Fix sampling and confidence intervals chapters:

* Remove frequentist interpretation except for one footnote. Replace with Bayesian.

* Remove all use of infer package.

* Remove section 10.4.

* Change 10.6 to follow the simple resampling work-flow. 

In both chapters, there is a standard sequence of examples: First, show/understand a simple example. Second, use a rough copy/paset code approach to show three examples, but with a single variable (shovel size, confidence interval) changing, usually with three values. Third, replace the copy/paste code with a function. Solve the problems with copy/paste! But still have the problem that a) you are creating a lot of objects, all of which you need to keep track of and b) nothing ensure that an object called shovel_50 was actually constructed by setting the size argument to 50. Solve these two problems with list-columns and map_ functions. First, do so with just the same 3 values for your variable, directly replacing the three separate function calls and objects from the previous version. (This will generally be done by creating a tibble with 3 rows, adding a column for the varable and then calling your function to add a new list-column.) Second, do it with 100 values, using the result for a cool plot.

Mention what a hypothesis test is and why it is stupid. Maybe we do that in every chapter as well? *Amatuers test. Professionals summarize*.


Feb 24: Regression chapters in MD are excellent. Two minor points:

We don't want to use coef(). It is too old and fragile, even if it works OK in certain situations. Instead, use tidy() and pull out what you want.
 
Uncertainty in simple linear regressions. Go slower. Show the results of each step. Show what each column (mod, data, etc) looks like as it is added to the tibble.

Also, we need six kinds of major surgery.

First, we do not want to use any of the MD-specific functions, like get_regression_table() or the other get_* hacks. Students should only use widely-used functions. Fortunately, the get_* functions are just wrapper for broom functions, so we will need to use tidy(), glance() and augment() instead. This will require changes in the text which explains the output. 

Second, we need to use bootstrap resampling, just like we did in the confidence interval chapter. Make that connection explicitly. Then do it. rep_sample_n, followed by group_by(replication), followed by run a regression and store the model results in a list column.  Then gather up and show confidence intervals for intercept and regression cofficient. Surprise! Those ranges are just like those provided by lm(). So, all we need is to use lm(). Then, continue with all the current explanations for what lm() results mean, except that the chapter currently includes no inference! That is because, in MD, they do regression before they do confidence intervals. So, they don't get to inference for regression until later chapters. We, on the other hand, can do inference now.

library(infer)
library(broom)
library(tidyverse)
x <- mtcars %>% 
  select(mpg, disp) %>% 
  rep_sample_n(size = 32, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) %>% 
  nest() %>% 
  mutate(mod = map(data, ~ lm(mpg ~ disp, data = .x))) %>% 
  mutate(reg_results = map(mod, ~ tidy(.x))) %>% 
  mutate(disp_coef = map_dbl(reg_results, ~ slice(.x, 2) %>% pull(estimate)))
  
Not sure this is the best approach. And lots of explaining the steps needs to be done! But, x now has a column, disp_coef, which are 1,000 draws from the posterior distribution --- more or less! Then we can:

quantile(x$disp_coef, c(0.025, 0.5, 0.975))

And compare those numbers with the resut of:

summary(lm(mpg ~ disp, data = mtcars))

And see that the percentiles match --- more or less. The Bayesian interpretation of lm results is justified. And so we can just use lm going forward when we run regressions for each county or whatever.

Third, the text descriptions, in later chapters, are frequentist. For us, everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

Fourth, there is no discussion about the Rubin Causal Model. We want to explicitly discuss how a regression connects to RCM, assuming that we are estimating a causal effect of some type. And, if we are not estimating causal effects (i.e., all we care about is prediction), then the mechanics of lm are the same, but the interpretation of the regression coefficient has no causal implications.  We want a series of tables illustrating potential outcomes and our estmation of them. Start with a table with ?, just as in the Appendix. We use linear regression to fill in these questions marks. Show table with question marks and then show table with question marks filled in with best guess. Then show table with question marks filled in with a confidence interval for the mean and then for a distribution of predicted values. The closer we can tie RCM to the different parts of a regression, the better.

Fifth, the chapter has no discussion about hypothesis tests. That is good. Motto: No tests! There is only the data, and models we create from the data, and the decisions we make with those models. But we still need to explain what hypothesis tests are, and why we don't care about them. MD does that in their chapters 9 and 10, chapters which we have not included in our book for precisely that reason. So, explain what a test is, and why we think it is a waste of time to do them, and why people do them anyway. Key issue: If p = 0.04 really makes you do something totally different than p = 0.06, then either you (or the system within which you are operating) is stupid.

Sixth, each chapter should finish with a new section which uses list-columns plus broom to estimate scores of models, and then pull out interesting models. See the gapminder examples from https://r4ds.had.co.nz/many-models.html#gapminder. We need the full tool set: nest, unnest and so on.


March 1: 13-classification.Rmd has two major flaws. First, the logistic regression section comes from a different source than MD. Which is fine. But we want it to have the same look-and-feel as chapters 11 and 12, as if the MD authors had written it. We can use the same data/examples as currently (or we can change them). The style/wording should be as similar to chapters 11 and 12 as 11/12 are to each other. Each time we show how boostrap justifies the use of lm/glm and, therefore, allows our Bayesian interpretation. Each time, we end with an example which runs 100 models and then pulls out and graphs an interesting subset. We need the same 6th part surgery here.

March 8: The second flaw of 13-classification.Rmd is that the sections about CART and random forest are disconnected from logistic regression. (And they came from a different source.) We want the chapter as a whole to work together. There should be a brief intro to the chapter discussing all the different ways we can handle classification problems, and mentioning the three that we will be using. (Indeed, it might be nice to use the same data across the three approaches!) Then, at the end, there should be some discussion about how they connect to each other, and how they connect to other key themes: like causal inference versus prediction. Indeed, the entire book needs to be infused with discussions of this issue.

March 15: The machine learning chapter is a mess, derived mostly from DS. A much better book is: https://bradleyboehmke.github.io/HOML/. But that won't be open for another year. And, it is too advanced. And it does not quite use all the latest machine learning approaches in R. But the style is nice, and perhaps worth emulating. A related issue is tying this work back to the last three weeks. After all, in each of the last three weeks we fitted hundreds of models. Isn't that what machine learning is? Sort of! Big difference is that, previously, the separate models have shared no data with each other. With machine learning, they will! How can we teach this in a way which is closest to the approaches they have learned? Are parsnip and recipe the packages to use? Here are other useful sources:

https://dnield.com/posts/tidymodels-intro/
https://github.com/rstudio-conf-2020/applied-ml

Only three weeks left.

Week ?: What is the next set of stuff worth covering after students understand the Rubin Causal Model? Good question! I am not even sure what those topics are. Perhaps all the ways we deal with observational data. What sorts of material do other intro books cover? Here are some relevant concepts:

unit/item nonresponse
ignorability
treat potential outcomes as fixed (can we do this with regression?)
missing data
regresssion towards the mean
prediction/classification
map/network/text
natural experiments
Conditional random assignment
Difference in differences
Regression discontinuity



# Potential Projects

Meld the material on functions and purrr into the concepts in the Probability/Bayes Chapters. We are not changing, I think, many of the words in these chapters, most of which are quite good. Instead, we are using these topics as an excuse to provide more lessons in functions/purrr. Also, edit Bayes chapter to make sense after probability chapter. Consider adding in intuition from Bayes for Beginner Book, especially decision trees, which map nicely to our simulation approach. 

Brief appendix about Tufte and other graphics luminaries?

Brief appendix about Leamer?

Use tidybayes package for better graphics throughout?

When discussing map/network/text data, make sure to link to these blog posts and provide a paragraph of discussion.

Paul Revere [social network](https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/) 
Federalist Papers [authorship](https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/)
Lady Tasting Tea

=================

# Things for Me

Add discussion of allowed variable names, the use of ``, and janitor::clean_names() early in the book. This is needed as background before we use tidy::broom() and similar functions.

Edit Bayes chapter, especially the end.

Fix sampling/CI chapters to discuss hypothesis tests and why we hate them. Should do this in every chapter through the core of the book.

# Random Notes and Questions

* *Bayes Theorem: A Visual Introduction for Beginners* does a nice job of teaching Bayesian inference with decision trees. Works really well! And easy (?) to simulate. Maybe we do that? Fits naturally (?) into list-columns and map functions. 

Looks like we can use sections from R4DS as long as we use the entire section, unchanged.

https://r4ds.had.co.nz/index.html
https://creativecommons.org/faq/#can-i-reuse-an-excerpt-of-a-larger-work-that-is-licensed-with-the-noderivs-restriction

How do references work in MD chapters?

I like the PDF version of Rafa's book available from Lean Pub. How hard is that to make?

Replace DS Unix stuff with unix stuff from UNIX workbench?

We should not need library(moderndive) at all. (But need to check on any data used from there.)

Get rid of use of infer library. Do all this the hard way, via the bootstrap and specific calculation of the test statistic.



Lots of pandoc-citeproc errors.

Replace all uses of kable with gt?

Fix "No additional resources" in Chapter 6 and 7. Standardize this section across all chapters. 545 is different.

Requires internet when using read_mnist() in some DS chapters. Annoying! Fix by including copy of mnist? By getting rid of these sections?

Why isn't preview_chapter() working with MD chapters? Why can't I simply knit one of the MD chapters? Gives weird error message about "Error in files2[[format]] : 
  attempt to select less than one element in get1index"
  

Links in STAT 545 not working despite addition of links.md file. How fix?


Take (?) material from: https://chabefer.github.io/STCI/; https://github.com/chabefer/SKY


Or should each of these be separate chapters so that we might mix and match things? Maybe we need 100+ chapters, each of which do simple things, largely unconnected to each other.

Add cookie photo to front

Add 545 and DS data download code to chapter 5?



========= Large Projects

## R Packages

Square away R packages. There should be one location with all the requirements. Here is a listing of the R packages used my MD, from their index.Rmd:

CRAN packages needed: "nycflights13", "ggplot2movies", "fivethirtyeight", "gapminder", "ISLR",tidyverse", "rmarkdown", "knitr", "janitor", "skimr","infer", "moderndive", "webshot", "mvtnorm", "remotes", "devtools", "dygraphs", "gridExtra", "kableExtra", "scales", "viridis", "ggrepel", "patchwork",

But what good is this, given that other packages are loaded elsewhere? Is there some standard way of handling this, perhaps with a DESCRIPTION file? Main annoyance is that new contributors have to try to compile the book a dozen times before it will work.

==
## Set Up Script

Consider the use of before_chapter_script: "_common.R" in the DS _bookdown.yml as well as the associated _common.R file. Is this an approach we should copy? The lack of this why I can't get all the DS chapters to work.

Combine _common.R, common.R and index.Rmd information into one place. Need to figure out how this works in bookdown. I think we need one file which only runs once when you make the book. That files does a bunch of stuff involving copying over files. But you don't maintain state after running that file, so any new functions are lost. Then you have a second file, like _common.R, which is run at the start of compiling each chapter.

==
## Bibiography

Deal with bibliography. Our source books use very different approaches.

I like the way that MD writes out new versions of citations associated with R packages that have been updated.

Note that logistic regression chapter has a bunch of entries we need from BYSH.

==
## References and Footnotes

The book has lots of references, especially to other chapters. Many of these don't work because the referred-to chapters don't exist. We need a thorough clean up.

Some chapters, like 03-productivity.Rmd have a lot of footnotes. Good or bad?

Seems like all chapters generate references at the end. That is fine, but it should be standardized. Or do all those references belong at the end.

==
## Specific Chapters

04-wrangling is a mess, especially in the way that the join material from MD and from 545 do not go well together. Should some of it be moved to 05?

06-functions last section in map_ functions and list columns should be created. We don't need to understand everything about these concepts, just enough to do what we need in the next few chapters.

 
========= Thoughts

Revisit the Prediction Game. Love this:

“The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at 10 he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred.”

— Immanuel Kant, Critique of Pure Reason

Broadening Your Statistical Horizons is a very cleanly put together book. We should make our book look like that.

````markdown
`r ''````{r}
plot(cars)
```
````

Key concepts which need to be put everywhere:

decisions need models
potential outcomes and causal effects
units, treatments, outcomes
randomization is magic: assignment to estimate causal effects, bootstrap to estimate uncertainty

Describe, predict, infer. Description of things you can see, prediction for things you will see and inference about things you will never see.

Prediction checks.

Bias/Variance == Underfitting/Overfitting

No Tests! Null hypthosis testing is a mistake. There is only the data, the models and the summaries therefrom.

heterogenous treatment effects; interaction terms

(See [modelDown](https://github.com/MI2DataLab/modelDown).) *[Regression and Other Stories](http://www.stat.columbia.edu/~gelman/regression/)* provides several examples of how to create, and document your creation of, such a model, e.g., section 13.5 (gun control) and section X (wells in Bangladesh).


# Standards

Follow MD as much as possible.

* R terms and objects (anything you might type in the console) in backticks.

* Functions names always include ().

* Package names are **bolded**.



# Summer

Should we move the whole thing to Netifly and use Github Actions to deploy? https://www.hvitfeldt.me/blog/bookdown-netlify-github-actions/

Should make Productivity chapter more connected to the lectures on Days 3 and 4. Cut material which is not directly relevant. Introduce the material in the same way that I introduce it in class. Someone who reads the chapter should find the lecture a simple repeat of what they have already seen.


Add more fancy plotting details to chapter 2. Key is to include all the necessary steps toward making the 538 plot. Discussion of the key parts of a graphic, especially title, subtitle and sources. themes(), starting with theme_minimal() but going on to cool stuff like fivethirtyeight and TV themes. ggtext. axis ajustments. Links to relevant readings in Healy. Also add a section on how to put a graphic in Rpubs. And also saving graphics.

Use almost every single one of these illustrations: https://github.com/allisonhorst/stats-illustrations

Lists show up in Chapter 4 without any previous discussion. We need a short intro to the major variable types before this. Also put characters ahead of factors. 

Does chapter 4 belong ahead of chapter 3? 

Should Productivity come right after Chapter 1? Should Chapter 1 have more fun stuff in it? That is, you make a fun plot or two at the end of Chapter 1, so you are motivated to get set up correctly, then you do the hard work of doing so in the Productivity chapter. Then the real work begins.

Chapter 2 should have faceting and other graphics fun.

https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.


https://committedtotape.shinyapps.io/freeR/

Clean up and organize images/ directory.

What is our plan for loading libraries and removing them when they are no longer need? Chapter 11 contained an annoying bug. Bug was that rvest and purrr both have pluck() as a function. Need to ensure that you get the purrr version if you need it. Bug only showed up when Chapter 5 (with rvest loaded) was used in the build.

