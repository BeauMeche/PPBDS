

# Summer 

The world confronts us. We must make decisions. The data we want is missing. So, we make models, with uncertainty, and then decide. 


Wardrop suggests doing dichotomous variables first, so as to fix all the important ideas.

## Other Topics

I am not even sure what those topics are. Perhaps all the ways we deal with observational data. What sorts of material do other intro books cover? Here are some relevant concepts:

unit/item nonresponse
ignorability
treat potential outcomes as fixed (can we do this with regression?)
missing data
regresssion towards the mean
prediction/classification
map/network/text
natural experiments
Conditional random assignment
Difference in differences
Regression discontinuity

other key themes: like causal inference versus prediction. Indeed, the entire book needs to be infused with discussions of this issue. Spend a week on that?




=================

# Things for Me

Add discussion of allowed variable names, the use of ``, and janitor::clean_names() early in the book. skimr() also. This is needed as background before we use tidy::broom() and similar functions.

Only use high quality packages, and recommend the same to readers.

Fix sampling/CI chapters to discuss hypothesis tests and why we hate them. Should do this in every chapter through the core of the book.

Use tidybayes package for better graphics throughout?

# Random Notes and Questions


How do references work in MD chapters?


Replace DS Unix stuff with unix stuff from UNIX workbench? Perhaps we introduce the **fs** package at the same time, showing how you can accomplish similar things from the terminal and the R console.

We should not need library(moderndive) at all. (But need to check on any data used from there.)


Get rid of use of infer library. Do all this the hard way, via the bootstrap and specific calculation of the test statistic. Example: https://juliasilge.com/blog/beer-production/

Lots of pandoc-citeproc errors.

Replace all uses of kable with gt.

Fix "No additional resources" in Chapter 6 and 7. Standardize this section across all chapters. 545 is different.


Why isn't preview_chapter() working with MD chapters? Why can't I simply knit one of the MD chapters? Gives weird error message about "Error in files2[[format]] : 
  attempt to select less than one element in get1index"
  

Links in STAT 545 not working despite addition of links.md file. How fix?


Take (?) material from: https://chabefer.github.io/STCI/; https://github.com/chabefer/SKY


Or should each of these be separate chapters so that we might mix and match things? Maybe we need 100+ chapters, each of which do simple things, largely unconnected to each other, or with explicit prerequisites. Maybe call these "sections" or "vignettes" or "lessons". Maybe each comes with a little video? And a learnr tutorial? Then, our chapters could be combinations of them? 

Too early to start thinking about two versions: 1005 and 1006, with 1006 as extra advanced sections in each chapter? Or maybe just astericked chapters for advanced material, not required for 1005.

Add 545 and DS data download code to chapter 5?



========= Large Projects

## R Packages

Square away R packages. There should be one location with all the requirements. Here is a listing of the R packages used my MD, from their index.Rmd:

CRAN packages needed: "nycflights13", "ggplot2movies", "fivethirtyeight", "gapminder", "ISLR",tidyverse", "rmarkdown", "knitr", "janitor", "skimr","infer", "moderndive", "webshot", "mvtnorm", "remotes", "devtools", "dygraphs", "gridExtra", "kableExtra", "scales", "viridis", "ggrepel", "patchwork",

But what good is this, given that other packages are loaded elsewhere? Is there some standard way of handling this, perhaps with a DESCRIPTION file? Main annoyance is that new contributors have to try to compile the book a dozen times before it will work.

==
## Set Up Script

Consider the use of before_chapter_script: "_common.R" in the DS _bookdown.yml as well as the associated _common.R file. Is this an approach we should copy? The lack of this why I can't get all the DS chapters to work.

Broadening Your Statistical Horizons is a very cleanly put together book. We should make our book look like that. Also HOML (https://bradleyboehmke.github.io/HOML/) is very cleanly and simply constructed. Use that as a base structure?

Combine _common.R, common.R and index.Rmd information into one place. Need to figure out how this works in bookdown. I think we need one file which only runs once when you make the book. That files does a bunch of stuff involving copying over files. But you don't maintain state after running that file, so any new functions are lost. Then you have a second file, like _common.R, which is run at the start of compiling each chapter.

==
## Bibiography

Deal with bibliography. Our source books use very different approaches. write_bib() is infamous for making trouble.

I like the way that MD writes out new versions of citations associated with R packages that have been updated.

==
## References and Footnotes

The book has lots of references, especially to other chapters. Many of these don't work because the referred-to chapters don't exist. We need a thorough clean up.

Some chapters, like 03-productivity.Rmd have a lot of footnotes. Good or bad?

Seems like all chapters generate references at the end. That is fine, but it should be standardized. Or do all those references belong at the end.

==
## Specific Chapters

04-wrangling is a mess, especially in the way that the join material from MD and from 545 do not go well together. Should some of it be moved to 05?

Need to introduce lists early, But when?

========= Thoughts

Revisit the Prediction Game. Love this:

“The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at 10 he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred.”

— Immanuel Kant, Critique of Pure Reason



````markdown
`r ''````{r}
plot(cars)
```
````

Key concepts which need to be put everywhere:

decisions need models
potential outcomes and causal effects
units, treatments, outcomes
randomization is magic: assignment to estimate causal effects, bootstrap to estimate uncertainty

Describe, predict, infer. Description of things you can see, prediction for things you will see and inference about things you will never see.

Prediction checks.

Bias/Variance == Underfitting/Overfitting

No Tests! Null hypthosis testing is a mistake. There is only the data, the models and the summaries therefrom

Describe an hypothesis test each chapter, and then dismiss it.

heterogenous treatment effects; interaction terms

(See [modelDown](https://github.com/MI2DataLab/modelDown).) *[Regression and Other Stories](http://www.stat.columbia.edu/~gelman/regression/)* provides several examples of how to create, and document your creation of, such a model, e.g., section 13.5 (gun control) and section X (wells in Bangladesh).


# Standards

Follow MD as much as possible.

* R terms and objects (anything you might type in the console) in backticks.

* Functions names always include ().

* Package names are **bolded**.

# Summer

## Themes to confirm over the summer

Drop all the frequentist nonsense except for a footnote at the first use with a confidence interval and an appendix which walks students through the Kuske arguments.

Chapters from sampling through machine learning should be parallel. To the greatest extent possible, they all do all these things.

Show the results of each step. Show what each column (mod, data, etc) looks like as it is added to the tibble.

Everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potentional outcomes to define precisely what "true value" you are talking about. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

Discuss RCM, assuming that we are estimating a causal effect of some type. And, if we are not estimating causal effects (i.e., all we care about is prediction), then the mechanics of lm are the same, but the interpretation of the regression coefficient has no causal implications.  We want a series of tables illustrating potential outcomes and our estmation of them. Start with a table with ?, just as in the Appendix. We use linear regression to fill in these questions marks. Show table with question marks and then show table with question marks filled in with best guess. Then show table with question marks filled in with a confidence interval for the mean and then for a distribution of predicted values. The closer we can tie RCM to the different parts of a regression, the better. But do this each chapter, not just regression.

Avoid discussion about hypothesis tests, except to dismiss them. But maybe we discuss and then dismiss them each chapter. Motto: No tests! There is only the data, and models we create from the data, and the decisions we make with those models. But we still need to explain what hypothesis tests are, and why we don't care about them. Explain what a test is, and why we think it is a waste of time to do them, and why people do them anyway. Key issue: If p = 0.04 really makes you do something totally different than p = 0.06, then either you (or the system within which you are operating) is stupid.

Each chapter should finish with a new section which uses list-columns plus broom to estimate scores of models, and then pull out interesting models. See the gapminder examples from https://r4ds.had.co.nz/many-models.html#gapminder. We need the full tool set: nest, unnest and so on.


## Other summer issues

Should we move the whole thing to Netifly and use Github Actions to deploy? https://www.hvitfeldt.me/blog/bookdown-netlify-github-actions/ Yes!


Lists show up in Chapter 4 without any previous discussion. We need a short intro to the major variable types before this. Also put characters ahead of factors. 



Chapter 2 should have faceting and other graphics fun.

https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.


https://committedtotape.shinyapps.io/freeR/

Clean up and organize images/ directory.

What is our plan for loading libraries and removing them when they are no longer need? Chapter 11 contained an annoying bug: rvest and purrr both have pluck() as a function. Need to ensure that you get the purrr version if you need it. Bug only showed up when Chapter 5 (with rvest loaded) was used in the build.



Figure out links in Chapter 4 and elsewhere . . .

Replace all photos of activities with photos from Harvard, using Tsai.

## Packages

When are different packages introduced? Where is this written down? Need to be intentional about what we introduced outside of the tidyverse --- janitor, ggthemes, reprex, fs, skimr, gt, googlesheets4, infer, gtsummary --- and when we do it.


## Other Stuff

Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this!

There should be on causal and one non-causal example in each chapter. Perhaps use trains throughout? Create a logistic regression example in which the dependent variable is moved X units more conservative, or did not.



This textbook looks interesting, and the guy has some cool graphics, but I don't think he is open sourcing it: http://nickchk.com/chapter5.pdf

googlesheet4 examples. Perhaps there is an Appendix with cool packages and a brief introduction?



Standard number formating. 2400 or 2,400 or $2400$ or . . . .

## Every Chapter from Sampling Forward

Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call?

Or maybe there are always two decisions: one forecasting and one causal.

Include discussion of RCM and potential outcomes, both in the context of this statistical technique and in the context of the empirical example.

Include a NHST and then a discussion of why NHST is stupid and full data/models are best for decision making.

Play the prediction game. That, perhaps, provides a useful framework for why NHST is stupid. Or, rather, you play the prediction game to figure out which statistical procedures are best --- and or, how well procedure X works --- and then use that information to make a decision.

Is there a way to print out messages during the book building process so that you know where the process is?

There is a lot of garbage in this project. Do we really need all the random files in data/ and rds/? Probably not. Figure out which we need and delete the rest. Perhaps combine them all in one directory? And try to do more calculations on the fly.

Is there really no discussion of p-values? There should be! And what about permutation tests? Maybe that is a useful early statistical programing exercise. Or maybe it would make a nice appendix, a second read after the Rubin Causal Model appendix.

Every model has a table highlighting what God would know, what we know, and then how we form a pdf about the truth. This is even so with sampling! I can think of two ways to thinking about this. One is with each row meaning a bead, and there are two potential outcomes, red and white. Until we pick that bead, both our "?". When we sample that bead, one column becomes TRUE and one becomes FALSE. And then we make inferences about the bead we can't see. Or, second way, it is a table with one column. (I think this is better.) Until we sample a bead, it is a "?". Then, it resolves to red/white. After drawing our sample, we make inferences about the other beads. In one sense, we are infering the value of p. In another, we are guessing what each of the "?" is, at least in expectation. But we can then also capture uncertainty! Each bead might be red or white, regardless of p. And there is uncertainty about p as well. This is the first time we are seeing both model uncertainty --- what is p? --- and predictive uncertainty --- what color is the next bead?

Also interesting to think about tables which we know are finite but we don't ever know how many rows, like number of living people in US right now (i.e., includes planes landing? someone whose heart has stopped beating but has not been "pronounced" dead?) 

Maybe we should restructure? Instead of thinking in terms of a chapter devoted to a specific model, like the regression chapters, we should instead think about chapters devoted to a problem. One problem is a right hand side variable that is continuous. One other problem is a right hand side variable that is 0/1. Of course, those chapters are too big! So, how to break them down? Maybe a chapter is defined as right hand size variable type (like continuous or 0/1) crossed with other factors. One such factor is left hand side available varibles: one continuous, one discrete, multiple and multiple with interactions. That would be eight chapters right there! Another factor might be modeling for causation or prediction. Hmm. But, lots of times, models are the same.

Think in terms of replacing our current 4 chapters from last third of the course. Ought to discuss causal versus predictive each time. Ought to discuss RCM each time. Ought to discuss over-fitting versus under-fitting each time. Ought to think about in-sample and out-of-sample. Ought to think in terms of decisions, and utilities each time. Those are the central themes, and should be repeated over-and-over again. We could use tidymodels syntax each time. Each model would have both a many-small-models example and a machine learning example. Predicted (or fitted) values and residuals (or errors). 

So, chap 1 continuous variables with one continuous covariate. Two models: lm and loess. chap 2 dichotomous variable with discrete convariate (first two categories, then several) and then also continuous. Two models: logistic and CART. chap 3 continuous variable with several covariates, both continuous and discrete. Two models: lm and ?. chap 4 other stuff, now that you understand all the key examples. Maybe federalist or social networks?

Or maybe: 1) continuous outcome, one covariate (lm and loess); 2) continuous outcome, multiple covariates (lm and neural network); 3) binary outcome, multiple covariates (glm and random forest), 4) multiple outcomes (like mnst?), multiple covariates (glm and deep learning). Concept of "machine learning" is used throughout, including tidymodels nomenclature. (Or should we do a time series chapter?)

Each chapter, at least in the last four, should only have one dataset. The cognitive load of switching is to high.

Put example in map appendix from Blackwell's slavery work.


Chapters 7, 8 and 9 use the bootstrap for two reasons. First, to build intution and provide justification for functions like lm. Second, to solve problems which can't be solved by standard functions. 

Machine learning in 10, 11 and 12.

Each chapter begins with a real example, a decision we must make (if only the prediction game), and then creates a model which we will use to help us. Bets are always offered. RCM discussed. Bias scenarios demonstrated. The secret weapon --- a model for each state or each year --- used.  



The problem we are trying to solve, not the tool we are using to solve it.

http://statprep.org/


