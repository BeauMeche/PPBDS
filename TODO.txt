

# Summer 

The world confronts us. We must make decisions. The data we want is missing. So, we make models, with uncertainty, and then decide. 

## Other Topics

I am not even sure what those topics are. Perhaps all the ways we deal with observational data. What sorts of material do other intro books cover? Here are some relevant concepts:

unit/item nonresponse
ignorability
treat potential outcomes as fixed (can we do this with regression?)
missing data
regresssion towards the mean
prediction/classification
map/network/text
natural experiments
Conditional random assignment
Difference in differences
Regression discontinuity

other key themes: like causal inference versus prediction. Indeed, the entire book needs to be infused with discussions of this issue. Spend a week on that?


# Summer People

Miro Bergram

Kayla Manning, Cameron Reaves, Yao Yu, 









=================

# Things for Me

Add discussion of allowed variable names, the use of ``, and janitor::clean_names() early in the book. skimr() also. This is needed as background before we use tidy::broom() and similar functions.

Only use high quality packages, and recommend the same to readers.

12 chapters only. Productivity goes to an Appendix; Bayes/Probability combined.

Fix sampling/CI chapters to discuss hypothesis tests and why we hate them. Should do this in every chapter through the core of the book.

Use tidybayes package for better graphics throughout?

# Random Notes and Questions


How do references work in MD chapters?


Replace DS Unix stuff with unix stuff from UNIX workbench? Perhaps we introduce the **fs** package at the same time, showing how you can accomplish similar things from the terminal and the R console.

We should not need library(moderndive) at all. (But need to check on any data used from there.)


Get rid of use of infer library. Do all this the hard way, via the bootstrap and specific calculation of the test statistic. Example: https://juliasilge.com/blog/beer-production/

Lots of pandoc-citeproc errors.

Replace all uses of kable with gt.

Fix "No additional resources" in Chapter 6 and 7. Standardize this section across all chapters. 545 is different.


Why isn't preview_chapter() working with MD chapters? Why can't I simply knit one of the MD chapters? Gives weird error message about "Error in files2[[format]] : 
  attempt to select less than one element in get1index"
  

Links in STAT 545 not working despite addition of links.md file. How fix?


Take (?) material from: https://chabefer.github.io/STCI/; https://github.com/chabefer/SKY


Or should each of these be separate chapters so that we might mix and match things? Maybe we need 100+ chapters, each of which do simple things, largely unconnected to each other, or with explicit prerequisites. Maybe call these "sections" or "vignettes" or "lessons". Maybe each comes with a little video? And a learnr tutorial? Then, our chapters could be combinations of them? 

Too early to start thinking about two versions: 1005 and 1006, with 1006 as extra advanced sections in each chapter? Or maybe just astericked chapters for advanced material, not required for 1005.

Add 545 and DS data download code to chapter 5?



========= Large Projects

## R Packages

Square away R packages. There should be one location with all the requirements. Here is a listing of the R packages used my MD, from their index.Rmd:

CRAN packages needed: "nycflights13", "ggplot2movies", "fivethirtyeight", "gapminder", "ISLR",tidyverse", "rmarkdown", "knitr", "janitor", "skimr","infer", "moderndive", "webshot", "mvtnorm", "remotes", "devtools", "dygraphs", "gridExtra", "kableExtra", "scales", "viridis", "ggrepel", "patchwork",

But what good is this, given that other packages are loaded elsewhere? Is there some standard way of handling this, perhaps with a DESCRIPTION file? Main annoyance is that new contributors have to try to compile the book a dozen times before it will work.

==
## Set Up Script

Consider the use of before_chapter_script: "_common.R" in the DS _bookdown.yml as well as the associated _common.R file. Is this an approach we should copy? The lack of this why I can't get all the DS chapters to work.

Broadening Your Statistical Horizons is a very cleanly put together book. We should make our book look like that. Also HOML (https://bradleyboehmke.github.io/HOML/) is very cleanly and simply constructed. Use that as a base structure?

Combine _common.R, common.R and index.Rmd information into one place. Need to figure out how this works in bookdown. I think we need one file which only runs once when you make the book. That files does a bunch of stuff involving copying over files. But you don't maintain state after running that file, so any new functions are lost. Then you have a second file, like _common.R, which is run at the start of compiling each chapter.

==
## Bibiography

Deal with bibliography. Our source books use very different approaches. write_bib() is infamous for making trouble.

I like the way that MD writes out new versions of citations associated with R packages that have been updated.

==
## References and Footnotes

The book has lots of references, especially to other chapters. Many of these don't work because the referred-to chapters don't exist. We need a thorough clean up.

Some chapters, like 03-productivity.Rmd have a lot of footnotes. Good or bad?

Seems like all chapters generate references at the end. That is fine, but it should be standardized. Or do all those references belong at the end.

==
## Specific Chapters

04-wrangling is a mess, especially in the way that the join material from MD and from 545 do not go well together. Should some of it be moved to 05?

Need to introduce lists early, But when?

========= Thoughts

Revisit the Prediction Game. Love this:

“The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at 10 he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred.”

— Immanuel Kant, Critique of Pure Reason



````markdown
`r ''````{r}
plot(cars)
```
````

Key concepts which need to be put everywhere:

decisions need models
potential outcomes and causal effects
units, treatments, outcomes
randomization is magic: assignment to estimate causal effects, bootstrap to estimate uncertainty

Describe, predict, infer. Description of things you can see, prediction for things you will see and inference about things you will never see.

Prediction checks.

Bias/Variance == Underfitting/Overfitting

No Tests! Null hypthosis testing is a mistake. There is only the data, the models and the summaries therefrom

Describe an hypothesis test each chapter, and then dismiss it.

heterogenous treatment effects; interaction terms

(See [modelDown](https://github.com/MI2DataLab/modelDown).) *[Regression and Other Stories](http://www.stat.columbia.edu/~gelman/regression/)* provides several examples of how to create, and document your creation of, such a model, e.g., section 13.5 (gun control) and section X (wells in Bangladesh).


# Standards

Follow MD as much as possible.

* R terms and objects (anything you might type in the console) in backticks.

* Functions names always include ().

* Package names are **bolded**.

# Summer

## Themes to confirm over the summer

Drop all the frequentist nonsense except for a footnote at the first use with a confidence interval and an appendix which walks students through the Kuske arguments.

Chapters from sampling through machine learning should be parallel. To the greatest extent possible, they all do all these things.

Show the results of each step. Show what each column (mod, data, etc) looks like as it is added to the tibble.

Everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potentional outcomes to define precisely what "true value" you are talking about. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

Discuss RCM, assuming that we are estimating a causal effect of some type. And, if we are not estimating causal effects (i.e., all we care about is prediction), then the mechanics of lm are the same, but the interpretation of the regression coefficient has no causal implications.  We want a series of tables illustrating potential outcomes and our estmation of them. Start with a table with ?, just as in the Appendix. We use linear regression to fill in these questions marks. Show table with question marks and then show table with question marks filled in with best guess. Then show table with question marks filled in with a confidence interval for the mean and then for a distribution of predicted values. The closer we can tie RCM to the different parts of a regression, the better. But do this each chapter, not just regression.

Avoid discussion about hypothesis tests, except to dismiss them. But maybe we discuss and then dismiss them each chapter. Motto: No tests! There is only the data, and models we create from the data, and the decisions we make with those models. But we still need to explain what hypothesis tests are, and why we don't care about them. Explain what a test is, and why we think it is a waste of time to do them, and why people do them anyway. Key issue: If p = 0.04 really makes you do something totally different than p = 0.06, then either you (or the system within which you are operating) is stupid.

Each chapter should finish with a new section which uses list-columns plus broom to estimate scores of models, and then pull out interesting models. See the gapminder examples from https://r4ds.had.co.nz/many-models.html#gapminder. We need the full tool set: nest, unnest and so on.


## Other summer issues

Should we move the whole thing to Netifly and use Github Actions to deploy? https://www.hvitfeldt.me/blog/bookdown-netlify-github-actions/ Yes!


Lists show up in Chapter 4 without any previous discussion. We need a short intro to the major variable types before this. Also put characters ahead of factors. 



Chapter 2 should have faceting and other graphics fun.

https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.


https://committedtotape.shinyapps.io/freeR/

Clean up and organize images/ directory.

What is our plan for loading libraries and removing them when they are no longer need? Chapter 11 contained an annoying bug: rvest and purrr both have pluck() as a function. Need to ensure that you get the purrr version if you need it. Bug only showed up when Chapter 5 (with rvest loaded) was used in the build.



Figure out links in Chapter 4 and elsewhere . . .

Replace all photos of activities with photos from Harvard, using Tsai.

## Packages

When are different packages introduced? Where is this written down? Need to be intentional about what we introduced outside of the tidyverse --- janitor, ggthemes, reprex, fs, skimr, gt, googlesheets4, infer, gtsummary --- and when we do it.


## Other Stuff

Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this!

There should be on causal and one non-causal example in each chapter. Perhaps use trains throughout? Create a logistic regression example in which the dependent variable is moved X units more conservative, or did not.



This textbook looks interesting, and the guy has some cool graphics, but I don't think he is open sourcing it: http://nickchk.com/chapter5.pdf

googlesheet4 examples. Perhaps there is an Appendix with cool packages and a brief introduction?



Standard number formating. 2400 or 2,400 or $2400$ or . . . .

## Every Chapter from Sampling Forward

Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call?

Or maybe there are always two decisions: one forecasting and one causal.

Include discussion of RCM and potential outcomes, both in the context of this statistical technique and in the context of the empirical example.

Include a NHST and then a discussion of why NHST is stupid and full data/models are best for decision making.

Play the prediction game. That, perhaps, provides a useful framework for why NHST is stupid. Or, rather, you play the prediction game to figure out which statistical procedures are best --- and or, how well procedure X works --- and then use that information to make a decision.

Is there a way to print out messages during the book building process so that you know where the process is?

There is a lot of garbage in this project. Do we really need all the random files in data/ and rds/? Probably not. Figure out which we need and delete the rest. Perhaps combine them all in one directory? And try to do more calculations on the fly.

Is there really no discussion of p-values? There should be! And what about permutation tests? Maybe that is a useful early statistical programing exercise. Or maybe it would make a nice appendix, a second read after the Rubin Causal Model appendix.

Every model has a table highlighting what God would know, what we know, and then how we form a pdf about the truth. This is even so with sampling! I can think of two ways to thinking about this. One is with each row meaning a bead, and there are two potential outcomes, red and white. Until we pick that bead, both our "?". When we sample that bead, one column becomes TRUE and one becomes FALSE. And then we make inferences about the bead we can't see. Or, second way, it is a table with one column. (I think this is better.) Until we sample a bead, it is a "?". Then, it resolves to red/white. After drawing our sample, we make inferences about the other beads. In one sense, we are infering the value of p. In another, we are guessing what each of the "?" is, at least in expectation. But we can then also capture uncertainty! Each bead might be red or white, regardless of p. And there is uncertainty about p as well. This is the first time we are seeing both model uncertainty --- what is p? --- and predictive uncertainty --- what color is the next bead?

Also interesting to think about tables which we know are finite but we don't ever know how many rows, like number of living people in US right now (i.e., includes planes landing? someone whose heart has stopped beating but has not been "pronounced" dead?) 

Maybe we should restructure? Instead of thinking in terms of a chapter devoted to a specific model, like the regression chapters, we should instead think about chapters devoted to a problem. One problem is a right hand side variable that is continuous. One other problem is a right hand side variable that is 0/1. Of course, those chapters are too big! So, how to break them down? Maybe a chapter is defined as right hand size variable type (like continuous or 0/1) crossed with other factors. One such factor is left hand side available varibles: one continuous, one discrete, multiple and multiple with interactions. That would be eight chapters right there! Another factor might be modeling for causation or prediction. Hmm. But, lots of times, models are the same.

Think in terms of replacing our current 4 chapters from last third of the course. Ought to discuss causal versus predictive each time. Ought to discuss RCM each time. Ought to discuss over-fitting versus under-fitting each time. Ought to think about in-sample and out-of-sample. Ought to think in terms of decisions, and utilities each time. Those are the central themes, and should be repeated over-and-over again. We could use tidymodels syntax each time. Each model would have both a many-small-models example and a machine learning example. Predicted (or fitted) values and residuals (or errors). 

So, chap 1 continuous variables with one continuous covariate. Two models: lm and loess. chap 2 dichotomous variable with discrete convariate (first two categories, then several) and then also continuous. Two models: logistic and CART. chap 3 continuous variable with several covariates, both continuous and discrete. Two models: lm and ?. chap 4 other stuff, now that you understand all the key examples. Maybe federalist or social networks?

Or maybe: 1) continuous outcome, one covariate (lm and loess); 2) continuous outcome, multiple covariates (lm and neural network); 3) binary outcome, multiple covariates (glm and random forest), 4) multiple outcomes (like mnst?), multiple covariates (glm and deep learning). Concept of "machine learning" is used throughout, including tidymodels nomenclature. (Or should we do a time series chapter?)

Each chapter, at least in the last four, should only have one dataset. The cognitive load of switching is to high.

Put example in map appendix from Blackwell's slavery work.


Chapters 7, 8 and 9 use the bootstrap for two reasons. First, to build intution and provide justification for functions like lm. Second, to solve problems which can't be solved by standard functions. 

Machine learning in 10, 11 and 12.

Each chapter begins with a real example, a decision we must make (if only the prediction game), and then creates a model which we will use to help us. Bets are always offered. RCM discussed. Bias scenarios demonstrated. The secret weapon --- a model for each state or each year --- used.  



The problem we are trying to solve, not the tool we are using to solve it.

# PPBDS, Version 2

## 1. Visualization (Productivity)

Include the current chapter 1 and 2. Lots of prettiness. Must give enough detail that you can do Congressional age.

Discussion of the key parts of a graphic, especially title, subtitle and sources. themes(), starting with theme_minimal() but going on to cool stuff like fivethirtyeight and TV themes. ggtext. axis ajustments. Links to relevant readings in Healy. Also add a section on how to put a graphic in Rpubs. And also saving graphics.

## 2. Tidyverse (Getting Help)

ifelse (and other similar commands?) belongs earlier in the book, like when we first use mutate(). What else might be introduced with the initial usages of mutate?

## 3. Wrangling (Full non-probabilistic RCM)

## 4. Functions (Shiny and Maps)

Maybe the functions chapter should introduce map functions independent of list-columns. For example, if you want to simulate dice throws in a tibble, you need (?) map function to enure that you get a different value for each row.

##  5. Probability 

Central idea: Teach probability solely on the basis of branching trees which eventually lead to outcomes from your sample set. Advantages: Easy to understand. Easy to set up simulation experiments. Encourages programming. Connects directly to Bayesian view, see McRealth book/lectures. Bayesian counting.

The notes below are often about class, but that should inform the chapter as well. Indeed, class should always look as much like the class as possible.

"grids," as used clearly in Chapter 1 of Thinking Bayes, are a good way of introducing Bayesian analysis. And a good programming exercise?

The bayesian_scratch example of the Downey cookie problem works very well. Students need some hints, however, about how to set things up. Most of them wanted to actually build the bowls, as I guess we did in a problem set. They did not immediately see that a simple tibble with well-chosen variables does the job just as well. If we were cool, we would do a cookie theme, and have a student pick a cookie out of the bowl.

It can take time, however. Probably a full class, even if you only do the cookie example. Don't use tabyl. Ought to make them use pivot_wider, calculate column sums, and make into a percentage, before showing the janitor. Almost certainly a good idea to end with putting a prior on the two bowls, and thereby understanding the Bayesian workflow from prior, to data, to posterior. (Is that right? Is the prior the number of rows?)

This all sets you up for the use of a more complex example in problem set #5.

The Albert article has a nice discussion of Bayes boxes and Bayes scatterplots. I think both make sense for in-class exercises this week. Perhaps pass out Bayes box handouts, with the boxes created but no numbers. Students fill them out together. (Need to tie this to Downey reading.) Then, show examples of Bayes scatterplots. Have students write code to create their own. Use purrr?

In code, they do the Bayesian box thing with 3 possible values for p. Then, change the code so we have 11 values. Then 1001, then 100,001. Still ought to be fast. And, each time, ask some standard questions that frequentist can't answer. What is likelihood of p > 0.4. What about p between 0.45 and 0.55? What is the middle 50% confidence interval for p? Answers don't change much as we extend the model, but they get more precise. And then wave hands for the continuous case.

The Rossman article has nice dice and coin examples. Perhaps use those in lecture?

Do a dice example where students each have their own priors and then we update after rolling dice, some fake or real. Do it with counts, not proportions.

Do contest where we start with a few poll responses, estimate some proportions, then get a few more responses, update our estimates, and so on. Each stage is, potentially, a new contest. See which approach wins the most contests. In the end, of course, they converge.

Can't just be "Repub ahead" as H_1. Need to be "Repub = 0.6" Without this hack, can't calculate the likelihood easily. Right?

Might Bayesian power discussion, as in Kruscke article be a useful problem set? Ties into the usage of fake data. Good way to introduce/teach type M and S errors?

Chapters 15 and 16 of the Bayes workbook has some interesting stuff. See page 330 for the use of the frequency format. Don't think in terms of percentages. Think in terms of counts.

       1000
      10   990
     8  2  95 895
     8/(8 + 95)

First, look at competing models. Who is ahead, D or R?
Second, add another model. D or R or tied?
Third, what is D percentage of support?


Show updating as each vote comes in. Then show that you get the same answer if you just include all the votes at once.

Start pair programming on Tuesday of week 5. Forces weaker students to really participate. Forces stronger students to help their peers. Use a single repo shared by two students, including one inviting the other. Show a merge conflict.

What fun examples can we do with Bayes? What prediction games? James-Stein estimators?

Assuming this is correct, we get to bring in prediction and betting. Then, we have the motivating question: What is a good estimate for the percentage of Democrats in this bucket? How do we combine information from the overall population and from our sample to come up with a good estimate, and confidence interval, for the percentage Democratic in that bucket? Perhaps this multi-level model is one of the last things we do. Even Mr P??

Want to be able to do this with just counting. No math. Albert's boxes seems sensible.

Are there physical set ups which would show bias? Make the red balls bigger? Make them fall to the bottom of the bowl? Maybe add the candy weighing estimate from Nolan/Gelman.

Motivating problem: Two buckets, each with different pair of die in them: one normal, one weird. You pick a bucket, then roll the die. Q: What are the odds of bucket 1 versus bucket 2? Start with dice function. Then build tibble as in the cookie problem. That takes a day, especially if we build the result by-hand, meaning pivot_wider and summing columns, as we should. Then, day 2, place a prior on the two buckets. Then, what if we through the dice more than once? This will be a list column. Scary! But all the other arguments go through. (I think!) Then, discuss continuous example of a coin, setting the stage for problem set question.

Change the ordering next year. First week will be Bayes. Perhaps we can do all the examples without list-columns. But we can get use to building things step-by-step, with the first step often being tibble(replication = 1:100). Or maybe, with Bayes, the first step is even simpler, building a Bayes box by hand. Then, we get a week of practice with this before we need to add purrr complications. Is there a way that Bayes boxes lead naturally to sampling and then to the bootstrap?


## 6. Sampling

Need to incorporate some notion of betting. The over/under line for percentage of red balls in the bowl. Can you show (maybe a problem set?) that the mean is the best place to put the over/under line? Play the prediction game. 

Can we incorporate some notion of uncertainty, even before we see the bootstrap in the next chapter? And, along with that notion of uncertainty, discuss a Bayesian interpretation.

Without what range would you offer 50/50 odds that the true percentage lies?



## 7. One Parameter (similar to current chapter, but also estimate median, 3rd biggest, IQR)
## 8. Two Parameter (two means, two medians, ratio of medians, difference in means)
## 9. Continuous left, discrete right (another way to do difference in means)
## 10. Continuous left, continuous right (regression and loess and other option)
## 11. Continuous left, multivariate (regression and neural network and randomforest)
## 12. Discrete left, multivariate (logistic and cart and deep learning)

## Productivity

Should make Productivity chapter more connected to the lectures on Days 3 and 4. Cut material which is not directly relevant. Introduce the material in the same way that I introduce it in class. Someone who reads the chapter should find the lecture a simple repeat of what they have already seen.

Productivity becomes an appendix in week 1. There is so much irrelevant material (like Windows tips for Mac users). Also, it could be made much shorter, with pointers to good info, like Happy Git for the UseR. 

## Getting Help

Split out this material to separate appendix

## Maps


## Shiny

## Animation

# More Appendices


Appedices have information that either a) a prof might reasonably decide not to assign or b) often contain material that students already know.

* Why Bayes?

* Brief appendix about Tufte and other graphics luminaries?

* Brief appendix about Leamer?

* Regression to the mean

* Garden of forking paths

* Type M and S errors.

* On permutation tests using Lady Tasting Tea

* Paul Revere [social network](https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/) 

* Federalist Papers [authorship](https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/)






