

# Summer 

The world confronts us. We must make decisions. The data we want is missing. So, we make models, with uncertainty, and then decide. 


Wardrop suggests doing dichotomous variables first, so as to fix all the important ideas.

## Other Topics

I am not even sure what those topics are. Perhaps all the ways we deal with observational data. What sorts of material do other intro books cover? Here are some relevant concepts:

unit/item nonresponse
ignorability
treat potential outcomes as fixed (can we do this with regression?)
missing data
regresssion towards the mean
prediction/classification
map/network/text
natural experiments
Conditional random assignment
Difference in differences
Regression discontinuity

other key themes: like causal inference versus prediction. Indeed, the entire book needs to be infused with discussions of this issue. Spend a week on that?




=================

# Things for Me



Use tidybayes package for better graphics throughout?

# Random Notes and Questions


How do references work in MD chapters?

https://twitter.com/aecoppock/status/1258406551888564226

We should not need library(moderndive) at all. (But need to check on any data used from there.)


Get rid of use of infer library. Do all this the hard way, via the bootstrap and specific calculation of the test statistic. Example: https://juliasilge.com/blog/beer-production/

Lots of pandoc-citeproc errors.

Replace all uses of kable with gt.

Fix "No additional resources" in Chapter 6 and 7. Standardize this section across all chapters. 545 is different.

  
Take (?) material from: https://chabefer.github.io/STCI/; https://github.com/chabefer/SKY


Too early to start thinking about two versions: 1005 and 1006, with 1006 as extra advanced sections in each chapter? Or maybe just astericked chapters for advanced material, not required for 1005.

Add 545 and DS data download code to chapter 5?



========= Large Projects

## R Packages

Square away R packages. There should be one location with all the requirements. Here is a listing of the R packages used my MD, from their index.Rmd:

CRAN packages needed: "nycflights13", "ggplot2movies", "fivethirtyeight", "gapminder", "ISLR",tidyverse", "rmarkdown", "knitr", "janitor", "skimr","infer", "moderndive", "webshot", "mvtnorm", "remotes", "devtools", "dygraphs", "gridExtra", "kableExtra", "scales", "viridis", "ggrepel", "patchwork",

But what good is this, given that other packages are loaded elsewhere? Is there some standard way of handling this, perhaps with a DESCRIPTION file? Main annoyance is that new contributors have to try to compile the book a dozen times before it will work.

==
## Set Up Script

Consider the use of before_chapter_script: "_common.R" in the DS _bookdown.yml as well as the associated _common.R file. Is this an approach we should copy? The lack of this why I can't get all the DS chapters to work.

Broadening Your Statistical Horizons is a very cleanly put together book. We should make our book look like that. Also HOML (https://bradleyboehmke.github.io/HOML/) is very cleanly and simply constructed. Use that as a base structure?

Combine _common.R, common.R and index.Rmd information into one place. Need to figure out how this works in bookdown. I think we need one file which only runs once when you make the book. That files does a bunch of stuff involving copying over files. But you don't maintain state after running that file, so any new functions are lost. Then you have a second file, like _common.R, which is run at the start of compiling each chapter.

==
## Bibiography

Deal with bibliography. Our source books use very different approaches. write_bib() is infamous for making trouble.

I like the way that MD writes out new versions of citations associated with R packages that have been updated.

==
## References and Footnotes

The book has lots of references, especially to other chapters. Many of these don't work because the referred-to chapters don't exist. We need a thorough clean up.

Some chapters, like 03-productivity.Rmd have a lot of footnotes. Good or bad?

Seems like all chapters generate references at the end. That is fine, but it should be standardized. Or do all those references belong at the end.


========= Thoughts

Revisit the Prediction Game. Love this:

“The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at 10 he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred.”

— Immanuel Kant, Critique of Pure Reason



````markdown
`r ''````{r}
plot(cars)
```
````

Key concepts which need to be put everywhere:

decisions need models
potential outcomes and causal effects
units, treatments, outcomes
randomization is magic: assignment to estimate causal effects, bootstrap to estimate uncertainty

Describe, predict, infer. Description of things you can see, prediction for things you will see and inference about things you will never see.

Prediction checks.

Bias/Variance == Underfitting/Overfitting

No Tests! Null hypthosis testing is a mistake. There is only the data, the models and the summaries therefrom

Describe an hypothesis test each chapter, and then dismiss it.

heterogenous treatment effects; interaction terms

(See [modelDown](https://github.com/MI2DataLab/modelDown).) *[Regression and Other Stories](http://www.stat.columbia.edu/~gelman/regression/)* provides several examples of how to create, and document your creation of, such a model, e.g., section 13.5 (gun control) and section X (wells in Bangladesh).


# Standards

Follow MD as much as possible.

* R terms and objects (anything you might type in the console) in backticks.

* Functions names always include ().

* Package names are **bolded**.

# Summer

## Themes to confirm over the summer

Drop all the frequentist nonsense except for a footnote at the first use with a confidence interval and an appendix which walks students through the Kuske arguments.

Chapters from sampling through machine learning should be parallel. To the greatest extent possible, they all do all these things.

Show the results of each step. Show what each column (mod, data, etc) looks like as it is added to the tibble.

Everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potentional outcomes to define precisely what "true value" you are talking about. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

Discuss RCM, assuming that we are estimating a causal effect of some type. And, if we are not estimating causal effects (i.e., all we care about is prediction), then the mechanics of lm are the same, but the interpretation of the regression coefficient has no causal implications.  We want a series of tables illustrating potential outcomes and our estmation of them. Start with a table with ?, just as in the Appendix. We use linear regression to fill in these questions marks. Show table with question marks and then show table with question marks filled in with best guess. Then show table with question marks filled in with a confidence interval for the mean and then for a distribution of predicted values. The closer we can tie RCM to the different parts of a regression, the better. But do this each chapter, not just regression.

Avoid discussion about hypothesis tests, except to dismiss them. But maybe we discuss and then dismiss them each chapter. Motto: No tests! There is only the data, and models we create from the data, and the decisions we make with those models. But we still need to explain what hypothesis tests are, and why we don't care about them. Explain what a test is, and why we think it is a waste of time to do them, and why people do them anyway. Key issue: If p = 0.04 really makes you do something totally different than p = 0.06, then either you (or the system within which you are operating) is stupid.

Each chapter should finish with a new section which uses list-columns plus broom to estimate scores of models, and then pull out interesting models. See the gapminder examples from https://r4ds.had.co.nz/many-models.html#gapminder. We need the full tool set: nest, unnest and so on.


## Other summer issues


https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.


https://committedtotape.shinyapps.io/freeR/

Clean up and organize images/ directory.


Replace all photos of activities with photos from Harvard, using Tsai.

## Packages




There should be on causal and one non-causal example in each chapter. Perhaps use trains throughout? Create a logistic regression example in which the dependent variable is moved X units more conservative, or did not.





googlesheet4 examples. Perhaps there is an Appendix with cool packages and a brief introduction?



Standard number formating. 2400 or 2,400 or $2400$ or . . . .

## Every Chapter from Sampling Forward

Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call?

Or maybe there are always two decisions: one forecasting and one causal.

Include discussion of RCM and potential outcomes, both in the context of this statistical technique and in the context of the empirical example.

Include a NHST and then a discussion of why NHST is stupid and full data/models are best for decision making.

Play the prediction game. That, perhaps, provides a useful framework for why NHST is stupid. Or, rather, you play the prediction game to figure out which statistical procedures are best --- and or, how well procedure X works --- and then use that information to make a decision.


There is a lot of garbage in this project. Do we really need all the random files in data/ and rds/? Probably not. Figure out which we need and delete the rest. Perhaps combine them all in one directory? And try to do more calculations on the fly.

Is there really no discussion of p-values? There should be! And what about permutation tests? Maybe that is a useful early statistical programing exercise. Or maybe it would make a nice appendix, a second read after the Rubin Causal Model appendix.




Think in terms of replacing our current 4 chapters from last third of the course. Ought to discuss causal versus predictive each time. Ought to discuss RCM each time. Ought to discuss over-fitting versus under-fitting each time. Ought to think about in-sample and out-of-sample. Ought to think in terms of decisions, and utilities each time. Those are the central themes, and should be repeated over-and-over again. We could use tidymodels syntax each time. Each model would have both a many-small-models example and a machine learning example. Predicted (or fitted) values and residuals (or errors). 




Each chapter, at least in the last four, should only have one dataset. The cognitive load of switching is to high.




Chapters 7, 8 and 9 use the bootstrap for two reasons. First, to build intution and provide justification for functions like lm. Second, to solve problems which can't be solved by standard functions. 

Machine learning in 10, 11 and 12.

Each chapter begins with a real example, a decision we must make (if only the prediction game), and then creates a model which we will use to help us. Bets are always offered. RCM discussed. Bias scenarios demonstrated. The secret weapon --- a model for each state or each year --- used.  


The problem we are trying to solve, not the tool we are using to solve it.

http://statprep.org/


