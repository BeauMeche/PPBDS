# Next meeting.

* 10 weeks left!

* Talk about list columns and map functions. Need to use them in almost every other chapter.

# Standards

Follow MD as much as possible.

* R terms and objects (anything you might type in the console) in backticks.

* Functions names always include ().

* Package names are **bolded**.

# Weekly Priorities

Jan 27: Do list columns and map_* functions at end of 06-functions.Rmd

Feb 3: Five hours on maps-Rmd and five hours on animation.Rmd. Neither is worth 10 hours of work.  

Feb 10: The univariate regression chapter in MD is excellent. But, we need six kinds of major surgery. 

First, we do not want to use any of the MD-specific functions, like get_regression_table() or the other get_* hacks. Students should only use widely-used functions. Fortunately, the get_* functions are just wrapper for broom functions, so we will need to use tidy(), glance() and augment() instead. This will require changes in the text which explains the output. 

Second, we need to use bootstrap resampling, just like we did in the confidence interval chapter. Make that connection explicitly. Then do it. rep_sample_n, followed by group_by(replication), followed by run a regression and store the model results in a list column.  Then gather up and show confidence intervals for intercept and regression cofficient. Surprise! Those ranges are just like those provided by lm(). So, all we need is to use lm(). Then, continue with all the current explanations for what lm() results means, except that the chapter currently includes no inference! That is because, in MD, they do regression before they do confidence intervals. So, they don't get to inference for regression until later chapters. We, on the other hand, can do inference now.

library(infer)
library(tidyverse)
x <- mtcars %>% 
  select(mpg, disp) %>% 
  rep_sample_n(size = 32, replace = TRUE, reps = 3) %>% 
  group_by(replicate) %>% 
  nest() %>% 
  mutate(mod = lm(mpg ~ disp, data = data))   # Not working!

Third, the text descriptions, in later chapters, are frequentist. For us, everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

Fourth, each chapter should finish with a new section which uses list-columns plus broom to estimate scores of models, and then pull out interesting models. See the gapminder examples from https://r4ds.had.co.nz/many-models.html#gapminder. Now we need the full tool set: nest, unnest and so on.

Fifth, the chapter has no discussion about hypothesis tests. That is good. Motto: No tests! There is only the data, and models we create from the data, and the decisions we make with those models. But we still need to explain what hypothesis tests are, and why we don't care about them. MD does that in their chapters 9 and 10, chapters which we have not included in our book for precisely that reason. So, explain what a test is, and why we think it is a waste of time to do them, and why people do them anyway. Key issue: If p = 0.04 really makes you do something totally different than p = 0.06, then either you (or the system within which you are operating) is stupid.

Feb 17: Do the same five part surgery on the multivariate regression chapter that you did last week on the univariate regression chapter.

Feb 24: 13-classification.Rmd has two major flaws. First, the logistic regression section comes from a different source than MD. Which is fine. But we want it to have the same look-and-feel as chapters 11 and 12, as if the MD authors had written it. We can use the same data/examples as currently (or we can change them). The style/wording should be as similar to chapters 11 and 12 as 11/12 are to each other. Each time we show how boostrap justifies the use of lm/glm and, therefore, allows our Bayesian interpretation. Each time, we end with an example which runs 100 models and then pulls out and graphs an interesting subset.

March 1: The second flaw of 13-classification.Rmd is that the sections about CART and random forest are disconnected from logistic regression. (And they came from a different source.) We want the chapter as a whole to work together. There should be a brief intro to the chapter discussing all the different ways we can handle classification problems, and mentioning the three that we will be using. (Indeed, it might be nice to use the same data across the three approaches!) Then, at the end, there should be some discussion about how they connect to each other, and how they connect to other key themes: like causal inference versus prediction. Indeed, the entire book needs to be infused with discussions of this issue.

March 8: 

March 15: The machine learning chapter is a mess, derived mostly from DS. A much better book is: https://bradleyboehmke.github.io/HOML/. But that won't be open for another year. And, it is too advanced. And it does not quite use all the latest machine learning approaches in R. But the style is nice, and perhaps worth emulating. A related issue is tying this work back to the last three weeks. After all, in each of the last three weeks we fitted hundreds of models. Isn't that what machine learning is? Sort of! Big difference is that, previously, the separate models have shared no data with each other. With machine learning, they will! How can we teach this in a way which is closest to the approaches they have learned? Are parsnip and recipe the packages to use?

Only three weeks left.

Week ?: What is the next set of stuff worth covering after students understand the Rubin Causal Model? Good question! I am not even sure what those topics are. Perhaps all the ways we deal with observational data. What sorts of material do other intro books cover? Here are some relevant concepts:

unit/item nonresponse
ignorability
treat potential outcomes as fixed (can we do this with regression?)
missing data
regresssion towards the mean
prediction/classification
map/network/text
natural experiments
Conditional random assignment
Difference in differences
Regression discontinuity



# Potential Projects

Meld the material on functions and purrr into the concepts in the Probability/Bayes Chapters. We are not changing, I think, many of the words in these chapters, most of which are quite good. Instead, we are using these topics as an excuse to provide more lessons in functions/purrr. Also, edit Bayes chapter to make sense after probability chapter. Consider adding in intuition from Bayes for Beginner Book, especially decision trees, which map nicely to our simulation approach. 

Meld potential outcomes into all later chapters in the book. Needs to be everywhere!

Brief appendix about Tufte and other graphics luminaries?

Brief appendix about Leamer?

When discussing map/network/text data, make sure to link to these blog posts and provide a paragraph of discussion.

Paul Revere [social network](https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/) 
Federalist Papers [authorship](https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/)
Lady Tasting Tea

=================

# Things for Me

Add discussion of allowed variable names, the use of ``, and janitor::clean_names() early in the book. This is needed as background before we use tidy::broom() and similar functions.

Edit Bayes chapter, especially the end.

Fix sampling/CI chapters to discuss hypothesis tests and why we hate them. Should do this in every chapter through the core of the book.

# Random Notes and Questions

* *Bayes Theorem: A Visual Introduction for Beginners* does a nice job of teaching Bayesian inference with decision trees. Works really well! And easy (?) to simulate. Maybe we do that? Fits naturally (?) into list-columns and map functions. 

Looks like we can use sections from R4DS as long as we use the entire section, unchanged.

https://r4ds.had.co.nz/index.html
https://creativecommons.org/faq/#can-i-reuse-an-excerpt-of-a-larger-work-that-is-licensed-with-the-noderivs-restriction

How do references work in MD chapters?

I like the PDF version of Rafa's book available from Lean Pub. How hard is that to make?

Replace DS Unix stuff with unix stuff from UNIX workbench?

We should not need library(moderndive) at all. (But need to check on any data used from there.)

Get rid of use of infer library. Do all this the hard way, via the bootstrap and specific calculation of the test statistic.



Lots of pandoc-citeproc errors.

Replace all uses of kable with gt?

Fix "No additional resources" in Chapter 6 and 7. Standardize this section across all chapters. 545 is different.

Requires internet when using read_mnist() in some DS chapters. Annoying! Fix by including copy of mnist? By getting rid of these sections?

Why isn't preview_chapter() working with MD chapters? Why can't I simply knit one of the MD chapters? Gives weird error message about "Error in files2[[format]] : 
  attempt to select less than one element in get1index"
  

Links in STAT 545 not working despite addition of links.md file. How fix?


Take (?) material from: https://chabefer.github.io/STCI/; https://github.com/chabefer/SKY


Or should each of these be separate chapters so that we might mix and match things? Maybe we need 100+ chapters, each of which do simple things, largely unconnected to each other.

Add cookie photo to front

Add 545 and DS data download code to chapter 5?



========= Large Projects

## R Packages

Square away R packages. There should be one location with all the requirements. Here is a listing of the R packages used my MD, from their index.Rmd:

CRAN packages needed: "nycflights13", "ggplot2movies", "fivethirtyeight", "gapminder", "ISLR",tidyverse", "rmarkdown", "knitr", "janitor", "skimr","infer", "moderndive", "webshot", "mvtnorm", "remotes", "devtools", "dygraphs", "gridExtra", "kableExtra", "scales", "viridis", "ggrepel", "patchwork",

But what good is this, given that other packages are loaded elsewhere? Is there some standard way of handling this, perhaps with a DESCRIPTION file? Main annoyance is that new contributors have to try to compile the book a dozen times before it will work.

==
## Set Up Script

Consider the use of before_chapter_script: "_common.R" in the DS _bookdown.yml as well as the associated _common.R file. Is this an approach we should copy? The lack of this why I can't get all the DS chapters to work.

Combine _common.R, common.R and index.Rmd information into one place. Need to figure out how this works in bookdown. I think we need one file which only runs once when you make the book. That files does a bunch of stuff involving copying over files. But you don't maintain state after running that file, so any new functions are lost. Then you have a second file, like _common.R, which is run at the start of compiling each chapter.

==
## Bibiography

Deal with bibliography. Our source books use very different approaches.

I like the way that MD writes out new versions of citations associated with R packages that have been updated.

Note that logistic regression chapter has a bunch of entries we need from BYSH.

==
## References and Footnotes

The book has lots of references, especially to other chapters. Many of these don't work because the referred-to chapters don't exist. We need a thorough clean up.

Some chapters, like 03-productivity.Rmd have a lot of footnotes. Good or bad?

Seems like all chapters generate references at the end. That is fine, but it should be standardized. Or do all those references belong at the end.

==
## Specific Chapters

04-wrangling is a mess, especially in the way that the join material from MD and from 545 do not go well together. Should some of it be moved to 05?

06-functions last section in map_ functions and list columns should be created. We don't need to understand everything about these concepts, just enough to do what we need in the next few chapters.

 
========= Thoughts

Revisit the Prediction Game. Love this:

“The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at 10 he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred.”

— Immanuel Kant, Critique of Pure Reason

Broadening Your Statistical Horizons is a very cleanly put together book. We should make our book look like that.

````markdown
`r ''````{r}
plot(cars)
```
````

Key concepts which need to be put everywhere:

decisions need models
potential outcomes and causal effects
units, treatments, outcomes
randomization is magic: assignment to estimate causal effects, bootstrap to estimate uncertainty

Describe, predict, infer. Description of things you can see, prediction for things you will see and inference about things you will never see.

Prediction checks.

Bias/Variance == Underfitting/Overfitting

No Tests! Null hypthosis testing is a mistake. There is only the data, the models and the summaries therefrom.

heterogenous treatment effects; interaction terms

(See [modelDown](https://github.com/MI2DataLab/modelDown).) *[Regression and Other Stories](http://www.stat.columbia.edu/~gelman/regression/)* provides several examples of how to create, and document your creation of, such a model, e.g., section 13.5 (gun control) and section X (wells in Bangladesh).


# Summer

Should we move the whole thing to Netifly and use Github Actions to deploy? https://www.hvitfeldt.me/blog/bookdown-netlify-github-actions/


