---
title: "Rubin Causal Model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(kableExtra)
library(tidyverse)
```

<!-- Note: much of this is taken from the Wikipedia page on the Rubin Causal
Model, which must be added to the attributions page. -->

<!-- Rubin Causal Model. That should be the title. Use https://en.wikipedia.org/wiki/Rubin_causal_model for starters. We can use all this text, both because it is CC and because I wrote it. Other concepts which should be mentioned (in whatever order you think best): -->

<!-- internal/external validity -->
<!-- Hawthorne effect -->
<!-- confounding -->
<!-- survey sampling -->
<!-- selection bias -->
<!-- permutation test -->

<!-- There is no need for any R code. We are just explaining things. No need, this pass, for any real data. Just use the fake data from Wikipedia page. -->


The Rubin causal model (RCM) is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes, named after Donald Rubin.^[It is also known as the Neymanâ€“Rubin causal model (Sekhon 2007). The name "Rubin causal model" was first coined by Paul W. Holland (1986).  The potential outcomes framework was first proposed by Jerzy Neyman in his 1923 Master's thesis (Neyman 1923), though he discussed it only in the context of completely randomized experiments (Rubin 2005). Rubin extended it into a general framework for thinking about causation in both observational and experimental studies (Sekhon 2007).]  

<!-- Rework: better to introduce concepts one at a time, via examples -->

<!-- Because of the fundamental problem of causal inference, unit-level causal effects cannot be directly observed. However, randomized experiments allow for the estimation of population-level causal effects (Rubin 1974). A randomized experiment assigns people randomly to treatments: college or no college. Because of this random assignment, the groups are (on average) equivalent, and the difference in income at age 40 can be attributed to the college assignment since that was the only difference between the groups. An estimate of the **average treatment effect** (or average causal effect) can then be obtained by computing the difference in means between the treated (college-attending) and control (not-college-attending) samples.  -->

<!-- Rewrote this to stick with one example (headache) -->

The Rubin causal model (RCM) is based on the idea of **potential outcomes.**  For example, let's say you took aspirin and now no longer have a headache. To measure the causal effect of taking the aspirin, we need to compare the outcome for you in one alternative future (where you took the aspirin) to another (where you did not). Since it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing. This dilemma is the "fundamental problem of causal inference."

Here's how Rubin defines a "causal effect": 

> Intuitively, the causal effect of one treatment, E, over another, C, for a particular unit and an interval of time from $t_1$ to $t_2$ is the difference between what would have happened at time $t_2$ if the unit had been exposed to E initiated at $t_1$ and what would have happened at $t_2$ if the unit had been exposed to C initiated at $t_1$: 'If an hour ago I had taken two aspirins instead of just a glass of water, my headache would now be gone,' or 'because an hour ago I took two aspirins instead of just a glass of water, my headache is now gone.' Our definition of the causal effect of the E versus C treatment will reflect this intuitive meaning.^[Rubin 1974.]

Thus, according to the RCM, the causal effect of your taking or not taking aspirin one hour ago is the difference between how your head would have felt in case 1 (taking the aspirin) and case 2 (not taking the aspirin). If your headache would remain without aspirin but disappear if you took aspirin, then the causal effect of taking aspirin is headache relief.

In most circumstances, we are interested in comparing two futures, one generally termed "treatment" and the other "control." These labels are somewhat arbitrary. The difference between the potential outcome under treatment and the potential outcome under control is called a "causal effect" or a "treatment effect."  The scenario that didn't actually happen, and thus that we didn't observe, is called a "counterfactual."

## Potential outcomes

Suppose that Joe is participating in an FDA test for a new hypertension drug. If we were omniscient, we would know the outcomes for Joe under both treatment (the new drug) and control (either no treatment or the current standard treatment). The causal effect, or treatment effect, is the difference between these two potential outcomes. 

<!-- Should we stick with this notation? -->

```{r first_example, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = "Joe",
       `$Y_t(u)$` = c("$130$"),
       `$Y_c(u)$` = c("$135$"),
       `$Y_t(u) - Y_c(u)$` = c("$-5$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

<!-- Should the numbers be written in math font? -->

$Y_{t}(u)$ is Joe's systolic blood pressure if he takes the new pill. In general, this notation expresses the potential outcome which results from a treatment, $t$, on a unit, $u$. Similarly, $Y_{c}(u)$ is the effect of a different treatment, $c$ or control, on a unit, $u$. In this case, $Y_{c}(u)$ is Joe's blood pressure if he doesn't take the pill. $Y_{t}(u)-Y_{c}(u)$ is the causal effect of taking the new drug.  Thus, Joe would have a blood pressure of $130$ if he took the pill and $135$ if he did not; the causal effect of taking the pill is $-5$.  Joe should take the pill!

From this table we only know the causal effect on Joe. Everyone else in the study might have an increase in blood pressure if they take the pill. However, regardless of what the causal effect is for the other subjects, the causal effect for Joe is lower blood pressure, relative to what his blood pressure would have been if he had not taken the pill.

## No causation without manipulation

In order for a potential outcome to make sense, it must be possible, at least *a priori*. For example, if there is no way for Joe, under any circumstance, to obtain the new drug, then $Y_{t}(u)$ is impossible for him. It can never happen. And if $Y_{t}(u)$ can never be observed, even in theory, then the causal effect of treatment on Joe's blood pressure is not defined. 

The causal effect of new drug is well defined because it is the simple difference of two potential outcomes, both of which might happen. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing or a different thing might happen.

This definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to happen, ever. For example, what is the causal effect of Joe's height on his weight? Naively, this seems a similar to our other examples. We just need to compare two potential outcomes: what would Joe's weight be under the treatment (where treatment is defined as being 3 inches taller) and what would Joe's weight be under the control (where control is defined as his current height).

A moment's reflection highlights the problem: we can't increase Joe's height. There is no way to observe, even conceptually, what Joe's weight would be if he were taller because there is no way to make him taller. We can't manipulate Joe's height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: **No causation without manipulation.**

## Average treatment effect

<!-- Calling this the "average treatment effect" rather than "average causal
effect" throughout -->

Consider a larger sample of patients: 

```{r average_treatment_effect, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$130$", "$130$", "$140$", "$145$", "$135$"),
       `$Y_c(u)$` = c("$135$", "$145$", "$145$", "$150$", "$140$", "$143$"),
       `$Y_t(u) - Y_c(u)$` = c("$-5$", "$-15$", "$-15$", "$-10$", "$+5$", "$-8$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

The causal effect is different for every subject, but the drug works for Joe, Mary, Sally and Bob because the causal effect is negative. Their blood pressure is lower with the drug than it would have been if each did not take the drug. For James, however, the drug causes an *increase* in blood pressure. 

One may calculate the **average treatment effect** (sometimes abbreviated ATE) by taking the mean of all the causal effects.  Here, the mean is $-8$.

Note that we can calculate the average treatment effect in this case because we are omniscient and know all the potential outcomes under both treatment and control.  We will soon see what assumptions we need to make in order to calculate the average treatment effect in the real world where only one potential outcome can ever be observed.

<!-- The next paragraph was cut; appears needlessly complicated at this stage.
-->

<!-- How we measure the response affects what inferences we draw. Suppose that we measure changes in blood pressure as a percentage change rather than in absolute values. Then, depending in the exact numbers, the average treatment effect might be an increase in blood pressure. For example, assume that George's blood pressure would be 154 under control and 140 with treatment. The absolute size of the causal effect is -14, but the percentage difference (in terms of the treatment level of 140) is -10%. If Sarah's blood pressure is 200 under treatment and 184 under control, then the causal effect in 16 in absolute terms but 8% in terms of the treatment value. A smaller absolute change in blood pressure (-14 versus 16) yields a larger percentage change (-10% versus 8%) for George. Even though the average treatment effect for George and Sarah is +1 in absolute terms, it is -1 in percentage terms.  -->

## Stable unit treatment value assumption (SUTVA)

We require that "the [potential outcome] observation on one unit should be unaffected by the particular assignment of treatments to the other units."^[Cox 1958, Â§2.4.] This is called the **Stable Unit Treatment Value Assumption** (**SUTVA**). 

In the context of our example, Joe's blood pressure should not depend on whether Mary receives the drug. But what if it does? Suppose that Joe and Mary live in the same house and Mary always cooks. The drug causes Mary to crave salty foods, so if she takes the drug she will cook with more salt than she would have otherwise. A high salt diet increases Joe's blood pressure. Therefore, his outcome will depend on both which treatment he received and which treatment Mary receives.

SUTVA violation makes causal inference more difficult. We can account for dependent observations by considering more treatments. We create 4 treatments by taking into account whether or not Mary receives treatment. 

```{r joe_mary, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe"),
       `Joe = c,<br />Mary = t` = c("$140$"),
       `Joe = t,<br />Mary = t` = c("$130$"),
       `Joe = c,<br />Mary = c` = c("$125$"),
       `Joe = t,<br />Mary = c` = c("$120$")) %>%
  
  # Then, we use the kable function to make it pretty. escape = FALSE is also
  # required for the linebreaks
  
  kable(escape = FALSE) %>%
  
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Recall that a causal effect is defined as the difference between two potential outcomes. In this case, there are multiple causal effects because there are more than two potential outcomes. One is the causal effect of the drug on Joe when Mary receives treatment ($130-140$). Another is the causal effect on Joe when Mary does not receive treatment ($120-125$). A third is the causal effect of Mary's treatment on Joe when Joe is not treated ($140-125$). Mary taking the drug has a larger causal effect on Joe than Joe himself taking the drug, and what's more, the effect is in the opposite dierction!

By considering more potential outcomes in this way, we can cause SUTVA to hold. However, if any units other than Joe are dependent on Mary, then we must consider further potential outcomes. The greater the number of dependent units, the more potential outcomes we must consider and the more complex the calculations become (consider an experiment with 20 different people, each of whose treatment status can affect outcomes for every one else). In order (easily) to estimate the causal effect of a single treatment relative to a control, SUTVA should hold. 

## The fundamental problem of causal inference

The results we have seen up to this point could never be measured in practice. It is impossible, by definition, to observe the effects of more than one treatment on a single subject. Joe cannot both take the pill and not take the pill at the same time! Therefore, the data we actually observe would look something like this: 

```{r effect_unknown, include = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$?$", "$100$", "$?$", "$?$", "$118$"),
       `$Y_c(u)$` = c("$?$", "$125$", "$?$", "$130$", "$120$", "$123$"),
       `$Y_t(u) - Y_c(u)$` = c("$?$", "$?$", "$?$", "$?$", "$?$", "$-5$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

```{r random_assignment, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$120$", "$?$", "$?$", "$115$", "$121.67$"),
       `$Y_c(u)$` = c("$?$", "$?$", "$125$", "$130$", "$?$", "$127.5$"),
       `$Y_t(u) - Y_c(u)$` = c("$?$", "$?$", "$?$", "$?$", "$?$", "$-5.83$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Question marks are responses that could not be observed. The **Fundamental Problem of Causal Inference** (Holland 1986) is that directly observing unit-level causal effects is impossible. However, this does not make *causal inference* impossible. Certain techniques and assumptions allow the fundamental problem to be overcome.

In particular, we may want to know whether we can get a good estimate of the average treatment effect.  If we knew that the drug lowered blood pressure on average, that would help us make decisions even if we can't observe the causal effect of the drug on individual people (because we don't observe counterfactuals).

The most natural place to start is with the difference the sample mean outcomes between treatment and control, $-5.83$.  Note that unlike earlier, this is an *estimate* of the ATE, not the ATE itself.  Why?  Because we have missing data!  For each unit, we only observe one of the potential outcomes.

<!-- Constant treatment effect is only relevant for predicting treatment effects
for individual units.  Without knowing the assignment mechanism, we can't know
if the mean is a good estimate of the ATE.  So this material will be reworked.
-->

<!-- One possible assumption is that the effect of the treatment is the same for each unit.  Using the data above, we can infer what Joe's potential outcome under control would have been if we assume a constant treatment effect: -->

<!-- $Y_{t}(u)=T+Y_{c}(u)$ -->

<!-- and -->

<!-- $Y_{t}(u)-T=Y_{c}(u)$. -->

<!-- Thus, we can infer the unobserved values if we assume a constant effect. The following table illustrates data consistent with the assumption of a constant effect.  -->

```{r constant_effect_assume, include = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$120$", "$100$", "$125$", "$115$", "$118$"),
       `$Y_c(u)$` = c("$135$", "$125$", "$105$", "$130$", "$120$", "$123$"),
       `$Y_t(u) - Y_c(u)$` = c("$-5$", "$-5$", "$-5$", "$-5$", "$-5$", "$-5$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

<!-- All of the subjects have the same causal effect even though they have different outcomes under the treatment.  -->

## The assignment mechanism

Is the difference in sample means between treated units and control units a good estimate of the average treatment effect?  That depends entirely on the method by which units are assigned treatment, which is called the **assignment mechanism**.

One such assignment mechanism is **randomization**. For each subject we could flip a coin to determine if she receives treatment. If we wanted five subjects to receive treatment, we could assign treatment to the first five names we pick out of a hat.

Randomized assignment is the best assignment mechanism for inferring average treatment effects because if the sample is large enough, the difference in sample means between treated and control units will be very close to the true average treatment effect in that sample.

However, when the sample size is small, our estimated average treatment effect may deviate considerably from the truth.  For example, the data in the last section were from just one possible random assignment, where Joe, Mary and James received the treatment.  Here's another possible random assignment, where Joe, Mary and Sally receive the treatment:

```{r random_assignment_2, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$120$", "$100$", "$?$", "$?$", "$115$"),
       `$Y_c(u)$` = c("$?$", "$?$", "$?$", "$130$", "$120$", "$123$"),
       `$Y_t(u) - Y_c(u)$` = c("$?$", "$?$", "$?$", "$?$", "$?$", "$-8.33$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Note that with one assignment, the estimate of the ATE is $-5.83$, while with another it is $-8.33$.  Assume that these data are the truth: 

```{r true_data, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$120$", "$100$", "$110$", "$115$", "$115$"),
       `$Y_c(u)$` = c("$115$", "$125$", "$125$", "$130$", "$120$", "$123$"),
       `$Y_t(u) - Y_c(u)$` = c("$+15$", "$-5$", "$-25$", "$-20$", "$-5$", "$-8$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Thus, the true average treatment effect is $-8$.  However, our estimates of the average treatment effect vary because our sample is small and the responses have a large variance. If the sample were larger and the variance were less, the average treatment effect would be closer to the true average treatment effect regardless of the specific units randomly assigned to treatment.

<!-- But the causal effect for these individuals is never equal to this average. The causal effect varies, as it generally (always?) does in real life. After assigning treatments randomly, we might estimate the causal effect as:  -->

<!-- A different random assignment of treatments yields a different estimate of the average treatment effect.  -->

<!-- Alternatively, suppose the mechanism assigns the treatment to all men and only to them.  -->

```{r assignment_men, include = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$?$", "$?$", "$110$", "$115$", "$118.33$"),
       `$Y_c(u)$` = c("$?$", "$125$", "$125$", "$?$", "$?$", "$125$"),
       `$Y_t(u) - Y_c(u)$` = c("$?$", "$?$", "$?$", "$?$", "$?$", "$-6.67$")) %>%

  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

<!-- Under this assignment mechanism, it is impossible for women to receive treatment and therefore impossible to determine the average treatment effect on female subjects. In order to make any inferences of causal effect on a subject, the probability that the subject receive treatment must be greater than 0 and less than 1.  -->

## The perfect doctor

With a large enough sample and randomized assignment, we can get a good estimate of the ATE.  Other assignment mechanisms, however, can make it difficult or impossible to engage in causal inference. Consider the use of the *perfect doctor* as an assignment mechanism. The perfect doctor knows how each subject will respond to the drug or the control and assigns each subject to the treatment that will most benefit her. The perfect doctor knows this information about a sample of patients: 

```{r true_data_perfect_doctor, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "Susie", "MEAN"),
       `$Y_t(u)$` = c("$130$", "$120$", "$100$", "$115$", "$120$", "$135$", "$120$"),
       `$Y_c(u)$` = c("$115$", "$125$", "$150$", "$125$", "$130$", "$105$", "$125$"),
       `$Y_t(u) - Y_c(u)$` = c("$+15$", "$-5$", "$-50$", "$-10$", "$-10$", "$+30$", "$-5$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Based on this knowledge she would make the following treatment assignments: 

```{r perfect_doctor, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`subject` = c("Joe", "Mary", "Sally", "Bob", "James", "Susie", "MEAN"),
       `$Y_t(u)$` = c("$?$", "$120$", "$100$", "$115$", "$120$", "$?$", "$113.75$"),
       `$Y_c(u)$` = c("$115$", "$?$", "$?$", "$?$", "$?$", "$105$", "$110$"),
       `$Y_t(u) - Y_c(u)$` = c("$?$", "$?$", "$?$", "$?$", "$?$", "$?$", "$+3.75$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

The perfect doctor distorts the averages of both $Y_t(u)$ and $Y_c(u)$ by filtering out poor responses to both the treatment and control. The difference between means is distorted in a direction that depends on the details. For instance, a subject like Susie who is harmed by taking the drug would be assigned to the control group by the perfect doctor and thus the negative effect of the drug would be masked.

The difference in means is thus no longer a good estimate of the ATE. In fact, it has the wrong sign! This is not merely a consequence of our small sample: even if the perfect doctor saw a million patients, we could not get a good estimate of the ATE.

If you are interested in causal inference, randomized experiments are the best approach. In many circumstances, however, randomized experiments are not possible due to ethical or practical concerns. In such scenarios there is by necessity a non-random assignment mechanism.

For instance, let's say you were interested in the effect of college attendence on earnings. People are not randomly assigned to attend college. Rather, people may choose to attend college based on their financial situation, parents' education, and so on. Many statistical methods have been developed for causal inference, such as propensity score matching. These methods attempt to correct for the assignment mechanism by finding control units similar to treatment units. What you should not do is naively compare the sample means under treatment and control and assume that is a good estimate of the ATE.  Without randomization, this could be very misleading!


## Conclusion

The causal effect of a treatment on a single unit at a point in time is the difference between the outcome variable with the treatment and without the treatment. The Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. You either take the aspirin now or you don't. As a consequence, assumptions must be made in order to estimate the missing counterfactuals.

<!-- This next part is more appropriate for an encylopedia than for a textbook:
-->

<!-- The Rubin causal model has also been connected to instrumental variables (Angrist, Imbens, and Rubin, 1996) and other techniques for causal inference. For more on the connections between the Rubin causal model, structural equation modeling, and other statistical methods for causal inference, see Morgan and Winship (2007). -->

## References

<!-- Need to integrate this with the rest of the book -->

<!-- Angrist, J.; Imbens, G.; Rubin, D. (1996). "Identification of Causal effects Using Instrumental Variables." *J. Amer. Statist. Assoc.* 91 (434): 444â€“455. doi:10.1080/01621459.1996.10476902. -->

Cox, D. R. (1958). *Planning of Experiments.* Wiley.

Holland, Paul W. (1986). "Statistics and Causal Inference." *J. Amer. Statist. Assoc.* 81 (396): 945â€“960. doi:10.1080/01621459.1986.10478354.

<!-- Morgan, S.; Winship, C. (2007). *Counterfactuals and Causal Inference: Methods and Principles for Social Research.* New York: Cambridge University Press. ISBN 978-0-521-67193-4. -->

Neyman, Jerzy. (1923). *Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes.* Master's Thesis. Excerpts reprinted in English, Statistical Science, Vol. 5, pp. 463â€“472. (D. M. Dabrowska, and T. P. Speed, Translators.)

Rubin, Donald (1974). "Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies." *J. Educ. Psychol.* 66 (5): 688â€“701 [p. 689]. doi:10.1037/h0037350.

Rubin, Donald (2005). "Causal Inference Using Potential Outcomes." *J. Amer. Statist. Assoc.* 100 (469): 322â€“331. doi:10.1198/016214504000001880.



