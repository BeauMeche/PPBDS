# Machine Learning {#machine-learning}

<!-- AR: I used material from this blog post: https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101
Should be added to the Acknowledgements section -->

We have learned four models: linear regression, logistic regression, CART, and random forest. But there are hundreds more! We need a consistent way to try lots of models and to compare them. Hence, we introduced in the last chapter the **tidymodels** collection of packages.

Also, how do we know which models are best? Recall that we came up with several different models for each of our example data sets. Which one should we use? The framework of **machine learning** helps.

Perhaps the most popular data science methodologies come from the field of machine learning. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple's Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars. Although today the terms "artificial intelligence" and "machine learning" are often used interchangeably, we make the following distinction: while the first artificial intelligence algorithms, such as those used by chess playing machines, implemented decision making based on programmable rules derived from theory or first principles, in machine learning decisions are based on algorithms **built with data**. 

## Data: Cooperative Congressional Election Study (CCES)

For this chapter, we'll be using data from the Cooperative Congressional Election Study, a large survey of Americans that asks many questions relevant to American politics and elections.  You can learn more about the study [here](https://cces.gov.harvard.edu/).

Let's start by downloading the file called "cumulative_2006_2018.rds" at the [CCES Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/II2DB6).  After you have downloaded it, load it into your environment with the name "cces" using `cces <- read_rds("cumulative_2006_2018.rds")`.  Be sure to load the **tidyverse** and **tidymodels** packages before running the code in this chapter.

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(tidymodels)

cces <- read_rds("rds/cumulative_2006_2018.rds")
```

Take a look at the `cces` object:

```{r}
cces
```

We have a lot of observations (more than 450,000), each of which is a single survey respondent.

The outcome we'll focus on for this chapter is *presidential approval.* 

Let's take a look at the presidential approval variable in the CCES. We see that it is an "int+lbl" object, which means that it an integer variable with labels, which we can see using the `print_labels()` function from the **haven** package:

```{r}
library(haven)

print_labels(cces$approval_pres)
```

We can see that there are 6 levels to this variable.  We'll `mutate()` the variable so it goes from 1 to 5, which 1 being "strongly disapprove" and 5 being "strongly approve," and 3 will be a middle category that combines responses from "never heard / not sure" and "neither approve nor disapprove," so that higher values indicate greater approval.

```{r}
cces <- cces %>%
  mutate(pres_approval = case_when(approval_pres == 4 ~ 1,
                                   approval_pres == 3 ~ 2,
                                   approval_pres %in% c(5, 6) ~ 3,
                                   approval_pres == 2 ~ 4,
                                   approval_pres == 1 ~ 5,
                                   TRUE ~ NA_real_))
```

We'll also `mutate()` some other variables for interpretability this chapter:

```{r}
cces <- cces %>%
  mutate(
    
    # Create variable for if the president is a Republican
    
    pres_gop = ifelse(year %in% c(2009:2016), 0, 1),
    
    # Combine "not sure" and "independent" on party identification scale
    
    pid7 = ifelse(pid7 == 8, 4, pid7),
    
    # Combine "not sure" and "moderate" on ideology scale
    
    ideo5 = ifelse(ideo5 == "Not Sure", 3, as.numeric(ideo5)),
    
    # Turn race and gender into factor variables
    
    race = as_factor(race),
    gender = as_factor(gender))
```

Finally, we'll `select()` the variables we'll be using and `glimpse()` our tibble:

```{r}
cces <- cces %>%
  select(pres_approval, pres_gop, pid7, ideo5, race, gender, educ, age, state)

glimpse(cces)
```

Now we have one outcome (`pres_approval`, which ranges from 1 to 5) and eight potential predictors:

- `pres_gop`: Binary variable for whether the president was a Republican when the survey was taken
- `pid7`: Respondent's party identification on a 1-7 scale from strong Democrat to strong Republican
- `ideo5`: Respondent's ideology on a 1-5 scale from very liberal to very conservative
- `race`: Respondent's race, from eight choices (white, black, Hispanic, Asian, Native American, Middle Eastern, mixed, or other)
- `gender`: Respondent's gender, from two choices (male and female)
- `educ`: Respondent's education on a 1-6 scale (no high school, high school, some college, two-year college, four-year college, and post-graduate education)
- `age`: Respondent's age
- `state`: Respondent's state

In past chapters, we've shown example models with one or two predictors, perhaps with an interaction.  When you have more variables in your dataset, how can you decide which predictors to include?  The techniques of machine learning can help answer this question.

For this chapter, we'll consider x possible models.  We have some intuitions about what should be in a useful model for presidential approval.  First, since the effect of all the predictors likely vary based on whether the president is a Democrat or Republican, all predictors should be interacted with `pres_gop`.  Also, party identification and self-reported ideology are likely large predictors of presidential approval, and thus should be included.  But what about the other variables?  Let's consider the following combinations:

1. `pid7` and `ideo5` alone
1. `pid7` and `ideo5` plus demographic variables (`race`, `gender`, `educ`, and `age`)
1. Same as above, but with interactions between `race` and `educ` and between `gender` and `age`
1. `pid7` and `ideo5` plus the respondent's `state`
1. The kitchen sink: demographic variables with the interactions in #3, plus `state`

Of course, these are a small subset of the possible models we could consider, either with the variables we have selected or with the larger set of all the variables in the CCES.  But we'll use these as examples for the machine learning techniques in this chapter; if you'd like, you can use the methods we learn here to test additional models.

Let's save these as `formula` objects in R, so we can easily access them later.  We'll start with the simplest model we'll consider, as `basic_form`:

```{r}
basic_form <- formula(pres_approval ~ pid7 * pres_gop + ideo5 * pres_gop)
```

Next, we can use `update()` to create the more complicated formulas.  `update()` takes as its first argument a formula and as its second argument the additions you want to make.  To keep all the predictors from the first formula and add more, you will start with `~ . + ` and then add more predictors, like so:

```{r}
demo_form <- update(basic_form,
                    ~ . + race * pres_gop + 
                      gender * pres_gop + 
                      educ * pres_gop + 
                      age * pres_gop)

demo_interact_form <- update(basic_form,
                             ~ . + race * educ * pres_gop + 
                               gender * age * pres_gop)

state_form <- update(basic_form,
                     ~ . + state * pres_gop)
```

Since the last model is the same as `demo_interact_form` but with `state * pres_gop` added, we'll use `update(demo_interact_form)` rather than `update(basic_form)` here:

```{r}
full_form <- update(demo_interact_form,
                    ~ . + state * pres_gop)
```

Now we have five `formula` objects we can use to fit models.

So we can access them easily, we'll save them in a tibble and give them easy-to-remember names:

```{r}
cces_formulas <- tibble(formula = c(basic_form,
                                   demo_form,
                                   demo_interact_form,
                                   state_form,
                                   full_form),
                       group = c("Basic model",
                                 "Demographic model",
                                 "Demographic interaction model",
                                 "State model",
                                 "Full model"))
```

## Notation

In machine learning, data comes in the form of:

1. the _outcome_ we want to predict and 
2. the _predictors_ that we will use to predict the outcome

We want to build an algorithm that takes predictor values as input and returns a prediction for the outcome when we don't know the outcome. The machine learning approach is to _train_ an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don't know the outcome.

Here we will use $Y$ to denote the outcome and $X_1, \dots, X_p$ to denote predictors. Note that predictors are sometimes referred to as *features* or *covariates*. We consider all these to be synonyms.

Prediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, $Y$ can be any one of $K$ classes. The number of classes can vary greatly across applications.

The general setup is as follows. We have a series of predictors and an unknown outcome we want to predict:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(dslabs)
tmp <- tibble(outcome="?", 
              'predictor 1' = "$X_1$",
              'predictor 2' = "$X_2$",
              'predictor 3' = "$X_3$",
              'predictor 4' = "$X_4$",
              'predictor 5' = "$X_5$")
if(knitr::is_html_output()){
  knitr::kable(tmp, "html", align = "c") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", align="c", escape = FALSE, booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

To _build a model_ that provides a prediction for any set of observed values $X_1=x_1, X_2=x_2, \dots X_5=x_5$, we collect data for which we know the outcome:

```{r, echo=FALSE}
n <- 2
tmp <- tibble(outcome = paste0("$y_{", 1:n,"}$"), 
              'predictor 1' = paste0("$x_{",1:n,",1}$"),
              'predictor 2' = paste0("$x_{",1:n,",2}$"),
              'predictor 3' = paste0("$x_{",1:n,",3}$"),
              'predictor 4' = paste0("$x_{",1:n,",4}$"),
              'predictor 5' = paste0("$x_{",1:n,",5}$"))
tmp_2 <- rbind(c("$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$"),
               c("$y_n$", "$x_{n,1}$","$x_{n,2}$","$x_{n,3}$","$x_{n,4}$","$x_{n,5}$"))
colnames(tmp_2) <- names(tmp)
tmp <- bind_rows(tmp, as_tibble(tmp_2))
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", escape = FALSE, booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

When the output is continuous we refer to the machine learning task as _prediction_, and the main output of the model is a function $f$ that automatically produces a prediction, denoted with $\hat{y}$, for any set of predictors: $\hat{y} = f(x_1, x_2, \dots, x_p)$.  We use the term _actual outcome_ to denote what we ended up observing. So we want the prediction $\hat{y}$ to match the actual outcome $y$ as well as possible. Because our outcome is continuous, our predictions $\hat{y}$ will not be either exactly right or wrong, but instead we will determine an _error_ defined as the difference between the prediction and the actual outcome $y - \hat{y}$.

When the outcome is categorical, we refer to the machine learning task as _classification_, and the main output of the model will be a _decision rule_ which prescribes which of the $K$ classes we should predict. In this scenario, most models provide functions of the predictors for each class $k$, $f_k(x_1, x_2, \dots, x_p)$, that are used to make this decision. When the data is binary a typical decision rules looks like this: if $f_1(x_1, x_2, \dots, x_p) > C$, predict category 1, if not the other category, with $C$ a predetermined cutoff. Because the outcomes are categorical, our predictions will be either right or wrong. 

## Evaluation metrics

Before we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by "better."

### Training and test sets

Ultimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don't know the outcome for one of these. We stop pretending we don't know the outcome to evaluate the algorithm, but only *after* we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the _training_ set. We refer to the group for which we pretend we don't know the outcome as the _test_ set. 

A standard way of generating the training and test sets is by randomly splitting the data.

We will then develop an algorithm using **only** the training set. Once we are done developing the algorithm, we will _freeze_ it and evaluate it using the test set.

But remember, **it is important that we optimize the model using only the training set**: the test set is only for evaluation. Evaluating an algorithm on the training set can lead to _overfitting_, which often results in dangerously over-optimistic assessments. 

### The loss function {#loss-function}

In this section, we describe how the general approach to defining "best" in machine learning is to define a _loss function_, which can be applied to both categorical and continuous data. 

The most commonly used loss function is the squared loss function. If $\hat{y}$ is our predictor and $y$ is the observed outcome, the squared loss function is simply:

$$
(\hat{y} - y)^2
$$

Because we often have a test set with many observations, say $N$, we use the mean squared error (MSE):

$$
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$

In practice, we often report the root mean squared error (RMSE), which is $\sqrt{\mbox{MSE}}$, because it is in the same units as the outcomes. But doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.

If the outcomes are binary, both RMSE and MSE are equivalent to accuracy, since $(\hat{y} - y)^2$ is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.

Note that there are loss functions other than the squared loss. For example, the _Mean Absolute Error_  uses absolute values, $|\hat{Y}_i - Y_i|$ instead of squaring the errors$(\hat{Y}_i - Y_i)^2$. However, in this book we focus on minimizing square loss since it is the most widely used.

## Evaluating with **tidymodels**

If you are embarking on learning **tidymodels**, you’ll need to use this same kind of code as the building blocks for any predictive modeling pipeline.

### **parsnip**: build the model

This step is really three, using only the [**parsnip** package](https://tidymodels.github.io/parsnip/):

```{r}
lm_spec <- 
  
  # Pick model
  
  linear_reg() %>%
  
  # Set engine
  
  set_engine("lm") %>%
  
  # Set mode
  
  set_mode("regression") 

lm_spec
```

To keep things simple, we'll only be evaluating linear regressions in this chapter, although there are many other modeling choices one could make for predicting presidential approval, some of which may be superior.^[For instance, linear regression allows for predicted values which are below 1 and above 5, which are theoretically forbidden.  Furthermore, linear regression assumes that the distance between each response category is the same, since the distance between 1 and 2 is the same as 2 and 3, and so on, but there may be real world "breakpoints," for instance if it is more important to go from neutral to somewhat approval than from somewhat approval to strong approval.  However, for the purposes of this chapter, we will proceed with linear regression.]

Things that are missing: data (we haven’t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html).

### **recipes**: not happening here, folks

This is where you would normally insert some code for *feature engineering* using the **recipes** package.  Feature engineering involves transforming your data to create different predictors, such as by taking log transforming, turning numerical variables into factors or vise versa, and so on.  We engaged in some rudimentary feature engineering when we `mutate()`d the CCES at the beginning of this chapter.  But for the purposes of this chapter, we will treat our data as-is.

### **rsample**: initial split

We’ll use the [**rsample** package](https://tidymodels.github.io/rsample/) to split the CCES up into two datasets: training and testing. The `initial_split()` function takes a dataset and splits it into a training and test set.  By default, 75% of the data is kept in the training set and the rest are allocated to the test set (this can be changed with the `prop` argument).  Because the split is done at random, we need to use `set.seed()` to ensure our results are replicable.

```{r}
set.seed(1234)

cces_split <- initial_split(cces)
cces_train <- training(cces_split)
cces_test <- testing(cces_split)
```

### Fitting the model once

Fitting a single model once is... not *exactly* the hardest part.

```{r}
lm_spec %>%
  
  # Train: get fitted model
  
  fit(basic_form, data = cces_train) %>%
  
  # Test: get predictions
  
  predict(new_data = cces_test) %>%
  
  # Compare: get metrics
  
  bind_cols(cces_test) %>%
  rmse(truth = pres_approval, estimate = .pred)
```

If you squint, you might see that we could make this into a function like below:

```{r}
fit_lm_holdout <- function(formula, train, test) {
  lm_spec %>%
    fit(formula, data = train) %>%
    predict(new_data = test) %>%
    bind_cols(test)
}
```

We can then create a tibble that has all the predictions for each model, using our old friend `map()`:

```{r}
cces_test_preds <- cces_formulas %>%
  mutate(preds = map(formula, ~ fit_lm_holdout(., cces_train, cces_test))) %>%
  unnest(preds)
```

Finally, we can use the `rmse()` function to compare our five specifications.

```{r}
cces_test_preds %>%
  group_by(group) %>%
  rmse(truth = pres_approval, estimate = .pred)
```

But, unfortunately, we shouldn’t be predicting with the test set over and over again like this. It isn’t good practice to predict with the test set more than one time. What is a good predictive modeler to do? We should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end — after we’ve compared different models, selected my features, and tuned my hyperparameters. How do you do this? You do [cross-validation](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html) with the training set, and you leave the testing set for [*the very last fit you do*](https://tidymodels.github.io/tune/reference/last_fit.html).

## Cross validation {#cross-validation}


In this section we introduce cross validation, one of the most important ideas in machine learning. 

In Section \@ref(loss-function), we described that a common goal of machine learning is to find an algorithm that produces predictors $\hat{Y}$ for an outcome $Y$ that minimizes the MSE:

$$
MSE = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$
There are two important characteristics of the MSE we should always keep in mind:

1. We can think our estimate of the MSE is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.

2. If we train an algorithm on the same dataset that we use to compute the MSE, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error.

Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the *true error*, a theoretical quantity, as the average of many *apparent errors* obtained by applying the algorithm to new random samples of the data, none of them used to train the algorithm. 

However, we only have available one set of outcomes: the ones we actually observed. Cross validation is based on the idea of generating a series of different random samples on which to apply our algorithm. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.

### K-fold cross validation


```{r, include=FALSE}
if(knitr::is_html_output()){
  knitr::opts_chunk$set(out.width = "500px",
                        out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"')
} else{
  knitr::opts_chunk$set(out.width = "35%")
}
```

The first approach we describe is _K-fold cross validation_. 

Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).

```{r, echo=FALSE}
knitr::include_graphics("images/cv-1.png")
```

But we don't get to see these independent datasets. 

```{r, echo=FALSE}
knitr::include_graphics("images/cv-2.png")
```

So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a _training set_ (blue) and a _test set_ (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.

We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing. 

```{r, echo=FALSE}
knitr::include_graphics("images/cv-3.png")
```

Let's reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting predictors, nothing! 

Now this presents a new problem because for most machine learning algorithms we need to select parameters. We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain.  This is where cross validation is most useful.

For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.

First, before we start the cross validation procedure, it is important to fix all the algorithm parameters. Although we will train the algorithm on the set of training sets, the parameters will be the same across all training sets.

Let's start by describing how to construct the first: we simply pick $M=N/K$ observations at random (we round if $M$ is not a round number) and think of these as a random sample. We call this the *validation set*:


```{r, echo=FALSE}
knitr::include_graphics("images/cv-4.png")
```

Now we can fit the model in the training set, then compute the MSE on the validation set.  Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take $K$ samples, not just one. In K-cross validation, we randomly split the observations into $K$ non-overlapping sets:


```{r, echo=FALSE}
knitr::include_graphics("images/cv-5.png")
```

Then, for our final estimate, we compute the average MSE across our $K$ samples.

A final step would be to select the set of parameters that minimizes the MSE.

We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:


```{r, echo=FALSE}
knitr::include_graphics("images/cv-6.png")
```

We can do cross validation again:

```{r, echo=FALSE}
knitr::include_graphics("images/cv-7.png")
```

and obtain a final estimate of our expected loss. However, note that this means that our entire compute time gets multiplied by $K$. You will soon learn that performing this task takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set.

Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.


```{r, echo=FALSE}
knitr::include_graphics("images/cv-8.png")
```


Now how do we pick the cross validation $K$? Large values of $K$ are preferable because the training data better imitates the original dataset. However, larger values of $K$ will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of $K=5$ and $K=10$ are popular.

### Bootstrap

One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick $K$ sets of some size at random.

One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the _bootstrap_. 

